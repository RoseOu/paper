<doc id="26700" url="https://en.wikipedia.org/wiki?curid=26700" title="Science">
Science

Science (from the Latin word "scientia", meaning "knowledge") is a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.

The earliest roots of science can be traced to Ancient Egypt and Mesopotamia in around 3500 to 3000 BCE. Their contributions to mathematics, astronomy, and medicine entered and shaped Greek natural philosophy of classical antiquity, whereby formal attempts were made to provide explanations of events in the physical world based on natural causes. After the fall of the Western Roman Empire, knowledge of Greek conceptions of the world deteriorated in Western Europe during the early centuries (400 to 1000 CE) of the Middle Ages but was preserved in the Muslim world during the Islamic Golden Age. The recovery and assimilation of Greek works and Islamic inquiries into Western Europe from the 10th to 13th century revived "natural philosophy", which was later transformed by the Scientific Revolution that began in the 16th century as new ideas and discoveries departed from previous Greek conceptions and traditions. The scientific method soon played a greater role in knowledge creation and it was not until the 19th century that many of the institutional and professional features of science began to take shape; along with the changing of "natural philosophy" to "natural science."

Modern science is typically divided into three major branches that consist of the natural sciences (e.g., biology, chemistry, and physics), which study nature in the broadest sense; the social sciences (e.g., economics, psychology, and sociology), which study individuals and societies; and the formal sciences (e.g., logic, mathematics, and theoretical computer science), which study abstract concepts. There is disagreement, however, on whether the formal sciences actually constitute a science as they do not rely on empirical evidence. Disciplines that use existing scientific knowledge for practical purposes, such as engineering and medicine, are described as applied sciences.

Science is based on research, which is commonly conducted in academic and research institutions as well as in government agencies and companies. The practical impact of scientific research has led to the emergence of science policies that seek to influence the scientific enterprise by prioritizing the development of commercial products, armaments, health care, and environmental protection.

Science in a broad sense existed before the modern era and in many historical civilizations. Modern science is distinct in its approach and successful in its results, so it now defines what science is in the strictest sense of the term. Science in its original sense was a word for a type of knowledge, rather than a specialized word for the pursuit of such knowledge. In particular, it was the type of knowledge which people can communicate to each other and share. For example, knowledge about the working of natural things was gathered long before recorded history and led to the development of complex abstract thought. This is shown by the construction of complex calendars, techniques for making poisonous plants edible, public works at national scale, such as those which harnessed the floodplain of the Yangtse with reservoirs, dams, and dikes, and buildings such as the Pyramids. However, no consistent conscious distinction was made between knowledge of such things, which are true in every community, and other types of communal knowledge, such as mythologies and legal systems. Metallurgy was known in prehistory, and the Vinča culture was the earliest known producer of bronze-like alloys. It is thought that early experimentation with heating and mixing of substances over time developed into alchemy.

Neither the words nor the concepts "science" and "nature" were part of the conceptual landscape in the ancient near east. The ancient Mesopotamians used knowledge about the properties of various natural chemicals for manufacturing pottery, faience, glass, soap, metals, lime plaster, and waterproofing; they also studied animal physiology, anatomy, and behavior for divinatory purposes and made extensive records of the movements of astronomical objects for their study of astrology. The Mesopotamians had intense interest in medicine and the earliest medical prescriptions appear in Sumerian during the Third Dynasty of Ur ( 2112 BCE – 2004 BCE). Nonetheless, the Mesopotamians seem to have had little interest in gathering information about the natural world for the mere sake of gathering information and mainly only studied scientific subjects which had obvious practical applications or immediate relevance to their religious system.

In classical antiquity, there is no real ancient analog of a modern scientist. Instead, well-educated, usually upper-class, and almost universally male individuals performed various investigations into nature whenever they could afford the time. Before the invention or discovery of the concept of "nature" (ancient Greek "phusis") by the Pre-Socratic philosophers, the same words tend to be used to describe the "natural" "way" in which a plant grows, and the "way" in which, for example, one tribe worships a particular god. For this reason, it is claimed these men were the first philosophers in the strict sense, and also the first people to clearly distinguish "nature" and "convention." Natural philosophy, the precursor of natural science, was thereby distinguished as the knowledge of nature and things which are true for every community, and the name of the specialized pursuit of such knowledge was "philosophy" – the realm of the first philosopher-physicists. They were mainly speculators or theorists, particularly interested in astronomy. In contrast, trying to use knowledge of nature to imitate nature (artifice or technology, Greek "technē") was seen by classical scientists as a more appropriate interest for artisans of lower social class.
The early Greek philosophers of the Milesian school, which was founded by Thales of Miletus and later continued by his successors Anaximander and Anaximenes, were the first to attempt to explain natural phenomena without relying on the supernatural. The Pythagoreans developed a complex number philosophy and contributed significantly to the development of mathematical science. The theory of atoms was developed by the Greek philosopher Leucippus and his student Democritus. The Greek doctor Hippocrates established the tradition of systematic medical science and is known as "The Father of Medicine".

A turning point in the history of early philosophical science was Socrates' example of applying philosophy to the study of human matters, including human nature, the nature of political communities, and human knowledge itself. The Socratic method as documented by Plato's dialogues is a dialectic method of hypothesis elimination: better hypotheses are found by steadily identifying and eliminating those that lead to contradictions. This was a reaction to the Sophist emphasis on rhetoric. The Socratic method searches for general, commonly held truths that shape beliefs and scrutinizes them to determine their consistency with other beliefs. Socrates criticized the older type of study of physics as too purely speculative and lacking in self-criticism. Socrates was later, in the words of his "Apology", accused of corrupting the youth of Athens because he did "not believe in the gods the state believes in, but in other new spiritual beings". Socrates refuted these claims, but was sentenced to death.

Aristotle later created a systematic programme of teleological philosophy: Motion and change is described as the actualization of potentials already in things, according to what types of things they are. In his physics, the Sun goes around the Earth, and many things have it as part of their nature that they are for humans. Each thing has a formal cause, a final cause, and a role in a cosmic order with an unmoved mover. The Socratics also insisted that philosophy should be used to consider the practical question of the best way to live for a human being (a study Aristotle divided into ethics and political philosophy). Aristotle maintained that man knows a thing scientifically "when he possesses a conviction arrived at in a certain way, and when the first principles on which that conviction rests are known to him with certainty".

The Greek astronomer Aristarchus of Samos (310–230 BCE) was the first to propose a heliocentric model of the universe, with the Sun at the center and all the planets orbiting it. Aristarchus's model was widely rejected because it was believed to violate the laws of physics. The inventor and mathematician Archimedes of Syracuse made major contributions to the beginnings of calculus and has sometimes been credited as its inventor, although his proto-calculus lacked several defining features. Pliny the Elder was a Roman writer and polymath, who wrote the seminal encyclopedia "Natural History", dealing with history, geography, medicine, astronomy, earth science, botany, and zoology.
Other scientists or proto-scientists in Antiquity were Theophrastus, Euclid, Herophilos, Hipparchus, Ptolemy, and Galen.

Because of the collapse of the Western Roman Empire due to the Migration Period an intellectual decline took place in the western part of Europe in the 400s. In contrast, the Byzantine Empire resisted the attacks from invaders, and preserved and improved upon the learning. John Philoponus, a Byzantine scholar in the 500s, questioned Aristotle's teaching of physics and to note its flaws. John Philoponus' criticism of Aristotelian principles of physics served as an inspiration to medieval scholars as well as to Galileo Galilei who ten centuries later, during the Scientific Revolution, extensively cited Philoponus in his works while making the case for why Aristotelian physics was flawed.

During late antiquity and the early Middle Ages, the Aristotelian approach to inquiries on natural phenomena was used. Aristotle's four causes prescribed that four "why" questions should be answered in order to explain things scientifically. Some ancient knowledge was lost, or in some cases kept in obscurity, during the fall of the Western Roman Empire and periodic political struggles. However, the general fields of science (or "natural philosophy" as it was called) and much of the general knowledge from the ancient world remained preserved through the works of the early Latin encyclopedists like Isidore of Seville. However, Aristotle's original texts were eventually lost in Western Europe, and only one text by Plato was widely known, the "Timaeus", which was the only Platonic dialogue, and one of the few original works of classical natural philosophy, available to Latin readers in the early Middle Ages. Another original work that gained influence in this period was Ptolemy's "Almagest", which contains a geocentric description of the solar system.

During late antiquity, in the Byzantine empire many Greek classical texts were preserved. Many Syriac translations were done by groups such as the Nestorians and Monophysites. They played a role when they translated Greek classical texts into Arabic under the Caliphate, during which many types of classical learning were preserved and in some cases improved upon. In addition, the neighboring Sassanid Empire established the medical Academy of Gondeshapur where Greek, Syriac and Persian physicians established the most important medical center of the ancient world during the 6th and 7th centuries.

The House of Wisdom was established in Abbasid-era Baghdad, Iraq,
where the Islamic study of Aristotelianism flourished. Al-Kindi (801–873) was the first of the Muslim Peripatetic philosophers, and is known for his efforts to introduce Greek and Hellenistic philosophy to the Arab world. The Islamic Golden Age flourished from this time until the Mongol invasions of the 13th century. Ibn al-Haytham (Alhazen), as well as his predecessor Ibn Sahl, was familiar with Ptolemy's "Optics", and used experiments as a means to gain knowledge. Alhazen disproved Ptolemy's theory of vision, but did not make any corresponding changes to Aristotle's metaphysics. Furthermore, doctors and alchemists such as the Persians Avicenna and Al-Razi also greatly developed the science of Medicine with the former writing the Canon of Medicine, a medical encyclopedia used until the 18th century and the latter discovering multiple compounds like alcohol. Avicenna's canon is considered to be one of the most important publications in medicine and they both contributed significantly to the practice of experimental medicine, using clinical trials and experiments to back their claims.

In Classical antiquity, Greek and Roman taboos had meant that dissection was usually banned in ancient times, but in Middle Ages it changed: medical teachers and students at Bologna began to open human bodies, and Mondino de Luzzi (c. 1275–1326) produced the ﬁrst known anatomy textbook based on human dissection.

By the eleventh century most of Europe had become Christian; stronger monarchies emerged; borders were restored; technological developments and agricultural innovations were made which increased the food supply and population. In addition, classical Greek texts started to be translated from Arabic and Greek into Latin, giving a higher level of scientific discussion in Western Europe.

By 1088, the first university in Europe (the University of Bologna) had emerged from its clerical beginnings. Demand for Latin translations grew (for example, from the Toledo School of Translators); western Europeans began collecting texts written not only in Latin, but also Latin translations from Greek, Arabic, and Hebrew. Manuscript copies of Alhazen's "Book of Optics" also propagated across Europe before 1240, as evidenced by its incorporation into Vitello's "Perspectiva". Avicenna's Canon was translated into Latin. In particular, the texts of Aristotle, Ptolemy, and Euclid, preserved in the Houses of Wisdom and also in the Byzantine Empire, were sought amongst Catholic scholars. The influx of ancient texts caused the Renaissance of the 12th century and the flourishing of a synthesis of Catholicism and Aristotelianism known as Scholasticism in western Europe, which became a new geographic center of science. An "experiment" in this period would be understood as a careful process of observing, describing, and classifying. One prominent scientist in this era was Roger Bacon. Scholasticism had a strong focus on revelation and dialectic reasoning, and gradually fell out of favour over the next centuries, as alchemy's focus on experiments that include direct observation and meticulous documentation slowly increased in importance.

New developments in optics played a role in the inception of the Renaissance, both by challenging long-held metaphysical ideas on perception, as well as by contributing to the improvement and development of technology such as the camera obscura and the telescope. Before what we now know as the Renaissance started, Roger Bacon, Vitello, and John Peckham each built up a scholastic ontology upon a causal chain beginning with sensation, perception, and finally apperception of the individual and universal forms of Aristotle. A model of vision later known as perspectivism was exploited and studied by the artists of the Renaissance. This theory uses only three of Aristotle's four causes: formal, material, and final.

In the sixteenth century, Copernicus formulated a heliocentric model of the solar system unlike the geocentric model of Ptolemy's "Almagest". This was based on a theorem that the orbital periods of the planets are longer as their orbs are farther from the centre of motion, which he found not to agree with Ptolemy's model.

Kepler and others challenged the notion that the only function of the eye is perception, and shifted the main focus in optics from the eye to the propagation of light. Kepler modelled the eye as a water-filled glass sphere with an aperture in front of it to model the entrance pupil. He found that all the light from a single point of the scene was imaged at a single point at the back of the glass sphere. The optical chain ends on the retina at the back of the eye. Kepler is best known, however, for improving Copernicus' heliocentric model through the discovery of Kepler's laws of planetary motion. Kepler did not reject Aristotelian metaphysics, and described his work as a search for the Harmony of the Spheres.
Galileo made innovative use of experiment and mathematics. However, he became persecuted after Pope Urban VIII blessed Galileo to write about the Copernican system. Galileo had used arguments from the Pope and put them in the voice of the simpleton in the work "Dialogue Concerning the Two Chief World Systems", which greatly offended Urban VIII.

In Northern Europe, the new technology of the printing press was widely used to publish many arguments, including some that disagreed widely with contemporary ideas of nature. René Descartes and Francis Bacon published philosophical arguments in favor of a new type of non-Aristotelian science. Descartes emphasized individual thought and argued that mathematics rather than geometry should be used in order to study nature. Bacon emphasized the importance of experiment over contemplation. Bacon further questioned the Aristotelian concepts of formal cause and final cause, and promoted the idea that science should study the laws of "simple" natures, such as heat, rather than assuming that there is any specific nature, or "formal cause", of each complex type of thing. This new science began to see itself as describing "laws of nature". This updated approach to studies in nature was seen as mechanistic. Bacon also argued that science should aim for the first time at practical inventions for the improvement of all human life.

As a precursor to the Age of Enlightenment, Isaac Newton and Gottfried Wilhelm Leibniz succeeded in developing a new physics, now referred to as classical mechanics, which could be confirmed by experiment and explained using mathematics (Newton (1687), "Philosophiæ Naturalis Principia Mathematica"). Leibniz also incorporated terms from Aristotelian physics, but now being used in a new non-teleological way, for example, "energy" and "potential" (modern versions of Aristotelian ""energeia" and "potentia""). This implied a shift in the view of objects: Where Aristotle had noted that objects have certain innate goals that can be actualized, objects were now regarded as devoid of innate goals. In the style of Francis Bacon, Leibniz assumed that different types of things all work according to the same general laws of nature, with no special formal or final causes for each type of thing. It is during this period that the word "science" gradually became more commonly used to refer to a "type of pursuit" of a type of knowledge, especially knowledge of nature – coming close in meaning to the old term "natural philosophy."

During this time, the declared purpose and value of science became producing wealth and inventions that would improve human lives, in the materialistic sense of having more food, clothing, and other things. In Bacon's words, "the real and legitimate goal of sciences is the endowment of human life with new inventions and riches", and he discouraged scientists from pursuing intangible philosophical or spiritual ideas, which he believed contributed little to human happiness beyond "the fume of subtle, sublime, or pleasing speculation".

Science during the Enlightenment was dominated by scientific societies and academies, which had largely replaced universities as centres of scientific research and development. Societies and academies were also the backbone of the maturation of the scientific profession. Another important development was the popularization of science among an increasingly literate population. Philosophes introduced the public to many scientific theories, most notably through the "Encyclopédie" and the popularization of Newtonianism by Voltaire as well as by Émilie du Châtelet, the French translator of Newton's "Principia".

Some historians have marked the 18th century as a drab period in the history of science; however, the century saw significant advancements in the practice of medicine, mathematics, and physics; the development of biological taxonomy; a new understanding of magnetism and electricity; and the maturation of chemistry as a discipline, which established the foundations of modern chemistry.

Enlightenment philosophers chose a short history of scientific predecessors – Galileo, Boyle, and Newton principally – as the guides and guarantors of their applications of the singular concept of nature and natural law to every physical and social field of the day. In this respect, the lessons of history and the social structures built upon it could be discarded.

The nineteenth century is a particularly important period in the history of science since during this era many distinguishing characteristics of contemporary modern science began to take shape such as: transformation of the life and physical sciences, frequent use of precision instruments, emergence of terms like "biologist", "physicist", "scientist"; slowly moving away from antiquated labels like "natural philosophy" and "natural history", increased professionalization of those studying nature lead to reduction in amateur naturalists, scientists gained cultural authority over many dimensions of society, economic expansion and industrialization of numerous countries, thriving of popular science writings and emergence of science journals.

Early in the 19th century, John Dalton suggested the modern atomic theory, based on Democritus's original idea of individible particles called "atoms".

Both John Herschel and William Whewell systematized methodology: the latter coined the term scientist. When Charles Darwin published "On the Origin of Species" he established evolution as the prevailing explanation of biological complexity. His theory of natural selection provided a natural explanation of how species originated, but this only gained wide acceptance a century later.

The laws of conservation of energy, conservation of momentum and conservation of mass suggested a highly stable universe where there could be little loss of resources. With the advent of the steam engine and the industrial revolution, there was, however, an increased understanding that all forms of energy as defined in physics were not equally useful： they did not have the same energy quality. This realization led to the development of the laws of thermodynamics, in which the free energy of the universe is seen as constantly declining: the entropy of a closed universe increases over time.

The electromagnetic theory was also established in the 19th century, and raised new questions which could not easily be answered using Newton's framework. The phenomena that would allow the deconstruction of the atom were discovered in the last decade of the 19th century: the discovery of X-rays inspired the discovery of radioactivity. In the next year came the discovery of the first subatomic particle, the electron.

Einstein's theory of relativity and the development of quantum mechanics led to the replacement of classical mechanics with a new physics which contains two parts that describe different types of events in nature.

In the first half of the century, the development of antibiotics and artificial fertilizer made global human population growth possible. At the same time, the structure of the atom and its nucleus was discovered, leading to the release of "atomic energy" (nuclear power). In addition, the extensive use of technological innovation stimulated by the wars of this century led to revolutions in transportation (automobiles and aircraft), the development of ICBMs, a space race, and a nuclear arms race.

The molecular structure of DNA was discovered in 1953. The discovery of the cosmic microwave background radiation in 1964 led to a rejection of the Steady State theory of the universe in favour of the Big Bang theory of Georges Lemaître.

The development of spaceflight in the second half of the century allowed the first astronomical measurements done on or near other objects in space, including manned landings on the Moon. Space telescopes lead to numerous discoveries in astronomy and cosmology.

Widespread use of integrated circuits in the last quarter of the 20th century combined with communications satellites led to a revolution in information technology and the rise of the global internet and mobile computing, including smartphones. The need for mass systematization of long, intertwined causal chains and large amounts of data led to the rise of the fields of systems theory and computer-assisted scientific modelling, which are partly based on the Aristotelian paradigm.

Harmful environmental issues such as ozone depletion, acidification, eutrophication and climate change came to the public's attention in the same period, and caused the onset of environmental science and environmental technology.

The Human Genome Project was completed in 2003, determining the sequence of nucleotide base pairs that make up human DNA, and identifying and mapping all of the genes of the human genome. Induced pluripotent stem cells were developed in 2006, a technology allowing adult cells to be transformed into stem cells capable of giving rise to any cell type found in the body, potentially of huge importance to the field of regenerative medicine.

With the discovery of the Higgs boson in 2012, the last particle predicted by the Standard Model of particle physics was found. In 2015, gravitational waves, predicted by general relativity a century before, were first observed.

Modern science is commonly divided into three major branches that consist of the natural sciences, social sciences, and formal sciences. Each of these branches comprise various specialized yet overlapping scientific disciplines that often possess their own nomenclature and expertise. Both natural and social sciences are empirical sciences as their knowledge are based on empirical observations and are capable of being tested for its validity by other researchers working under the same conditions.

There are also closely related disciplines that use science, such as engineering and medicine, which are sometimes described as applied sciences. The relationships between the branches of science are summarized by the following table.
Natural science is concerned with the description, prediction, and understanding of natural phenomena based on empirical evidence from observation and experimentation. It can be divided into two main branches: life science (or biological science) and physical science. Physical science is subdivided into branches, including physics, chemistry, astronomy and earth science. These two branches may be further divided into more specialized disciplines. Modern natural science is the successor to the natural philosophy that began in Ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science. Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and so on. Today, "natural history" suggests observational descriptions aimed at popular audiences.

Social science is concerned with society and the relationships among individuals within a society. It has many branches that include, but are not limited to, anthropology, archaeology, communication studies, economics, history, human geography, jurisprudence, linguistics, political science, psychology, public health, and sociology. Social scientists may adopt various philosophical theories to study individuals and society. For example, positivist social scientists use methods resembling those of the natural sciences as tools for understanding society, and so define science in its stricter modern sense. Interpretivist social scientists, by contrast, may use social critique or symbolic interpretation rather than constructing empirically falsifiable theories, and thus treat science in its broader sense. In modern academic practice, researchers are often eclectic, using multiple methodologies (for instance, by combining both quantitative and qualitative research). The term "social research" has also acquired a degree of autonomy as practitioners from various disciplines share in its aims and methods.

Formal science is involved in the study of formal systems. It includes mathematics, systems theory, and theoretical computer science. The formal sciences share similarities with the other two branches by relying on objective, careful, and systematic study of an area of knowledge. They are, however, different from the empirical sciences as they rely exclusively on deductive reasoning, without the need for empirical evidence, to verify their abstract concepts. The formal sciences are therefore "a priori" disciplines and because of this, there is disagreement on whether they actually constitute a science. Nevertheless, the formal sciences play an important role in the empirical sciences. Calculus, for example, was initially invented to understand motion in physics. Natural and social sciences that rely heavily on mathematical applications include mathematical physics, mathematical chemistry, mathematical biology, mathematical finance, and mathematical economics.

Scientific research can be labeled as either basic or applied research. Basic research is the search for knowledge and applied research is the search for solutions to practical problems using this knowledge. Although some scientific research is applied research into specific problems, a great deal of our understanding comes from the curiosity-driven undertaking of basic research. This leads to options for technological advance that were not planned or sometimes even imaginable. This point was made by Michael Faraday when allegedly in response to the question "what is the "use" of basic research?" he responded: "Sir, what is the use of a new-born child?". For example, research into the effects of red light on the human eye's rod cells did not seem to have any practical purpose; eventually, the discovery that our night vision is not troubled by red light would lead search and rescue teams (among others) to adopt red light in the cockpits of jets and helicopters. Finally, even basic research can take unexpected turns, and there is some sense in which the scientific method is built to harness luck.

Scientific research involves using the scientific method, which seeks to objectively explain the events of nature in a reproducible way. An explanatory thought experiment or hypothesis is put forward as explanation using principles such as parsimony (also known as "Occam's Razor") and are generally expected to seek consilience – fitting well with other accepted facts related to the phenomena. This new explanation is used to make falsifiable predictions that are testable by experiment or observation. The predictions are to be posted before a confirming experiment or observation is sought, as proof that no tampering has occurred. Disproof of a prediction is evidence of progress. This is done partly through observation of natural phenomena, but also through experimentation that tries to simulate natural events under controlled conditions as appropriate to the discipline (in the observational sciences, such as astronomy or geology, a predicted observation might take the place of a controlled experiment). Experimentation is especially important in science to help establish causal relationships (to avoid the correlation fallacy).

When a hypothesis proves unsatisfactory, it is either modified or discarded. If the hypothesis survived testing, it may become adopted into the framework of a scientific theory, a logically reasoned, self-consistent model or framework for describing the behavior of certain natural phenomena. A theory typically describes the behavior of much broader sets of phenomena than a hypothesis; commonly, a large number of hypotheses can be logically bound together by a single theory. Thus a theory is a hypothesis explaining various other hypotheses. In that vein, theories are formulated according to most of the same scientific principles as hypotheses. In addition to testing hypotheses, scientists may also generate a model, an attempt to describe or depict the phenomenon in terms of a logical, physical or mathematical representation and to generate new hypotheses that can be tested, based on observable phenomena.

While performing experiments to test hypotheses, scientists may have a preference for one outcome over another, and so it is important to ensure that science as a whole can eliminate this bias. This can be achieved by careful experimental design, transparency, and a thorough peer review process of the experimental results as well as any conclusions. After the results of an experiment are announced or published, it is normal practice for independent researchers to double-check how the research was performed, and to follow up by performing similar experiments to determine how dependable the results might be. Taken in its entirety, the scientific method allows for highly creative problem solving while minimizing any effects of subjective bias on the part of its users (especially the confirmation bias).

John Ziman points out that intersubjective verifiability is fundamental to the creation of all scientific knowledge. Ziman shows how scientists can identify patterns to each other across centuries; he refers to this ability as "perceptual consensibility." He then makes consensibility, leading to consensus, the touchstone of reliable knowledge.

Mathematics is essential in the formation of hypotheses, theories, and laws in the natural and social sciences. For example, it is used in quantitative scientific modeling, which can generate new hypotheses and predictions to be tested. It is also used extensively in observing and collecting measurements. Statistics, a branch of mathematics, is used to summarize and analyze data, which allow scientists to assess the reliability and variability of their experimental results.

Computational science applies computing power to simulate real-world situations, enabling a better understanding of scientific problems than formal mathematics alone can achieve. According to the Society for Industrial and Applied Mathematics, computation is now as important as theory and experiment in advancing scientific knowledge.

Scientists usually take for granted a set of basic assumptions that are needed to justify the scientific method: (1) that there is an objective reality shared by all rational observers; (2) that this objective reality is governed by natural laws; (3) that these laws can be discovered by means of systematic observation and experimentation. Philosophy of science seeks a deep understanding of what these underlying assumptions mean and whether they are valid.

The belief that scientific theories should and do represent metaphysical reality is known as realism. It can be contrasted with anti-realism, the view that the success of science does not depend on it being accurate about unobservable entities such as electrons. One form of anti-realism is idealism, the belief that the mind or consciousness is the most basic essence, and that each mind generates its own reality. In an idealistic world view, what is true for one mind need not be true for other minds.

There are different schools of thought in philosophy of science. The most popular position is empiricism, which holds that knowledge is created by a process involving observation and that scientific theories are the result of generalizations from such observations. Empiricism generally encompasses inductivism, a position that tries to explain the way general theories can be justified by the finite number of observations humans can make and hence the finite amount of empirical evidence available to confirm scientific theories. This is necessary because the number of predictions those theories make is infinite, which means that they cannot be known from the finite amount of evidence using deductive logic only. Many versions of empiricism exist, with the predominant ones being Bayesianism and the hypothetico-deductive method.
Empiricism has stood in contrast to rationalism, the position originally associated with Descartes, which holds that knowledge is created by the human intellect, not by observation. Critical rationalism is a contrasting 20th-century approach to science, first defined by Austrian-British philosopher Karl Popper. Popper rejected the way that empiricism describes the connection between theory and observation. He claimed that theories are not generated by observation, but that observation is made in the light of theories and that the only way a theory can be affected by observation is when it comes in conflict with it. Popper proposed replacing verifiability with falsifiability as the landmark of scientific theories and replacing induction with falsification as the empirical method. Popper further claimed that there is actually only one universal method, not specific to science: the negative method of criticism, trial and error. It covers all products of the human mind, including science, mathematics, philosophy, and art.

Another approach, instrumentalism, colloquially termed "shut up and multiply," emphasizes the utility of theories as instruments for explaining and predicting phenomena. It views scientific theories as black boxes with only their input (initial conditions) and output (predictions) being relevant. Consequences, theoretical entities, and logical structure are claimed to be something that should simply be ignored and that scientists shouldn't make a fuss about (see interpretations of quantum mechanics). Close to instrumentalism is constructive empiricism, according to which the main criterion for the success of a scientific theory is whether what it says about observable entities is true.

Thomas Kuhn argued that the process of observation and evaluation takes place within a paradigm, a logically consistent "portrait" of the world that is consistent with observations made from its framing. He characterized "normal science" as the process of observation and "puzzle solving" which takes place within a paradigm, whereas "revolutionary science" occurs when one paradigm overtakes another in a paradigm shift. Each paradigm has its own distinct questions, aims, and interpretations. The choice between paradigms involves setting two or more "portraits" against the world and deciding which likeness is most promising. A paradigm shift occurs when a significant number of observational anomalies arise in the old paradigm and a new paradigm makes sense of them. That is, the choice of a new paradigm is based on observations, even though those observations are made against the background of the old paradigm. For Kuhn, acceptance or rejection of a paradigm is a social process as much as a logical process. Kuhn's position, however, is not one of relativism.

Finally, another approach often cited in debates of scientific skepticism against controversial movements like "creation science" is methodological naturalism. Its main point is that a difference between natural and supernatural explanations should be made and that science should be restricted methodologically to natural explanations. That the restriction is merely methodological (rather than ontological) means that science should not consider supernatural explanations itself, but should not claim them to be wrong either. Instead, supernatural explanations should be left a matter of personal belief outside the scope of science. Methodological naturalism maintains that proper science requires strict adherence to empirical study and independent verification as a process for properly developing and evaluating explanations for observable phenomena. The absence of these standards, arguments from authority, biased observational studies and other common fallacies are frequently cited by supporters of methodological naturalism as characteristic of the non-science they criticize.

A scientific theory is empirical and is always open to falsification if new evidence is presented. That is, no theory is ever considered strictly certain as science accepts the concept of fallibilism. The philosopher of science Karl Popper sharply distinguished truth from certainty. He wrote that scientific knowledge "consists in the search for truth," but it "is not the search for certainty ... All human knowledge is fallible and therefore uncertain.

New scientific knowledge rarely results in vast changes in our understanding. According to psychologist Keith Stanovich, it may be the media's overuse of words like "breakthrough" that leads the public to imagine that science is constantly proving everything it thought was true to be false. While there are such famous cases as the theory of relativity that required a complete reconceptualization, these are extreme exceptions. Knowledge in science is gained by a gradual synthesis of information from different experiments by various researchers across different branches of science; it is more like a climb than a leap. Theories vary in the extent to which they have been tested and verified, as well as their acceptance in the scientific community. For example, heliocentric theory, the theory of evolution, relativity theory, and germ theory still bear the name "theory" even though, in practice, they are considered factual.
Philosopher Barry Stroud adds that, although the best definition for "knowledge" is contested, being skeptical and entertaining the "possibility" that one is incorrect is compatible with being correct. Therefore, scientists adhering to proper scientific approaches will doubt themselves even once they possess the truth. The fallibilist C. S. Peirce argued that inquiry is the struggle to resolve actual doubt and that merely quarrelsome, verbal, or hyperbolic doubt is fruitless – but also that the inquirer should try to attain genuine doubt rather than resting uncritically on common sense. He held that the successful sciences trust not to any single chain of inference (no stronger than its weakest link) but to the cable of multiple and various arguments intimately connected.

Stanovich also asserts that science avoids searching for a "magic bullet"; it avoids the single-cause fallacy. This means a scientist would not ask merely "What is "the" cause of ...", but rather "What "are" the most significant "causes" of ...". This is especially the case in the more macroscopic fields of science (e.g. psychology, physical cosmology). Research often analyzes few factors at once, but these are always added to the long list of factors that are most important to consider. For example, knowing the details of only a person's genetics, or their history and upbringing, or the current situation may not explain a behavior, but a deep understanding of all these variables combined can be very predictive.

Scientific research is published in an enormous range of scientific literature. Scientific journals communicate and document the results of research carried out in universities and various other research institutions, serving as an archival record of science. The first scientific journals, "Journal des Sçavans" followed by the "Philosophical Transactions", began publication in 1665. Since that time the total number of active periodicals has steadily increased. In 1981, one estimate for the number of scientific and technical journals in publication was 11,500. The United States National Library of Medicine currently indexes 5,516 journals that contain articles on topics related to the life sciences. Although the journals are in 39 languages, 91 percent of the indexed articles are published in English.

Most scientific journals cover a single scientific field and publish the research within that field; the research is normally expressed in the form of a scientific paper. Science has become so pervasive in modern societies that it is generally considered necessary to communicate the achievements, news, and ambitions of scientists to a wider populace.

Science magazines such as "New Scientist", "Science & Vie", and "Scientific American" cater to the needs of a much wider readership and provide a non-technical summary of popular areas of research, including notable discoveries and advances in certain fields of research. Science books engage the interest of many more people. Tangentially, the science fiction genre, primarily fantastic in nature, engages the public imagination and transmits the ideas, if not the methods, of science.

Recent efforts to intensify or develop links between science and non-scientific disciplines such as literature or more specifically, poetry, include the "Creative Writing Science" resource developed through the Royal Literary Fund.

Discoveries in fundamental science can be world-changing. For example:

The replication crisis is an ongoing methodological crisis primarily affecting parts of the social and life sciences in which scholars have found that the results of many scientific studies are difficult or impossible to replicate or reproduce on subsequent investigation, either by independent researchers or by the original researchers themselves. The crisis has long-standing roots; the phrase was coined in the early 2010s as part of a growing awareness of the problem. The replication crisis represents an important body of research in metascience, which aims to improve the quality of all scientific research while reducing waste.

An area of study or speculation that masquerades as science in an attempt to claim a legitimacy that it would not otherwise be able to achieve is sometimes referred to as pseudoscience, fringe science, or junk science. Physicist Richard Feynman coined the term "cargo cult science" for cases in which researchers believe they are doing science because their activities have the outward appearance of science but actually lack the "kind of utter honesty" that allows their results to be rigorously evaluated. Various types of commercial advertising, ranging from hype to fraud, may fall into these categories. Science has been described as "the most important tool" for separating valid claims from invalid ones.

There can also be an element of political or ideological bias on all sides of scientific debates. Sometimes, research may be characterized as "bad science," research that may be well-intended but is actually incorrect, obsolete, incomplete, or over-simplified expositions of scientific ideas. The term "scientific misconduct" refers to situations such as where researchers have intentionally misrepresented their published data or have purposely given credit for a discovery to the wrong person.

The scientific community is a group of all interacting scientists, along with their respective societies and institutions.

Scientists are individuals who conduct scientific research to advance knowledge in an area of interest. The term "scientist" was coined by William Whewell in 1833. In modern times, many professional scientists are trained in an academic setting and upon completion, attain an academic degree, with the highest degree being a doctorate such as a Doctor of Philosophy (PhD), Doctor of Medicine (MD), or Doctor of Engineering (DEng). Many scientists pursue careers in various sectors of the economy such as academia, industry, government, and nonprofit organizations.

Scientists exhibit a strong curiosity about reality, with some scientists having a desire to apply scientific knowledge for the benefit of health, nations, environment, or industries. Other motivations include recognition by their peers and prestige. The Nobel Prize, a widely regarded prestigious award, is awarded annually to those who have achieved scientific advances in the fields of medicine, physics, chemistry, and economics.

Science has historically been a male-dominated field, with some notable exceptions. Women faced considerable discrimination in science, much as they did in other areas of male-dominated societies, such as frequently being passed over for job opportunities and denied credit for their work. For example, Christine Ladd (1847–1930) was able to enter a PhD program as "C. Ladd"; Christine "Kitty" Ladd completed the requirements in 1882, but was awarded her degree only in 1926, after a career which spanned the algebra of logic (see truth table), color vision, and psychology. Her work preceded notable researchers like Ludwig Wittgenstein and Charles Sanders Peirce. The achievements of women in science have been attributed to their defiance of their traditional role as laborers within the domestic sphere.

In the late 20th century, active recruitment of women and elimination of institutional discrimination on the basis of sex greatly increased the number of women scientists, but large gender disparities remain in some fields; in the early 21st century over half of new biologists were female, while 80% of PhDs in physics are given to men. In the early part of the 21st century, women in the United States earned 50.3% of bachelor's degrees, 45.6% of master's degrees, and 40.7% of PhDs in science and engineering fields. They earned more than half of the degrees in psychology (about 70%), social sciences (about 50%), and biology (about 50-60%) but earned less than half the degrees in the physical sciences, earth sciences, mathematics, engineering, and computer science. Lifestyle choice also plays a major role in female engagement in science; women with young children are 28% less likely to take tenure-track positions due to work-life balance issues, and female graduate students' interest in careers in research declines dramatically over the course of graduate school, whereas that of their male colleagues remains unchanged.

Learned societies for the communication and promotion of scientific thought and experimentation have existed since the Renaissance. Many scientists belong to a learned society that promotes their respective scientific discipline, profession, or group of related disciplines. Membership may be open to all, may require possession of some scientific credentials, or may be an honor conferred by election. Most scientific societies are non-profit organizations, and many are professional associations. Their activities typically include holding regular conferences for the presentation and discussion of new research results and publishing or sponsoring academic journals in their discipline. Some also act as professional bodies, regulating the activities of their members in the public interest or the collective interest of the membership. Scholars in the sociology of science argue that learned societies are of key importance and their formation assists in the emergence and development of new disciplines or professions.

The professionalization of science, begun in the 19th century, was partly enabled by the creation of distinguished academy of sciences in a number of countries such as the Italian in 1603, the British Royal Society in 1660, the French in 1666, the American National Academy of Sciences in 1863, the German Kaiser Wilhelm Institute in 1911, and the Chinese Academy of Sciences in 1928. International scientific organizations, such as the International Council for Science, have since been formed to promote cooperation between the scientific communities of different nations.

Science policy is an area of public policy concerned with the policies that affect the conduct of the scientific enterprise, including research funding, often in pursuance of other national policy goals such as technological innovation to promote commercial product development, weapons development, health care and environmental monitoring. Science policy also refers to the act of applying scientific knowledge and consensus to the development of public policies. Science policy thus deals with the entire domain of issues that involve the natural sciences. In accordance with public policy being concerned about the well-being of its citizens, science policy's goal is to consider how science and technology can best serve the public.

State policy has influenced the funding of public works and science for thousands of years, particularly within civilizations with highly organized governments such as imperial China and the Roman Empire. Prominent historical examples include the Great Wall of China, completed over the course of two millennia through the state support of several dynasties, and the Grand Canal of the Yangtze River, an immense feat of hydraulic engineering begun by Sunshu Ao (孫叔敖 7th c. BCE), Ximen Bao (西門豹 5th c.BCE), and Shi Chi (4th c. BCE). This construction dates from the 6th century BCE under the Sui Dynasty and is still in use today. In China, such state-supported infrastructure and scientific research projects date at least from the time of the Mohists, who inspired the study of logic during the period of the Hundred Schools of Thought and the study of defensive fortifications like the Great Wall of China during the Warring States period.

Public policy can directly affect the funding of capital equipment and intellectual infrastructure for industrial research by providing tax incentives to those organizations that fund research. Vannevar Bush, director of the Office of Scientific Research and Development for the United States government, the forerunner of the National Science Foundation, wrote in July 1945 that "Science is a proper concern of government."

Scientific research is often funded through a competitive process in which potential research projects are evaluated and only the most promising receive funding. Such processes, which are run by government, corporations, or foundations, allocate scarce funds. Total research funding in most developed countries is between 1.5% and 3% of GDP. In the OECD, around two-thirds of research and development in scientific and technical fields is carried out by industry, and 20% and 10% respectively by universities and government. The government funding proportion in certain industries is higher, and it dominates research in social science and humanities. Similarly, with some exceptions (e.g. biotechnology) government provides the bulk of the funds for basic scientific research. Many governments have dedicated agencies to support scientific research. Prominent scientific organizations include the National Science Foundation in the United States, the National Scientific and Technical Research Council in Argentina, Commonwealth Scientific and Industrial Research Organisation (CSIRO) in Australia, in France, the Max Planck Society and in Germany, and CSIC in Spain. In commercial research and development, all but the most research-oriented corporations focus more heavily on near-term commercialisation possibilities rather than "blue-sky" ideas or technologies (such as nuclear fusion).

The public awareness of science relates to the attitudes, behaviors, opinions, and activities that make up the relations between science and the general public. it integrates various themes and activities such as science communication, science museums, science festivals, science fairs, citizen science, and science in popular culture. Social scientists have devised various metrics to measure the public understanding of science such as factual knowledge, self-reported knowledge, and structural knowledge.

The mass media face a number of pressures that can prevent them from accurately depicting competing scientific claims in terms of their credibility within the scientific community as a whole. Determining how much weight to give different sides in a scientific debate may require considerable expertise regarding the matter. Few journalists have real scientific knowledge, and even beat reporters who know a great deal about certain scientific issues may be ignorant about other scientific issues that they are suddenly asked to cover.

Politicization of science occurs when government, business, or advocacy groups use legal or economic pressure to influence the findings of scientific research or the way it is disseminated, reported, or interpreted. Many factors can act as facets of the politicization of science such as populist anti-intellectualism, perceived threats to religious beliefs, postmodernist subjectivism, and fear for business interests. Politicization of science is usually accomplished when scientific information is presented in a way that emphasizes the uncertainty associated with the scientific evidence. Tactics such as shifting conversation, failing to acknowledge facts, and capitalizing on doubt of scientific consensus have been used to gain more attention for views that have been undermined by scientific evidence. Examples of issues that have involved the politicization of science include the global warming controversy, health effects of pesticides, and health effects of tobacco.


Publications

Resources


</doc>
<doc id="26701" url="https://en.wikipedia.org/wiki?curid=26701" title="List of current United States senators">
List of current United States senators

The United States Senate consists of 100 members, two from each of the 50 states. Below is a list of U.S. senators in the 116th United States Congress.



</doc>
<doc id="26703" url="https://en.wikipedia.org/wiki?curid=26703" title="Statistic">
Statistic

A statistic (singular) or sample statistic is any quantity computed from values in a sample, often the mean. Technically speaking, a statistic can be calculated by applying any mathematical function to the values found in a sample of data.

In statistics, there is an important distinction between a statistic and a parameter. "Parameter" refers to any characteristic of a population under study. When it is not possible or practical to directly measure the value of a population parameter, statistical methods are used to infer the likely value of the parameter on the basis of a statistic computed from a sample taken from the population. When a statistic is used to estimate a population parameter, is called an estimator. It can be proved that the mean of a sample is an unbiased estimator of the population mean. This means that the average of multiple sample means will tend to converge to the true mean of the population.

Formally, statistical theory defines a statistic as a function of a sample where the function itself is independent of the unknown estimands; that is, the function is strictly a function of the data. The term statistic is used both for the function and for the value of the function on a given sample.

When a statistic (a function) is being used for a specific purpose, it may be referred to by a name indicating its purpose: in descriptive statistics, a descriptive statistic is used to describe the data; in estimation theory, an estimator is used to estimate a parameter of the distribution (population); in statistical hypothesis testing, a test statistic is used to test a hypothesis. However, a single statistic can be used for multiple purposes – for example the sample mean can be used to describe a data set, to estimate the population mean, or to test a hypothesis.

Some examples of statistics are:

There are a variety of functions that are used to calculate statistics. Some include:

A statistic is an "observable" random variable, which differentiates it both from a "parameter" that is a generally unobservable quantity describing a property of a statistical population, and from an unobservable random variable, such as the difference between an observed measurement and a population average. A parameter can only be computed exactly if entire population can be observed without error; for instance, in a perfect census or for a population of standardized test takers.

Statisticians often contemplate a parameterized family of probability distributions, any member of which could be the distribution of some measurable aspect of each member of a population, from which a sample is drawn randomly. For example, the parameter may be the average height of 25-year-old men in North America. The height of the members of a sample of 100 such men are measured; the average of those 100 numbers is a statistic. The average of the heights of all members of the population is not a statistic unless that has somehow also been ascertained (such as by measuring every member of the population). The average height that would be calculated using "all" of the individual heights of "all" 25-year-old North American men is a parameter, and not a statistic.

Important potential properties of statistics include completeness, consistency, sufficiency, unbiasedness, minimum mean square error, low variance, robustness, and computational convenience.

Information of a statistic on model parameters can be defined in several ways. The most common is the Fisher information, which is defined on the statistic model induced by the statistic. Kullback information measure can also be used.




</doc>
<doc id="26707" url="https://en.wikipedia.org/wiki?curid=26707" title="Sverige (disambiguation)">
Sverige (disambiguation)

Sverige is the Swedish language name for Sweden.

Sverige may also refer to:




</doc>
<doc id="26709" url="https://en.wikipedia.org/wiki?curid=26709" title="Sean Connery">
Sean Connery

Sir Thomas Sean Connery (born 25 August 1930) is a Scottish retired actor and producer, who has won an Academy Award, two BAFTA Awards (one being a BAFTA Academy Fellowship Award), and three Golden Globes, including the Cecil B. DeMille Award and a Henrietta Award.

Connery was the first actor to portray the character James Bond in film, starring in seven Bond films (every film from "Dr. No" to "You Only Live Twice", plus "Diamonds Are Forever" and "Never Say Never Again"), between 1962 and 1983. In 1988, Connery won the Academy Award for Best Supporting Actor for his role in "The Untouchables". His films also include "Marnie" (1964), "Murder on the Orient Express" (1974), "The Man Who Would Be King" (1975), "The Name of the Rose" (1986), "Highlander" (1986), "Indiana Jones and the Last Crusade" (1989), "The Hunt for Red October" (1990), "Dragonheart" (1996), "The Rock" (1996), and "Finding Forrester" (2000).

Connery has been polled in "The Sunday Herald" as "The Greatest Living Scot" and in a EuroMillions survey as "Scotland's Greatest Living National Treasure". He was voted by "People" magazine as both the “Sexiest Man Alive” in 1989 and the “Sexiest Man of the Century” in 1999. Connery was knighted in the 2000 New Year Honours for services to film drama.

Thomas Sean Connery, named Thomas after his grandfather, was born in Fountainbridge, Edinburgh, Scotland on 25 August 1930. His mother, Euphemia "Effie" McBain McLean, was a cleaning woman. She was born the daughter of Neil McLean and Helen Forbes Ross, and named after her father's mother 'Euphemia McBain', wife of John McLean and daughter of William McBain from Ceres in Fife. Connery's father, Joseph Connery, was a factory worker and lorry driver. His paternal grandfather's parents emigrated to Scotland from Ireland in the mid-19th century. The remainder of his family was of Scottish descent, and his maternal great-grandparents were native Scottish Gaelic speakers from Fife (unusually, for a speaker of the language), and Uig on Skye.
His father was a Roman Catholic, and his mother was a Protestant. He has a younger brother, Neil. Connery has said that he was called Sean, his middle name, long before becoming an actor, explaining that when he was young he had an Irish friend named Séamus and that those who knew them both had decided to call Connery by his middle name whenever both were present. He was generally referred to in his youth as "Tommy". Although he was small in primary school, he grew rapidly around the age of 12, reaching his full adult height of at 18. He was known during his teen years as "Big Tam", and has stated that he lost his virginity to an adult woman in an ATS uniform at the age of 14.

Connery's first job was as a milkman in Edinburgh with St. Cuthbert's Co-operative Society. In 2009, Connery recalled a conversation in a taxi:

Connery then joined the Royal Navy, during which time he acquired two tattoos, of which his official website says "unlike many tattoos, his were not frivolous—his tattoos reflect two of his lifelong commitments: his family and Scotland. ... One tattoo is a tribute to his parents and reads 'Mum and Dad,' and the other is self-explanatory, 'Scotland Forever.'"

Connery was later discharged from the navy on medical grounds because of a duodenal ulcer, a condition that affected most of the males in previous generations of his family. Afterwards, he returned to the co-op, then worked as, among other things, a lorry driver, a lifeguard at Portobello swimming baths, a labourer, an artist's model for the Edinburgh College of Art, and after a suggestion by former Mr. Scotland, Archie Brennan, a coffin polisher. The modelling earned him 15 shillings an hour. Artist Richard Demarco, at the time a student who painted several early pictures of Connery, described him as "very straight, slightly shy, too, too beautiful for words, a virtual Adonis".

Connery began bodybuilding at the age of 18, and from 1951 trained heavily with Ellington, a former gym instructor in the British Army. While his official website claims he was third in the 1950 Mr. Universe contest, most sources place him in the 1953 competition, either third in the Junior class or failing to place in the Tall Man classification. Connery stated that he was soon deterred from bodybuilding when he found that the Americans frequently beat him in competitions because of sheer muscle size and, unlike Connery, refused to participate in athletic activity which could make them lose muscle mass.

Connery was a keen footballer, having played for Bonnyrigg Rose in his younger days. He was offered a trial with East Fife. While on tour with "South Pacific", Connery played in a football match against a local team that Matt Busby, manager of Manchester United, happened to be scouting. According to reports, Busby was impressed with his physical prowess and offered Connery a contract worth £25 a week () immediately after the game. Connery admits that he was tempted to accept, but he recalls, "I realised that a top-class footballer could be over the hill by the age of 30, and I was already 23. I decided to become an actor and it turned out to be one of my more intelligent moves."

Looking to pick up some extra money, Connery helped out backstage at the King's Theatre in late 1951. He became interested in the proceedings, and a career was launched. During a bodybuilding competition held in London in 1953, one of the competitors mentioned that auditions were being held for a production of "South Pacific", and Connery landed a small part as one of the Seabees chorus boys. By the time the production reached Edinburgh, he had been given the part of Marine Cpl Hamilton Steeves and was understudying two of the juvenile leads, and his salary was raised from £12 to £14–10s a week. The production returned the following year out of popular demand, and Connery was promoted to the featured role of Lieutenant Buzz Adams, which Larry Hagman had portrayed in the West End.

While in Edinburgh, Connery was targeted by the Valdor gang, one of the most violent in the city. He was first approached by them in a billiard hall where he prevented them from stealing his jacket and was later followed by six gang members to a 15-foot-high balcony at the Palais. There Connery launched an attack singlehandedly against the gang members, grabbing one by the throat and another by a biceps and cracked their heads together. From then on he was treated with great respect by the gang and gained a reputation as a "hard man".

Connery first met Michael Caine at a party during the production of "South Pacific" in 1954, and the two later became close friends. During the production of "South Pacific" at the Opera House, Manchester over the Christmas period of 1954, Connery developed a serious interest in the theatre through American actor Robert Henderson who lent him copies of the Henrik Ibsen works "Hedda Gabler", "The Wild Duck", and "When We Dead Awaken", and later listed works by the likes of Marcel Proust, Leo Tolstoy, Ivan Turgenev, George Bernard Shaw, James Joyce and William Shakespeare for him to digest. Henderson urged him to take elocution lessons and got him parts at the Maida Vale Theatre in London. He had already begun a film career, having been an extra in Herbert Wilcox's 1954 musical "Lilacs in the Spring" alongside Anna Neagle.

Although Connery had secured several roles as extras, he was struggling to make ends meet, and was forced to accept a part-time job as a babysitter for journalist Peter Noble and his actress wife Mary, which earned him 10 shillings a night. He met Hollywood actress Shelley Winters one night at Noble's house, who described Connery as "one of the tallest and most charming and masculine Scotsmen" she'd ever seen, and later spent many evenings with the Connery brothers drinking beer. Around this time Connery was residing at TV presenter Llew Gardner's house. Henderson landed Connery a role in a £6 a week Q Theatre production of Agatha Christie's "Witness for the Prosecution", during which he met and became friends with fellow-Scot Ian Bannen. This role was followed by "Point of Departure" and "A Witch in Time" at Kew, a role as Pentheus opposite Yvonne Mitchell in "The Bacchae" at the Oxford Playhouse, and a role opposite Jill Bennett in Eugene O'Neill's production of "Anna Christie".

During his time at the Oxford Theatre, Connery won a brief part as a boxer in the TV series "The Square Ring", before being spotted by Canadian director Alvin Rakoff, who gave him multiple roles in "The Condemned", shot on location in Dover in Kent. In 1956, Connery appeared in the theatrical production of "Epitaph", and played a minor role as a hoodlum in the "Ladies of the Manor" episode of the BBC Television police series "Dixon of Dock Green". This was followed by small television parts in "Sailor of Fortune" and "The Jack Benny Program".

In early 1957, Connery hired agent Richard Hatton who got him his first film role, as Spike, a minor gangster with a speech impediment in Montgomery Tully's "No Road Back" alongside Skip Homeier, Paul Carpenter, Patricia Dainton and Norman Wooland. In April 1957, Rakoff—after being disappointed by Jack Palance—decided to give the young actor his first chance in a leading role, and cast Connery as Mountain McLintock in BBC Television's production of "Requiem For a Heavyweight", which also starred Warren Mitchell and Jacqueline Hill. He then played a rogue lorry driver, Johnny Yates, in Cy Endfield's "Hell Drivers" (1957) alongside Stanley Baker, Herbert Lom, Peggy Cummins and Patrick McGoohan. Later in 1957, Connery appeared in Terence Young's poorly received MGM action picture "Action of the Tiger" opposite Van Johnson, Martine Carol, Herbert Lom and Gustavo Rojo; the film was shot on location in southern Spain. He also had a minor role in Gerald Thomas's thriller "Time Lock" (1957) as a welder, appearing alongside Robert Beatty, Lee Patterson, Betty McDowall and Vincent Winter; this commenced filming on 1 December 1956 at Beaconsfield Studios.

Connery had a major role in the melodrama "Another Time, Another Place" (1958) as a British reporter named Mark Trevor, caught in a love affair opposite Lana Turner and Barry Sullivan. During filming, star Turner's possessive gangster boyfriend, Johnny Stompanato, who was visiting from Los Angeles, believed she was having an affair with Connery. Connery and Turner had attended West End shows and London restaurants together. Stompanato stormed onto the film set and pointed a gun at Connery, only to have Connery disarm him and knock him flat on his back. Stompanato was banned from the set. Two Scotland Yard detectives advised Stompanato to leave and escorted him to the airport, where he boarded a plane back to the US. Connery later recounted that he had to lie low for a while after receiving threats from men linked to Stompanato's boss, Mickey Cohen.

In 1959, Connery landed a leading role in Robert Stevenson's Walt Disney Productions film "Darby O'Gill and the Little People" (1959) alongside Albert Sharpe, Janet Munro, and Jimmy O'Dea. The film is a tale about a wily Irishman and his battle of wits with leprechauns. Upon the film's initial release, A. H. Weiler of "The New York Times" praised the cast (save Connery whom he described as "merely tall, dark, and handsome") and thought the film an "overpoweringly charming concoction of standard Gaelic tall stories, fantasy and romance." He also had prominent television roles in Rudolph Cartier's 1961 productions of "Adventure Story" and "Anna Karenina" for BBC Television, in the latter of which he co-starred with Claire Bloom.

Connery's breakthrough came in the role of British secret agent James Bond. He was reluctant to commit to a film series, but understood that if the films succeeded, his career would greatly benefit. He played 007 in the first five Bond films: "Dr. No" (1962), "From Russia with Love" (1963), "Goldfinger" (1964), "Thunderball" (1965), and "You Only Live Twice" (1967) – then appeared again as Bond in "Diamonds Are Forever" (1971) and "Never Say Never Again" (1983). All seven films were commercially successful. James Bond, as portrayed by Connery, was selected as the third-greatest hero in cinema history by the American Film Institute.

Connery's selection for the role of James Bond owed a lot to Dana Broccoli, wife of producer Albert "Cubby" Broccoli, who is reputed to have been instrumental in persuading her husband that Connery was the right man. James Bond's creator, Ian Fleming, originally doubted Connery's casting, saying, "He's not what I envisioned of James Bond looks", and "I'm looking for Commander Bond and not an overgrown stunt-man", adding that Connery (muscular, 6' 2", and a Scot) was unrefined. Fleming's girlfriend Blanche Blackwell told him that Connery had the requisite sexual charisma, and Fleming changed his mind after the successful "Dr. No" première. He was so impressed, he wrote Connery's heritage into the character. In his 1964 novel "You Only Live Twice", Fleming wrote that Bond's father was Scottish and from Glencoe.

Connery's portrayal of Bond owes much to stylistic tutelage from director Terence Young, which helped polish the actor while using his physical grace and presence for the action. Lois Maxwell, who played Miss Moneypenny, related that "Terence took Sean under his wing. He took him to dinner, showed him how to walk, how to talk, even how to eat." The tutoring was successful; Connery received thousands of fan letters a week after "Dr. No’s" opening, and the actor became a major male sex symbol in film.

During the filming of "Thunderball" in 1965, Connery's life was in danger in the sequence with the sharks in Emilio Largo's pool. He had been concerned about this threat when he read the script. Connery insisted that Ken Adam build a special Plexiglas partition inside the pool, but this was not a fixed structure, and one of the sharks managed to pass through it. He had to abandon the pool immediately. In 2005, "From Russia with Love" was adapted by Electronic Arts into a video game, titled "", which featured all-new voice work by Connery, recorded by Terry Manning in the Bahamas, as well as his likeness, and those of several of the film's supporting cast.

Although Bond had made him a star, Connery grew tired of the role and the pressure the franchise put on him, saying "[I am] fed up to here with the whole Bond bit" and "I have always hated that damned James Bond. I'd like to kill him". Michael Caine said of the situation, "If you were his friend in these early days you didn't raise the subject of Bond. He was, and is, a much better actor than just playing James Bond, but he became synonymous with Bond. He'd be walking down the street and people would say, "Look, there's James Bond." That was particularly upsetting to him."

While making the Bond films, Connery also starred in other films such as Alfred Hitchcock's "Marnie" (1964) and "The Hill" (1965). Connery was offered the lead role in Michaelangelo Antonioni's film about "swinging London", "Blowup" (1966), but turned it down because Antonioni would not show him the complete script: only a summary that was stored in a cigarette packet.

Having played Bond six times, Connery's global popularity was such that he shared a Golden Globe Henrietta Award with Charles Bronson for "World Film Favorite – Male" in 1972. He appeared in John Huston’s "The Man Who Would Be King" (1975), starring opposite Michael Caine, with both actors regarding it as their favourite film. The same year, he appeared in "The Wind and the Lion", and in 1976 played Robin Hood in "Robin and Marian" where he starred opposite Audrey Hepburn who played Maid Marian. Film critic Roger Ebert – who had praised the double act of Connery and Caine in "The Man Who Would Be King" – praised Connery’s chemistry with Hepburn, writing, "Connery and Hepburn seem to have arrived at a tacit understanding between themselves about their characters. They glow. They really do seem in love." In the 1970s Connery was part of ensemble casts in films such as "Murder on the Orient Express" (1974) with Vanessa Redgrave and John Gielgud, and "A Bridge Too Far" (1977) co-starring Dirk Bogarde and Laurence Olivier.

In 1981, Connery appeared in the film "Time Bandits" as Agamemnon. The casting choice derives from a joke Michael Palin included in the script, in which he describes the character removing his mask as being "Sean Connery — or someone of equal but cheaper stature". When shown the script, Connery was happy to play the supporting role. In 1982, Connery narrated "G'olé!", the official film of the 1982 FIFA World Cup.
Connery agreed to reprise Bond as an ageing agent 007 in "Never Say Never Again", released in October 1983. The title, contributed by his wife, refers to his earlier statement that he would "never again" play Bond. Although the film performed well at the box office, it was plagued with production problems: strife between the director and producer, financial problems, the Fleming estate trustees' attempts to halt the film, and Connery's wrist being broken by fight choreographer, Steven Seagal. As a result of his negative experiences during filming, Connery became unhappy with the major studios and did not make any films for two years. Following the successful European production "The Name of the Rose" (1986), for which he won a BAFTA award, Connery's interest in more commercial material was revived. That same year, a supporting role in "Highlander" showcased his ability to play older mentors to younger leads, which became a recurring role in many of his later films. The following year, his acclaimed performance as a hard-nosed Irish-American cop in "The Untouchables" (1987) earned him his only Academy Award for Best Supporting Actor.

His subsequent box-office hits included "Indiana Jones and the Last Crusade" (1989), in which he played Henry Jones, Sr., the title character's father, "The Hunt for Red October" (1990) (where he was reportedly called in at two weeks' notice), "The Russia House" (1990), "The Rock" (1996), and "Entrapment" (1999). In 1996, he voiced the role of Draco the dragon in the film "Dragonheart". In 1998, Connery received a BAFTA Academy Fellowship Award.

Connery's later films included several box office and critical disappointments such as "First Knight" (1995), "Just Cause" (1995), "The Avengers" (1998), and "The League of Extraordinary Gentlemen" (2003); he received positive reviews for his performance in "Finding Forrester" (2000). He also received a Crystal Globe for outstanding artistic contribution to world cinema. In a 2003 poll conducted by Channel 4 Connery was ranked eighth on their list of the 100 Greatest Movie Stars. The failure of "The League of Extraordinary Gentlemen" was especially frustrating for Connery, who sensed during shooting that the production was "going off the rails" announced that the director, Stephen Norrington should be "locked up for insanity", and spent considerable effort in trying to salvage the film through the editing process, ultimately deciding to retire from acting rather than go through such stress ever again. 

Connery was offered the role of Gandalf in "The Lord of the Rings" series but declined it, claiming he didn't understand the script. Connery was reportedly offered $30 million along with 15 percent of the worldwide box office receipts for the role, which—had he accepted—would have earned him $450 million. Connery also turned down the opportunity to appear as the Architect in "The Matrix" trilogy for similar reasons. Connery's disillusionment with the "idiots now making films in Hollywood" was cited as a reason for his eventual decision to retire from film-making. In 2005 he recorded voiceovers for a new video game version of his Bond film "From Russia with Love". In an interview on the game disc, Connery stated that he was very happy that the producers of the game (EA Games) had approached him to voice Bond.

When Connery received the American Film Institute's Lifetime Achievement Award on 8 June 2006, he confirmed his retirement from acting. On 7 June 2007, he denied rumours that he would appear in the fourth "Indiana Jones" film, stating that "retirement is just too much damned fun". In 2010, a bronze bust sculpture of Connery was placed in Tallinn, the capital of Estonia. Connery briefly came out of retirement in 2012 by voice acting the title character in the animated movie "Sir Billi the Vet". Connery served as executive producer for an expanded 80-minute version.

During the production of "South Pacific" in the mid-1950s, Connery dated a "dark-haired beauty with a ballerina's figure", Carol Sopel, but was warned off by her Jewish family. He then dated Julie Hamilton, daughter of documentary filmmaker and feminist Jill Craigie. Given Connery's rugged appearance and rough charm, Hamilton initially thought he was an appalling person and was not attracted to him until she saw him in a kilt, declaring him to be the most beautiful thing she'd ever seen in her life. He also shared a mutual attraction with jazz singer Maxine Daniels, whom he met at the Empire Theatre. He made a pass at her, but she informed him that she was already happily married with a baby daughter. Connery was married to actress Diane Cilento from 1962 to 1973. They had a son, actor Jason Connery. In her autobiography in 2006 she alleged that he had abused her mentally and physically during their relationship; Connery had been quoted as saying that occasionally hitting a woman was "no big deal". Connery cancelled an appearance at the Scottish Parliament because of the controversy, and said he had been misquoted and that any abuse of women was unacceptable.
Connery has been married to Moroccan-French painter Micheline Roquebrune (born 1929) since 1975. A keen golfer, Connery owned the Domaine de Terre Blanche in the South of France for twenty years (from 1979) where he planned to build his dream golf course on the of land; the dream was realised when he sold it to German billionaire Dietmar Hopp in 1999. He has been awarded an honorary rank of "Shodan" (1st dan) in Kyokushin karate.

Connery was knighted by Elizabeth II at an investiture ceremony at Holyrood Palace in Edinburgh on 5 July 2000. He had been nominated for a knighthood in 1997 and 1998, but these nominations were reported to have been vetoed by Donald Dewar due to Connery's political views.
Sean Connery has a villa in Kranidi, Greece. His neighbour is Willem-Alexander of the Netherlands, with whom he shares a helicopter platform. Michael Caine (who co-starred with Connery in "The Man Who Would Be King" in 1975) is among Connery's closest friends. Connery is a keen supporter of Scottish Premiership football club Rangers F.C., having changed his allegiance from Celtic.

Connery is a member of the Scottish National Party, a centre-left political party campaigning for Scottish independence from the United Kingdom, and has supported the party financially and through personal appearances. His funding of the SNP ceased in 2001, when the UK Parliament passed legislation that prohibited overseas funding of political activities in the UK.

In response to accusations that he is a tax exile, Connery released documents in 2003 showing that he had paid £3.7 million in UK taxes between 1997/98 and 2002/03; critics pointed out that had he been continuously resident in the UK for tax purposes, his tax rate would have been far higher. In the run-up to the 2014 Scottish independence referendum, Connery's brother Neil said that Connery would not come to Scotland to rally independence supporters since his tax exile status greatly limited the number of days he could spend in the country.

After Connery sold his Marbella villa in 1999, Spanish authorities launched an investigation into alleged tax evasion by him and his wife, alleging that the Spanish treasury had been defrauded of £5.5 million. Connery was subsequently cleared by the Spanish officials but his wife and 16 others were charged with attempting to defraud the Spanish treasury.




</doc>
<doc id="26714" url="https://en.wikipedia.org/wiki?curid=26714" title="Sculpture">
Sculpture

Sculpture is the branch of the visual arts that operates in three dimensions. It is one of the plastic arts. Durable sculptural processes originally used carving (the removal of material) and modelling (the addition of material, as clay), in stone, metal, ceramics, wood and other materials but, since Modernism, there has been an almost complete freedom of materials and process. A wide variety of materials may be worked by removal such as carving, assembled by welding or modelling, or molded or cast.

Sculpture in stone survives far better than works of art in perishable materials, and often represents the majority of the surviving works (other than pottery) from ancient cultures, though conversely traditions of sculpture in wood may have vanished almost entirely. However, most ancient sculpture was brightly painted, and this has been lost.

Sculpture has been central in religious devotion in many cultures, and until recent centuries large sculptures, too expensive for private individuals to create, were usually an expression of religion or politics. Those cultures whose sculptures have survived in quantities include the cultures of the ancient Mediterranean, India and China, as well as many in Central and South America and Africa.

The Western tradition of sculpture began in ancient Greece, and Greece is widely seen as producing great masterpieces in the classical period. During the Middle Ages, Gothic sculpture represented the agonies and passions of the Christian faith. The revival of classical models in the Renaissance produced famous sculptures such as Michelangelo's "David". Modernist sculpture moved away from traditional processes and the emphasis on the depiction of the human body, with the making of constructed sculpture, and the presentation of found objects as finished art works.

A basic distinction is between sculpture in the round, free-standing sculpture, such as statues, not attached (except possibly at the base) to any other surface, and the various types of relief, which are at least partly attached to a background surface. Relief is often classified by the degree of projection from the wall into low or bas-relief, high relief, and sometimes an intermediate mid-relief. Sunk-relief is a technique restricted to ancient Egypt. Relief is the usual sculptural medium for large figure groups and narrative subjects, which are difficult to accomplish in the round, and is the typical technique used both for architectural sculpture, which is attached to buildings, and for small-scale sculpture decorating other objects, as in much pottery, metalwork and jewellery. Relief sculpture may also decorate steles, upright slabs, usually of stone, often also containing inscriptions.

Another basic distinction is between subtractive carving techniques, which remove material from an existing block or lump, for example of stone or wood, and modelling techniques which shape or build up the work from the material. Techniques such as casting, stamping and moulding use an intermediate matrix containing the design to produce the work; many of these allow the production of several copies.
The term "sculpture" is often used mainly to describe large works, which are sometimes called monumental sculpture, meaning either or both of sculpture that is large, or that is attached to a building. But the term properly covers many types of small works in three dimensions using the same techniques, including coins and medals, hardstone carvings, a term for small carvings in stone that can take detailed work.

The very large or "colossal" statue has had an enduring appeal since antiquity; the largest on record at is the 2018 Indian Statue of Unity. Another grand form of portrait sculpture is the equestrian statue of a rider on horse, which has become rare in recent decades. The smallest forms of life-size portrait sculpture are the "head", showing just that, or the bust, a representation of a person from the chest up. Small forms of sculpture include the figurine, normally a statue that is no more than tall, and for reliefs the plaquette, medal or coin.

Modern and contemporary art have added a number of non-traditional forms of sculpture, including sound sculpture, light sculpture, environmental art, environmental sculpture, street art sculpture, kinetic sculpture (involving aspects of physical motion), land art, and site-specific art. Sculpture is an important form of public art. A collection of sculpture in a garden setting can be called a sculpture garden.

One of the most common purposes of sculpture is in some form of association with religion. Cult images are common in many cultures, though they are often not the colossal statues of deities which characterized ancient Greek art, like the Statue of Zeus at Olympia. The actual cult images in the innermost sanctuaries of Egyptian temples, of which none have survived, were evidently rather small, even in the largest temples. The same is often true in Hinduism, where the very simple and ancient form of the lingam is the most common. Buddhism brought the sculpture of religious figures to East Asia, where there seems to have been no earlier equivalent tradition, though again simple shapes like the "bi" and "cong" probably had religious significance.

Small sculptures as personal possessions go back to the earliest prehistoric art, and the use of very large sculpture as public art, especially to impress the viewer with the power of a ruler, goes back at least to the Great Sphinx of some 4,500 years ago. In archaeology and art history the appearance, and sometimes disappearance, of large or monumental sculpture in a culture is regarded as of great significance, though tracing the emergence is often complicated by the presumed existence of sculpture in wood and other perishable materials of which no record remains; the totem pole is an example of a tradition of monumental sculpture in wood that would leave no traces for archaeology. The ability to summon the resources to create monumental sculpture, by transporting usually very heavy materials and arranging for the payment of what are usually regarded as full-time sculptors, is considered a mark of a relatively advanced culture in terms of social organization. Recent unexpected discoveries of ancient Chinese bronze age figures at Sanxingdui, some more than twice human size, have disturbed many ideas held about early Chinese civilization, since only much smaller bronzes were previously known. Some undoubtedly advanced cultures, such as the Indus Valley civilization, appear to have had no monumental sculpture at all, though producing very sophisticated figurines and seals. The Mississippian culture seems to have been progressing towards its use, with small stone figures, when it collapsed. Other cultures, such as ancient Egypt and the Easter Island culture, seem to have devoted enormous resources to very large-scale monumental sculpture from a very early stage.
The collecting of sculpture, including that of earlier periods, goes back some 2,000 years in Greece, China and Mesoamerica, and many collections were available on semi-public display long before the modern museum was invented. From the 20th century the relatively restricted range of subjects found in large sculpture expanded greatly, with abstract subjects and the use or representation of any type of subject now common. Today much sculpture is made for intermittent display in galleries and museums, and the ability to transport and store the increasingly large works is a factor in their construction. Small decorative figurines, most often in ceramics, are as popular today (though strangely neglected by modern and Contemporary art) as they were in the Rococo, or in ancient Greece when Tanagra figurines were a major industry, or in East Asian and Pre-Columbian art. Small sculpted fittings for furniture and other objects go well back into antiquity, as in the Nimrud ivories, Begram ivories and finds from the tomb of Tutankhamun.

Portrait sculpture began in Egypt, where the Narmer Palette shows a ruler of the 32nd century BCE, and Mesopotamia, where we have 27 surviving statues of Gudea, who ruled Lagash c. 2144–2124 BCE. In ancient Greece and Rome, the erection of a portrait statue in a public place was almost the highest mark of honour, and the ambition of the elite, who might also be depicted on a coin. In other cultures such as Egypt and the Near East public statues were almost exclusively the preserve of the ruler, with other wealthy people only being portrayed in their tombs. Rulers are typically the only people given portraits in Pre-Columbian cultures, beginning with the Olmec colossal heads of about 3,000 years ago. East Asian portrait sculpture was entirely religious, with leading clergy being commemorated with statues, especially the founders of monasteries, but not rulers, or ancestors. The Mediterranean tradition revived, initially only for tomb effigies and coins, in the Middle Ages, but expanded greatly in the Renaissance, which invented new forms such as the personal portrait medal.

Animals are, with the human figure, the earliest subject for sculpture, and have always been popular, sometimes realistic, but often imaginary monsters; in China animals and monsters are almost the only traditional subjects for stone sculpture outside tombs and temples. The kingdom of plants is important only in jewellery and decorative reliefs, but these form almost all the large sculpture of Byzantine art and Islamic art, and are very important in most Eurasian traditions, where motifs such as the palmette and vine scroll have passed east and west for over two millennia.

One form of sculpture found in many prehistoric cultures around the world is specially enlarged versions of ordinary tools, weapons or vessels created in impractical precious materials, for either some form of ceremonial use or display or as offerings. Jade or other types of greenstone were used in China, Olmec Mexico, and Neolithic Europe, and in early Mesopotamia large pottery shapes were produced in stone. Bronze was used in Europe and China for large axes and blades, like the Oxborough Dirk.

The materials used in sculpture are diverse, changing throughout history. The classic materials, with outstanding durability, are metal, especially bronze, stone and pottery, with wood, bone and antler less durable but cheaper options. Precious materials such as gold, silver, jade, and ivory are often used for small luxury works, and sometimes in larger ones, as in chryselephantine statues. More common and less expensive materials were used for sculpture for wider consumption, including hardwoods (such as oak, box/boxwood, and lime/linden); terracotta and other ceramics, wax (a very common material for models for casting, and receiving the impressions of cylinder seals and engraved gems), and cast metals such as pewter and zinc (spelter). But a vast number of other materials have been used as part of sculptures, in ethnographic and ancient works as much as modern ones.

Sculptures are often painted, but commonly lose their paint to time, or restorers. Many different painting techniques have been used in making sculpture, including tempera, oil painting, gilding, house paint, aerosol, enamel and sandblasting.

Many sculptors seek new ways and materials to make art. One of Pablo Picasso's most famous sculptures included bicycle parts. Alexander Calder and other modernists made spectacular use of painted steel. Since the 1960s, acrylics and other plastics have been used as well. Andy Goldsworthy makes his unusually ephemeral sculptures from almost entirely natural materials in natural settings. Some sculpture, such as ice sculpture, sand sculpture, and gas sculpture, is deliberately short-lived. Recent sculptors have used stained glass, tools, machine parts, hardware and consumer packaging to fashion their works. Sculptors sometimes use found objects, and Chinese scholar's rocks have been appreciated for many centuries.

Stone sculpture is an ancient activity where pieces of rough natural stone are shaped by the controlled removal of stone. Owing to the permanence of the material, evidence can be found that even the earliest societies indulged in some form of stone work, though not all areas of the world have such abundance of good stone for carving as Egypt, Greece, India and most of Europe. Petroglyphs (also called rock engravings) are perhaps the earliest form: images created by removing part of a rock surface which remains "in situ", by incising, pecking, carving, and abrading. Monumental sculpture covers large works, and architectural sculpture, which is attached to buildings. Hardstone carving is the carving for artistic purposes of semi-precious stones such as jade, agate, onyx, rock crystal, sard or carnelian, and a general term for an object made in this way. Alabaster or mineral gypsum is a soft mineral that is easy to carve for smaller works and still relatively durable. Engraved gems are small carved gems, including cameos, originally used as seal rings.

The copying of an original statue in stone, which was very important for ancient Greek statues, which are nearly all known from copies, was traditionally achieved by "pointing", along with more freehand methods. Pointing involved setting up a grid of string squares on a wooden frame surrounding the original, and then measuring the position on the grid and the distance between grid and statue of a series of individual points, and then using this information to carve into the block from which the copy is made.

Bronze and related copper alloys are the oldest and still the most popular metals for cast metal sculptures; a cast bronze sculpture is often called simply a "bronze". Common bronze alloys have the unusual and desirable property of expanding slightly just before they set, thus filling the finest details of a mold. Their strength and lack of brittleness (ductility) is an advantage when figures in action are to be created, especially when compared to various ceramic or stone materials (see marble sculpture for several examples). Gold is the softest and most precious metal, and very important in jewellery; with silver it is soft enough to be worked with hammers and other tools as well as cast; repoussé and chasing are among the techniques used in gold and silversmithing.

Casting is a group of manufacturing processes by which a liquid material (bronze, copper, glass, aluminum, iron) is (usually) poured into a mold, which contains a hollow cavity of the desired shape, and then allowed to solidify. The solid casting is then ejected or broken out to complete the process, although a final stage of "cold work" may follow on the finished cast. Casting may be used to form hot liquid metals or various materials that "cold set" after mixing of components (such as epoxies, concrete, plaster and clay). Casting is most often used for making complex shapes that would be otherwise difficult or uneconomical to make by other methods. The oldest surviving casting is a copper Mesopotamian frog from 3200 BCE. Specific techniques include lost-wax casting, plaster mold casting and sand casting.

Welding is a process where different pieces of metal are fused together to create different shapes and designs. There are many different forms of welding, such as Oxy-fuel welding, Stick welding, MIG welding, and TIG welding. Oxy-fuel is probably the most common method of welding when it comes to creating steel sculptures because it is the easiest to use for shaping the steel as well as making clean and less noticeable joins of the steel. The key to Oxy-fuel welding is heating each piece of metal to be joined evenly until all are red and have a shine to them. Once that shine is on each piece, that shine will soon become a 'pool' where the metal is liquified and the welder must get the pools to join together, fusing the metal. Once cooled off, the location where the pools joined are now one continuous piece of metal. Also used heavily in Oxy-fuel sculpture creation is forging. Forging is the process of heating metal to a certain point to soften it enough to be shaped into different forms. One very common example is heating the end of a steel rod and hitting the red heated tip with a hammer while on an anvil to form a point. In between hammer swings, the forger rotates the rod and gradually forms a sharpened point from the blunt end of a steel rod.

Glass may be used for sculpture through a wide range of working techniques, though the use of it for large works is a recent development. It can be carved, with considerable difficulty; the Roman Lycurgus Cup is all but unique. Hot casting can be done by ladling molten glass into molds that have been created by pressing shapes into sand, carved graphite or detailed plaster/silica molds. Kiln casting glass involves heating chunks of glass in a kiln until they are liquid and flow into a waiting mold below it in the kiln. Glass can also be blown and/or hot sculpted with hand tools either as a solid mass or as part of a blown object. More recent techniques involve chiseling and bonding plate glass with polymer silicates and UV light.

Pottery is one of the oldest materials for sculpture, as well as clay being the medium in which many sculptures cast in metal are originally modelled for casting. Sculptors often build small preliminary works called maquettes of ephemeral materials such as plaster of Paris, wax, unfired clay, or plasticine. Many cultures have produced pottery which combines a function as a vessel with a sculptural form, and small figurines have often been as popular as they are in modern Western culture. Stamps and moulds were used by most ancient civilizations, from ancient Rome and Mesopotamia to China.

Wood carving has been extremely widely practiced, but survives much less well than the other main materials, being vulnerable to decay, insect damage, and fire. It therefore forms an important hidden element in the art history of many cultures. Outdoor wood sculpture does not last long in most parts of the world, so that we have little idea how the totem pole tradition developed. Many of the most important sculptures of China and Japan in particular are in wood, and the great majority of African sculpture and that of Oceania and other regions.

Wood is light, so suitable for masks and other sculpture intended to be carried, and can take very fine detail. It is also much easier to work than stone. It has been very often painted after carving, but the paint wears less well than the wood, and is often missing in surviving pieces. Painted wood is often technically described as "wood and polychrome". Typically a layer of gesso or plaster is applied to the wood, and then the paint is applied to that.

Worldwide, sculptors have usually been tradesmen whose work is unsigned; in some traditions, for example China, where sculpture did not share the prestige of literati painting, this has affected the status of sculpture itself. Even in ancient Greece, where sculptors such as Phidias became famous, they appear to have retained much the same social status as other artisans, and perhaps not much greater financial rewards, although some signed their works. In the Middle Ages artists such as the 12th-century Gislebertus sometimes signed their work, and were sought after by different cities, especially from the Trecento onwards in Italy, with figures such as Arnolfo di Cambio, and Nicola Pisano and his son Giovanni. Goldsmiths and jewellers, dealing with precious materials and often doubling as bankers, belonged to powerful guilds and had considerable status, often holding civic office. Many sculptors also practised in other arts; Andrea del Verrocchio also painted, and Giovanni Pisano, Michelangelo, and Jacopo Sansovino were architects. Some sculptors maintained large workshops. Even in the Renaissance the physical nature of the work was perceived by Leonardo da Vinci and others as pulling down the status of sculpture in the arts, though the reputation of Michelangelo perhaps put this long-held idea to rest.

From the High Renaissance artists such as Michelangelo, Leone Leoni and Giambologna could become wealthy, and ennobled, and enter the circle of princes, after a period of sharp argument over the relative status of sculpture and painting. Much decorative sculpture on buildings remained a trade, but sculptors producing individual pieces were recognised on a level with painters. From the 18th century or earlier sculpture also attracted middle-class students, although it was slower to do so than painting. Women sculptors took longer to appear than women painters, and were less prominent until the 20th century.

Aniconism remained restricted to Judaism, which did not accept figurative sculpture until the 19th century, before expanding to Early Christianity, which initially accepted large sculptures. In Christianity and Buddhism, sculpture became very significant. Christian Eastern Orthodoxy has never accepted monumental sculpture, and Islam has consistently rejected nearly all figurative sculpture, except for very small figures in reliefs and some animal figures that fulfill a useful function, like the famous lions supporting a fountain in the Alhambra. Many forms of Protestantism also do not approve of religious sculpture. There has been much iconoclasm of sculpture from religious motives, from the Early Christians, the Beeldenstorm of the Protestant Reformation to the 2001 destruction of the Buddhas of Bamyan by the Taliban.

The earliest undisputed examples of sculpture belong to the Aurignacian culture, which was located in Europe and southwest Asia and active at the beginning of the Upper Paleolithic. As well as producing some of the earliest known cave art, the people of this culture developed finely-crafted stone tools, manufacturing pendants, bracelets, ivory beads, and bone-flutes, as well as three-dimensional figurines.

The 30 cm tall Löwenmensch found in the Hohlenstein Stadel area of Germany is an anthropomorphic lion-man figure carved from woolly mammoth ivory. It has been dated to about 35–40,000BP, making it, along with the Venus of Hohle Fels, the oldest known uncontested example of figurative art.

Much surviving prehistoric art is small portable sculptures, with a small group of female Venus figurines such as the Venus of Willendorf (24–26,000BP) found across central Europe. The Swimming Reindeer of about 13,000 years ago is one of the finest of a number of Magdalenian carvings in bone or antler of animals in the art of the Upper Paleolithic, although they are outnumbered by engraved pieces, which are sometimes classified as sculpture. Two of the largest prehistoric sculptures can be found at the Tuc d'Audobert caves in France, where around 12–17,000 years ago a masterful sculptor used a spatula-like stone tool and fingers to model a pair of large bison in clay against a limestone rock.

With the beginning of the Mesolithic in Europe figurative sculpture greatly reduced, and remained a less common element in art than relief decoration of practical objects until the Roman period, despite some works such as the Gundestrup cauldron from the European Iron Age and the Bronze Age Trundholm sun chariot.

From the ancient Near East, the over-life sized stone Urfa Man from modern Turkey comes from about 9,000 BCE, and the 'Ain Ghazal Statues from around 7200 and 6500 BCE. These are from modern Jordan, made of lime plaster and reeds, and about half life-size; there are 15 statues, some with two heads side by side, and 15 busts. Small clay figures of people and animals are found at many sites across the Near East from the Pre-Pottery Neolithic, and represent the start of a more-or-less continuous tradition in the region. 

The Protoliterate period in Mesopotamia, dominated by Uruk, saw the production of sophisticated works like the Warka Vase and cylinder seals. The Guennol Lioness is an outstanding small limestone figure from Elam of about 3000–2800 BCE, part human and part lioness. A little later there are a number of figures of large-eyed priests and worshippers, mostly in alabaster and up to a foot high, who attended temple cult images of the deity, but very few of these have survived. Sculptures from the Sumerian and Akkadian period generally had large, staring eyes, and long beards on the men. Many masterpieces have also been found at the Royal Cemetery at Ur (c. 2650 BCE), including the two figures of a "Ram in a Thicket", the "Copper Bull" and a bull's head on one of the Lyres of Ur.

From the many subsequent periods before the ascendency of the Neo-Assyrian Empire in the 10th century BCE Mesopotamian art survives in a number of forms: cylinder seals, relatively small figures in the round, and reliefs of various sizes, including cheap plaques of moulded pottery for the home, some religious and some apparently not. The Burney Relief is an unusually elaborate and relatively large (20 x 15 inches, 50 x 37 cm) terracotta plaque of a naked winged goddess with the feet of a bird of prey, and attendant owls and lions. It comes from the 18th or 19th centuries BCE, and may also be moulded. Stone stelae, votive offerings, or ones probably commemorating victories and showing feasts, are also found from temples, which unlike more official ones lack inscriptions that would explain them; the fragmentary Stele of the Vultures is an early example of the inscribed type, and the Assyrian Black Obelisk of Shalmaneser III a large and solid late one.

The conquest of the whole of Mesopotamia and much surrounding territory by the Assyrians created a larger and wealthier state than the region had known before, and very grandiose art in palaces and public places, no doubt partly intended to match the splendour of the art of the neighbouring Egyptian empire. Unlike earlier states, the Assyrians could use easily carved stone from northern Iraq, and did so in great quantity. The Assyrians developed a style of extremely large schemes of very finely detailed narrative low reliefs in stone for palaces, with scenes of war or hunting; the British Museum has an outstanding collection, including the "Lion Hunt of Ashurbanipal" and the Lachish reliefs showing a campaign. They produced very little sculpture in the round, except for colossal guardian figures of the human-headed lamassu, which are sculpted in high relief on two sides of a rectangular block, with the heads effectively in the round (and also five legs, so that both views seem complete). Even before dominating the region they had continued the cylinder seal tradition with designs which are often exceptionally energetic and refined.

The monumental sculpture of ancient Egypt is world-famous, but refined and delicate small works exist in much greater numbers. The Egyptians used the distinctive technique of sunk relief, which is well suited to very bright sunlight. The main figures in reliefs adhere to the same figure convention as in painting, with parted legs (where not seated) and head shown from the side, but the torso from the front, and a standard set of proportions making up the figure, using 18 "fists" to go from the ground to the hair-line on the forehead. This appears as early as the Narmer Palette from Dynasty I. However, there as elsewhere the convention is not used for minor figures shown engaged in some activity, such as the captives and corpses. Other conventions make statues of males darker than females ones. Very conventionalized portrait statues appear from as early as Dynasty II, before 2,780 BCE, and with the exception of the art of the Amarna period of Ahkenaten, and some other periods such as Dynasty XII, the idealized features of rulers, like other Egyptian artistic conventions, changed little until after the Greek conquest.

Egyptian pharaohs were always regarded as deities, but other deities are much less common in large statues, except when they represent the pharaoh "as" another deity; however the other deities are frequently shown in paintings and reliefs. The famous row of four colossal statues outside the main temple at Abu Simbel each show Rameses II, a typical scheme, though here exceptionally large. Small figures of deities, or their animal personifications, are very common, and found in popular materials such as pottery. Most larger sculpture survives from Egyptian temples or tombs; by Dynasty IV (2680–2565 BCE) at the latest the idea of the Ka statue was firmly established. These were put in tombs as a resting place for the "ka" portion of the soul, and so we have a good number of less conventionalized statues of well-off administrators and their wives, many in wood as Egypt is one of the few places in the world where the climate allows wood to survive over millennia. The so-called reserve heads, plain hairless heads, are especially naturalistic. Early tombs also contained small models of the slaves, animals, buildings and objects such as boats necessary for the deceased to continue his lifestyle in the afterworld, and later "Ushabti" figures.

The first distinctive style of ancient Greek sculpture developed in the Early Bronze Age Cycladic period (3rd millennium BCE), where marble figures, usually female and small, are represented in an elegantly simplified geometrical style. Most typical is a standing pose with arms crossed in front, but other figures are shown in different poses, including a complicated figure of a harpist seated on a chair.

The subsequent Minoan and Mycenaean cultures developed sculpture further, under influence from Syria and elsewhere, but it is in the later Archaic period from around 650 BCE that the kouros developed. These are large standing statues of naked youths, found in temples and tombs, with the kore as the clothed female equivalent, with elaborately dressed hair; both have the "archaic smile". They seem to have served a number of functions, perhaps sometimes representing deities and sometimes the person buried in a grave, as with the Kroisos Kouros. They are clearly influenced by Egyptian and Syrian styles, but the Greek artists were much more ready to experiment within the style.

During the 6th century Greek sculpture developed rapidly, becoming more naturalistic, and with much more active and varied figure poses in narrative scenes, though still within idealized conventions. Sculptured pediments were added to temples, including the Parthenon in Athens, where the remains of the pediment of around 520 using figures in the round were fortunately used as infill for new buildings after the Persian sack in 480 BCE, and recovered from the 1880s on in fresh unweathered condition. Other significant remains of architectural sculpture come from Paestum in Italy, Corfu, Delphi and the Temple of Aphaea in Aegina (much now in Munich). Most Greek sculpture originally included at least some colour; the Ny Carlsberg Glyptotek Museum in Copenhagen, Denmark, has done extensive research and recreation of the original colours.

There are fewer original remains from the first phase of the Classical period, often called the Severe style; free-standing statues were now mostly made in bronze, which always had value as scrap. The Severe style lasted from around 500 in reliefs, and soon after 480 in statues, to about 450. The relatively rigid poses of figures relaxed, and asymmetrical turning positions and oblique views became common, and deliberately sought. This was combined with a better understanding of anatomy and the harmonious structure of sculpted figures, and the pursuit of naturalistic representation as an aim, which had not been present before. Excavations at the Temple of Zeus, Olympia since 1829 have revealed the largest group of remains, from about 460, of which many are in the Louvre.

The "High Classical" period lasted only a few decades from about 450 to 400, but has had a momentous influence on art, and retains a special prestige, despite a very restricted number of original survivals. The best known works are the Parthenon Marbles, traditionally (since Plutarch) executed by a team led by the most famous ancient Greek sculptor Phidias, active from about 465–425, who was in his own day more famous for his colossal chryselephantine Statue of Zeus at Olympia (c. 432), one of the Seven Wonders of the Ancient World, his "Athena Parthenos" (438), the cult image of the Parthenon, and "Athena Promachos", a colossal bronze figure that stood next to the Parthenon; all of these are lost but are known from many representations. He is also credited as the creator of some life-size bronze statues known only from later copies whose identification is controversial, including the "Ludovisi Hermes".

The High Classical style continued to develop realism and sophistication in the human figure, and improved the depiction of drapery (clothes), using it to add to the impact of active poses. Facial expressions were usually very restrained, even in combat scenes. The composition of groups of figures in reliefs and on pediments combined complexity and harmony in a way that had a permanent influence on Western art. Relief could be very high indeed, as in the Parthenon illustration below, where most of the leg of the warrior is completely detached from the background, as were the missing parts; relief this high made sculptures more subject to damage. The Late Classical style developed the free-standing female nude statue, supposedly an innovation of Praxiteles, and developed increasingly complex and subtle poses that were interesting when viewed from a number of angles, as well as more expressive faces; both trends were to be taken much further in the Hellenistic period.

The Hellenistic period is conventionally dated from the death of Alexander the Great in 323 BCE, and ending either with the final conquest of the Greek heartlands by Rome in 146 BCE or with the final defeat of the last remaining successor-state to Alexander's empire after the Battle of Actium in 31 BCE, which also marks the end of Republican Rome. It is thus much longer than the previous periods, and includes at least two major phases: a "Pergamene" style of experimentation, exuberance and some sentimentality and vulgarity, and in the 2nd century BCE a classicising return to a more austere simplicity and elegance; beyond such generalizations dating is typically very uncertain, especially when only later copies are known, as is usually the case. The initial Pergamene style was not especially associated with Pergamon, from which it takes its name, but the very wealthy kings of that state were among the first to collect and also copy Classical sculpture, and also commissioned much new work, including the famous Pergamon Altar whose sculpture is now mostly in Berlin and which exemplifies the new style, as do the Mausoleum at Halicarnassus (another of the Seven Wonders), the famous "Laocoön and his Sons" in the Vatican Museums, a late example, and the bronze original of "The Dying Gaul" (illustrated at top), which we know was part of a group actually commissioned for Pergamon in about 228 BCE, from which the Ludovisi Gaul was also a copy. The group called the Farnese Bull, possibly a 2nd-century marble original, is still larger and more complex,
Hellenistic sculpture greatly expanded the range of subjects represented, partly as a result of greater general prosperity, and the emergence of a very wealthy class who had large houses decorated with sculpture, although we know that some examples of subjects that seem best suited to the home, such as children with animals, were in fact placed in temples or other public places. For a much more popular home decoration market there were Tanagra figurines, and those from other centres where small pottery figures were produced on an industrial scale, some religious but others showing animals and elegantly dressed ladies. Sculptors became more technically skilled in representing facial expressions conveying a wide variety of emotions and the portraiture of individuals, as well representing different ages and races. The reliefs from the Mausoleum are rather atypical in that respect; most work was free-standing, and group compositions with several figures to be seen in the round, like the "Laocoon" and the Pergamon group celebrating victory over the Gauls became popular, having been rare before. The Barberini Faun, showing a satyr sprawled asleep, presumably after drink, is an example of the moral relaxation of the period, and the readiness to create large and expensive sculptures of subjects that fall short of the heroic.

After the conquests of Alexander Hellenistic culture was dominant in the courts of most of the Near East, and some of Central Asia, and increasingly being adopted by European elites, especially in Italy, where Greek colonies initially controlled most of the South. Hellenistic art, and artists, spread very widely, and was especially influential in the expanding Roman Republic and when it encountered Buddhism in the easternmost extensions of the Hellenistic area. The massive so-called Alexander Sarcophagus found in Sidon in modern Lebanon, was probably made there at the start of the period by expatriate Greek artists for a Hellenized Persian governor. The wealth of the period led to a greatly increased production of luxury forms of small sculpture, including engraved gems and cameos, jewellery, and gold and silverware.

Early Roman art was influenced by the art of Greece and that of the neighbouring Etruscans, themselves greatly influenced by their Greek trading partners. An Etruscan speciality was near life size tomb effigies in terracotta, usually lying on top of a sarcophagus lid propped up on one elbow in the pose of a diner in that period. As the expanding Roman Republic began to conquer Greek territory, at first in Southern Italy and then the entire Hellenistic world except for the Parthian far east, official and patrician sculpture became largely an extension of the Hellenistic style, from which specifically Roman elements are hard to disentangle, especially as so much Greek sculpture survives only in copies of the Roman period. By the 2nd century BCE, "most of the sculptors working at Rome" were Greek, often enslaved in conquests such as that of Corinth (146 BCE), and sculptors continued to be mostly Greeks, often slaves, whose names are very rarely recorded. Vast numbers of Greek statues were imported to Rome, whether as booty or the result of extortion or commerce, and temples were often decorated with re-used Greek works.

A native Italian style can be seen in the tomb monuments, which very often featured portrait busts, of prosperous middle-class Romans, and portraiture is arguably the main strength of Roman sculpture. There are no survivals from the tradition of masks of ancestors that were worn in processions at the funerals of the great families and otherwise displayed in the home, but many of the busts that survive must represent ancestral figures, perhaps from the large family tombs like the Tomb of the Scipios or the later mausolea outside the city. The famous bronze head supposedly of Lucius Junius Brutus is very variously dated, but taken as a very rare survival of Italic style under the Republic, in the preferred medium of bronze. Similarly stern and forceful heads are seen on coins of the Late Republic, and in the Imperial period coins as well as busts sent around the Empire to be placed in the basilicas of provincial cities were the main visual form of imperial propaganda; even Londinium had a near-colossal statue of Nero, though far smaller than the 30-metre-high Colossus of Nero in Rome, now lost.
The Romans did not generally attempt to compete with free-standing Greek works of heroic exploits from history or mythology, but from early on produced historical works in relief, culminating in the great Roman triumphal columns with continuous narrative reliefs winding around them, of which those commemorating Trajan (CE 113) and Marcus Aurelius (by 193) survive in Rome, where the Ara Pacis ("Altar of Peace", 13 BCE) represents the official Greco-Roman style at its most classical and refined. Among other major examples are the earlier re-used reliefs on the Arch of Constantine and the base of the Column of Antoninus Pius (161), Campana reliefs were cheaper pottery versions of marble reliefs and the taste for relief was from the imperial period expanded to the sarcophagus. All forms of luxury small sculpture continued to be patronized, and quality could be extremely high, as in the silver Warren Cup, glass Lycurgus Cup, and large cameos like the Gemma Augustea, Gonzaga Cameo and the "Great Cameo of France". For a much wider section of the population, moulded relief decoration of pottery vessels and small figurines were produced in great quantity and often considerable quality.

After moving through a late 2nd-century "baroque" phase, in the 3rd century, Roman art largely abandoned, or simply became unable to produce, sculpture in the classical tradition, a change whose causes remain much discussed. Even the most important imperial monuments now showed stumpy, large-eyed figures in a harsh frontal style, in simple compositions emphasizing power at the expense of grace. The contrast is famously illustrated in the Arch of Constantine of 315 in Rome, which combines sections in the new style with roundels in the earlier full Greco-Roman style taken from elsewhere, and the "Four Tetrarchs" (c. 305) from the new capital of Constantinople, now in Venice. Ernst Kitzinger found in both monuments the same "stubby proportions, angular movements, an ordering of parts through symmetry and repetition and a rendering of features and drapery folds through incisions rather than modelling... The hallmark of the style wherever it appears consists of an emphatic hardness, heaviness and angularity—in short, an almost complete rejection of the classical tradition".

This revolution in style shortly preceded the period in which Christianity was adopted by the Roman state and the great majority of the people, leading to the end of large religious sculpture, with large statues now only used for emperors. However, rich Christians continued to commission reliefs for sarcophagi, as in the Sarcophagus of Junius Bassus, and very small sculpture, especially in ivory, was continued by Christians, building on the style of the consular diptych.

The Early Christians were opposed to monumental religious sculpture, though continuing Roman traditions in portrait busts and sarcophagus reliefs, as well as smaller objects such as the consular diptych. Such objects, often in valuable materials, were also the main sculptural traditions (as far as is known) of the barbaric civilizations of the Migration period, as seen in the objects found in the 6th-century burial treasure at Sutton Hoo, and the jewellery of Scythian art and the hybrid Christian and animal style productions of Insular art. Following the continuing Byzantine tradition, Carolingian art revived ivory carving, often in panels for the treasure bindings of grand illuminated manuscripts, as well as crozier heads and other small fittings.

Byzantine art, though producing superb ivory reliefs and architectural decorative carving, never returned to monumental sculpture, or even much small sculpture in the round. However, in the West during the Carolingian and Ottonian periods there was the beginnings of a production of monumental statues, in courts and major churches. This gradually spread; by the late 10th and 11th century there are records of several apparently life-size sculptures in Anglo-Saxon churches, probably of precious metal around a wooden frame, like the Golden Madonna of Essen. No Anglo-Saxon example has survived, and survivals of large non-architectural sculpture from before 1,000 are exceptionally rare. Much the finest is the Gero Cross, of 965–970, which is a crucifix, which was evidently the commonest type of sculpture; Charlemagne had set one up in the Palatine Chapel in Aachen around 800. These continued to grow in popularity, especially in Germany and Italy. The rune stones of the Nordic world, the Pictish stones of Scotland and possibly the high cross reliefs of Christian Great Britain, were northern sculptural traditions that bridged the period of Christianization.

From about 1000 there was a general rebirth of artistic production in all Europe, led by general economic growth in production and commerce, and the new style of Romanesque art was the first medieval style to be used in the whole of Western Europe. The new cathedrals and pilgrim's churches were increasingly decorated with architectural stone reliefs, and new focuses for sculpture developed, such as the tympanum over church doors in the 12th century, and the inhabited capital with figures and often narrative scenes. Outstanding abbey churches with sculpture include in France Vézelay and Moissac and in Spain Silos.

Romanesque art was characterised by a very vigorous style in both sculpture and painting. The capitals of columns were never more exciting than in this period, when they were often carved with complete scenes with several figures. The large wooden crucifix was a German innovation right at the start of the period, as were free-standing statues of the enthroned Madonna, but the high relief was above all the sculptural mode of the period. Compositions usually had little depth, and needed to be flexible to squeeze themselves into the shapes of capitals, and church typanums; the tension between a tightly enclosing frame, from which the composition sometimes escapes, is a recurrent theme in Romanesque art. Figures still often varied in size in relation to their importance portraiture hardly existed.

Objects in precious materials such as ivory and metal had a very high status in the period, much more so than monumental sculpture — we know the names of more makers of these than painters, illuminators or architect-masons. Metalwork, including decoration in enamel, became very sophisticated, and many spectacular shrines made to hold relics have survived, of which the best known is the Shrine of the Three Kings at Cologne Cathedral by Nicholas of Verdun. The bronze Gloucester candlestick and the brass font of 1108–17 now in Liège are superb examples, very different in style, of metal casting, the former highly intricate and energetic, drawing on manuscript painting, while the font shows the Mosan style at its most classical and majestic. The bronze doors, a triumphal column and other fittings at Hildesheim Cathedral, the Gniezno Doors, and the doors of the Basilica di San Zeno in Verona are other substantial survivals. The aquamanile, a container for water to wash with, appears to have been introduced to Europe in the 11th century, and often took fantastic zoomorphic forms; surviving examples are mostly in brass. Many wax impressions from impressive seals survive on charters and documents, although Romanesque coins are generally not of great aesthetic interest.

The Cloisters Cross is an unusually large ivory crucifix, with complex carving including many figures of prophets and others, which has been attributed to one of the relatively few artists whose name is known, Master Hugo, who also illuminated manuscripts. Like many pieces it was originally partly coloured. The Lewis chessmen are well-preserved examples of small ivories, of which many pieces or fragments remain from croziers, plaques, pectoral crosses and similar objects.

The Gothic period is essentially defined by Gothic architecture, and does not entirely fit with the development of style in sculpture in either its start or finish. The facades of large churches, especially around doors, continued to have large typanums, but also rows of sculpted figures spreading around them. The statues on the Western (Royal) Portal at Chartres Cathedral (c. 1145) show an elegant but exaggerated columnar elongation, but those on the south transept portal, from 1215 to 1220, show a more naturalistic style and increasing detachment from the wall behind, and some awareness of the classical tradition. These trends were continued in the west portal at Reims Cathedral of a few years later, where the figures are almost in the round, as became usual as Gothic spread across Europe.

In Italy Nicola Pisano (1258–1278) and his son Giovanni developed a style that is often called Proto-Renaissance, with unmistakable influence from Roman sarcophagi and sophisticated and crowded compositions, including a sympathetic handling of nudity, in relief panels on their pulpit of Siena Cathedral (1265–68), the Fontana Maggiore in Perugia, and Giovanni's pulpit in Pistoia of 1301. Another revival of classical style is seen in the International Gothic work of Claus Sluter and his followers in Burgundy and Flanders around 1400. Late Gothic sculpture continued in the North, with a fashion for very large wooden sculpted altarpieces with increasingly virtuoso carving and large numbers agitated expressive figures; most surviving examples are in Germany, after much iconoclasm elsewhere. Tilman Riemenschneider, Veit Stoss and others continued the style well into the 16th century, gradually absorbing Italian Renaissance influences.

Life-size tomb effigies in stone or alabaster became popular for the wealthy, and grand multi-level tombs evolved, with the Scaliger Tombs of Verona so large they had to be moved outside the church. By the 15th century there was an industry exporting Nottingham alabaster altar reliefs in groups of panels over much of Europe for economical parishes who could not afford stone retables. Small carvings, for a mainly lay and often female market, became a considerable industry in Paris and some other centres. Types of ivories included small devotional polyptychs, single figures, especially of the Virgin, mirror-cases, combs, and elaborate caskets with scenes from Romances, used as engagement presents. The very wealthy collected extravagantly elaborate jewelled and enamelled metalwork, both secular and religious, like the Duc de Berry's Holy Thorn Reliquary, until they ran short of money, when they were melted down again for cash.

Renaissance sculpture proper is often taken to begin with the famous competition for the doors of the Florence Baptistry in 1403, from which the trial models submitted by the winner, Lorenzo Ghiberti, and Filippo Brunelleschi survive. Ghiberti's doors are still in place, but were undoubtedly eclipsed by his second pair for the other entrance, the so-called "Gates of Paradise", which took him from 1425 to 1452, and are dazzlingly confident classicizing compositions with varied depths of relief allowing extensive backgrounds. The intervening years had seen Ghiberti's early assistant Donatello develop with seminal statues including his "Davids" in marble (1408–09) and bronze (1440s), and his Equestrian statue of Gattamelata, as well as reliefs. A leading figure in the later period was Andrea del Verrocchio, best known for his equestrian statue of Bartolomeo Colleoni in Venice; his pupil Leonardo da Vinci designed an equine sculpture in 1482 "The Horse" for Milan-but only succeeded in making a clay model which was destroyed by French archers in 1499, and his other ambitious sculptural plans were never completed.

The period was marked by a great increase in patronage of sculpture by the state for public art and by the wealthy for their homes; especially in Italy, public sculpture remains a crucial element in the appearance of historic city centres. Church sculpture mostly moved inside just as outside public monuments became common. Portrait sculpture, usually in busts, became popular in Italy around 1450, with the Neapolitan Francesco Laurana specializing in young women in meditative poses, while Antonio Rossellino and others more often depicted knobbly-faced men of affairs, but also young children. The portrait medal invented by Pisanello also often depicted women; relief plaquettes were another new small form of sculpture in cast metal.

Michelangelo was an active sculptor from about 1500 to 1520, and his great masterpieces including his "David", "Pietà", "Moses", and pieces for the Tomb of Pope Julius II and Medici Chapel could not be ignored by subsequent sculptors. His iconic David (1504) has a "contrapposto" pose, borrowed from classical sculpture. It differs from previous representations of the subject in that David is depicted before his battle with Goliath and not after the giant's defeat. Instead of being shown victorious, as Donatello and Verocchio had done, David looks tense and battle ready.

As in painting, early Italian Mannerist sculpture was very largely an attempt to find an original style that would top the achievement of the High Renaissance, which in sculpture essentially meant Michelangelo, and much of the struggle to achieve this was played out in commissions to fill other places in the Piazza della Signoria in Florence, next to Michelangelo's "David". Baccio Bandinelli took over the project of "Hercules and Cacus" from the master himself, but it was little more popular than it is now, and maliciously compared by Benvenuto Cellini to "a sack of melons", though it had a long-lasting effect in apparently introducing relief panels on the pedestal of statues. Like other works of his and other Mannerists it removes far more of the original block than Michelangelo would have done. Cellini's bronze "Perseus with the head of Medusa" is certainly a masterpiece, designed with eight angles of view, another Mannerist characteristic, but is indeed mannered compared to the "David"s of Michelangelo and Donatello. Originally a goldsmith, his famous gold and enamel Salt Cellar (1543) was his first sculpture, and shows his talent at its best. As these examples show, the period extended the range of secular subjects for large works beyond portraits, with mythological figures especially favoured; previously these had mostly been found in small works.

Small bronze figures for collector's cabinets, often mythological subjects with nudes, were a popular Renaissance form at which Giambologna, originally Flemish but based in Florence, excelled in the later part of the century, also creating life-size sculptures, of which two joined the collection in the Piazza della Signoria. He and his followers devised elegant elongated examples of the "figura serpentinata", often of two intertwined figures, that were interesting from all angles.

In Baroque sculpture, groups of figures assumed new importance, and there was a dynamic movement and energy of human forms— they spiralled around an empty central vortex, or reached outwards into the surrounding space. Baroque sculpture often had multiple ideal viewing angles, and reflected a general continuation of the Renaissance move away from the relief to sculpture created in the round, and designed to be placed in the middle of a large space—elaborate fountains such as Bernini's Fontana dei Quattro Fiumi (Rome, 1651), or those in the Gardens of Versailles were a Baroque speciality. The Baroque style was perfectly suited to sculpture, with Gian Lorenzo Bernini the dominating figure of the age in works such as "The Ecstasy of St Theresa" (1647–1652). Much Baroque sculpture added extra-sculptural elements, for example, concealed lighting, or water fountains, or fused sculpture and architecture to create a transformative experience for the viewer. Artists saw themselves as in the classical tradition, but admired Hellenistic and later Roman sculpture, rather than that of the more "Classical" periods as they are seen today.

The Protestant Reformation brought an almost total stop to religious sculpture in much of Northern Europe, and though secular sculpture, especially for portrait busts and tomb monuments, continued, the Dutch Golden Age has no significant sculptural component outside goldsmithing. Partly in direct reaction, sculpture was as prominent in Roman Catholicism as in the late Middle Ages. Statues of rulers and the nobility became increasingly popular. In the 18th century much sculpture continued on Baroque lines—the Trevi Fountain was only completed in 1762. Rococo style was better suited to smaller works, and arguably found its ideal sculptural form in early European porcelain, and interior decorative schemes in wood or plaster such as those in French domestic interiors and Austrian and Bavarian pilgrimage churches.

The Neoclassical style that arrived in the late 18th century gave great emphasis to sculpture. Jean-Antoine Houdon exemplifies the penetrating portrait sculpture the style could produce, and Antonio Canova's nudes the idealist aspect of the movement. The Neoclassical period was one of the great ages of public sculpture, though its "classical" prototypes were more likely to be Roman copies of Hellenistic sculptures. In sculpture, the most familiar representatives are the Italian Antonio Canova, the Englishman John Flaxman and the Dane Bertel Thorvaldsen. The European neoclassical manner also took hold in the United States, where its pinnacle occurred somewhat later and is exemplified in the sculptures of Hiram Powers.

Greco-Buddhist art is the artistic manifestation of Greco-Buddhism, a cultural syncretism between the Classical Greek culture and Buddhism, which developed over a period of close to 1000 years in Central Asia, between the conquests of Alexander the Great in the 4th century BCE, and the Islamic conquests of the 7th century CE. Greco-Buddhist art is characterized by the strong idealistic realism of Hellenistic art and the first representations of the Buddha in human form, which have helped define the artistic (and particularly, sculptural) canon for Buddhist art throughout the Asian continent up to the present. Though dating is uncertain, it appears that strongly Hellenistic styles lingered in the East for several centuries after they had declined around the Mediterranean, as late as the 5th century CE. Some aspects of Greek art were adopted while others did not spread beyond the Greco-Buddhist area; in particular the standing figure, often with a relaxed pose and one leg flexed, and the flying cupids or victories, who became popular across Asia as apsaras. Greek foliage decoration was also influential, with Indian versions of the Corinthian capital appearing.

The origins of Greco-Buddhist art are to be found in the Hellenistic Greco-Bactrian kingdom (250–130 BCE), located in today's Afghanistan, from which Hellenistic culture radiated into the Indian subcontinent with the establishment of the small Indo-Greek kingdom (180–10 BCE). Under the Indo-Greeks and then the Kushans, the interaction of Greek and Buddhist culture flourished in the area of Gandhara, in today's northern Pakistan, before spreading further into India, influencing the art of Mathura, and then the Hindu art of the Gupta empire, which was to extend to the rest of South-East Asia. The influence of Greco-Buddhist art also spread northward towards Central Asia, strongly affecting the art of the Tarim Basin and the Dunhuang Caves, and ultimately the sculpted figure in China, Korea, and Japan.

Chinese ritual bronzes from the Shang and Western Zhou Dynasties come from a period of over a thousand years from c. 1500 BCE, and have exerted a continuing influence over Chinese art. They are cast with complex patterned and zoomorphic decoration, but avoid the human figure, unlike the huge figures only recently discovered at Sanxingdui. The spectacular Terracotta Army was assembled for the tomb of Qin Shi Huang, the first emperor of a unified China from 221–210 BCE, as a grand imperial version of the figures long placed in tombs to enable the deceased to enjoy the same lifestyle in the afterlife as when alive, replacing actual sacrifices of very early periods. Smaller figures in pottery or wood were placed in tombs for many centuries afterwards, reaching a peak of quality in Tang dynasty tomb figures. The tradition of unusually large pottery figures persisted in China, through Tang sancai tomb figures to later Buddhist statues such as the near life-size set of Yixian glazed pottery luohans and later figures for temples and tombs. These came to replace earlier equivalents in wood.

Native Chinese religions do not usually use cult images of deities, or even represent them, and large religious sculpture is nearly all Buddhist, dating mostly from the 4th to the 14th century, and initially using Greco-Buddhist models arriving via the Silk Road. Buddhism is also the context of all large portrait sculpture; in total contrast to some other areas, in medieval China even painted images of the emperor were regarded as private. Imperial tombs have spectacular avenues of approach lined with real and mythological animals on a scale matching Egypt, and smaller versions decorate temples and palaces.

Small Buddhist figures and groups were produced to a very high quality in a range of media, as was relief decoration of all sorts of objects, especially in metalwork and jade. In the earlier periods, large quantities of sculpture were cut from the living rock in pilgrimage cave-complexes, and as outside rock reliefs. These were mostly originally painted. In notable contrast to literati painters, sculptors of all sorts were regarded as artisans and very few names are recorded. From the Ming dynasty onwards, statuettes of religious and secular figures were produced in Chinese porcelain and other media, which became an important export.

Towards the end of the long Neolithic Jōmon period, some pottery vessels were "flame-rimmed" with extravagant extensions to the rim that can only be called sculptural, and very stylized pottery dogū figures were produced, many with the characteristic "snow-goggle" eyes. During the Kofun period of the 3rd to 6th century CE, haniwa terracotta figures of humans and animals in a simplistic style were erected outside important tombs. The arrival of Buddhism in the 6th century brought with it sophisticated traditions in sculpture, Chinese styles mediated via Korea. The 7th-century Hōryū-ji and its contents have survived more intact than any East Asian Buddhist temple of its date, with works including a "Shaka Trinity" of 623 in bronze, showing the historical Buddha flanked by two bodhisattvas and also the Guardian Kings of the Four Directions.

The wooden image (9th century) of Shakyamuni, the "historic" Buddha, enshrined in a secondary building at the Murō-ji, is typical of the early Heian sculpture, with its ponderous body, covered by thick drapery folds carved in the hompa-shiki (rolling-wave) style, and its austere, withdrawn facial expression. The Kei school of sculptors, particularly Unkei, created a new, more realistic style of sculpture.

Almost all subsequent significant large sculpture in Japan was Buddhist, with some Shinto equivalents, and after Buddhism declined in Japan in the 15th century, monumental sculpture became largely architectural decoration and less significant. However sculptural work in the decorative arts was developed to a remarkable level of technical achievement and refinement in small objects such as inro and netsuke in many materials, and metal "" or Japanese sword mountings. In the 19th century there were export industries of small bronze sculptures of extreme virtuosity, ivory and porcelain figurines, and other types of small sculpture, increasingly emphasizing technical accomplishment.

The first known sculpture in the Indian subcontinent is from the Indus Valley civilization (3300–1700 BCE), found in sites at Mohenjo-daro and Harappa in modern-day Pakistan. These include the famous small bronze female dancer. However, such figures in bronze and stone are rare and greatly outnumbered by pottery figurines and stone seals, often of animals or deities very finely depicted. After the collapse of the Indus Valley civilization there is little record of sculpture until the Buddhist era, apart from a hoard of copper figures of (somewhat controversially) c. 1500 BCE from Daimabad. Thus the great tradition of Indian monumental sculpture in stone appears to begin, relative to other cultures, and the development of Indian civilization, relatively late, with the reign of Asoka from 270 to 232 BCE, and the Pillars of Ashoka he erected around India, carrying his edicts and topped by famous sculptures of animals, mostly lions, of which six survive. Large amounts of figurative sculpture, mostly in relief, survive from Early Buddhist pilgrimage stupas, above all Sanchi; these probably developed out of a tradition using wood that also embraced Hinduism.

The pink sandstone Hindu, Jain and Buddhist sculptures of Mathura from the 1st to 3rd centuries CE reflected both native Indian traditions and the Western influences received through the Greco-Buddhist art of Gandhara, and effectively established the basis for subsequent Indian religious sculpture. The style was developed and diffused through most of India under the Gupta Empire (c. 320–550) which remains a "classical" period for Indian sculpture, covering the earlier Ellora Caves, though the Elephanta Caves are probably slightly later. Later large-scale sculpture remains almost exclusively religious, and generally rather conservative, often reverting to simple frontal standing poses for deities, though the attendant spirits such as apsaras and yakshi often have sensuously curving poses. Carving is often highly detailed, with an intricate backing behind the main figure in high relief. The celebrated bronzes of the Chola dynasty (c. 850–1250) from south India, many designed to be carried in processions, include the iconic form of Shiva as Nataraja, with the massive granite carvings of Mahabalipuram dating from the previous Pallava dynasty.

The sculpture of the region tends to be characterised by a high degree of ornamentation, as seen in the great monuments of Hindu and Buddhist Khmer sculpture (9th to 13th centuries) at Angkor Wat and elsewhere, the enormous 9th-century Buddhist complex at Borobudur in Java, and the Hindu monuments of Bali. Both of these include many reliefs as well as figures in the round; Borobudur has 2,672 relief panels, 504 Buddha statues, many semi-concealed in openwork stupas, and many large guardian figures.

In Thailand and Laos, sculpture was mainly of Buddha images, often gilded, both large for temples and monasteries, and small figurines for private homes. Traditional sculpture in Myanmar emerged before the Bagan period. As elsewhere in the region, most of the wood sculptures of the Bagan and Ava periods have been lost. In later periods Chinese influence predominated in Vietnam, Laos and Cambodia, and more wooden sculpture survives from across the region.

Islam is famously aniconic, so the vast majority of sculpture is arabesque decoration in relief or openwork, based on vegetable motifs, but tending to geometrical abstract forms. In the very early Mshatta Facade (740s), now mostly in Berlin, there are animals within the dense arabesques in high relief, and figures of animals and men in mostly low relief are found in conjunction with decoration on many later pieces in various materials, including metalwork, ivory and ceramics.

Figures of animals in the round were often acceptable for works used in private contexts if the object was clearly practical, so medieval Islamic art contains many metal animals that are aquamaniles, incense burners or supporters for fountains, as in the stone lions supporting the famous one in the Alhambra, culminating in the largest medieval Islamic animal figure known, the Pisa Griffin. In the same way, luxury hardstone carvings such as dagger hilts and cups may be formed as animals, especially in Mughal art. The degree of acceptability of such relaxations of strict Islamic rules varies between periods and regions, with Islamic Spain, Persia and India often leading relaxation, and is typically highest in courtly contexts.

Historically, with the exception of some monumental Egyptian sculpture, most African sculpture was created in wood and other organic materials that have not survived from earlier than a few centuries ago; older pottery figures are found from a number of areas. Masks are important elements in the art of many peoples, along with human figures, often highly stylized. There is a vast variety of styles, often varying within the same context of origin depending on the use of the object, but wide regional trends are apparent; sculpture is most common among "groups of settled cultivators in the areas drained by the Niger and Congo rivers" in West Africa. Direct images of deities are relatively infrequent, but masks in particular are or were often made for religious ceremonies; today many are made for tourists as "airport art". African masks were an influence on European Modernist art, which was inspired by their lack of concern for naturalistic depiction.

The Nubian Kingdom of Kush in modern Sudan was in close and often hostile contact with Egypt, and produced monumental sculpture mostly derivative of styles to the north. In West Africa, the earliest known sculptures are from the Nok culture which thrived between 500 BCE and 500 CE in modern Nigeria, with clay figures typically with elongated bodies and angular shapes. Later West African cultures developed bronze casting for reliefs to decorate palaces like the famous Benin Bronzes, and very fine naturalistic royal heads from around the Yoruba town of Ife in terracotta and metal from the 12th–14th centuries. Akan goldweights are a form of small metal sculptures produced over the period 1400–1900, some apparently representing proverbs and so with a narrative element rare in African sculpture, and royal regalia included impressive gold sculptured elements.

Many West African figures are used in religious rituals and are often coated with materials placed on them for ceremonial offerings. The Mande-speaking peoples of the same region make pieces of wood with broad, flat surfaces and arms and legs are shaped like cylinders. In Central Africa, however, the main distinguishing characteristics include heart-shaped faces that are curved inward and display patterns of circles and dots.

Populations in the African Great Lakes are not known for their sculpture. However, one style from the region is pole sculptures, carved in human shapes and decorated with geometric forms, while the tops are carved with figures of animals, people, and various objects. These poles are, then, placed next to graves and are associated with death and the ancestral world. The culture known from Great Zimbabwe left more impressive buildings than sculpture but the eight soapstone Zimbabwe Birds appear to have had a special significance and were mounted on monoliths. Modern Zimbabwean sculptors in soapstone have achieved considerable international success. Southern Africa's oldest known clay figures date from 400 to 600 CE and have cylindrical heads with a mixture of human and animal features.

The creation of sculptures in Ethiopia and Eritrea can be traced back to its ancient past with the kingdoms of Dʿmt and Aksum. Christian art was established in Ethiopia with the conversion from paganism to Christianity in the 4th century CE, during the reign of king Ezana of Axum. Christian imagery decorated churches during the Asksumite period and later eras. For instance, at Lalibela, life-size saints were carved into the Church of Bet Golgotha; by tradition these were made during the reign of the Zagwe ruler Gebre Mesqel Lalibela in the 12th century, but they were more likely crafted in the 15th century during the Solomonic dynasty. However, the Church of Saint George, Lalibela, one of several examples of rock cut architecture at Lalibela containing intricate carvings, was built in the 10th–13th centuries as proven by archaeology.

In ancient Sudan, the development of sculpture stretches from the simple pottery of the Kerma culture beginning around 2500 BC to the monumental statuary and architecture of the Kingdom of Kush, its last phase—the Meroitic period—ending around 350 CE (with its conquest by Ethiopia's Aksum). Beyond pottery items, the Kerma culture also made furniture that contained sculptures, such as gold cattle hoofs as the legs of beds. Sculpture during the Kingdom of Kush included full-sized statues (especially of kings and queens), smaller figurines (most commonly depicting royal servants), and reliefs in stone, which were influenced by the contemporary ancient Egyptian sculptural tradition.

Sculpture in what is now Latin America developed in two separate and distinct areas, Mesoamerica in the north and Peru in the south. In both areas, sculpture was initially of stone, and later of terracotta and metal as the civilizations in these areas became more technologically proficient. The Mesoamerican region produced more monumental sculpture, from the massive block-like works of the Olmec and Toltec cultures, to the superb low reliefs that characterize the Mayan and Aztec cultures. In the Andean region, sculptures were typically small, but often show superb skill.

In North America, wood was sculpted for totem poles, masks, utensils, War canoes and a variety of other uses, with distinct variation between different cultures and regions. The most developed styles are those of the Pacific Northwest Coast, where a group of elaborate and highly stylized formal styles developed forming the basis of a tradition that continues today. In addition to the famous totem poles, painted and carved house fronts were complemented by carved posts inside and out, as well as mortuary figures and other items. Among the Inuit of the far north, traditional carving styles in ivory and soapstone are still continued.

The arrival of European Catholic culture readily adapted local skills to the prevailing Baroque style, producing enormously elaborate retablos and other mostly church sculptures in a variety of hybrid styles. The most famous of such examples in Canada is the altar area of the Notre Dame Basilica in Montreal, Quebec, which was carved by peasant "habitant" labourers. Later, artists trained in the Western academic tradition followed European styles until in the late 19th century they began to draw again on indigenous influences, notably in the Mexican baroque grotesque style known as Churrigueresque. Aboriginal peoples also adapted church sculpture in variations on Carpenter Gothic; one famous example is the "Church of the Holy Cross" in Skookumchuck Hot Springs, British Columbia.

The history of sculpture in the United States after Europeans' arrival reflects the country's 18th-century foundation in Roman republican civic values and Protestant Christianity. Compared to areas colonized by the Spanish, sculpture got off to an extremely slow start in the British colonies, with next to no place in churches, and was only given impetus by the need to assert nationality after independence. American sculpture of the mid- to late-19th century was often classical, often romantic, but showed a bent for a dramatic, narrative, almost journalistic realism. Public buildings during the last quarter of the 19th century and the first half of the 20th century often provided an architectural setting for sculpture, especially in relief. By the 1930s the International Style of architecture and design and art deco characterized by the work of Paul Manship and Lee Lawrie and others became popular. By the 1950s, traditional sculpture education would almost be completely replaced by a Bauhaus-influenced concern for abstract design. Minimalist sculpture replaced the figure in public settings and architects almost completely stopped using sculpture in or on their designs. Modern sculptors (21st century) use both classical and abstract inspired designs. Beginning in the 1980s, there was a swing back toward figurative public sculpture; by 2000, many of the new public pieces in the United States were figurative in design.

Modern classicism contrasted in many ways with the classical sculpture of the 19th century which was characterized by commitments to naturalism (Antoine-Louis Barye)—the melodramatic (François Rude) sentimentality (Jean-Baptiste Carpeaux)—or a kind of stately grandiosity (Lord Leighton). Several different directions in the classical tradition were taken as the century turned, but the study of the live model and the post-Renaissance tradition was still fundamental to them.
Auguste Rodin was the most renowned European sculptor of the early 20th century. He is often considered a sculptural Impressionist, as are his students including Camille Claudel, and Hugo Rheinhold, attempting to model of a fleeting moment of ordinary life.
Modern classicism showed a lesser interest in naturalism and a greater interest in formal stylization. Greater attention was paid to the rhythms of volumes and spaces—as well as greater attention to the contrasting qualities of surface (open, closed, planar, broken etc.) while less attention was paid to story-telling and convincing details of anatomy or costume. Greater attention was given to psychological effect than to physical realism, and influences from earlier styles worldwide were used.

Early masters of modern classicism included: Aristide Maillol, Alexander Matveyev, Joseph Bernard, Antoine Bourdelle, Georg Kolbe, Libero Andreotti, Gustav Vigeland, Jan Stursa, Constantin Brâncuși. As the century progressed, modern classicism was adopted as the national style of the two great European totalitarian empires: Nazi Germany and Soviet Russia, who co-opted the work of earlier artists such as Kolbe and Wilhelm Lehmbruck in Germany and Matveyev in Russia. Over the 70 years of the USSR, new generations of sculptors were trained and chosen within their system, and a distinct style, socialist realism, developed, that returned to the 19th century's emphasis on melodrama and naturalism.

Classical training was rooted out of art education in Western Europe (and the Americas) by 1970 and the classical variants of the 20th century were marginalized in the history of modernism. But classicism continued as the foundation of art education in the Soviet academies until 1990, providing a foundation for expressive figurative art throughout eastern Europe and parts of the Middle East. By the year 2000, the European classical tradition retains a wide appeal to the public but awaits an educational tradition to revive its contemporary development.

Some of the modern classical became either more decorative/art deco (Paul Manship, Jose de Creeft, Carl Milles) or more abstractly stylized or more expressive (and Gothic) (Anton Hanak, Wilhelm Lehmbruck, Ernst Barlach, Arturo Martini)—or turned more to the Renaissance (Giacomo Manzù, Venanzo Crocetti) or stayed the same (Charles Despiau, Marcel Gimond).

Modernist sculpture movements include Cubism, Geometric abstraction, De Stijl, Suprematism, Constructivism, Dadaism, Surrealism, Futurism, Formalism Abstract expressionism, Pop-Art, Minimalism, Land art, and Installation art among others.

In the early days of the 20th century, Pablo Picasso revolutionized the art of sculpture when he began creating his "constructions" fashioned by combining disparate objects and materials into one constructed piece of sculpture; the sculptural equivalent of the collage in two-dimensional art. The advent of Surrealism led to things occasionally being described as "sculpture" that would not have been so previously, such as "involuntary sculpture" in several senses, including coulage. In later years Picasso became a prolific potter, leading, with interest in historic pottery from around the world, to a revival of ceramic art, with figures such as George E. Ohr and subsequently Peter Voulkos, Kenneth Price, and Robert Arneson. Marcel Duchamp originated the use of the "found object" (French: objet trouvé) or "readymade" with pieces such as "Fountain" (1917).

Similarly, the work of Constantin Brâncuși at the beginning of the century paved the way for later abstract sculpture. In revolt against the naturalism of Rodin and his late-19th-century contemporaries, Brâncuși distilled subjects down to their essences as illustrated by the elegantly refined forms of his "Bird in Space" series (1924).

Brâncuși's impact, with his vocabulary of reduction and abstraction, is seen throughout the 1930s and 1940s, and exemplified by artists such as Gaston Lachaise, Sir Jacob Epstein, Henry Moore, Alberto Giacometti, Joan Miró, Julio González, Pablo Serrano, Jacques Lipchitz and by the 1940s abstract sculpture was impacted and expanded by Alexander Calder, Len Lye, Jean Tinguely, and Frederick Kiesler who were pioneers of Kinetic art.

Modernist sculptors largely missed out on the huge boom in public art resulting from the demand for war memorials for the two World Wars, but from the 1950s the public and commissioning bodies became more comfortable with Modernist sculpture and large public commissions both abstract and figurative became common. Picasso was commissioned to make a maquette for a huge -high public sculpture, the so-called "Chicago Picasso" (1967). His design was ambiguous and somewhat controversial, and what the figure represents is not clear; it could be a bird, a horse, a woman or a totally abstract shape.

During the late 1950s and the 1960s abstract sculptors began experimenting with a wide array of new materials and different approaches to creating their work. Surrealist imagery, anthropomorphic abstraction, new materials and combinations of new energy sources and varied surfaces and objects became characteristic of much new modernist sculpture. Collaborative projects with landscape designers, architects, and landscape architects expanded the outdoor site and contextual integration. Artists such as Isamu Noguchi, David Smith, Alexander Calder, Jean Tinguely, Richard Lippold, George Rickey, Louise Bourgeois, and Louise Nevelson came to characterize the look of modern sculpture.

By the 1960s Abstract expressionism, Geometric abstraction and Minimalism, which reduces sculpture to its most essential and fundamental features, predominated. Some works of the period are: the Cubi works of David Smith, and the welded steel works of Sir Anthony Caro, as well as welded sculpture by a large variety of sculptors, the large-scale work of John Chamberlain, and environmental installation scale works by Mark di Suvero. Other Minimalists include Tony Smith, Donald Judd, Robert Morris, Anne Truitt, Giacomo Benevelli, Arnaldo Pomodoro, Richard Serra, Dan Flavin, Carl Andre, and John Safer who added motion and monumentality to the theme of purity of line.

During the 1960s and 1970s figurative sculpture by modernist artists in stylized forms was made by artists such as Leonard Baskin, Ernest Trova, George Segal, Marisol Escobar, Paul Thek, Robert Graham in a classic articulated style, and Fernando Botero bringing his painting's 'oversized figures' into monumental sculptures.

Site specific and environmental art works are represented by artists: Andy Goldsworthy, Walter De Maria, Richard Long, Richard Serra, Robert Irwin, George Rickey and Christo and Jeanne-Claude led contemporary abstract sculpture in new directions. Artists created environmental sculpture on expansive sites in the 'land art in the American West' group of projects. These land art or 'earth art' environmental scale sculpture works exemplified by artists such as Robert Smithson, Michael Heizer, James Turrell (Roden Crater). Eva Hesse, Sol LeWitt, Jackie Winsor, Keith Sonnier, Bruce Nauman and Dennis Oppenheim among others were pioneers of Postminimalist sculpture.

Also during the 1960s and 1970s artists as diverse as Eduardo Paolozzi, Chryssa, Claes Oldenburg, George Segal, Edward Kienholz, Nam June Paik, Wolf Vostell, Duane Hanson, and John DeAndrea explored abstraction, imagery and figuration through video art, environment, light sculpture, and installation art in new ways.

Conceptual art is art in which the concept(s) or idea(s) involved in the work take precedence over traditional aesthetic and material concerns. Works include "One and Three Chairs", 1965, is by Joseph Kosuth, and "An Oak Tree" by Michael Craig-Martin, and those of Joseph Beuys, James Turrell and Jacek Tylicki.

Some modern sculpture forms are now practiced outdoors, as environmental art and environmental sculpture, often in full view of spectators. Light sculpture, street art sculpture and site-specific art also often make use of the environment. Ice sculpture is a form of ephemeral sculpture that uses ice as the raw material. It is popular in China, Japan, Canada, Sweden, and Russia. Ice sculptures feature decoratively in some cuisines, especially in Asia. Kinetic sculptures are sculptures that are designed to move, which include mobiles. Snow sculptures are usually carved out of a single block of snow about on each side and weighing about 20–30 tons. The snow is densely packed into a form after having been produced by artificial means or collected from the ground after a snowfall. Sound sculptures take the form of indoor sound installations, outdoor installations such as aeolian harps, automatons, or be more or less near conventional musical instruments. Sound sculpture is often site-specific. Art toys have become another format for contemporary artists since the late 1990s, such as those produced by Takashi Murakami and Kid Robot, designed by Michael Lau, or hand-made by Michael Leavitt (artist).

Sculptures are sensitive to environmental conditions such as temperature, humidity and exposure to light and ultraviolet light. Acid rain can also cause damage to certain building materials and historical monuments. This results when sulfuric acid in the rain chemically reacts with the calcium compounds in the stones (limestone, sandstone, marble and granite) to create gypsum, which then flakes off.

At any time many contemporary sculptures have usually been on display in public places; theft was not a problem as pieces were instantly recognisable. In the early 21st century the value of metal rose to such an extent that theft of massive bronze sculpture for the value of the metal became a problem; sculpture worth millions being stolen and melted down for the relatively low value of the metal, a tiny fraction of the value of the artwork.





</doc>
<doc id="26715" url="https://en.wikipedia.org/wiki?curid=26715" title="Slashdot">
Slashdot

Slashdot (sometimes abbreviated as /.) is a social news website that originally billed itself as "News for Nerds. Stuff that Matters". It features news stories on science, technology, and politics that are submitted and evaluated by site users and editors. Each story has a comments section attached to it where users can add online comments. The website was founded in 1997 by Hope College students Rob Malda, also known as "CmdrTaco", and classmate Jeff Bates, also known as "Hemos". In 2012, they sold it to DHI Group, Inc. (i.e., Dice Holdings International, which created the Dice.com website for tech job seekers). In January 2016, BIZX acquired Slashdot Media, including both slashdot.org and SourceForge.. In December 2019, BIZX rebranded to Slashdot Media.

Summaries of stories and links to news articles are submitted by Slashdot's own users, and each story becomes the topic of a threaded discussion among users. Discussion is moderated by a user-based moderation system. Randomly selected moderators are assigned points (typically 5) which they can use to rate a comment. Moderation applies either "−1" or "+1" to the current rating, based on whether the comment is perceived as either "normal", "offtopic", "insightful", "redundant", "interesting", or "troll" (among others).

The site's comment and moderation system is administered by its own open source content management system, Slash, which is available under the GNU General Public License. In 2012, "Slashdot" had around 3.7 million unique visitors per month and received over 5300 comments per day. The site has won more than 20 awards, including People's Voice Awards in 2000 for "Best Community Site" and "Best News Site". At its peak use, a news story posted to the site with a link could overwhelm some smaller or independent sites. This phenomenon was known as the "Slashdot effect".

Slashdot was preceded by Rob Malda's personal website "Chips & Dips", which launched in October 1997, featured a single "rant" each day about something that interested its author – typically something to do with Linux or open source software. At the time, Malda was a student at Hope College in Holland, Michigan, majoring in computer science. The site became "Slashdot" in September 1997 under the slogan "News for Nerds. Stuff that Matters," and quickly became a hotspot on the Internet for news and information of interest to computer geeks.

The name "Slashdot" came from a somewhat "obnoxious parody of a URL" – when Malda registered the domain, he desired to make a name that was "silly and unpronounceable" – try pronouncing out, "h-t-t-p-colon-slash-slash-slashdot-dot-org". By June 1998, the site was seeing as many as 100,000 page views per day and advertisers began to take notice. Slashdot was co-founded by Rob Malda and Jeff Bates. By December 1998, Slashdot had net revenues of $18,000, yet its Internet profile was higher and revenues were expected to increase.

On June 29, 1999, the site was sold to Linux megasite Andover.net for $1.5 million in cash and $7 million in Andover stock at the Initial public offering (IPO) price. Part of the deal was contingent upon the continued employment of Malda and Bates and on the achievement of certain "milestones". With the acquisition of Slashdot, Andover.net could now advertise itself as "the leading Linux/Open Source destination on the Internet". Andover.net merged with VA Linux on February 3, 2000, changed its name to SourceForge, Inc. on May 24, 2007, and then became Geeknet, Inc. on November 4, 2009.

Slashdot's 10,000th article was posted after two and a half years on February 24, 2000, and the 100,000th article was posted on December 11, 2009 after 12 years online.
During the first 12 years, the most active story with the most responses posted was the post-2004 US Presidential Election article "Kerry Concedes Election To Bush" with 5,687 posts. This followed the creation of a new article section, "politics.slashdot.org", created at the start of the 2004 election on September 7, 2004. Many of the most popular stories are political, with "Strike on Iraq" (March 19, 2003) the second-most-active article and "Barack Obama Wins US Presidency" (November 5, 2008) the third-most-active. The rest of the 10 most active articles are an article announcing the 2005 London bombings, and several articles about Evolution vs. Intelligent Design, Saddam Hussein's capture, and "Fahrenheit 9/11". Articles about Microsoft and its Windows Operating System are popular. A thread posted in 2002 titled "What's Keeping You On Windows?" was the 10th-most-active story, and an article about Windows 2000/NT4 source-code leaks the most visited article with more than 680,000 hits. Some controversy erupted on March 9, 2001 after an anonymous user posted the full text of Scientology's "Operating Thetan Level Three" (OT III) document in a comment attached to a Slashdot article. The Church of Scientology demanded that Slashdot remove the document under the Digital Millennium Copyright Act. A week later, in a long article, Slashdot editors explained their decision to remove the page while providing links and information on how to get the document from other sources.

Slashdot Japan was launched on May 28, 2001 (although the first article was published April 5, 2001) and is an official offshoot of the US-based Web site. the site was owned by OSDN-Japan, Inc., and carried some of the US-based Slashdot articles as well as localized stories. An external site, "New Media Services", has reported the importance of Online Moderation last December 1, 2011. On Valentine's Day 2002, founder Rob Malda proposed to longtime girlfriend Kathleen Fent using the front page of Slashdot. They were married on December 8, 2002, in Las Vegas, Nevada. Slashdot implemented a paid subscription service on March 1, 2002. Slashdot's subscription model works by allowing users to pay a small fee to be able to view pages without banner ads, starting at a rate of $5 per 1,000 page views – non-subscribers may still view articles and respond to comments, with banner ads in place. On March 6, 2003, subscribers were given the ability to see articles 10 to 20 minutes before they are released to the public. Slashdot altered its threaded discussion forum display software to explicitly show domains for links in articles, as "users made a sport out of tricking unsuspecting readers into visiting <nowiki>[</nowiki>Goatse.cx<nowiki>]</nowiki>."
In observance of April Fools' Day in 2006, Slashdot temporarily changed its signature teal color theme to a warm palette of bubblegum pink and changed its masthead from the usual, "News for Nerds" motto to, "OMG!!! Ponies!!!" Editors joked that this was done to increase female readership. In another supposed April Fools' Day joke, User Achievement tags were introduced on April 1, 2009. This system allowed users to be tagged with various achievements, such as "The Tagger" for tagging a story or "Member of the {1,2,3,4,5} Digit UID Club" for having a Slashdot UID consisting of a certain number of digits. While it was posted on April Fools' Day to allow for certain joke achievements, the system is real. Slashdot unveiled its newly redesigned site on June 4, 2006, following a CSS Redesign Competition. The winner of the competition was Alex Bendiken, who built on the initial CSS framework of the site. The new site looks similar to the old one but is more polished with more rounded curves, collapsible menus, and updated fonts. On November 9 that same year, Malda wrote that Slashdot attained 16,777,215 (or 2 − 1) comments, which broke the database for three hours until the administrators fixed the problem.

On January 25, 2011, the site launched its third major redesign in its 13.5-year history, which gutted the HTML and CSS, and updated the graphics. On August 25, 2011, Malda resigned as Editor-in-Chief with immediate effect. He did not mention any plans for the future, other than spending more time with his family, catching up on some reading, and possibly writing a book. His final farewell message received over 1,400 comments within 24 hours on the site. On December 7, 2011, Slashdot announced that it would start to push what the company described as "sponsored" Ask Slashdot questions. On March 28, 2012, Slashdot launched Slashdot TV. Two months later, in May 2012, Slashdot launched SlashBI, SlashCloud, and SlashDataCenter, three websites dedicated to original journalistic content. The websites proved controversial, with longtime Slashdot users commenting that the original content ran counter to the website's longtime focus on user-generated submissions. Nick Kolakowski, the editor of the three websites, told The Next Web that the websites were “meant to complement Slashdot with an added layer of insight into a very specific area of technology, without interfering with Slashdot’s longtime focus on tech-community interaction and discussion.” Despite the debate, articles published on SlashCloud and SlashBI attracted attention from io9, NPR, Nieman Lab, Vanity Fair, and other publications.

In September 2012, Slashdot, SourceForge, and Freecode were acquired by online job site Dice.com for $20 million, and incorporated into a subsidiary known as Slashdot Media. While initially stating that there were no plans for major changes to Slashdot, in October 2013, Slashdot launched a "beta" for a significant redesign of the site, which featured a simpler appearance and commenting system. While initially an opt-in beta, the site automatically began migrating selected users to the new design in February 2014; the rollout led to a negative response from many longtime users, upset by the added visual complexity, and the removal of features, such as comment viewing, that distinguished Slashdot from other news sites. An organized boycott of the site was held from February 10 to 17, 2014. The "beta" site was eventually shelved. In July 2015, Dice announced that it planned to sell Slashdot and SourceForge; in particular, the company stated in a filing that it was unable to "successfully [leverage] the Slashdot user base to further Dice's digital recruitment business".

On January 27, 2016, the two sites were sold to the San Diego-based BizX, LLC for an undisclosed amount.

It was run by its founder, Rob "CmdrTaco" Malda, from 1998 until 2011. He shared editorial responsibilities with several other editors including Timothy Lord, Patrick "Scuttlemonkey" McGarry, Jeff "Soulskill" Boehm, Rob "Samzenpus" Rozeboom, and Keith Dawson. Jonathan "cowboyneal" Pater is another popular editor of Slashdot, who came to work for Slashdot as a programmer and systems administrator. His online nickname (handle), CowboyNeal, is inspired by a Grateful Dead tribute to Neal Cassady in their song, "That's It for the Other One". He is best known as the target of the usual comic poll option, a tradition started by Chris DiBona.

Slashdot runs on Slash, a content management system available under the GNU General Public License. Early versions of Slash were written by Rob Malda in the spring of 1998. After Andover.net bought Slashdot in June 1999, Slash remains Free software and anyone can contribute to development.

Slashdot's editors are primarily responsible for selecting and editing the primary stories that are posted daily by submitters. The editors provide a one-paragraph summary for each story and a link to an external website where the story originated. Each story becomes the topic for a threaded discussion among the site's users. A user-based moderation system is employed to filter out abusive or offensive comments. Every comment is initially given a score of "−1" to "+2", with a default score of "+1" for registered users, "0" for anonymous users (Anonymous Coward), "+2" for users with high "karma", or "−1" for users with low "karma". As moderators read comments attached to articles, they click to moderate the comment, either up ("+1") or down ("−1"). Moderators may choose to attach a particular descriptor to the comments as well, such as "normal", "offtopic", "flamebait", "troll", "redundant", "insightful", "interesting", "informative", "funny", "overrated", or "underrated", with each corresponding to a "−1" or "+1" rating. So a comment may be seen to have a rating of "+1 insightful" or "−1 troll". Comments are very rarely deleted, even if they contain hateful remarks.

Starting in August of 2019 anonymous comments and postings have been disabled.

Moderation points add to a user's rating, which is known as "karma" on Slashdot. Users with high "karma" are eligible to become moderators themselves. The system does not promote regular users as "moderators" and instead assigns five moderation points at a time to users based on the number of comments they have entered in the system – once a user's moderation points are used up, they can no longer moderate articles (though they can be assigned more moderation points at a later date). Paid staff editors have an unlimited number of moderation points. A given comment can have any integer score from "−1" to "+5", and registered users of Slashdot can set a personal threshold so that no comments with a lesser score are displayed. For instance, a user reading Slashdot at level "+5" will only see the highest rated comments, while a user reading at level "−1" will see a more "unfiltered, anarchic version". A meta-moderation system was implemented on September 7, 1999, to moderate the moderators and help contain abuses in the moderation system. Meta-moderators are presented with a set of moderations that they may rate as either "fair" or "unfair". For each moderation, the meta-moderator sees the original comment and the reason assigned by the moderator (e.g. "troll", "funny"), and the meta-moderator can click to see the context of comments surrounding the one that was moderated.

Slashdot features discussion forums on a variety of technology- and science-related topics, or "News for Nerds", as its motto states. Articles are divided into the following sections:


Slashdot uses a system of "tags" where users can categorize a story to group them together and sorting them. Tags are written in all lowercase, with no spaces, and limited to 64 characters. For example, articles could be tagged as being about "security" or "mozilla". Some articles are tagged with longer tags, such as "whatcouldpossiblygowrong" (expressing the perception of catastrophic risk), "suddenoutbreakofcommonsense" (used when the community feels that the subject has finally figured out something obvious), "correlationnotcausation" (used when scientific articles lack direct evidence; see correlation does not imply causation), or "getyourasstomars" (commonly seen in articles about Mars or space exploration).

As an online community with primarily user-generated content, many in-jokes and internet memes have developed over the course of the site's history. A popular meme (based on an unscientific Slashdot user poll) is, "In Soviet Russia, "noun" "verb" you!" This type of joke has its roots in the 1960s or earlier, and is known as a "Russian reversal". Other popular memes usually pertain to computing or technology, such as "Imagine a Beowulf cluster of these", "But does it run Linux?", or "Netcraft now confirms: BSD (or some other software package or item) is dying." Users will also typically refer to articles referring to data storage and data capacity by inquiring how much it is in units of Libraries of Congress. Sometimes bandwidth speeds are referred to in units of Libraries of Congress per second. When numbers are quoted, people will comment that the number happens to be the "combination to their luggage" (a reference to the Mel Brooks film Spaceballs) and express false anger at the person who revealed it.

Slashdotters often use the abbreviation TFA which stands for "The fucking article" or RTFA ("Read the fucking article"), which itself is derived from the abbreviation RTFM. Usage of this abbreviation often exposes comments from posters who have not read the article linked to in the main story. Slashdotters typically like to mock then United States Senator Ted Stevens' 2006 description of the Internet as a "series of tubes" or former Microsoft CEO Steve Ballmer's chair-throwing incident from 2005. Microsoft founder Bill Gates is a popular target of jokes by Slashdotters, and all stories about Microsoft were once identified with a graphic of Gates looking like a Borg from "". Many Slashdotters have long talked about the supposed release of "Duke Nukem Forever", which was promised in 1997 but was delayed indefinitely (the game was eventually released in 2011). References to the game are commonly brought up in other articles about software packages that are not yet in production even though the announced delivery date has long passed (see vaporware). Having a low Slashdot user identifier (user ID) is highly valued since they are assigned sequentially; having one is a sign that someone has an older account and has contributed to the site longer. For Slashdot's 10-year anniversary in 2007, one of the items auctioned off in the charity auction for the Electronic Frontier Foundation was a 3-digit Slashdot user ID.

In 2006, Slashdot had approximately 5.5 million users per month and in January 2013, the site's Alexa rank was 2,000, with the average user spending 3 minutes and 18 seconds per day on the site and 82,665 sites linking in. By 2019 this had reduced to Alexa rank 5,194.

The primary stories on the site consist of a short synopsis paragraph, a link to the original story, and a lengthy discussion section, all contributed by users. At its peak, discussion on stories could get up to 10,000 posts per day. Slashdot has been considered a pioneer in user-driven content, influencing other sites such as Google News and Wikipedia.
There has been a dip in readership as of 2011, primarily due to the increase of technology-related blogs and Twitter feeds.

In 2002, approximately 50% of Slashdot's traffic consisted of people who simply check out the headlines and click through, while others participate in discussion boards and take part in the community. Many links in Slashdot stories caused the linked site to get swamped by heavy traffic and its server to collapse. This was known as the "Slashdot effect", a term first coined on February 15, 1999 that refers to an article about a "new generation of niche Web portals driving unprecedented amounts of traffic to sites of interest".

Slashdot has received over twenty awards, including People's Voice Awards in 2000 in both of the categories for which it was nominated ("Best Community Site" and "Best News Site"). It was also voted as one of "Newsweek"s favorite technology Web sites and rated in Yahoo!'s Top 100 Web sites as the "Best Geek Hangout" (2001). The main antagonists in the 2004 novel "Century Rain", by Alastair Reynolds – The Slashers – are named after Slashdot users. The site was mentioned briefly in the 2000 novel "Cosmonaut Keep", written by Ken MacLeod. Several tech celebrities have stated that they either checked the website regularly or participated in its discussion forums using an account. Some of these celebrities include: Apple co-founder Steve Wozniak, writer and actor Wil Wheaton, and id Software technical director John Carmack.



</doc>
<doc id="26716" url="https://en.wikipedia.org/wiki?curid=26716" title="South Australia">
South Australia

South Australia (abbreviated as SA) is a state in the southern central part of Australia. It covers some of the most arid parts of the country. With a total land area of , it is the fourth-largest of Australia's states and territories by area, and fifth largest by population. It has a total of 1.7 million people, and its population is the second most highly centralised in Australia, after Western Australia, with more than 77 percent of South Australians living in the capital, Adelaide, or its . Other population centres in the state are relatively small; Mount Gambier, the second largest centre, has a population of 28,684.

South Australia shares borders with all of the other mainland states, and with the Northern Territory; it is bordered to the west by Western Australia, to the north by the Northern Territory, to the north-east by Queensland, to the east by New South Wales, to the south-east by Victoria, and to the south by the Great Australian Bight. The state comprises less than 8 percent of the Australian population and ranks fifth in population among the six states and two territories. The majority of its people reside in greater Metropolitan Adelaide. Most of the remainder are settled in fertile areas along the south-eastern coast and River Murray. The state's colonial origins are unique in Australia as a freely settled, planned British province, rather than as a convict settlement. Colonial government commenced on 28 December 1836, when the members of the council were sworn in near the Old Gum Tree.

As with the rest of the continent, the region has a long history of human occupation by numerous tribes and languages. The South Australian Company established a temporary settlement at Kingscote, Kangaroo Island, on 26 July 1836, five months before Adelaide was founded. The guiding principle behind settlement was that of "systematic colonisation", a theory espoused by Edward Gibbon Wakefield that was later employed by the New Zealand Company. The goal was to establish the province as a centre of civilisation for free immigrants, promising civil liberties and religious tolerance. Although its history is marked by economic hardship, South Australia has remained politically innovative and culturally vibrant. Today, it is known for its fine wine and numerous cultural festivals. The state's economy is dominated by the agricultural, manufacturing and mining industries.

Evidence of human activity in South Australia dates back as far as 20,000 years, with flint mining activity and rock art in the Koonalda Cave on the Nullarbor Plain. In addition wooden spears and tools were made in an area now covered in peat bog in the South East. Kangaroo Island was inhabited long before the island was cut off by rising sea levels.
The first recorded European sighting of the South Australian coast was in 1627 when the Dutch ship the "Gulden Zeepaert", captained by François Thijssen, examined and mapped a section of the coastline as far east as the Nuyts Archipelago. Thijssen named the whole of the country eastward of the Leeuwin "Nuyts Land", after a distinguished passenger on board; the Hon. Pieter Nuyts, one of the Councillors of India.

The coastline of South Australia was first mapped by Matthew Flinders and Nicolas Baudin in 1802, excepting the inlet later named the Port Adelaide River which was first discovered in 1831 by Captain Collet Barker and later accurately charted in 1836–37 by Colonel William Light, leader of the South Australian Colonization Commissioners' 'First Expedition' and first Surveyor-General of South Australia.

The land which now forms the state of South Australia was claimed for Britain in 1788 as part of the colony of New South Wales. Although the new colony included almost two-thirds of the continent, early settlements were all on the eastern coast and only a few intrepid explorers ventured this far west. It took more than forty years before any serious proposal to establish settlements in the south-western portion of New South Wales were put forward.

On 15 August 1834, the British Parliament passed the South Australia Act 1834 ("Foundation Act"), which empowered His Majesty to erect and establish a province or provinces in southern Australia. The act stated that the land between 132° and 141° east longitude and from 26° south latitude to the southern ocean would be allotted to the colony, and it would be convict-free.
In contrast to the rest of Australia, "terra nullius" did not apply to the new province. The Letters Patent, which used the enabling provisions of the South Australia Act 1834 to fix the boundaries of the Province of South Australia, provided that "nothing in those our Letters Patent shall affect or be construed to affect the rights of any Aboriginal Natives of the said Province to the actual occupation and enjoyment in their own Persons or in the Persons of their Descendants of any Lands therein now actually occupied or enjoyed by such Natives." Although the patent guaranteed land rights under force of law for the indigenous inhabitants it was ignored by the South Australian Company authorities and squatters. Despite strong reference to the rights of the native population in the initial proclamation by the Governor, there were many conflicts and deaths in the Australian Frontier Wars in South Australia.
Survey was required before settlement of the province, and the Colonization Commissioners for South Australia appointed William Light as the leader of its 'First Expedition', tasked with examining 1500 miles of the South Australian coastline and selecting the best site for the capital, and with then planning and surveying the site of the city into one-acre Town Sections and its surrounds into 134-acre Country Sections.

Eager to commence the establishment of their whale and seal fisheries, the South Australian Company sought, and obtained, the Commissioners' permission to send Company ships to South Australia, in advance of the surveys and ahead of the Commissioners' colonists.

The Company's settlement of seven vessels and 636 people was temporarily made at Kingscote on Kangaroo Island, until the official site of the capital was selected by William Light, where the City of Adelaide is currently located. The first immigrants arrived at Holdfast Bay (near the present day Glenelg) in November 1836.

The commencement of colonial government was proclaimed on 28 December 1836, now known as Proclamation Day.

South Australia is the only Australian state to have never received British convicts. Another free settlement, Swan River colony was established in 1829 but Western Australia later sought convict labour, and in 1849 Western Australia was formally constituted as a penal colony. Although South Australia was constituted such that convicts could never be transported to the Province, some emancipated or escaped convicts or expirees made their own way there, both prior to 1836, or later, and may have constituted 1–2% of the early population.

The plan for the province was that it would be an experiment in reform, addressing the problems perceived in British society. There was to be religious freedom and no established religion. Sales of land to colonists created an Emigration Fund to pay the costs of transferring a poor young labouring population to South Australia. In early 1838 the colonists became concerned after it was reported that convicts who had escaped from the eastern states may make their way to South Australia. The South Australia Police was formed in April 1838 to protect the community and enforce government regulations. Their principal role was to run the first temporary gaol, a two-room hut.

The current flag of South Australia was adopted on 13 January 1904, and is a British blue ensign defaced with the state badge. The badge is described as a piping shrike with wings outstretched on a yellow disc. The state badge is believed to have been designed by Robert Craig of Adelaide's School of Design.

The terrain consists largely of arid and semi-arid rangelands, with several low mountain ranges. The most important (but not tallest) is the Mount Lofty-Flinders Ranges system, which extends north about from Cape Jervis to the northern end of Lake Torrens. The highest point in the state is not in those ranges; Mount Woodroffe () is in the Musgrave Ranges in the extreme northwest of the state. The south-western portion of the state consists of the sparsely inhabited Nullarbor Plain, fronted by the cliffs of the Great Australian Bight. Features of the coast include Spencer Gulf and the Eyre and Yorke Peninsulas that surround it.

The principal industries and exports of South Australia are wheat, wine and wool. More than half of Australia's wines are produced in the South Australian wine regions which principally include: Barossa Valley, Clare Valley, McLaren Vale, Coonawarra, the Riverland and the Adelaide Hills. "See South Australian wine."

South Australia has boundaries with every other Australian mainland state and territory except the Australian Capital Territory and the Jervis Bay Territory. The Western Australia border has a history involving the South Australian government astronomer, Dodwell, and the Western Australian Government Astronomer, Curlewis, marking the border on the ground in the 1920s.

In 1863, that part of New South Wales to the north of South Australia was annexed to South Australia, by letters patent, as the "Northern Territory of South Australia", which became shortened to the Northern Territory (6 July 1863). The Northern Territory was handed to the federal government in 1911 and became a separate territory.

According to Australian maps, South Australia's south coast is flanked by the Southern Ocean, but official international consensus defines the Southern Ocean as extending north from the pole only to 60°S or 55°S, at least 17 degrees of latitude further south than the most southern point of South Australia. Thus the south coast is officially adjacent to the south-most portion of the Indian Ocean. "See Southern Ocean: Existence and definitions".

The southern part of the state has a Mediterranean climate, while the rest of the state has either an arid or semi-arid climate. South Australia's main temperature range is in January and in July. 
The highest maximum temperature was recorded as at Oodnadatta on 2 January 1960, which is also the highest official temperature recorded in Australia. The lowest minimum temperature was at Yongala on 20 July 1976.

South Australia's average annual employment for 2009–10 was 800,600 persons, 18% higher than for 2000–01. For the corresponding period, national average annual employment rose by 22%.

South Australia's largest employment sector is health care and social assistance, surpassing manufacturing in SA as the largest employer since 2006–07. In 2009–10, manufacturing in SA had average annual employment of 83,700 persons compared with 103,300 for health care and social assistance. Health care and social assistance represented nearly 13% of the state average annual employment.

The retail trade is the second largest employer in SA (2009–10), with 91,900 jobs, and 12 per cent of the state workforce.

The manufacturing industry plays an important role in South Australia's economy, generating 11.7% of the state's gross state product (GSP) and playing a large part in exports. The manufacturing industry consists of automotive (44% of total Australian production, 2006) and component manufacturing, pharmaceuticals, defence technology (2.1% of GSP, 2002–03) and electronic systems (3.0% of GSP in 2006). South Australia's economy relies on exports more than any other state in Australia.

State export earnings stood at A$10 billion per year and grew by 8.8% from 2002 to 2003. Production of South Australian food and drink (including agriculture, horticulture, aquaculture, fisheries and manufacturing) is a $10 billion industry.

South Australia's credit rating was upgraded to AAA by Standard & Poor's Rating Agency in September 2004 and to AAA by Moody's Rating Agency November 2004, the highest credit ratings achievable by any company or sovereign. The State had previously lost these ratings in the State Bank collapse. However, in 2012 Standard & Poor's downgraded the state's credit rating to AA+ due to declining revenues, new spending initiatives and a weaker than expected budgetary outlook.

South Australia's Gross State Product was A$48.9 billion starting 2004, making it A$32,996 per capita. Exports for 2006 were valued at $9.0bn with imports at $6.2bn. Private Residential Building Approvals experienced 80% growth over the year of 2006.

South Australia's economy includes the following major industries: meat and meat preparations, wheat, wine, wool and sheepskins, machinery, metal and metal manufactures, fish and crustaceans, road vehicles and parts, and petroleum products. Other industries, such as education and defence technology, are of growing importance.

South Australia receives the least amount of federal funding for its local road network of all states on a per capita and a per kilometre basis.

In 2013, South Australia was named by Commsec Securities as the second lowest performing economy in Australia. While some sources have pointed at weak retail spending and capital investment, others have attributed poor performance to declines in public spending.

South Australia has the lead over other Australian states for its commercialisation and commitment to renewable energy. It is now the leading producer of wind power in Australia. Renewable energy is a growing source of electricity in South Australia, and there is potential for growth from this particular industry of the state's economy. The Hornsdale Power Reserve is a bank of grid-connected batteries adjacent to the Hornsdale Wind Farm in South Australia's Mid-North region. At the time of construction in late 2017, it was billed as the largest lithium-ion battery in the world.

The Olympic Dam mine near Roxby Downs in northern South Australia is the largest deposit of uranium in the world, possessing more than a third of the world's low-cost recoverable reserves and 70% of Australia's. The mine, owned and operated by BHP Billiton, presently accounts for 9% of global uranium production. The Olympic Dam mine is also the world's fourth-largest remaining copper deposit, and the world's fifth largest gold deposit. There was a proposal to vastly expand the operations of the mine, making it the largest open-cut mine in the world, but in 2012 the BHP Billiton board decided not to go ahead with it at that time due to then lower commodity prices.

Crown land held in right of South Australia is managed under the Crown Land Management Act 2009.

South Australia is a constitutional monarchy with the Queen of Australia as sovereign, and the Governor of South Australia as her representative. It is a state of the Commonwealth of Australia. The bicameral Parliament of South Australia consists of the lower house known as the House of Assembly and the upper house known as the Legislative Council. General elections are held every four years, the last being the 2018 election.

Initially, the Governor of South Australia held almost total power, derived from the letters patent of the imperial government to create the colony. He was accountable only to the British Colonial Office, and thus democracy did not exist in the colony. A new body was created to advise the governor on the administration of South Australia in 1843 called the Legislative Council. It consisted of three representatives of the British Government and four colonists appointed by the governor. The governor retained total executive power.

In 1851, the Imperial Parliament enacted the Australian Colonies Government Act which allowed for the election of representatives to each of the colonial legislatures and the drafting of a constitution to properly create representative and responsible government in South Australia. Later that year, propertied male colonists were allowed to vote for 16 members on a new 24 seat Legislative Council. Eight members continued to be appointed by the governor.

The main responsibility of this body was to draft a constitution for South Australia. The body drafted the most democratic constitution ever seen in the British Empire and provided for universal manhood suffrage. It created the bicameral Parliament of South Australia. For the first time in the colony, the executive was elected by the people and the colony used the Westminster system, where the government is the party or coalition that exerts a majority in the House of Assembly.

Women's suffrage in Australia took a leap forward – enacted in 1895 and taking effect from the 1896 colonial election, South Australia was the first in Australia and only the second in the world after New Zealand to allow women to vote, and the first in the world to allow women to stand for election. In 1897 Catherine Helen Spence was the first woman in Australia to be a candidate for political office when she was nominated to be one of South Australia's delegates to the conventions that drafted the constitution. South Australia became an original state of the Commonwealth of Australia on 1 January 1901.

South Australia is divided into 74 local government areas. Local councils are responsible for functions delegated by the South Australian parliament, such as road infrastructure and waste management. Council revenue comes mostly from property taxes and government grants.

As at March 2018 the population of South Australia was 1,733,500.
A majority of the state's population lives within Greater Adelaide's metropolitan area which had an estimated population of 
1,333,927 in June 2017. Other significant population centres include Mount Gambier (29,505), Victor Harbor-Goolwa (26,334), Whyalla (21,976), Murray Bridge (18,452), Port Lincoln (16,281), Port Pirie (14,267), and Port Augusta (13,957).

At the 2016 census, the most commonly nominated ancestries were: 
28.9% of the population was born overseas at the 2016 census. The five largest groups of overseas-born were from England (5.8%), India (1.6%), China (1.5%), Italy (1.1%) and Vietnam (0.9%).

2% of the population, or 34,184 people, identified as Indigenous Australians (Aboriginal Australians and Torres Strait Islanders) in 2016.

At the 2016 census, 78.2% of the population spoke only English at home. The other languages most commonly spoken at home were Italian (1.7%), Standard Mandarin (1.7%), Greek (1.4%) Vietnamese (1.1%), and Cantonese (0.6%).

At the 2016 census, overall 53.9% of responses identified some variant of Christianity. 9% of respondents chose not to state a religion. The most commonly nominated responses were 'No Religion' (35.4%), Catholicism (18%), Anglicanism (10%) and Uniting Church (7.1%).

On 1 January 2009, the school leaving age was raised to 17 (having previously been 15 and then 16). Education is compulsory for all children until age 17, unless they are working or undergoing other training. The majority of students stay on to complete their South Australian Certificate of Education (SACE). School education is the responsibility of the South Australian government, but the public and private education systems are funded jointly by it and the Commonwealth Government.

The South Australian Government provides, to schools on a per student basis, 89 percent of the total Government funding while the Commonwealth contributes 11 percent. Since the early 1970s it has been an ongoing controversy that 68 percent of Commonwealth funding (increasing to 75% by 2008) goes to private schools that are attended by 32% of the states students. Private schools often refute this by saying that they receive less State Government funding than public schools and in 2004 the main private school funding came from the Australian government, not the state government.

On 14 June 2013, South Australia became the third Australian state to sign up to the Australian Federal Government's Gonski Reform Program. This will see funding for primary and secondary education to South Australia increased by $1.1 billion before 2019.

There are three public and four private universities in South Australia. The three public universities are the University of Adelaide (established 1874, third oldest in Australia), Flinders University (est. 1966) and the University of South Australia (est. 1991). The four private universities are Torrens University Australia (est. 2013), Carnegie Mellon University - Australia (est. 2006), University College London's School of Energy and Resources (Australia), and Cranfield University. All six have their main campus in the Adelaide metropolitan area: Adelaide and UniSA on North Terrace in the city; CMU, UCL and Cranfield are co-located on Victoria Square in the city, and Flinders at Bedford Park.

Tertiary vocational education is provided by a range of Registered Training Organisations (RTOs) which are regulated at Commonwealth level. The range of RTOs delivering education include public, private and 'enterprise' providers i.e. employing organisations who run an RTO for their own employees or members.

The largest public provider of vocational education is TAFE South Australia which is made up of colleges throughout the state, many of these in rural areas, providing tertiary education to as many people as possible. In South Australia, TAFE is funded by the state government and run by the South Australian Department of Further Education, Employment, Science and Technology (DFEEST). Each TAFE SA campus provides a range of courses with its own specialisation.

After settlement, the major form of transport in South Australia was ocean transport. Limited land transport was provided by horses and bullocks. In the mid 19th century, the state began to develop a widespread rail network, although a coastal shipping network continued until the post war period.

Roads began to improve with the introduction of motor transport. By the late 19th century, road transport dominated internal transport in South Australia.

South Australia has four interstate rail connections, to Perth via the Nullarbor Plain, to Darwin through the centre of the continent, to New South Wales through Broken Hill, and to Melbourne–which is the closest capital city to Adelaide.

Rail transport is important for many mines in the north of the state.

The capital Adelaide has a commuter rail network made of electric and diesel electric powered multiple units, with 6 lines between them.

South Australia has extensive road networks linking towns and other states. Roads are also the most common form of transport within the major metropolitan areas with car transport predominating. Public transport in Adelaide is mostly provided by buses and trams with regular services throughout the day.

Adelaide Airport provides regular flights to other capitals, major South Australian towns and many international locations. The airport also has daily flights to several Asian hub airports. Adelaide Metro buses J1 and J1X connect to the city (approx. 30 minutes travel time). Standard fares apply and tickets may be purchased from the driver. Maximum charge (September 2016) for Metroticket is $5.30; off-peak and seniors discounts may apply.
The River Murray was formerly an important trade route for South Australia, with paddle steamers linking inland areas and the ocean at Goolwa.

South Australia has a container port at Port Adelaide. There are also numerous important ports along the coast for minerals and grains.

The passenger terminal at Port Adelaide periodically sees cruise liners.

Kangaroo Island is dependent on the Sea Link ferry service between Cape Jervis and Penneshaw.

South Australia has been known as "the Festival State" for many years, for its abundance of arts and gastronomic festivals. While much of the arts scene is concentrated in Adelaide, the state government has supported regional arts actively since the 1990s. One of the manifestations of this was the creation of Country Arts SA, created in 1992.

Diana Laidlaw did much to further the arts in South Australia during her term as Arts Minister from 1993 to 2002, and after Mike Rann assumed government in 2002, he created a strategic plan in 2004 (updated 2007) which included furthering and promoting the arts in South Australia under the topic heading "Objective 4:
Fostering Creativity and Innovation".

In September 2019, with the arts portfolio now subsumed within the Department of the Premier and Cabinet (DPC) after the election of Steven Marshall as Premier, and the 2004 strategic plan having been deleted from the website in 2018, the ""Arts and Culture Plan, South Australia 2019–2024"" was created by the Department. Marshall said when launching the plan: “The arts sector in South Australia is already very strong but it’s been operating without a plan for 20 years”. However the plan does not signal any new government support, even after the government’s cuts to arts funding when Arts South Australia was absorbed into DPC in 2018. Specific propoals within the plan include an “Adelaide in 100 Objects” walking tour, a new shared ticketing system for small to medium arts bodies, a five-year-plan to revitalise regional art centres, creation of an arts-focussed high school, and a new venue for the Adelaide Symphony Orchestra.

Australian rules football (AFL) is the most popular spectator sport in South Australia, with South Australians having the highest attendance rate in Australia.

South Australia fields two teams in the Australian Football League national competition: the Adelaide Football Club and Port Adelaide Football Club. As of 2015 the two clubs were in the top five in terms of membership numbers, with both clubs' membership figures reaching over 60,000. Both teams have used the Adelaide Oval as their home ground since 2014, having previously used Football Park (AAMI Stadium).

The South Australian National Football League, which was the premier league in the state before the advent of the Australian Football League, is a popular local league comprising ten teams (Sturt, Port Adelaide, Adelaide, West Adelaide, South Adelaide, North Adelaide, Norwood, Woodville/West Torrens, Glenelg and Central District).

The South Australian Amateur Football League comprises 68 member clubs playing over 110 matches per week across ten senior divisions and three junior divisions. The SAAFL is one of Australia's largest and strongest Australian rules football associations.

Cricket is the most popular summer sport in South Australia and attracts big crowds. South Australia has a cricket team, the West End Redbacks, who play at Adelaide Oval in the Adelaide Park Lands during the summer; they won their first title since 1996 in the summer of 2010–11. Many international matches have been played at the Adelaide Oval; it was one of the host cities of 2015 Cricket World Cup, and for many years it hosted the Australia Day One Day International. South Australia is also home to the Adelaide Strikers, an Australian men's professional Twenty20 cricket team, that competes in Australia's domestic Twenty20 cricket competition, the Big Bash League.

Adelaide United represents South Australia in soccer in the men's A-League and women's W-League. The club's home ground is Hindmarsh Stadium (Coopers Stadium), but occasionally plays games at the Adelaide Oval.

The club was founded in 2003 and are the 2015–16 season champions of the A-League. The club was also premier in the inaugural 2005–06 A-League season, finishing 7 points clear of the rest of the competition, before finishing 3rd in the finals. Adelaide United was also a Grand Finalist in the 2006–07 and 2008–09 seasons. Adelaide is the only A-League club to have progressed past the group stages of the Asian Champions League on more than one occasion.

Adelaide City remains South Australia's most successful club, having won three National Soccer League titles and three NSL Cups. City was the first side from South Australia to ever win a continental title when it claimed the 1987 Oceania Club Championship and it has also won a record 17 South Australian championships and 17 Federation Cups.

West Adelaide became the first South Australian club to be crowned Australian champion when it won the 1978 National Soccer League title. Like City, it now competes in the National Premier Leagues South Australia and the two clubs contest the Adelaide derby.

Basketball also has a big following in South Australia, with the Adelaide 36ers playing out of an 8,070 seat stadium in Findon. The 36ers have won four championships in the last 20 years in the National Basketball League. The Titanium Security Arena, located in Findon, is the home of basketball in the state.

Mount Gambier also has a national basketball team – the Mount Gambier Pioneers. The Pioneers play at the Icehouse (Mount Gambier Basketball Stadium) which seats over 1,000 people and is also home to the Mount Gambier Basketball Association. The Pioneers won the South Conference in 2003 and the Final in 2003; this team was rated second in the top five teams to have ever played in the league. In 2012, the club entered its 25th season, with a roster of 10 senior players (two imports) and three development squad players.

Australia's premier motor sport series, the Supercars Championship, has visited South Australia each year since 1999. South Australia's Supercars event, the Adelaide 500, is staged on the Adelaide Street Circuit, a temporary track laid out through the streets and parklands to the east of the Adelaide city centre. Attendance for the 2010 event totalled 277,800. An earlier version of the Adelaide Street Circuit played host to the Australian Grand Prix, a round of the FIA Formula One World Championship, each year from 1985 to 1995.

Mallala Motor Sport Park, a permanent circuit located near the town of Mallala, 58 km north of Adelaide, caters for both state and national level motor sport throughout the year.

The Bend Motorsport Park, is another permanent circuit, located just outside of Tailem Bend.

Sixty-three percent of South Australian children took part in organised sports in 2002–2003.

The ATP Adelaide was a tennis tournament held from 1972 to 2008 that then moved to Brisbane and was replaced with The World Tennis Challenge a Professional Exhibition Tournament that is part of the Australian Open Series. Also, the Royal Adelaide Golf Club has hosted nine editions of the Australian Open, with the most recent being in 1998.

The state has hosted the Tour Down Under cycle race since 1999.







</doc>
<doc id="26725" url="https://en.wikipedia.org/wiki?curid=26725" title="Slime mold">
Slime mold

Slime mold or slime mould is an informal name given to several kinds of unrelated eukaryotic organisms that can live freely as single cells, but can aggregate together to form multicellular reproductive structures. Slime molds were formerly classified as fungi but are no longer considered part of that kingdom. Although not forming a single monophyletic clade, they are grouped within the paraphyletic group referred to as kingdom Protista.

More than 900 species of slime mold occur globally. Their common name refers to part of some of these organisms' life cycles where they can appear as gelatinous "slime". This is mostly seen with the Myxogastria, which are the only macroscopic slime molds. Most slime molds are smaller than a few centimeters, but some species may reach sizes up to several square meters and masses up to 30 grams.

Many slime molds, mainly the "cellular" slime molds, do not spend most of their time in this state. When food is abundant, these slime molds exist as single-celled organisms. When food is in short supply, many of these single-celled organisms will congregate and start moving as a single body. In this state they are sensitive to airborne chemicals and can detect food sources. They can readily change the shape and function of parts, and may form stalks that produce fruiting bodies, releasing countless spores, light enough to be carried on the wind or hitch a ride on passing animals.

They feed on microorganisms that live in any type of dead plant material. They contribute to the decomposition of dead vegetation, and feed on bacteria, yeasts, and fungi. For this reason, slime molds are usually found in soil, lawns, and on the forest floor, commonly on deciduous logs. In tropical areas they are also common on inflorescences and fruits, and in aerial situations (e.g., in the canopy of trees). In urban areas, they are found on mulch or in the leaf mold in rain gutters, and also grow in air conditioners, especially when the drain is blocked.

Slime molds, as a group, are polyphyletic. They were originally represented by the subkingdom Gymnomycota in the Fungi kingdom and included the defunct phyla Myxomycota, Acrasiomycota, and Labyrinthulomycota. Today, slime molds have been divided among several supergroups, "none" of which is included in the kingdom Fungi.

Slime molds can generally be divided into two main groups.

In more strict terms, slime molds comprise the mycetozoan group of the amoebozoa. Mycetozoa include the following three groups:

Even at this level of classification there are conflicts to be resolved. Recent molecular evidence shows that, while the first two groups are likely to be monophyletic, the protosteloids are likely to be polyphyletic. For this reason, scientists are currently trying to understand the relationships among these three groups.

The most commonly encountered are the Myxogastria. A common slime mold that forms tiny brown tufts on rotting logs is "Stemonitis". Another form, which lives in rotting logs and is often used in research, is "Physarum polycephalum". In logs, it has the appearance of a slimy web-work of yellow threads, up to a few feet in size. "Fuligo" forms yellow crusts in mulch.

The "Dictyosteliida", cellular slime molds, are distantly related to the plasmodial slime molds and have a very different lifestyle. Their amoebae do not form huge coenocytes, and remain individual. They live in similar habitats and feed on microorganisms. When food is depleted and they are ready to form sporangia, they do something radically different. They release signal molecules into their environment, by which they find each other and create swarms. These amoeba then join up into a tiny multicellular slug-like coordinated creature, which crawls to an open lit place and grows into a fruiting body. Some of the amoebae become spores to begin the next generation, but some of the amoebae sacrifice themselves to become a dead stalk, lifting the spores up into the air.

The protosteloids have characters intermediate between the previous two groups, but they are much smaller, the fruiting bodies only forming one to a few spores.

Non-amoebozoan slime moulds include:

Slime molds begin life as amoeba-like cells. These unicellular amoebae are commonly haploid and feed on bacteria. These amoebae can mate if they encounter the correct mating type and form zygotes that then grow into plasmodia. These contain many nuclei without cell membranes between them, and can grow to meters in size. The species "Fuligo septica" is often seen as a slimy yellow network in and on rotting logs. The amoebae and the plasmodia engulf microorganisms. The plasmodium grows into an interconnected network of protoplasmic strands.

Within each protoplasmic strand, the cytoplasmic contents rapidly stream. If one strand is carefully watched for about 50 seconds, the cytoplasm can be seen to slow, stop, and then reverse direction. The streaming protoplasm within a plasmodial strand can reach speeds of up to 1.35 mm per second, which is the fastest rate recorded for any microorganism. Migration of the plasmodium is accomplished when more protoplasm streams to advancing areas and protoplasm is withdrawn from rear areas. When the food supply wanes, the plasmodium will migrate to the surface of its substrate and transform into rigid fruiting bodies. The fruiting bodies or sporangia are what are commonly seen. They superficially look like fungi or molds but are not related to the true fungi. These sporangia will then release spores which hatch into amoebae to begin the life cycle again.

Slime molds are isogamous organisms, which means their sex cells are all the same size. There are over 900 species of slime molds that exist today. "Physarum polycephalum" is one species that has three sex genes--"mat"A, "mat"B, and "mat"C. The first two types have thirteen separate variations. "Mat"C, however, only has three variations. Each sexually mature slime mold contains two copies of each of the three sex genes. When "Physarum polycephalum" is ready to make its sex cells, it grows a bulbous extension of its body to contain them. Each cell is created with a random combination of the genes that the slime mold contains within its genome. Therefore, it can create cells with up to eight different gene types. Once these cells are released, they are independent and tasked with finding another cell it is able to fuse with. Other "Physarum polycephalum" may contain different combinations of the "mat"A, "mat"B, and "mat"C genes, allowing over 500 possible variations. It is advantageous for organisms with this type of reproductive cells to have many sexes because the likelihood of the cells finding a partner is greatly increased. At the same time, the risk of inbreeding as drastically reduced.

"Dictyostelium discoideum" is another species of slime mold that has many different sexes. When this organism has entered the stage of reproduction, is releases an attractant, called "acrasin". Acrasin is made up of cyclic adenosine monophosphate, or cyclic AMP. Cyclic AMP is crucial in passing hormone signals between sex cells. When it comes time for the cells to fuse, "Dictyostelium discoideum" has mating types of its own that dictate which cells are compatible with each other. These include NC-4, WS 582, WS 583, WS 584, WS 5-1, WS 7, WS 10, WS 11-1, WS 28-1, WS 57-6, and WS 112b. A scientific study demonstrated the compatibility of these eleven mating types of "Dictyostelium discoideum" by monitoring the formation of macrocysts. For example, WS 583 is very compatible with WS 582, but not NC-4. It was concluded that cell contact between the compatible mating types needs to occur before macrocysts can form.

In "Myxogastria", the plasmodial portion of the life cycle only occurs after syngamy, which is the fusion of cytoplasm and nuclei of myxoamoebae or swarm cells. The diploid zygote becomes a multinucleated plasmodium through multiple nuclear divisions without further cell division. Myxomycete plasmodia are multinucleate masses of protoplasm that move by cytoplasmic streaming. In order for the plasmodium to move, cytoplasm must be diverted towards the leading edge from the lagging end. This process results in the plasmodium advancing in fan-like fronts. As it moves, plasmodium also gains nutrients through the phagocytosis of bacteria and small pieces of organic matter.

The plasmodium also has the ability to subdivide and establish separate plasmodia. Conversely, separate plasmodia that are genetically similar and compatible can fuse together to create a larger plasmodium. In the event that conditions become dry, the plasmodium will form a sclerotium, essentially a dry and dormant state. In the event that conditions become moist again the sclerotium absorbs water and an active plasmodium is restored. When the food supply wanes, the Myxomycete plasmodium will enter the next stage of its life cycle forming haploid spores, often in a well-defined sporangium or other spore-bearing structure.

When a slime mold mass or mound is physically separated, the cells find their way back to re-unite. Studies on "Physarum polycephalum" have even shown an ability to learn and predict periodic unfavorable conditions in laboratory experiments. John Tyler Bonner, a professor of ecology known for his studies of slime molds, argues that they are "no more than a bag of amoebae encased in a thin slime sheath, yet they manage to have various behaviours that are equal to those of animals who possess muscles and nerves with ganglia – that is, simple brains."

Atsushi Tero of Hokkaido University grew "Physarum" in a flat wet dish, placing the mold in a central position representing Tokyo and oat flakes surrounding it corresponding to the locations of other major cities in the Greater Tokyo Area. As "Physarum" avoids bright light, light was used to simulate mountains, water and other obstacles in the dish. The mold first densely filled the space with plasmodia, and then thinned the network to focus on efficiently connected branches. The network strikingly resembled Tokyo's rail system.

Slime mould "Physarum polycephalum" was also used by Andrew Adamatzky from the University of the West of England and his colleagues world-wide in experimental laboratory approximations of motorway networks of 14 geographical areas: Australia, Africa, Belgium, Brazil, Canada, China, Germany, Iberia, Italy, Malaysia, Mexico, the Netherlands, UK and USA.




</doc>
<doc id="26737" url="https://en.wikipedia.org/wiki?curid=26737" title="Substitution splice">
Substitution splice

The substitution splice or stop trick is a cinematic special effect in which filmmakers achieve an appearance, disappearance, or transformation by altering one or more selected aspects of the mise-en-scène between two shots
while maintaining the same framing and other aspects of the scene in both shots. The effect is usually polished by careful editing to establish a seamless cut and optimal moment of change. It has also been referred to as stop motion substitution or stop-action. 

The pioneering French filmmaker Georges Méliès claimed to have accidentally developed the stop trick, as he wrote in "Les Vues Cinématographiques" in 1907 (translated from French):

According to the film scholar Jacques Deslandes, it is more likely that Méliès discovered the trick by carefully examining a print of the Edison Manufacturing Company's 1895 film "The Execution of Mary Stuart", in which a primitive version of the trick appears. And certainly the trick was used in the widely seen 1903 film, "The Great Train Robbery", when a dummy is substituted for an actor in the coal car. In any case, the substitution splice was both the first special effect Méliès perfected, and the most important in his body of work.

Film historians such as Richard Abel and Elizabeth Ezra established that much of the effect was the result of Méliès's careful frame matching during the editing process, creating a seamless match cut out of two separately staged shots. Indeed, Méliès often used substitution splicing not as an obvious special effect, but as an inconspicuous editing technique, matching and combining short takes into one apparently seamless longer shot. Substitution splicing could become even more seamless when the film was colored by hand, as many of Méliès's films were; the addition of painted color acts as a sleight of hand technique allowing the cuts to pass by unnoticed.

The substitution splice was the most popular cinematic special effect in trick films and early film fantasies, especially those that evolved from the stage tradition of the "féerie". Segundo de Chomón is among the other filmmakers who used substitution splicing to create elaborate fantasy effects. D.W. Griffith's 1909 film "The Curtain Pole", starring Mack Sennett, used substitution splices for comedic effect. The transformations made possible by the substitution splice were so central to early fantasy films that, in France, such films were often described simply as "scènes à transformation".

This technique is different from the stop motion technique, in which the entire shot is created frame by frame.


</doc>
<doc id="26739" url="https://en.wikipedia.org/wiki?curid=26739" title="Stop">
Stop

Stop may refer to:














</doc>
<doc id="26740" url="https://en.wikipedia.org/wiki?curid=26740" title="Scandinavia">
Scandinavia

Scandinavia ( ) is a region in Northern Europe, with strong historical, cultural, and linguistic ties. The term "Scandinavia" in local usage covers the three kingdoms of Denmark, Norway, and Sweden. The majority national languages of these three belong to the Scandinavian dialect continuum, and are mutually intelligible North Germanic languages. In English usage, Scandinavia also sometimes refers to the Scandinavian Peninsula, or to the broader region including Finland and Iceland, which is always known locally as the Nordic countries.

While part of the Nordic countries, the remote Norwegian islands of Svalbard and Jan Mayen are not in Scandinavia; neither is Greenland, a constituent country within the Kingdom of Denmark. The Faroe Islands are sometimes included.

The name Scandinavia originally referred to the former Danish, now Swedish, region of Scania. "Scandinavia" and "Scandinavian" entered usage in the late 18th century, being introduced by the early linguistic and cultural Scandinavist movement. The majority of the population of Scandinavia are descended from several North Germanic tribes who originally inhabited the southern part of Scandinavia and spoke a Germanic language that evolved into Old Norse. Icelanders and the Faroese are to a significant extent descended from the Norse and are therefore often seen as Scandinavian. Finland is mainly populated by Finns, with a minority of approximately 5% of Swedish speakers. A small minority of Sami people live in the extreme north of Scandinavia. The Danish, Norwegian and Swedish languages form a dialect continuum and are known as the Scandinavian languages—all of which are considered mutually intelligible with one another. Faroese and Icelandic, sometimes referred to as insular Scandinavian languages, are intelligible in continental Scandinavian languages only to a limited extent. Finnish and Meänkieli are closely related to each other and more distantly to the Sami languages, but are entirely unrelated to the Scandinavian languages. Apart from these, German, Yiddish and Romani are recognized minority languages in parts of Scandinavia.

"Scandinavia" refers to Denmark, Norway and Sweden. Some sources argue for the inclusion of the Faroe Islands, Finland and Iceland, though that broader region is usually known in the countries concerned as "Norden", or the Nordic countries.

The use of "Scandinavia" as a convenient general term for Denmark, Norway and Sweden is fairly recent. According to some historians, it was adopted and introduced in the eighteenth century, at a time when the ideas about a common heritage started to appear and develop into early literary and linguistic Scandinavism. Before this time, the term "Scandinavia" was familiar mainly to classical scholars through Pliny the Elder's writings and was used vaguely for Scania and the southern region of the peninsula.

As a political term, "Scandinavia" was first used by students agitating for pan-Scandinavianism in the 1830s. The popular usage of the term in Sweden, Denmark and Norway as a unifying concept became established in the nineteenth century through poems such as Hans Christian Andersen's "I am a Scandinavian" of 1839. After a visit to Sweden, Andersen became a supporter of early political Scandinavism. In a letter describing the poem to a friend, he wrote: "All at once I understood how related the Swedes, the Danes and the Norwegians are, and with this feeling I wrote the poem immediately after my return: 'We are one people, we are called Scandinavians!'".

The clearest example of the use of the term "Scandinavia" as a political and societal construct is the unique position of Finland, based largely on the fact that most of modern-day Finland was part of Sweden for more than six centuries (see: Finland under Swedish rule), thus to much of the world associating Finland with all of Scandinavia. But the creation of a Finnish identity is unique in the region in that it was formed in relation to two different imperial models, the Swedish and the Russian.

The term is often defined according to the conventions of the cultures that lay claim to the term in their own use. When a speaker wants to explicitly include Finland alongside Scandinavia-proper, the geographic terms Fenno-Scandinavia or Fennoscandia are sometimes used in English, although these terms are hardly if at all used within Scandinavia.

Various promotional agencies of the Nordic countries in the United States (such as The American-Scandinavian Foundation, established in 1910 by the Danish American industrialist Niels Poulsen) serve to promote market and tourism interests in the region. Today, the five Nordic heads of state act as the organization's patrons and according to the official statement by the organization its mission is "to promote the Nordic region as a whole while increasing the visibility of Denmark, Finland, Iceland, Norway and Sweden in New York City and the United States". The official tourist boards of Scandinavia sometimes cooperate under one umbrella, such as the Scandinavian Tourist Board. The cooperation was introduced for the Asian market in 1986, when the Swedish national tourist board joined the Danish national tourist board to coordinate intergovernmental promotion of the two countries. Norway's government entered one year later. All five Nordic governments participate in the joint promotional efforts in the United States through the Scandinavian Tourist Board of North America.

While the term "Scandinavia" is commonly used for Denmark, Norway and Sweden, the term "Nordic countries" is used unambiguously for Denmark, Norway, Sweden, Finland and Iceland, including their associated territories (Svalbard, Greenland, the Faroe Islands and the Åland Islands). Scandinavia can thus be considered a subset of the Nordic countries. Furthermore, the term Fennoscandia refers to Scandinavia, Finland and Karelia, excluding Denmark and overseas territories, but the usage of this term is restricted to geology when speaking of the Fennoscandian Shield (Baltic Shield).

In addition to the mainland Scandinavian countries of:
The Nordic countries also consist of:

Whereas the term "Scandinavia" is relatively straightforward as traditionally relating to the three kingdoms of Denmark, Norway and Sweden there exists some ambiguity as regards the ethnic aspect of the concept in the modern era. Traditionally, the term refers specifically to the majority peoples of Denmark, Norway and Sweden, their states, their Germanic languages and their culture. In the modern era, the term will often include minority peoples such as the Sami and Meänkieli speakers in a political and to some extent cultural sense as they are citizens of Scandinavian countries and speak Scandinavian languages either as their first or second language. However, Scandinavian is still also seen as an ethnic term for the Germanic majority peoples of Scandinavia and as such the inclusion of Sami and Finnish speakers can be seen as controversial within these groups.

Scandinavia and Scania ("Skåne", the southernmost province of Sweden) are thought to go back to the Proto-Germanic compound *"Skaðin-awjō" (the edh represented in Latin by t or d), which appears later in Old English as "Scedenig" and in Old Norse as "Skáney". The earliest identified source for the name Scandinavia is Pliny the Elder's "Natural History", dated to the first century AD.

Various references to the region can also be found in Pytheas, Pomponius Mela, Tacitus, Ptolemy, Procopius and Jordanes, usually in the form of Scandza. It is believed that the name used by Pliny may be of West Germanic origin, originally denoting Scania. According to some scholars, the Germanic stem can be reconstructed as *"skaðan-" and meaning "danger" or "damage". The second segment of the name has been reconstructed as *"awjō", meaning "land on the water" or "island". The name Scandinavia would then mean "dangerous island", which is considered to refer to the treacherous sandbanks surrounding Scania. Skanör in Scania, with its long Falsterbo reef, has the same stem ("skan") combined with -"ör", which means "sandbanks".

The Old Norse goddess name "Skaði", along with "Sca(n)dinavia" and "Skáney", may be related to Proto-Germanic "*skaðwa-" (meaning "shadow"). Scholar John McKinnell comments that this etymology suggests that the goddess Skaði may have once been a personification of the geographical region of Scandinavia or associated with the underworld.

Pliny's descriptions of "Scatinavia" and surrounding areas are not always easy to decipher. Writing in the capacity of a Roman admiral, he introduces the northern region by declaring to his Roman readers that there are 23 islands ""Romanis armis cognitae"" ("known to Roman arms") in this area. According to Pliny, the ""clarissima"" ("most famous") of the region's islands is "Scatinavia", of unknown size. There live the Hilleviones. The belief that Scandinavia was an island became widespread among classical authors during the first century and dominated descriptions of Scandinavia in classical texts during the centuries that followed.

Pliny begins his description of the route to "Scatinavia" by referring to the mountain of Saevo ("mons Saevo ibi"), the Codanus Bay ("Codanus sinus") and the Cimbrian promontory. The geographical features have been identified in various ways. By some scholars, "Saevo" is thought to be the mountainous Norwegian coast at the entrance to Skagerrak and the Cimbrian peninsula is thought to be Skagen, the north tip of Jutland, Denmark. As described, Saevo and Scatinavia can also be the same place.

Pliny mentions Scandinavia one more time: in Book VIII he says that the animal called "achlis" (given in the accusative, "achlin", which is not Latin) was born on the island of Scandinavia. The animal grazes, has a big upper lip and some mythical attributes.

The name "Scandia", later used as a synonym for Scandinavia, also appears in Pliny's "Naturalis Historia" ("Natural History"), but is used for a group of Northern European islands which he locates north of Britannia. "Scandia" thus does not appear to be denoting the island Scadinavia in Pliny's text. The idea that ""Scadinavia"" may have been one of the ""Scandiae"" islands was instead introduced by Ptolemy (c. 90 – c. 168 AD), a mathematician, geographer and astrologer of Roman Egypt. He used the name ""Skandia"" for the biggest, most easterly of the three ""Scandiai"" islands, which according to him were all located east of Jutland.

Neither Pliny's nor Ptolemy's lists of Scandinavian tribes include the Suiones mentioned by Tacitus. Some early Swedish scholars of the Swedish Hyperborean school and of the 19th-century romantic nationalism period proceeded to synthesize the different versions by inserting references to the Suiones, arguing that they must have been referred to in the original texts and obscured over time by spelling mistakes or various alterations.

The Latin names in Pliny's text gave rise to different forms in medieval Germanic texts. In Jordanes' history of the Goths (AD 551), the form "Scandza" is the name used for their original home, separated by sea from the land of Europe (chapter 1, 4). Where Jordanes meant to locate this quasi-legendary island is still a hotly debated issue, both in scholarly discussions and in the nationalistic discourse of various European countries. The form "Scadinavia" as the original home of the Langobards appears in Paulus Diaconus' "Historia Langobardorum", but in other versions of "Historia Langobardorum" appear the forms "Scadan", "Scandanan", "Scadanan" and "Scatenauge". Frankish sources used "Sconaowe" and Aethelweard, an Anglo-Saxon historian, used "Scani". In "Beowulf", the forms "Scedenige" and "Scedeland" are used while the Alfredian translation of Orosius and Wulfstan's travel accounts used the Old English "Sconeg".

The earliest Sami yoik texts written down refer to the world as "Skadesi-suolo" (north Sami) and "Skađsuâl" (east Sami), meaning "Skaði's island". Svennung considers the Sami name to have been introduced as a loan word from the North Germanic languages; "Skaði" is the giant stepmother of Freyr and Freyja in Norse mythology. It has been suggested that Skaði to some extent is modeled on a Sami woman. The name for Skade's father Thjazi is known in Sami as "Čáhci", "the waterman"; and her son with Odin, Saeming, can be interpreted as a descendant of "Saam" the Sami population. Older joik texts give evidence of the old Sami belief about living on an island and state that the wolf is known as "suolu gievra", meaning "the strong one on the island". The Sami place name "Sulliidčielbma" means "the island's threshold" and "Suoločielgi" means "the island's back".

In recent substrate studies, Sami linguists have examined the initial cluster sk- in words used in Sami and concluded that sk- is a phonotactic structure of alien origin.

Another possibility is that all or part of the segments of the name came from the Mesolithic people inhabiting the region. In modernity, Scandinavia is a peninsula, but between approximately 10,300 and 9,500 years ago the southern part of Scandinavia was an island separated from the northern peninsula, with water exiting the Baltic Sea through the area where Stockholm is now located.

Some Basque scholars have presented the idea that the segment "sk" that appears in "*Skaðinawjō" is connected to the name for the Euzko peoples, akin to Basques, that populated Paleolithic Europe. According to one scholar, Scandinavian people share particular genetic markers with the Basque people.

The geography of Scandinavia is extremely varied. Notable are the Norwegian fjords, the Scandinavian Mountains, the flat, low areas in Denmark and the archipelagos of Sweden and Norway. Sweden has many lakes and moraines, legacies of the ice age, which ended about ten millennia ago.

The southern and by far most populous regions of Scandinavia have a temperate climate. Scandinavia extends north of the Arctic Circle, but has relatively mild weather for its latitude due to the Gulf Stream. Many of the Scandinavian mountains have an alpine tundra climate.

The climate varies from north to south and from west to east: a marine west coast climate () typical of western Europe dominates in Denmark, southernmost part of Sweden and along the west coast of Norway reaching north to 65°N, with orographic lift giving more mm/year precipitation (<5000 mm) in some areas in western Norway. The central part – from Oslo to Stockholm – has a humid continental climate (Dfb), which gradually gives way to subarctic climate (Dfc) further north and cool marine west coast climate (Cfc) along the northwestern coast. A small area along the northern coast east of the North Cape has tundra climate (Et) as a result of a lack of summer warmth. The Scandinavian Mountains block the mild and moist air coming from the southwest, thus northern Sweden and the Finnmarksvidda plateau in Norway receive little precipitation and have cold winters. Large areas in the Scandinavian mountains have alpine tundra climate.

The warmest temperature ever recorded in Scandinavia is 38.0 °C in Målilla (Sweden). The coldest temperature ever recorded is −52.6 °C in Vuoggatjålme (Sweden). The coldest month was February 1985 in Vittangi (Sweden) with a mean of −27.2 °C.

Southwesterly winds further warmed by foehn wind can give warm temperatures in narrow Norwegian fjords in winter. Tafjord has recorded 17.9 °C in January and Sunndal 18.9 °C in February.

Two language groups have coexisted on the Scandinavian peninsula since prehistory—the North Germanic languages (Scandinavian languages) and the Sami languages. Due to later migrations, Finnish and Yiddish have also been spoken for over a hundred years. Denmark also has a minority of German-speakers. More recent migrations has added even more languages. Apart from Sami and the languages of minority groups speaking a variant of the majority language of a neighboring state, the following minority languages in Scandinavia are protected under the European Charter for Regional or Minority Languages: Yiddish, Romani Chib/Romanes and Romani.

The North Germanic languages of Scandinavia are traditionally divided into an East Scandinavian branch (Danish and Swedish) and a West Scandinavian branch (Norwegian, Icelandic and Faroese), but because of changes appearing in the languages since 1600 the East Scandinavian and West Scandinavian branches are now usually reconfigured into Insular Scandinavian ("ö-nordisk"/"øy-nordisk") featuring Icelandic and Faroese and Continental Scandinavian ("Skandinavisk"), comprising Danish, Norwegian and Swedish.

The modern division is based on the degree of mutual comprehensibility between the languages in the two branches. The populations of the Scandinavian countries, with common Scandinavian roots in language, can—at least with some training—understand each other's standard languages as they appear in print and are heard on radio and television.

The reason Danish, Swedish and the two official written versions of Norwegian ("Nynorsk" and "Bokmål") are traditionally viewed as different languages, rather than dialects of one common language, is that each is a well-established standard language in its respective country.

Danish, Swedish and Norwegian have since medieval times been influenced to varying degrees by Middle Low German and standard German. That influence came from not just proximity but also that Denmark and later Denmark-Norway ruling over the German speaking region of Holstein, and in Sweden with its close trade with the Hanseatic League.

Norwegians are accustomed to variation and may perceive Danish and Swedish only as slightly more distant dialects. This is because they have two official written standards, in addition to the habit of strongly holding on to local dialects. The people of Stockholm, Sweden and Copenhagen, Denmark have the greatest difficulty in understanding other Scandinavian languages. In the Faroe Islands and Iceland, learning Danish is mandatory. This causes Faroese people as well as Icelandic people to become bilingual in two very distinct North Germanic languages, making it relatively easy for them to understand the other two Mainland Scandinavian languages.

Although Iceland was under the political control of Denmark until a much later date (1918), very little influence and borrowing from Danish has occurred in the Icelandic language. Icelandic remained the preferred language among the ruling classes in Iceland. Danish was not used for official communications, most of the royal officials were of Icelandic descent and the language of the church and law courts remained Icelandic.

The Scandinavian languages are (as a language family) unrelated to Finnish, Estonian and Sami languages, which as Uralic languages are distantly related to Hungarian. Owing to the close proximity, there is still a great deal of borrowing from the Swedish and Norwegian languages in the Finnish and Sami languages. The long history of linguistic influence of Swedish on Finnish is also due to the fact that Finnish, the language of the majority in Finland, was treated as a minority language while Finland was part of Sweden. Finnish-speakers had to learn Swedish in order to advance to higher positions. Swedish spoken in today's Finland includes a lot of words that are borrowed from Finnish, whereas the written language remains closer to that of Sweden.

Finland is officially bilingual, with Finnish and Swedish having mostly the same status at national level. Finland's majority population are Finns, whose mother tongue is either Finnish (approximately 95%), Swedish or both. The Swedish-speakers live mainly on the coastline starting from approximately the city of Porvoo (in the Gulf of Finland) up to the city of Kokkola (in the Bay of Bothnia). The Åland Islands, an autonomous province of Finland situated in the Baltic Sea between Finland and Sweden, are entirely Swedish-speaking. Children are taught the other official language at school: for Swedish-speakers this is Finnish (usually from the 3rd grade), while for Finnish-speakers it is Swedish (usually from the 3rd, 5th or 7th grade).

Finnish speakers constitute a language minority in Sweden and Norway. Meänkieli and Kven are Finnish dialects spoken in Swedish Lapland and Norwegian Lapland.

The Sami languages are indigenous minority languages in Scandinavia. They belong to their own branch of the Uralic language family and are unrelated to the North Germanic languages other than by limited grammatical (particularly lexical) characteristics resulting from prolonged contact. Sami is divided into several languages or dialects. Consonant gradation is a feature in both Finnish and northern Sami dialects, but it is not present in south Sami, which is considered to have a different language history. According to the Sami Information Centre of the Sami Parliament in Sweden, southern Sami may have originated in an earlier migration from the south into the Scandinavian peninsula.

The economies of the countries of Scandinavia are amongst the highest in Europe. There is a generous welfare system in Sweden, Denmark, Norway and Finland.

During a period of Christianization and state formation in the 10th–13th centuries, numerous Germanic petty kingdoms and chiefdoms were unified into three kingdoms:

The three Scandinavian kingdoms joined in 1387 in the Kalmar Union under Queen Margaret I of Denmark. Sweden left the union in 1523 under King Gustav Vasa. In the aftermath of Sweden's secession from the Kalmar Union, civil war broke out in Denmark and Norway—the Protestant Reformation followed. When things had settled, the Norwegian Privy Council was abolished—it assembled for the last time in 1537. A personal union, entered into by the kingdoms of Denmark and Norway in 1536, lasted until 1814. Three sovereign successor states have subsequently emerged from this unequal union: Denmark, Norway and Iceland.

The borders between the three countries got the shape they have had since in the middle of the seventeenth century: In the 1645 Treaty of Brömsebro, Denmark–Norway ceded the Norwegian provinces of Jämtland, Härjedalen and Idre and Särna, as well as the Baltic Sea islands of Gotland and Ösel (in Estonia) to Sweden. The Treaty of Roskilde, signed in 1658, forced Denmark–Norway to cede the Danish provinces Scania, Blekinge, Halland, Bornholm and the Norwegian provinces of Båhuslen and Trøndelag to Sweden. The 1660 Treaty of Copenhagen forced Sweden to return Bornholm and Trøndelag to Denmark–Norway, and to give up its recent claims to the island Funen.

In the east, Finland, was a fully incorporated part of Sweden since medieval times until the Napoleonic wars, when it was ceded to Russia. Despite many wars over the years since the formation of the three kingdoms, Scandinavia has been politically and culturally close.

Denmark–Norway as a historiographical name refers to the former political union consisting of the kingdoms of Denmark and Norway, including the Norwegian dependencies of Iceland, Greenland and the Faroe Islands. The corresponding adjective and demonym is Dano-Norwegian. During Danish rule, Norway kept its separate laws, coinage and army as well as some institutions such as a royal chancellor. Norway's old royal line had died out with the death of Olav IV in 1387, but Norway's remaining a hereditary kingdom became an important factor for the Oldenburg dynasty of Denmark–Norway in its struggles to win elections as kings of Denmark.

The Treaty of Kiel (14 January 1814) formally dissolved the Dano-Norwegian union and ceded the territory of Norway proper to the King of Sweden, but Denmark retained Norway's overseas possessions. However, widespread Norwegian resistance to the prospect of a union with Sweden induced the governor of Norway, crown prince Christian Frederick (later Christian VIII of Denmark), to call a constituent assembly at Eidsvoll in April 1814. The assembly drew up a liberal constitution and elected Christian Frederick to the throne of Norway. Following a Swedish invasion during the summer, the peace conditions of the Convention of Moss (14 August 1814) specified that king Christian Frederik had to resign, but Norway would keep its independence and its constitution within a personal union with Sweden. Christian Frederik formally abdicated on 10 August 1814 and returned to Denmark. The Norwegian parliament Storting elected king Charles XIII of Sweden as king of Norway on 4 November.

The Storting dissolved the union between Sweden and Norway in 1905, after which the Norwegians elected Prince Charles of Denmark as king of Norway: he reigned as Haakon VII.

The influence of Scandinavism as a Scandinavist political movement was in the middle of the nineteenth century between the First Schleswig War (1848–1850) and the Second Schleswig War (1864).

The Swedish king also proposed a unification of Denmark, Norway and Sweden into a single united kingdom. The background for the proposal was the tumultuous events during the Napoleonic Wars in the beginning of the century. This war resulted in Finland (formerly the eastern third of Sweden) becoming the Russian Grand Duchy of Finland in 1809 and Norway ("de jure" in union with Denmark since 1387, although "de facto" treated as a province) becoming independent in 1814, but thereafter swiftly forced to accept a personal union with Sweden. The dependent territories Iceland, the Faroe Islands and Greenland, historically part of Norway, remained with Denmark in accordance with the Treaty of Kiel. Sweden and Norway were thus united under the Swedish monarch, but Finland's inclusion in the Russian Empire excluded any possibility for a political union between Finland and any of the other Nordic countries.

The end of the Scandinavian political movement came when Denmark was denied the military support promised from Sweden and Norway to annex the (Danish) Duchy of Schleswig, which together with the (German) Duchy of Holstein had been in personal union with Denmark. The Second war of Schleswig followed in 1864, a brief but disastrous war between Denmark and Prussia (supported by Austria). Schleswig-Holstein was conquered by Prussia and after Prussia's success in the Franco-Prussian War a Prussian-led German Empire was created and a new power-balance of the Baltic sea countries was established. The Scandinavian Monetary Union, established in 1873, lasted until World War I.





</doc>
<doc id="26741" url="https://en.wikipedia.org/wiki?curid=26741" title="Stockholm">
Stockholm

Stockholm () is the capital and most populous urban area of Sweden. 972,647 people live in the municipality, approximately 1.6 million in the urban area, and 2.4 million in the metropolitan area. The city stretches across fourteen islands where Lake Mälaren flows into the Baltic Sea. Outside the city to the east, and along the coast, is the island chain of the Stockholm archipelago. The area has been settled since the Stone Age, in the 6th millennium BC, and was founded as a city in 1252 by Swedish statesman Birger Jarl. It is also the county seat of Stockholm County.

Stockholm is the cultural, media, political, and economic centre of Sweden. The Stockholm region alone accounts for over a third of the country's GDP, and is among the top 10 regions in Europe by GDP per capita. It is an important global city, and the main centre for corporate headquarters in the Nordic region. The city is home to some of Europe's top ranking universities, such as the Stockholm School of Economics, Karolinska Institute and KTH Royal Institute of Technology. It hosts the annual Nobel Prize ceremonies and banquet at the Stockholm Concert Hall and Stockholm City Hall. One of the city's most prized museums, the Vasa Museum, is the most visited non-art museum in Scandinavia. The Stockholm metro, opened in 1950, is well known for the decor of its stations; it has been called the longest art gallery in the world. Sweden's national football arena is located north of the city centre, in Solna. Ericsson Globe, the national indoor arena, is in the southern part of the city. The city was the host of the 1912 Summer Olympics, and hosted the equestrian portion of the 1956 Summer Olympics otherwise held in Melbourne, Victoria, Australia.

Stockholm is the seat of the Swedish government and most of its agencies, including the highest courts in the judiciary, and the official residencies of the Swedish monarch and the Prime Minister. The government has its seat in the Rosenbad building, the Riksdag (Swedish parliament) is seated in the Parliament House, and the Prime Minister's residence is adjacent at Sager House. Stockholm Palace is the official residence and principal workplace of the Swedish monarch, while Drottningholm Palace, a World Heritage Site on the outskirts of Stockholm, serves as the Royal Family's private residence.

After the Ice Age, around 8,000 BC, there were already many people living in what is today the Stockholm area, but as temperatures dropped, inhabitants moved south. Thousands of years later, as the ground thawed, the climate became tolerable and the lands became fertile, people began to migrate back to the North. At the intersection of the Baltic Sea and lake Mälaren is an archipelago site where the Old Town of Stockholm was first built from about 1000 CE by Vikings. They had a positive trade impact on the area because of the trade routes they created.

Stockholm's location appears in Norse sagas as Agnafit, and in Heimskringla in connection with the legendary king Agne. The earliest written mention of the name Stockholm dates from 1252, by which time the mines in Bergslagen made it an important site in the iron trade. The first part of the name () means log in Swedish, although it may also be connected to an old German word () meaning fortification. The second part of the name () means islet, and is thought to refer to the islet Helgeandsholmen in central Stockholm. According to the "Eric Chronicles" the city is said to have been founded by Birger Jarl to protect Sweden from sea invasions made by Karelians after the pillage of Sigtuna on Lake Mälaren in the summer of 1187.

Stockholm's core, the present Old Town () was built on the central island next to Helgeandsholmen from the mid-13th century onward. The city originally rose to prominence as a result of the Baltic trade of the Hanseatic League. Stockholm developed strong economic and cultural linkages with Lübeck, Hamburg, Gdańsk, Visby, Reval, and Riga during this time. Between 1296 and 1478 Stockholm's City Council was made up of 24 members, half of whom were selected from the town's German-speaking burghers.

The strategic and economic importance of the city made Stockholm an important factor in relations between the Danish Kings of the Kalmar Union and the national independence movement in the 15th century. The Danish King Christian II was able to enter the city in 1520. On 8 November 1520, a massacre of opposition figures called the Stockholm Bloodbath took place and set off further uprisings that eventually led to the breakup of the Kalmar Union. With the accession of Gustav Vasa in 1523 and the establishment of royal power, the population of Stockholm began to grow, reaching 10,000 by 1600.

The 17th century saw Sweden grow into a major European power, reflected in the development of the city of Stockholm. From 1610 to 1680 the population multiplied sixfold. In 1634, Stockholm became the official capital of the Swedish empire. Trading rules were also created that gave Stockholm an essential monopoly over trade between foreign merchants and other Swedish and Scandinavian territories. In 1697, Tre Kronor (castle) burned and was replaced by Stockholm Palace.

In 1710, a plague killed about 20,000 (36 percent) of the population. After the end of the Great Northern War the city stagnated. Population growth halted and economic growth slowed. The city was in shock after having lost its place as the capital of a Great power. However, Stockholm maintained its role as the political center of Sweden and continued to develop culturally under Gustav III.

By the second half of the 19th century, Stockholm had regained its leading economic role. New industries emerged and Stockholm was transformed into an important trade and service center as well as a key gateway point within Sweden. The population also grew dramatically during this time, mainly through immigration. At the end of the 19th century, less than 40% of the residents were Stockholm-born. Settlement began to expand outside the city limits. The 19th century saw the establishment of a number of scientific institutes, including the Karolinska Institutet. The General Art and Industrial Exposition was held in 1897. From 1887 to 1953 the Old Stockholm telephone tower was a landmark; originally built to link phone lines, it became redundant after these were buried, and it was latterly used for advertising.
Stockholm became a modern, technologically advanced, and ethnically diverse city in the latter half of the 20th century. Many historical buildings were torn down during the modernist era, including substantial parts of the historical district of Klara, and replaced with modern architecture. However, in many other parts of Stockholm (such as in Gamla stan, Södermalm, Östermalm, Kungsholmen and Vasastan), many "old" buildings, blocks and streets built before the modernism and functionalism movements took off in Sweden (around 1930–35) survived this era of demolition. Throughout the century, many industries shifted away from industrial activities into more high-tech and service industry areas.

Currently, Stockholm's metropolitan area is one of the fastest-growing regions in Europe, and its population is expected to number 2.5 million by 2024. As a result of this massive population growth, there has been a proposal to build densely packed high-rise buildings in the city center connected by elevated walkways.

Stockholm is located on Sweden's east coast, where the freshwater Lake Mälaren — Sweden's third-largest lake — flows out into the Baltic Sea. The central parts of the city consist of fourteen islands that are continuous with the Stockholm archipelago. The geographical city center is situated on the water, in Riddarfjärden bay. Over 30% of the city area is made up of waterways and another 30% is made up of parks and green spaces.

Positioned at the eastern end of the Central Swedish lowland, the city's location reflect the early orientation of Swedish trade toward the Baltic region.

Stockholm belongs to the Temperate deciduous forest biome, which means the climate is very similar to that of the far northeastern area of the United States and coastal Nova Scotia in Canada. The average annual temperature is . The average rainfall is a year. The deciduous forest has four distinct seasons, spring, summer, autumn, and winter. In the autumn the leaves change color. During the winter months, the trees lose their leaves.

For details about the other municipalities in the Stockholm area, see the pertinent articles. North of Stockholm Municipality: Järfälla, Solna, Täby, Sollentuna, Lidingö, Upplands Väsby, Österåker, Sigtuna, Sundbyberg, Danderyd, Vallentuna, Ekerö, Upplands-Bro, Vaxholm, and Norrtälje. South of Stockholm: Huddinge, Nacka, Botkyrka, Haninge, Tyresö, Värmdö, Södertälje, Salem, Nykvarn and Nynäshamn.

Stockholm Municipality is an administrative unit defined by geographical borders. The semi-official name for the municipality is "City of Stockholm" ("Stockholms stad" in Swedish). As a municipality, the City of Stockholm is subdivided into district councils, which carry responsibility for primary schools, social, leisure and cultural services within their respective areas. The municipality is usually described in terms of its three main parts: Innerstaden (Stockholm City Centre), Söderort (Southern Stockholm) and Västerort (Western Stockholm). The districts of these parts are:


The modern centre Norrmalm (concentrated around the town square Sergels torg) is the largest shopping district in Sweden. It is the most central part of Stockholm in business and shopping.

Stockholm has an oceanic climate (Köppen: "Cfb") with humid continental ("Dfb") influences. Although winters are cold, average temperatures generally remain above 0 °C for much of the year. Summers are mild, and precipitation occurs throughout the year.

Due to the city's high northerly latitude, the length of the day varies widely from more than 18 hours around midsummer to only around 6 hours in late December. The nights from late May until mid-July are bright even when cloudy. Stockholm has relatively mild weather compared to other locations at a similar latitude, or even farther south. With an average of just over 1800 hours of sunshine per year, it is also one of the sunniest cities in Northern Europe, receiving more sunshine than Paris, London and a few other major European cities of a more southerly latitude. Because of the urban heat island effect and the prevailing wind traveling overland rather than sea during summer months, Stockholm has the warmest July months of the Nordic capitals. Stockholm has an annual average snow cover between 75 and 100 days.

In spite of its mild climate, Stockholm is located further north than parts of Canada that are above the Arctic tree line at sea level.

Summers average daytime high temperatures of and lows of around , but temperatures can reach on some days. Days above occur on average 1.55 days per year (1992–2011). Days between and are relatively common especially in July and August. Night-time lows of above are rare, and hot summer nights vary from . Winters generally bring cloudy weather with the most precipitation falling in December and January (as rain or as snow). The average winter temperatures range from , and occasionally drop below in the outskirts. Spring and autumn are generally cool to mild.

The climate table below presents weather data from the years 1981–2010 although the official Köppen reference period was from 1961–1990. According to ongoing measurements, the temperature has increased during the years 1991–2009 as compared with the last series. This increase averages about overall months. Warming is most pronounced during the winter months, with an increase of more than in January. For the 2002–2014 measurements some further increases have been found, although some months such as June have been relatively flat.

The highest temperature ever recorded in Stockholm was on 3 July 1811; the lowest was on 20 January 1814. The temperature has not dropped to below since 10 January 1987.

Annual precipitation is with around 170 wet days and light to moderate rainfall throughout the year. The precipitation is not uniformly distributed throughout the year. The second half of the year receives 50% more than the first half. Snowfall occurs mainly from December through March. Snowfall may occasionally occur in late October as well as in April.

In Stockholm, the aurora borealis can occasionally be observed.
Stockholm's location just south of the 60th parallel north means that the number of daylight hours is relatively small during winter – about six hours – while in June and the first half of July, the nights are relatively short, with about 18 hours of daylight. Around the summer solstice the sun never reaches further below the horizon than 7.3 degrees. This gives the sky a bright blue colour in summer once the sun has set, because it does not get any darker than nautical twilight. Also, when looking straight up towards the zenith, few stars are visible after the sun has gone down. This is not to be confused with the midnight sun, which occurs north of the Arctic Circle, around 7 degrees farther north.

The Stockholm Municipal Council () is the name of the local assembly. Its 101 councillors are elected concurrently with general elections, held at the same time as the elections to the Riksdag and county councils. The Council convenes twice every month at Stockholm City Hall, and the meetings are open to the public. The matters on which the councillors decide have generally already been drafted and discussed by various boards and committees. Once decisions are referred for practical implementation, the employees of the City administrations and companies take over.

The elected majority has a Mayor and eight Vice Mayors. The Mayor and each majority Vice Mayor is a head of a department, with responsibility for a particular area of operation, such as City Planning. The opposition also has four Vice Mayors, but they hold no executive power. Together the Mayor and the 12 Vice Mayors form the Council of Mayors, and they prepare matters for the City Executive Board. The Mayor holds a special position among the Vice Mayors, chairing both the Council of Mayors and the City Executive Board.

The City Executive Board () is elected by the City Council and can be thought of as the equivalent of a cabinet. The City Executive Board renders an opinion in all matters decided by the Council and bears the overall responsibility for follow-up, evaluation and execution of its decisions. The Board is also responsible for financial administration and long-term development. The City Executive Board consists of 13 members, who represent both the majority and the opposition. Its meetings are not open to the public.

Following the 2018 Stockholm municipal election a majority of seats in the municipal council is at present held by a center/right-wing majority and the Mayor of Stockholm () is Anna Konig Jerlmyr from the Moderate Party.

The vast majority of Stockholm residents work in the service industry, which accounts for roughly 85% of jobs in Stockholm. The almost total absence of heavy industry (and fossil fuel power plants) makes Stockholm one of the world's cleanest metropolises. The last decade has seen a significant number of jobs created in high technology companies. Large employers include IBM, Ericsson, and Electrolux. A major IT centre is located in Kista, in northern Stockholm.

Stockholm is Sweden's financial centre. Major Swedish banks, such as Swedbank, Handelsbanken, and SEB, are headquartered in Stockholm, as are the major insurance companies Skandia, Folksam and Trygg-Hansa. Stockholm is also home to Sweden's foremost stock exchange, the Stockholm Stock Exchange ("Stockholmsbörsen"). Additionally, about 45% of Swedish companies with more than 200 employees are headquartered in Stockholm. Noted clothes retailer H&M is also headquartered in the city. In recent years, tourism has played an important part in the city's economy. Stockholm County is ranked as the 10th largest visitor destination in Europe, with over 10 million commercial overnight stays per year. Among 44 European cities Stockholm had the 6th highest growth in number of nights spent in the period 2004–2008.

The largest companies in Stockholm, by number of employees (2017)

The city-owned company Stokab started in 1994 to build a fiber-optic network throughout the municipality as a level playing field for all operators (City of Stockholm, 2011). Around a decade later, the network was long making it the longest optic fiber network in the world and now has over 90 operators and 450 enterprises as customers. 2011 was the final year of a three-year project which brought fiber to 100% of public housing, meaning an extra 95,000 houses were added. (City of Stockholm, 2011)

Research and higher education in the sciences started in Stockholm in the 18th century, with education in medicine and various research institutions such as the Stockholm Observatory. The medical education was eventually formalized in 1811 as Karolinska Institutet. KTH Royal Institute of Technology ("Swedish: Kungliga Tekniska högskolan") was founded in 1827 and is currently Scandinavia's largest higher education institute of technology with 13,000 students. Stockholm University, founded in 1878 with university status granted in 1960, has 52,000 students . It also incorporates many historical institutions, such as the Observatory, the Swedish Museum of Natural History, and the botanical garden "Bergianska trädgården". The Stockholm School of Economics, founded in 1909, is one of the few private institutions of higher education in Sweden.

In the fine arts, educational institutions include the Royal College of Music, which has a history going back to the conservatory founded as part of the Royal Swedish Academy of Music in 1771, the Royal University College of Fine Arts, which has a similar historical association with the Royal Swedish Academy of Arts and a foundation date of 1735, and the Swedish National Academy of Mime and Acting, which is the continuation of the school of the Royal Dramatic Theatre, once attended by Greta Garbo. Other schools include the design school Konstfack, founded in 1844, the University College of Opera (founded in 1968, but with older roots), the University College of Dance, and the "Stockholms Musikpedagogiska Institut" (the University College of Music Education).

The Södertörn University College was founded in 1995 as a multi-disciplinary institution for southern Metropolitan Stockholm, to balance the many institutions located in the northern part of the region.

Other institutes of higher education are:

The biggest complaint from students of higher education in Stockholm is the lack of student accommodations, the difficulty in finding other accommodations and the high rent.

The Stockholm region is home to around 22% of Sweden's total population, and accounts for about 29% of its gross domestic product. The geographical notion of "Stockholm" has changed over time. By the turn of the 19th century, Stockholm largely consisted of the area today known as City Centre, roughly or one-fifth of the current municipal area. In the ensuing decades several other areas were incorporated (such as Brännkyrka Municipality in 1913, at which time it had 25,000 inhabitants, and Spånga in 1949). The municipal border was established in 1971; with the exception of Hansta, in 1982 purchased by Stockholm Municipality from Sollentuna Municipality and today a nature reserve.

Of the population of 935,619 in 2016, 461,677 were men and 473,942 women. The average age is 40 years; 40.1% of the population is between 20 and 44 years. 382,887 people, or 40.9% of the population, over the age 15 were unmarried. 259,153 people, or 27.7% of the population, were married. 99,524 or 10.6% of the population, had been married but divorced. 299,925 people or 32.1% of Stockholm's residents are of an immigrant or non-Swedish background.

As of October 2018, there were 201,821 foreign-born people in Stockholm. The largest group of them are the Finns (17,000), followed by Iraqis (16,275), Poles (11,994) and Iranians (11,429).

Residents of Stockholm are known as Stockholmers (""stockholmare""). Languages spoken in Greater Stockholm outside of Swedish include Finnish, one of the official minority languages of Sweden; and English, as well as Albanian, Bosnian, Syriac, Arabic, Turkish, Kurdish, Persian, Dutch, Spanish, Serbian and Croatian.

The entire Stockholm metropolitan area, consisting of 26 municipalities, has a population of over 2.2 million, making it the most populous city in the Nordic region. The Stockholm urban area, defined only for statistical purposes, had a total population of 1,630,738 in 2015. In the following municipalities some of the districts are contained within the Stockholm urban area, though not all:

Apart from being Sweden's capital, Stockholm houses many national cultural institutions. The Stockholm region is home to three of Sweden's World Heritage Sites – spots judged as invaluable places that belong to all of humanity: The Drottningholm Palace, Skogskyrkogården (The Woodland Cemetery) and Birka. In 1998, Stockholm was named European Capital of Culture.

Authors connected to Stockholm include the poet and songwriter Carl Michael Bellman (1740–1795), novelist and dramatist August Strindberg (1849–1912), and novelist Hjalmar Söderberg (1869–1941), all of whom made Stockholm part of their works.

Martin Beck is a fictional Swedish police detective from Stockholm, who is the main character in a series of 10 novels by Maj Sjöwall and Per Wahlöö, collectively titled The Story of a Crime, and often based in Stockholm.

Other authors with notable heritage in Stockholm were the Nobel Prize laureate Eyvind Johnson (1900–1976) and the popular poet and composer Evert Taube (1890–1976). The novelist Per Anders Fogelström (1917–1998) wrote a popular series of historical novels depicting life in Stockholm from the mid-18th to mid-20th century.

The city's oldest section is Gamla stan (Old Town), located on the original small islands of the city's earliest settlements and still featuring the medieval street layout. Some notable buildings of Gamla Stan are the large German Church ("Tyska kyrkan") and several mansions and palaces: the "Riddarhuset" (the House of Nobility), the Bonde Palace, the Tessin Palace and the Oxenstierna Palace.

The oldest building in Stockholm is the Riddarholmskyrkan from the late 13th century. After a fire in 1697 when the original medieval castle was destroyed, Stockholm Palace was erected in a baroque style. Storkyrkan Cathedral, the episcopal seat of the Bishop of Stockholm, stands next to the castle. It was founded in the 13th century but is clad in a baroque exterior dating to the 18th century.

As early as the 15th century, the city had expanded outside of its original borders. Some pre-industrial, small-scale buildings from this era can still be found in Södermalm. During the 19th century and the age of industrialization Stockholm grew rapidly, with plans and architecture inspired by the large cities of the continent such as Berlin and Vienna. Notable works of this time period include public buildings such as the Royal Swedish Opera and private developments such as the luxury housing developments on Strandvägen.

In the 20th century, a nationalistic push spurred a new architectural style inspired by medieval and renaissance ancestry as well as influences of the Jugend/Art Nouveau style. A key landmark of Stockholm, the Stockholm City Hall, was erected 1911–1923 by architect Ragnar Östberg. Other notable works of these times are the Stockholm Public Library and the World Heritage Site Skogskyrkogården.
In the 1930s modernism characterized the development of the city as it grew. New residential areas sprang up such as the development on Gärdet while industrial development added to the growth, such as the KF manufacturing industries on Kvarnholmen located in the Nacka Municipality. In the 1950s, suburban development entered a new phase with the introduction of the Stockholm metro. The modernist developments of Vällingby and Farsta were internationally praised. In the 1960s this suburban development continued but with the aesthetic of the times, the industrialized and mass-produced blocks of flats received a large amount of criticism.

At the same time that this suburban development was taking place, the most central areas of the inner city were being redesigned, known as "Norrmalmsregleringen". Sergels Torg, with its five high-rise office towers was created in the 1960s, followed by the total clearance of large areas to make room for new development projects. The most notable buildings from this period include the ensemble of the House of Culture, City Theatre and the Riksbank at Sergels Torg, designed by architect Peter Celsing.

In the 1980s, the planning ideas of modernism were starting to be questioned, resulting in suburbs with a denser planning, such as Skarpnäck. In the 1990s this idea was taken further with the development of and old industrial area close to the inner city, resulting in a sort of mix of modernistic and urban planning in the new area of Hammarby Sjöstad.

The municipality has appointed an official "board of beauty" called "Skönhetsrådet" to protect and preserve the beauty of the city.

Stockholm's architecture (along with Visby, Gotland) provided the inspiration for Japanese anime director Hayao Miyazaki as he sought to evoke an idealized city untouched by World War. His creation, called "Koriko", draws directly from what Miyazaki felt was Stockholm's sense of well-established architectural unity, vibrancy, independence, and safety.

Stockholm is one of the most crowded museum-cities in the world with around 100 museums, visited by millions of people every year.

The Vasa Museum () is a maritime museum on Djurgården which displays the only almost fully intact 17th century ship that has ever been salvaged, the 64-gun warship "Vasa" that sank on her maiden voyage in 1628.

The Nationalmuseum houses the largest collection of art in the country: 16,000 paintings and 30,000 objects of art handicraft. The collection dates back to the days of Gustav Vasa in the 16th century, and has since been expanded with works by artists such as Rembrandt, and Antoine Watteau, as well as constituting a main part of Sweden's art heritage, manifested in the works of Alexander Roslin, Anders Zorn, Johan Tobias Sergel, Carl Larsson, Carl Fredrik Hill and Ernst Josephson. From the year 2013 to 2018 the museum was closed due to a restoration of the building.

Moderna Museet (Museum of Modern Art) is Sweden's national museum of modern art. It has works by noted modern artists such as Picasso and Salvador Dalí.

Skansen (in English: the Sconce) is a combined open-air museum and zoo, located on the island of Djurgården. It was founded in 1891 by Artur Hazelius (1833–1901) to show the way of life in the different parts of Sweden before the industrial era.

Other notable museums (in alphabetical order):

Stockholm has a vibrant art scene with a number of internationally recognized art centres and commercial galleries. Amongst others privately sponsored initiatives such as Bonniers Konsthall, Magasin 3, and state supported institutions such as Tensta Konsthall and Index all show leading international and national artists. In the last few years a gallery district has emerged around Hudiksvallsgatan where leading galleries such as Andréhn-Schiptjenko, Brändström & Stene have located. Other important commercial galleries include Nordenhake, Milliken Gallery and Galleri Magnus Karlsson.

The Stockholm suburbs are places with diverse cultural background. Some areas in the inner suburbs, including those of Skärholmen, Tensta, Jordbro, Fittja, Husby, Brandbergen, Rinkeby, Rissne, Kista, Hagsätra, Hässelby, Farsta, Rågsved, Flemingsberg, and the outer suburb of Södertälje, have high percentages of immigrants or second generation immigrants. These mainly come from the Middle East (Assyrians, Syriacs, Turks and Kurds) also Bosnians and Serbs, but there are also immigrants from Africa, Southeast Asia and Latin America. Other parts of the inner suburbs, such as Täby, Danderyd, Lidingö, Flysta and, as well as some of the suburbs mentioned above, have a majority of ethnic Swedes.

Distinguished among Stockholm's many theatres are the Royal Dramatic Theatre ("Kungliga Dramatiska Teatern"), one of Europe's most renowned theatres, and the Royal Swedish Opera, inaugurated in 1773.

Other notable theatres are the Stockholm City Theatre (Stockholms stadsteater), the Peoples Opera ("Folkoperan"), the Modern Theatre of Dance ("Moderna dansteatern"), the China Theatre, the Göta Lejon Theatre, the Mosebacke Theatre, and the Oscar Theatre.

Gröna Lund is an amusement park located on the island of Djurgården. This amusement park has over 30 attractions and many restaurants. It is a popular tourist attraction and visited by thousands of people every day. It is open from the end of April to the middle of September. Gröna Lund also serves as a concert venue.

Stockholm is the media centre of Sweden. It has four nationwide daily newspapers and is also the central location of the publicly funded radio (SR) and television (SVT). In addition, all other major television channels have their base in Stockholm, such as: TV3, TV4 and TV6. All major magazines are also located to Stockholm, as are the largest literature publisher, the Bonnier group. The worlds best selling video game "Minecraft" was created in Stockholm by Markus 'Notch' Persson in 2009, and its company Mojang is currently headquartered there.

The most popular spectator sports are football and ice hockey. The three most popular football clubs in Stockholm are AIK, Djurgårdens IF and Hammarby IF, who all play in the first tier, Allsvenskan. AIK play at Sweden's national stadium for football, Friends Arena in Solna, with a capacity of 54,329. The Europa League final of 2017 was played the 24th of May between AFC Ajax and Manchester United on Friends Arena. Manchester United won the trophy after a 2-0 victory.

Djurgårdens IF and Hammarby play at Tele2 Arena in Johanneshov, with a capacity of 30,000 spectators.

All three clubs are multi-sport clubs, which have ice hockey teams; Djurgårdens IF play in the first tier, AIK in the second and Hammarby in the third tier, as well as teams in bandy, basketball, floorball and other sports, including individual sports.

Historically, the city was the host of the 1912 Summer Olympics. From those days stem the Stockholms Olympiastadion which has since hosted numerous sports events, notably football and athletics. Other major sport arenas are Friends Arena the new national football stadium, Stockholm Globe Arena, a multi-sport arena and one of the largest spherical buildings in the world and the nearby indoor arena Hovet.

Besides the 1912 Summer Olympics, Stockholm hosted the 1956 Summer Olympics Equestrian Games and the UEFA Euro 1992. The city was also second runner up in the 2004 Summer Olympics bids. Stockholm hosted the 1958 FIFA World Cup. Stockholm is bid jointly with Åre for the 2026 Winter Olympics competing against the other joint bid of Milan/Cortina d'Ampezzo, Italy, if awarded it would have been the second city to host both Summer and Winter Olympics after Beijing and for the 2026 Winter Paralympics if awarded it would also have been the second city to host both Summer and Winter Paralympics also after Beijing and with Åre it will also be to host all three winter event including Winter Olympic Games, Winter Paralympic Games and the Special Olympics World Winter Games in which Åre will host in 2021 along with Östersund. Stockholm first bid for the Winter Olympics for 2022 Winter Olympics, but withdrew its bid in 2014 due to financial matters.

Stockholm also hosted all but one of the Nordic Games, a winter multi-sport event that predated the Winter Olympics.

In 2015, the Stockholms Kungar Rugby league club were formed. They are Stockholm's first Rugby league team and will play in Sweden's National Rugby league championship.

Every year Stockholm is host to the ÖTILLÖ Swimrun World Championship.

Stockholm has hosted the Stockholm Open, an ATP World Tour 250 series professional tennis tournament annually since 1969. Each year since 1995, the tournament has been hosted at the Kungliga tennishallen.

There are over 1000 restaurants in Stockholm. Stockholm boasts a total of ten Michelin star restaurants, two with two stars and one with three stars.


Stockholm is one of the cleanest capitals in the world. The city was granted the 2010 European Green Capital Award by the EU Commission; this was Europe's first "green capital". Applicant cities were evaluated in several ways: climate change, local transport, public green areas, air quality, noise, waste, water consumption, waste water treatment, sustainable utilisation of land, biodiversity and environmental management. Out of 35 participant cities, eight finalists were chosen: Stockholm, Amsterdam, Bristol, Copenhagen, Freiburg, Hamburg, Münster, and Oslo. Some of the reasons why Stockholm won the 2010 European Green Capital Award were: its integrated administrative system, which ensures that environmental aspects are considered in budgets, operational planning, reporting, and monitoring; its cut in carbon dioxide emissions by 25% per capita in ten years; and its decision towards being fossil fuel free by 2050. Stockholm has long demonstrated concern for the environment. The city's current environmental program is the fifth since the first one was established in the mid-1970s. In 2011, Stockholm passed the title of European Green Capital to Hamburg, Germany.

In the beginning of 2010, Stockholm launched the program Professional Study Visits in order to share the city's green best practices. The program provides visitors with the opportunity to learn how to address issues such as waste management, urban planning, carbon dioxide emissions, and sustainable and efficient transportation system, among others.

According to the European Cities Monitor 2010, Stockholm is the best city in terms of freedom from pollution. Surrounded by 219 nature reserves, Stockholm has around 1,000 green spaces, which corresponds to 30% of the city's area. Founded in 1995, the Royal National City Park is the world's first legally protected "national urban park". For a description of the formation process, value assets and implementation of the legal protection of The Royal National Urban Park, see Schantz 2006 The water in Stockholm is so clean that people can dive and fish in the centre of the city. The waters of downtown Stockholm serve as spawning grounds for multiple fish species including trout and salmon, though human intervention is needed to keep populations up. Regarding CO2 emissions, the government's target is that Stockholm will be CO2 free before 2050.

Stockholm used to have problematic levels of particulates (PM10) due to studded winter tires, but as of 2016 the levels are below limits, after street-specific bans. Instead the current (2016) problem is nitrogen oxides emitted by diesel vehicles. In 2016 the average levels for urban background (roof of Torkel Knutssonsgatan) were: NO 11 μg/m, NO 14 μg/m, PM10 12 μg/m, PM2.5 4.9 μg/m, soot 0.4 μg/m, ultrafine particles 6200/cm, CO 0.2 mg/m, SO 0.4 μg/m, ozone 51 μg/m. For urban street level (the densely trafficked Hornsgatan) the average levels were: NO 43 μg/m, NO 104 μg/m, PM10 23 μg/m, PM2.5 5.9 μg/m, soot 1.0 μg/m, ultrafine particles 17100/cm, CO 0.3 mg/m, ozone 31 μg/m.

Stockholm has an extensive public transport system. It consists of the Stockholm Metro (), which consist of three color-coded main lines (green, red and blue) with seven actual lines (10, 11, 13, 14, 17, 18, 19); the Stockholm commuter rail () which runs on the state-owned railroads on six lines (40, 41, 42, 43, 44, 48); four light rail/tramway lines (7, 12, 21, and 22); the 891 mm narrow-gauge railway Roslagsbanan, on three lines (27, 28, 29) in the northeastern part; the local railway Saltsjöbanan, on two lines (25, 26) in the southeastern part; a large number of bus lines, and the inner-city Djurgården ferry. The overwhelming majority of the land-based public transport in Stockholm County (save for the airport buses/airport express trains and other few commercially viable bus lines) is organized under the common umbrella of Storstockholms Lokaltrafik (SL), an aktiebolag wholly owned by Stockholm County Council. Since the 1990s, the operation and maintenance of the SL public transport services are contracted out to independent companies bidding for contracts, such as MTR, which currently operate the Metro. The archipelago boat traffic is handled by Waxholmsbolaget, which is also wholly owned by the County Council.
SL has a common ticket system in the entire Stockholm County, which allows for easy travel between different modes of transport. The tickets are of two main types, single ticket and travel cards, both allowing for unlimited travel with SL in the entire Stockholm County for the duration of the ticket validity. On 1 April 2007, a zone system (A, B, C) and price system was introduced. Single tickets were available in forms of cash ticket, individual unit pre-paid tickets, pre-paid ticket slips of 8, sms-ticket and machine ticket. Cash tickets bought at the point of travel were the most expensive and pre-paid tickets slips of 8 are the cheapest. A single ticket costs 32 SEK with the card and 45 SEK without and is valid for 75 minutes. The duration of the travel card validity depended on the exact type; they were available from 24 hours up to a year. As of 2018, a 30-day card costs 860 SEK. Tickets of all these types were available with reduced prices for students and persons under 20 and over 65 years of age. On 9 January 2017, the zone system was removed, and the cost of the tickets was increased.

With an estimated cost of SEK 16.8 billion (January 2007 price level), which equals 2.44 billion US dollars, the City Line, an environmentally certified project, comprises a -long commuter train tunnel (in rock and water) beneath Stockholm, with two new stations (Stockholm City and Stockholm Odenplan), and a -long railway bridge at Årsta. The City Line was built by the Swedish Transport Administration in co-operation with the City of Stockholm, Stockholm County Council, and Stockholm Transport, SL. As Stockholm Central Station is overloaded, the purpose of this project was to double the city's track capacity and improve service efficiency. Operations began in July 2017.

Between Riddarholmen and Söder Mälarstrand, the City Line runs through a submerged concrete tunnel. As a green project, the City Line includes the purification of waste water; noise reduction through sound-attenuating tracks; the use of synthetic diesel, which provides users with clean air; and the recycling of excavated rocks.

Stockholm is at the junction of the European routes E4, E18 and E20. A half-completed motorway ring road exists on the south, west and north sides of the City Centre. The northern section of the ring road, Norra Länken, opened for traffic in 2015 while the final subsea eastern section is being discussed as a future project. A bypass motorway for traffic between Northern and Southern Sweden, Förbifart Stockholm, is currently being built. The many islands and waterways make extensions of the road system both complicated and expensive, and new motorways are often built as systems of tunnels and bridges.

Stockholm has a congestion pricing system, Stockholm congestion tax, in use on a permanent basis since 1 August 2007, after having had a seven-month trial period in the first half of 2006. The City Centre is within the congestion tax zone. All the entrances and exits of this area have unmanned control points operating with automatic number plate recognition. All vehicles entering or exiting the congestion tax affected area, with a few exceptions, have to pay 10–20 SEK (1.09–2.18 EUR, 1.49–2.98 USD) depending on the time of day between 06:30 and 18:29. The maximum tax amount per vehicle per day is 60 SEK (6.53 EUR, ). Payment is done by various means within 14 days after one has passed one of the control points; one cannot pay at the control points.

After the trial period was over, consultative referendums were held in Stockholm Municipality and several other municipalities in Stockholm County. The then-reigning government (Persson Cabinet) stated that they would only take into consideration the results of the referendum in Stockholm Municipality. The opposition parties (Alliance for Sweden) stated that if they were to form a cabinet after the general election—which was held the same day as the congestion tax referendums—they would take into consideration the referendums held in several of the other municipalities in Stockholm County as well. The results of the referendums were that the Stockholm Municipality voted for the congestion tax, while the other municipalities voted against it. The opposition parties won the general election and a few days before they formed government (Reinfeldt Cabinet) they announced that the congestion tax would be reintroduced in Stockholm, but that the revenue would go entirely to road construction in and around Stockholm. During the trial period and according to the agenda of the previous government the revenue went entirely to public transport.

Stockholm has regular ferry lines to Helsinki and Turku in Finland (commonly called "Finlandsfärjan"); Tallinn, Estonia; Riga, Latvia, Åland islands and to Saint Petersburg. The large Stockholm archipelago is served by the archipelago boats of Waxholmsbolaget (owned and subsidized by Stockholm County Council).

Between April and October, during the warmer months, it is possible to rent Stockholm City Bikes by purchasing a bike card online or through retailers. Cards allow users to rent bikes from any Stockholm City Bikes stand spread across the city and return them in any stand. There are two types of cards: the Season Card (valid from 1 April to 31 October) and the 3-day card. When their validity runs out they can be reactivated and are therefore reusable. Bikes can be used for up to three hours per loan and can be rented from Monday to Sunday from 6 am to 10 pm.


Arlanda Express airport rail link runs between Arlanda Airport and central Stockholm. With a journey of 20 minutes, the train ride is the fastest way of traveling to the city center. 
Arlanda Central Station is also served by commuter, regional and intercity trains.

Additionally, there are also bus lines, Flygbussarna, that run between central Stockholm and all the airports.

Stockholm Central Station has train connections to many Swedish cities as well as to Oslo, Norway and Copenhagen, Denmark. The popular X 2000 service to Gothenburg takes three hours. Most of the trains are run by SJ AB.

Stockholm often performs well in international rankings, some of which are mentioned below:




</doc>
<doc id="26742" url="https://en.wikipedia.org/wiki?curid=26742" title="Stamp collecting">
Stamp collecting

Stamp collecting is the collecting of postage stamps and related objects. It is related to philately, which is the study of stamps. It has been one of the world's most popular hobbies since the late nineteenth century with the rapid growth of the postal service, as a never-ending stream of new stamps was produced by countries that sought to advertise their distinctiveness through their stamps.

Stamp collecting is generally accepted as one of the areas that make up the wider subject of philately, which is the study of stamps. A philatelist may, but does not have to, collect stamps. It is not uncommon for the term "philatelist" to be used to mean a stamp collector. Many casual stamp collectors accumulate stamps for sheer enjoyment and relaxation without worrying about the tiny details. The creation of a large or comprehensive collection, however, generally requires some philatelic knowledge and will usually contain areas of philatelic studies.

Postage stamps are often collected for their historical value and geographical aspects and also for the many subjects depicted on them, ranging from ships, horses, and birds to kings, queens and presidents.

Sales of postage stamps are an important source of income for some countries whose stamp issues may exceed their postal needs, but have designs that appeal to many stamp collectors.

It has been suggested that John Bourke, Receiver General of Stamp Dues in Ireland, was the first collector. In 1774 he assembled a book of the existing embossed revenue stamps, ranging in value from 6 pounds to half a penny, as well as the hand stamped charge marks that were used with them. His collection is preserved in the Royal Irish Academy, Dublin.

Postage stamp collecting began at the same time that stamps were first issued, and by 1860 thousands of collectors and stamp dealers were appearing around the world as this new study and hobby spread across Europe, European colonies, the United States and other parts of the world.

The first postage stamp, the Penny Black, was issued by Britain in May 1840 and pictured a young Queen Victoria. It was produced without perforations (imperforate) and consequently had to be cut from the sheet with scissors in order to be used. While unused examples of the Penny Black are quite scarce, used examples are quite common, and may be purchased for $20 to $200, depending upon condition.

People started to collect stamps almost immediately. One of the earliest and most notable was John Edward Gray. In 1862, Gray stated that he "began to collect postage stamps shortly after the system was established and before it had become a rage".

Women stamp collectors date from the earliest days of postage stamp collecting. One of the earliest was Adelaide Lucy Fenton who wrote articles in the 1860s for the journal "The Philatelist" under the name Herbert Camoens.

As the hobby and study of stamps began to grow, stamp albums and stamp related literature began to surface, and by the early 1880s publishers like Stanley Gibbons made a business out of this advent.

Children and teenagers were early collectors of stamps in the 1860s and 1870s. Many adults dismissed it as a childish pursuit but later many of those same collectors, as adults, began to systematically study the available postage stamps and publish books about them. Some stamps, such as the triangular issues of the Cape of Good Hope, have become legendary.

Stamp collecting is a less popular hobby in the early 21st century than it was a hundred years ago. In 2013, the Wall Street Journal estimated the global number of stamp collectors was around 60 million. Tens of thousands of stamp dealers supply them with stamps along with stamp albums, catalogues and other publications. There are also thousands of stamp (philatelic) clubs and organizations that provide them with the history and other aspects of stamps. Today, though the number of collectors is somewhat less, stamp collecting is still one of the world's most popular indoor hobbies.

A few basic items of equipment are recommended for proper stamp collection. Stamp tongs help to handle stamps safely, a magnifying glass helps in viewing fine details and an album is a convenient way to store stamps. The stamps need to be attached to the pages of the album in some way, and stamp hinges are a cheap and simple way to do this. However, hinging stamps can damage them, thus reducing their value; today many collectors prefer more expensive "hingeless mounts". Issued in various sizes, these are clear, chemically neutral thin plastic holders that open to receive stamps and are gummed on the back so that they stick to album pages. Another alternative is a stockbook, where the stamps drop into clear pockets without the need for a mount. Stamps should be stored away from light, heat and moisture or they will be damaged.

Stamps can be displayed according to the collector's wishes, by country, topic, or even by size, which can create a display pleasing to the eye. There are no rules and it is entirely a matter for the individual collector to decide. Albums can be commercially purchased, downloaded or created by the collector. In the latter cases, using acid free paper provides better long-term stamp protection.

Many collectors ask their family and friends to save stamps for them from their mail. Although the stamps received by major businesses and those kept by elderly relatives may be of international and historical interest, the stamps received from family members are often of the definitive sort. Definitives seem mundane but, considering their variety of colours, watermarks, paper differences, perforations and printing errors, they can fill many pages in a collection. Introducing either variety or specific focus to a collection can require the purchasing of stamps, either from a dealer or online. Online stamp collector clubs often contain a platform for buying/selling and trading. Large numbers of relatively recent stamps, often still attached to fragments or envelopes, may be obtained cheaply and easily. Rare and old stamps can also be obtained, but these can be very expensive.

Duplicate stamps are those a collector already has and are not required, therefore, to fill a gap in a collection. Duplicate stamps can be sold or traded, so they are an important medium of exchange among collectors.

Many dealers sell stamps through the Internet while others have neighborhood shops which are among the best resources for beginning and intermediate collectors. Some dealers also jointly set up week-end stamp markets called "bourses" that move around a region from week to week. They also meet collectors at regional exhibitions and stamp shows.

A worldwide collection would be enormous, running to thousands of volumes, and would be incredibly expensive to acquire. Many consider that Count Philipp von Ferrary's collection at the beginning of the 20th century was the most complete ever formed. Many collectors limit their collecting to particular countries, certain time periods or particular subjects (called "topicals") like birds or aircraft.

Some of the more popular collecting areas include:


There are thousands of organizations for collectors: local stamp clubs, special-interest groups, and national organizations. Most nations have a national collectors' organization, such as the American Philatelic Society in the United States and the Philatelic Traders Society in United Kingdom. The Internet has greatly expanded the availability of information and made it easier to obtain stamps and other philatelic material. The American Topical Association (ATA) is now a part of the APS and promotes thematic collecting as well as encouraging sub-groups of numerous topics.

Stamp clubs and philatelic societies can add a social aspect to stamp collecting and provide a forum where novices can meet experienced collectors. Although such organizations are often advertised in stamp magazines and online, the relatively small number of collectors – especially outside urban areas – means that a club may be difficult to set up and sustain. The Internet partially solves this problem, as the association of collectors online is not limited by geographical distance. For this reason, many highly specific stamp clubs have been established on the Web, with international membership.

Organizations such as the Cinderella Stamp Club (UK) retain hundreds of members interested in a specific aspect of collecting. Social organizations, such as the Lions Club and Rotary International, have also formed stamp collecting groups specific to those stamps that are issued from many countries worldwide that display the organization's logo.

Rare stamps are often old and many have interesting stories attached to them. Some include:

Stamp catalogues are the primary tool used by serious collectors to organize their collections, and for the identification and valuation of stamps. Most stamp shops have stamp catalogues available for purchase. A few catalogues are offered online, either free or for a fee. There are hundreds of different catalogues, most specializing in particular countries or periods. Collector clubs tend to provide free catalogues to their members.

The stamp collection assembled by French-Austrian aristocrat Philipp von Ferrary (1850–1917) at the beginning of the 20th century is widely considered the most complete stamp collection ever formed (or likely to be formed). It included, for example, all of the rare stamps described above that had been issued by 1917. However, as Ferrary was an Austrian citizen, the collection was broken up and sold by the French government after the First World War, as war reparations. A close rival was Thomas Tapling (1855–1891), whose Tapling Collection was donated to the British Museum.

Several European monarchs were keen stamp collectors, including King George V of the United Kingdom and King Carol II of Romania. King George V possessed one of the most valuable stamp collections in the world and became President of the Royal Philatelic Society. His collection was passed on to Queen Elizabeth II who, while not a serious philatelist, has a collection of British and Commonwealth first day covers which she started in 1952.

U.S. President Franklin Delano Roosevelt was a stamp collector; he designed several American commemorative stamps during his term. Late in life Ayn Rand renewed her childhood interest in stamps and became an enthusiastic collector. Several entertainment and sport personalities have been known to be collectors. Freddie Mercury, lead singer of the band Queen, collected stamps as a child. His childhood stamp album is in the collection of the British Postal Museum & Archive. John Lennon of The Beatles was a childhood stamp collector. His stamp album is held by the National Postal Museum.

Former world chess champion Anatoly Karpov has amassed a huge stamp collection over the decades, led by stamps from Belgium and Belgian Congo, that has been estimated to be worth $15 million.






</doc>
<doc id="26743" url="https://en.wikipedia.org/wiki?curid=26743" title="Sigmund Freud">
Sigmund Freud

Sigmund Freud ( ; ; born Sigismund Schlomo Freud; 6 May 1856 – 23 September 1939) was an Austrian neurologist and the founder of psychoanalysis, a clinical method for treating psychopathology through dialogue between a patient and a psychoanalyst.

Freud was born to Galician Jewish parents in the Moravian town of Freiberg, in the Austrian Empire. He qualified as a doctor of medicine in 1881 at the University of Vienna. Upon completing his habilitation in 1885, he was appointed a docent in neuropathology and became an affiliated professor in 1902. Freud lived and worked in Vienna, having set up his clinical practice there in 1886. In 1938, Freud left Austria to escape the Nazis. He died in exile in the United Kingdom in 1939.

In founding psychoanalysis, Freud developed therapeutic techniques such as the use of free association and discovered transference, establishing its central role in the analytic process. Freud's redefinition of sexuality to include its infantile forms led him to formulate the Oedipus complex as the central tenet of psychoanalytical theory. His analysis of dreams as wish-fulfillments provided him with models for the clinical analysis of symptom formation and the underlying mechanisms of repression. On this basis Freud elaborated his theory of the unconscious and went on to develop a model of psychic structure comprising id, ego and super-ego. Freud postulated the existence of libido, a sexualised energy with which mental processes and structures are invested and which generates erotic attachments, and a death drive, the source of compulsive repetition, hate, aggression and neurotic guilt. In his later works, Freud developed a wide-ranging interpretation and critique of religion and culture.

Though in overall decline as a diagnostic and clinical practice, psychoanalysis remains influential within psychology, psychiatry, and psychotherapy, and across the humanities. It thus continues to generate extensive and highly contested debate with regard to its therapeutic efficacy, its scientific status, and whether it advances or is detrimental to the feminist cause. Nonetheless, Freud's work has suffused contemporary Western thought and popular culture. W. H. Auden's 1940 poetic tribute to Freud describes him as having created "a whole climate of opinion / under whom we conduct our different lives."

Freud was born to Jewish parents in the Moravian town of Freiberg, in the Austrian Empire (later Příbor, Czech Republic), the first of eight children. Both of his parents were from Galicia, a province straddling modern-day West Ukraine and Poland. His father, Jakob Freud (1815–1896), a wool merchant, had two sons, Emanuel (1833–1914) and Philipp (1836–1911), by his first marriage. Jakob's family were Hasidic Jews, and although Jakob himself had moved away from the tradition, he came to be known for his Torah study. He and Freud's mother, Amalia Nathansohn, who was 20 years younger and his third wife, were married by Rabbi Isaac Noah Mannheimer on 29 July 1855. They were struggling financially and living in a rented room, in a locksmith's house at Schlossergasse 117 when their son Sigmund was born. He was born with a caul, which his mother saw as a positive omen for the boy's future.

In 1859, the Freud family left Freiberg. Freud's half brothers emigrated to Manchester, England, parting him from the "inseparable" playmate of his early childhood, Emanuel's son, John. Jakob Freud took his wife and two children (Freud's sister, Anna, was born in 1858; a brother, Julius born in 1857, had died in infancy) firstly to Leipzig and then in 1860 to Vienna where four sisters and a brother were born: Rosa (b. 1860), Marie (b. 1861), Adolfine (b. 1862), Paula (b. 1864), Alexander (b. 1866). In 1865, the nine-year-old Freud entered the "Leopoldstädter Kommunal-Realgymnasium", a prominent high school. He proved to be an outstanding pupil and graduated from the Matura in 1873 with honors. He loved literature and was proficient in German, French, Italian, Spanish, English, Hebrew, Latin and Greek.

Freud entered the University of Vienna at age 17. He had planned to study law, but joined the medical faculty at the university, where his studies included philosophy under Franz Brentano, physiology under Ernst Brücke, and zoology under Darwinist professor Carl Claus. In 1876, Freud spent four weeks at Claus's zoological research station in Trieste, dissecting hundreds of eels in an inconclusive search for their male reproductive organs. In 1877 Freud moved to Ernst Brücke's physiology laboratory where he spent six years comparing the brains of humans and other vertebrates with those of frogs and invertebrates such as crayfish and lampreys. His research work on the biology of nervous tissue proved seminal for the subsequent discovery of the neuron in the 1890s. Freud's research work was interrupted in 1879 by the obligation to undertake a year's compulsory military service. The lengthy downtimes enabled him to complete a commission to translate four essays from John Stuart Mill's collected works. He graduated with an MD in March 1881.

In 1882, Freud began his medical career at the Vienna General Hospital. His research work in cerebral anatomy led to the publication of an influential paper on the palliative effects of cocaine in 1884 and his work on aphasia would form the basis of his first book "On the Aphasias: a Critical Study", published in 1891. Over a three-year period, Freud worked in various departments of the hospital. His time spent in Theodor Meynert's psychiatric clinic and as a locum in a local asylum led to an increased interest in clinical work. His substantial body of published research led to his appointment as a university lecturer or docent in neuropathology in 1885, a non-salaried post but one which entitled him to give lectures at the University of Vienna.

In 1886, Freud resigned his hospital post and entered private practice specializing in "nervous disorders". The same year he married Martha Bernays, the granddaughter of Isaac Bernays, a chief rabbi in Hamburg. They had six children: Mathilde (b. 1887), Jean-Martin (b. 1889), Oliver (b. 1891), Ernst (b. 1892), Sophie (b. 1893), and Anna (b. 1895). From 1891 until they left Vienna in 1938, Freud and his family lived in an apartment at Berggasse 19, near Innere Stadt, a historical district of Vienna.
In 1896, Minna Bernays, Martha Freud's sister, became a permanent member of the Freud household after the death of her fiancé. The close relationship she formed with Freud led to rumours, started by Carl Jung, of an affair. The discovery of a Swiss hotel log of 13 August 1898, signed by Freud whilst travelling with his sister-in-law, has been presented as evidence of the affair.

Freud began smoking tobacco at age 24; initially a cigarette smoker, he became a cigar smoker. He believed smoking enhanced his capacity to work and that he could exercise self-control in moderating it. Despite health warnings from colleague Wilhelm Fliess, he remained a smoker, eventually suffering a buccal cancer. Freud suggested to Fliess in 1897 that addictions, including that to tobacco, were substitutes for masturbation, "the one great habit."

Freud had greatly admired his philosophy tutor, Brentano, who was known for his theories of perception and introspection. Brentano discussed the possible existence of the unconscious mind in his "Psychology from an Empirical Standpoint" (1874). Although Brentano denied its existence, his discussion of the unconscious probably helped introduce Freud to the concept. Freud owned and made use of Charles Darwin's major evolutionary writings, and was also influenced by Eduard von Hartmann's "The Philosophy of the Unconscious" (1869). Other texts of importance to Freud were by Fechner and Herbart with the latter's "Psychology as Science" arguably considered to be of underrated significance in this respect. Freud also drew on the work of Theodor Lipps who was one of the main contemporary theorists of the concepts of the unconscious and empathy.

Though Freud was reluctant to associate his psychoanalytic insights with prior philosophical theories, attention has been drawn to analogies between his work and that of both Schopenhauer and Nietzsche, both of whom he claimed not to have read until late in life. One historian concluded, based on Freud's correspondence with his adolescent friend Eduard Silberstein, that Freud read Nietzsche's "The Birth of Tragedy" and the first two of the "Untimely Meditations" when he was seventeen. In 1900, the year of Nietzsche's death, Freud bought his collected works; he told his friend, Fliess, that he hoped to find in Nietzsche's works "the words for much that remains mute in me." Later, he said he had not yet opened them. Freud came to treat Nietzsche's writings "as texts to be resisted far more than to be studied." His interest in philosophy declined after he had decided on a career in neurology.

Freud read William Shakespeare in English throughout his life, and it has been suggested that his understanding of human psychology may have been partially derived from Shakespeare's plays.

Freud's Jewish origins and his allegiance to his secular Jewish identity were of significant influence in the formation of his intellectual and moral outlook, especially with respect to his intellectual non-conformism, as he was the first to point out in his "Autobiographical Study". They would also have a substantial effect on the content of psychoanalytic ideas, particularly in respect of their common concerns with depth interpretation and "the bounding of desire by law".

In October 1885, Freud went to Paris on a three-month fellowship to study with Jean-Martin Charcot, a renowned neurologist who was conducting scientific research into hypnosis. He was later to recall the experience of this stay as catalytic in turning him toward the practice of medical psychopathology and away from a less financially promising career in neurology research. Charcot specialized in the study of hysteria and susceptibility to hypnosis, which he frequently demonstrated with patients on stage in front of an audience.

Once he had set up in private practice back in Vienna in 1886, Freud began using hypnosis in his clinical work. He adopted the approach of his friend and collaborator, Josef Breuer, in a type of hypnosis which was different from the French methods he had studied, in that it did not use suggestion. The treatment of one particular patient of Breuer's proved to be transformative for Freud's clinical practice. Described as Anna O., she was invited to talk about her symptoms while under hypnosis (she would coin the phrase "talking cure" for her treatment). In the course of talking in this way, her symptoms became reduced in severity as she retrieved memories of traumatic incidents associated with their onset.

The inconsistent results of Freud's early clinical work eventually led him to abandon hypnosis, having concluded that more consistent and effective symptom relief could be achieved by encouraging patients to talk freely, without censorship or inhibition, about whatever ideas or memories occurred to them. In conjunction with this procedure, which he called "free association", Freud found that patients' dreams could be fruitfully analyzed to reveal the complex structuring of unconscious material and to demonstrate the psychic action of repression which, he had concluded, underlay symptom formation. By 1896 he was using the term "psychoanalysis" to refer to his new clinical method and the theories on which it was based.

Freud's development of these new theories took place during a period in which he experienced heart irregularities, disturbing dreams and periods of depression, a "neurasthenia" which he linked to the death of his father in 1896 and which prompted a "self-analysis" of his own dreams and memories of childhood. His explorations of his feelings of hostility to his father and rivalrous jealousy over his mother's affections led him to fundamentally revise his theory of the origin of the neuroses.

On the basis of his early clinical work, Freud had postulated that unconscious memories of sexual molestation in early childhood were a necessary precondition for the psychoneuroses (hysteria and obsessional neurosis), a formulation now known as Freud's seduction theory. In the light of his self-analysis, Freud abandoned the theory that every neurosis can be traced back to the effects of infantile sexual abuse, now arguing that infantile sexual scenarios still had a causative function, but it did not matter whether they were real or imagined and that in either case they became pathogenic only when acting as repressed memories.

This transition from the theory of infantile sexual trauma as a general explanation of how all neuroses originate to one that presupposes an autonomous infantile sexuality provided the basis for Freud's subsequent formulation of the theory of the Oedipus complex.

Freud described the evolution of his clinical method and set out his theory of the psychogenetic origins of hysteria, demonstrated in a number of case histories, in "Studies on Hysteria" published in 1895 (co-authored with Josef Breuer). In 1899 he published "The Interpretation of Dreams" in which, following a critical review of existing theory, Freud gives detailed interpretations of his own and his patients' dreams in terms of wish-fulfillments made subject to the repression and censorship of the "dream work". He then sets out the theoretical model of mental structure (the unconscious, pre-conscious and conscious) on which this account is based. An abridged version, "On Dreams", was published in 1901. In works which would win him a more general readership, Freud applied his theories outside the clinical setting in "The Psychopathology of Everyday Life" (1901) and "Jokes and their Relation to the Unconscious" (1905). In "Three Essays on the Theory of Sexuality", published in 1905, Freud elaborates his theory of infantile sexuality, describing its "polymorphous perverse" forms and the functioning of the "drives", to which it gives rise, in the formation of sexual identity. The same year he published "Fragment of an Analysis of a Case of Hysteria", which became one of his more famous and controversial case studies.

During this formative period of his work, Freud valued and came to rely on the intellectual and emotional support of his friend Wilhelm Fliess, a Berlin-based ear, nose and throat specialist whom he had first met 1887. Both men saw themselves as isolated from the prevailing clinical and theoretical mainstream because of their ambitions to develop radical new theories of sexuality. Fliess developed highly eccentric theories of human biorhythms and a nasogenital connection which are today considered pseudoscientific. He shared Freud's views on the importance of certain aspects of sexuality — masturbation, coitus interruptus, and the use of condoms — in the etiology of what were then called the "actual neuroses," primarily neurasthenia and certain physically manifested anxiety symptoms. They maintained an extensive correspondence from which Freud drew on Fliess's speculations on infantile sexuality and bisexuality to elaborate and revise his own ideas. His first attempt at a systematic theory of the mind, his "Project for a Scientific Psychology" was developed as a metapsychology with Fliess as interlocutor. However, Freud's efforts to build a bridge between neurology and psychology were eventually abandoned after they had reached an impasse, as his letters to Fliess reveal, though some ideas of the "Project" were to be taken up again in the concluding chapter of "The Interpretation of Dreams".

Freud had Fliess repeatedly operate on his nose and sinuses to treat "nasal reflex neurosis", and subsequently referred his patient Emma Eckstein to him. According to Freud, her history of symptoms included severe leg pains with consequent restricted mobility, as well as stomach and menstrual pains. These pains were, according to Fliess's theories, caused by habitual masturbation which, as the tissue of the nose and genitalia were linked, was curable by removal of part of the middle turbinate. Fliess's surgery proved disastrous, resulting in profuse, recurrent nasal bleeding – he had left a half-metre of gauze in Eckstein's nasal cavity - the subsequent removal of which left her permanently disfigured. At first, though aware of Fliess's culpability – Freud fled from the remedial surgery in horror – he could only bring himself to delicately intimate in his correspondence to Fliess the nature of his disastrous role and in subsequent letters maintained a tactful silence on the matter or else returned to the face-saving topic of Eckstein's hysteria. Freud ultimately, in light of Eckstein's history of adolescent self-cutting and irregular nasal (and menstrual) bleeding, concluded that Fliess was "completely without blame", as Eckstein's post-operative haemorrhages were hysterical "wish-bleedings" linked to "an old wish to be loved in her illness" and triggered as a means of "rearousing [Freud's] affection". Eckstein nonetheless continued her analysis with Freud. She was restored to full mobility and went on to practice psychoanalysis herself.

Freud, who had called Fliess "the Kepler of biology", later concluded that a combination of a homoerotic attachment and the residue of his "specifically Jewish mysticism" lay behind his loyalty to his Jewish friend and his consequent over-estimation of both his theoretical and clinical work. Their friendship came to an acrimonious end with Fliess angry at Freud's unwillingness to endorse his general theory of sexual periodicity and accusing him of collusion in the plagiarism of his work. After Fliess failed to respond to Freud's offer of collaboration over publication of his "Three Essays on the Theory of Sexuality" in 1906, their relationship came to an end.

In 1902, Freud at last realised his long-standing ambition to be made a university professor. The title "professor extraordinarius" was important to Freud for the recognition and prestige it conferred, there being no salary or teaching duties attached to the post (he would be granted the enhanced status of "professor ordinarius" in 1920). Despite support from the university, his appointment had been blocked in successive years by the political authorities and it was secured only with the intervention of one of his more influential ex-patients, a Baroness Marie Ferstel, who (supposedly) had to bribe the minister of education with a valuable painting.

With his prestige thus enhanced, Freud continued with the regular series of lectures on his work which, since the mid-1880s as a docent of Vienna University, he had been delivering to small audiences every Saturday evening at the lecture hall of the university's psychiatric clinic.

From the autumn of 1902, a number of Viennese physicians who had expressed interest in Freud's work were invited to meet at his apartment every Wednesday afternoon to discuss issues relating to psychology and neuropathology. This group was called the Wednesday Psychological Society ("Psychologische Mittwochs-Gesellschaft") and it marked the beginnings of the worldwide psychoanalytic movement.

Freud founded this discussion group at the suggestion of the physician Wilhelm Stekel. Stekel had studied medicine at the University of Vienna under Richard von Krafft-Ebing. His conversion to psychoanalysis is variously attributed to his successful treatment by Freud for a sexual problem or as a result of his reading "The Interpretation of Dreams", to which he subsequently gave a positive review in the Viennese daily newspaper "Neues Wiener Tagblatt".

The other three original members whom Freud invited to attend, Alfred Adler, Max Kahane, and Rudolf Reitler, were also physicians and all five were Jewish by birth. Both Kahane and Reitler were childhood friends of Freud. Kahane had attended the same secondary school and both he and Reitler went to university with Freud. They had kept abreast of Freud's developing ideas through their attendance at his Saturday evening lectures. In 1901, Kahane, who first introduced Stekel to Freud's work, had opened an out-patient psychotherapy institute of which he was the director in Bauernmarkt, in Vienna. In the same year, his medical textbook, "Outline of Internal Medicine for Students and Practicing Physicians," was published. In it, he provided an outline of Freud's psychoanalytic method. Kahane broke with Freud and left the Wednesday Psychological Society in 1907 for unknown reasons and in 1923 committed suicide. Reitler was the director of an establishment providing thermal cures in Dorotheergasse which had been founded in 1901. He died prematurely in 1917. Adler, regarded as the most formidable intellect among the early Freud circle, was a socialist who in 1898 had written a health manual for the tailoring trade. He was particularly interested in the potential social impact of psychiatry.

Max Graf, a Viennese musicologist and father of "Little Hans", who had first encountered Freud in 1900 and joined the Wednesday group soon after its initial inception, described the ritual and atmosphere of the early meetings of the society:

The gatherings followed a definite ritual. First one of the members would present a paper. Then, black coffee and cakes were served; cigar and cigarettes were on the table and were consumed in great quantities. After a social quarter of an hour, the discussion would begin. The last and decisive word was always spoken by Freud himself. There was the atmosphere of the foundation of a religion in that room. Freud himself was its new prophet who made the heretofore prevailing methods of psychological investigation appear superficial.
By 1906, the group had grown to sixteen members, including Otto Rank, who was employed as the group's paid secretary. In the same year, Freud began a correspondence with Carl Gustav Jung who was by then already an academically acclaimed researcher into word-association and the Galvanic Skin Response, and a lecturer at Zurich University, although still only an assistant to Eugen Bleuler at the Burghölzli Mental Hospital in Zürich. In March 1907, Jung and Ludwig Binswanger, also a Swiss psychiatrist, travelled to Vienna to visit Freud and attend the discussion group. Thereafter, they established a small psychoanalytic group in Zürich. In 1908, reflecting its growing institutional status, the Wednesday group was reconstituted as the Vienna Psychoanalytic Society with Freud as President, a position he relinquished in 1910 in favor of Adler in the hope of neutralizing his increasingly critical standpoint.

The first woman member, Margarete Hilferding, joined the Society in 1910 and the following year she was joined by Tatiana Rosenthal and Sabina Spielrein who were both Russian psychiatrists and graduates of the Zürich University medical school. Prior to the completion of her studies, Spielrein had been a patient of Jung at the Burghölzli and the clinical and personal details of their relationship became the subject of an extensive correspondence between Freud and Jung. Both women would go on to make important contributions to the work of the Russian Psychoanalytic Society founded in 1910.

Freud's early followers met together formally for the first time at the Hotel Bristol, Salzburg on 27 April 1908. This meeting, which was retrospectively deemed to be the first International Psychoanalytic Congress, was convened at the suggestion of Ernest Jones, then a London-based neurologist who had discovered Freud's writings and begun applying psychoanalytic methods in his clinical work. Jones had met Jung at a conference the previous year and they met up again in Zürich to organize the Congress. There were, as Jones records, "forty-two present, half of whom were or became practicing analysts." In addition to Jones and the Viennese and Zürich contingents accompanying Freud and Jung, also present and notable for their subsequent importance in the psychoanalytic movement were Karl Abraham and Max Eitingon from Berlin, Sándor Ferenczi from Budapest and the New York-based Abraham Brill.

Important decisions were taken at the Congress with a view to advancing the impact of Freud's work. A journal, the "Jahrbuch für psychoanalytische und psychopathologishe Forschungen", was launched in 1909 under the editorship of Jung. This was followed in 1910 by the monthly "Zentralblatt für Psychoanalyse" edited by Adler and Stekel, in 1911 by "Imago", a journal devoted to the application of psychoanalysis to the field of cultural and literary studies edited by Rank and in 1913 by the "Internationale Zeitschrift für Psychoanalyse", also edited by Rank. Plans for an international association of psychoanalysts were put in place and these were implemented at the Nuremberg Congress of 1910 where Jung was elected, with Freud's support, as its first president.

Freud turned to Brill and Jones to further his ambition to spread the psychoanalytic cause in the English-speaking world. Both were invited to Vienna following the Salzburg Congress and a division of labour was agreed with Brill given the translation rights for Freud's works, and Jones, who was to take up a post at the University of Toronto later in the year, tasked with establishing a platform for Freudian ideas in North American academic and medical life. Jones's advocacy prepared the way for Freud's visit to the United States, accompanied by Jung and Ferenczi, in September 1909 at the invitation of Stanley Hall, president of Clark University, Worcester, Massachusetts, where he gave five lectures on psychoanalysis.

The event, at which Freud was awarded an Honorary Doctorate, marked the first public recognition of Freud's work and attracted widespread media interest. Freud's audience included the distinguished neurologist and psychiatrist James Jackson Putnam, Professor of Diseases of the Nervous System at Harvard, who invited Freud to his country retreat where they held extensive discussions over a period of four days. Putnam's subsequent public endorsement of Freud's work represented a significant breakthrough for the psychoanalytic cause in the United States. When Putnam and Jones organised the founding of the American Psychoanalytic Association in May 1911 they were elected president and secretary respectively. Brill founded the New York Psychoanalytic Society the same year. His English translations of Freud's work began to appear from 1909.

Some of Freud's followers subsequently withdrew from the International Psychoanalytical Association (IPA) and founded their own schools.

From 1909, Adler's views on topics such as neurosis began to differ markedly from those held by Freud. As Adler's position appeared increasingly incompatible with Freudianism, a series of confrontations between their respective viewpoints took place at the meetings of the Viennese Psychoanalytic Society in January and February 1911. In February 1911, Adler, then the president of the society, resigned his position. At this time, Stekel also resigned his position as vice president of the society. Adler finally left the Freudian group altogether in June 1911 to found his own organization with nine other members who had also resigned from the group. This new formation was initially called "Society for Free Psychoanalysis" but it was soon renamed the "Society for Individual Psychology". In the period after World War I, Adler became increasingly associated with a psychological position he devised called individual psychology.

In 1912, Jung published "Wandlungen und Symbole der Libido" (published in English in 1916 as "Psychology of the Unconscious") making it clear that his views were taking a direction quite different from those of Freud. To distinguish his system from psychoanalysis, Jung called it analytical psychology. Anticipating the final breakdown of the relationship between Freud and Jung, Ernest Jones initiated the formation of a secret Committee of loyalists charged with safeguarding the theoretical coherence and institutional legacy of the psychoanalytic movement. Formed in the autumn of 1912, the Committee comprised Freud, Jones, Abraham, Ferenczi, Rank, and Hanns Sachs. Max Eitingon joined the Committee in 1919. Each member pledged himself not to make any public departure from the fundamental tenets of psychoanalytic theory before he had discussed his views with the others. After this development, Jung recognised that his position was untenable and resigned as editor of the "Jarhbuch" and then as president of the IPA in April 1914. The Zürich Society withdrew from the IPA the following July.

Later the same year, Freud published a paper entitled "", the German original being first published in the "Jahrbuch", giving his view on the birth and evolution of the psychoanalytic movement and the withdrawal of Adler and Jung from it.

The final defection from Freud's inner circle occurred following the publication in 1924 of Rank's "The Trauma of Birth" which other members of the committee read as, in effect, abandoning the Oedipus Complex as the central tenet of psychoanalytic theory. Abraham and Jones became increasingly forceful critics of Rank and though he and Freud were reluctant to end their close and long-standing relationship the break finally came in 1926 when Rank resigned from his official posts in the IPA and left Vienna for Paris. His place on the committee was taken by Anna Freud. Rank eventually settled in the United States where his revisions of Freudian theory were to influence a new generation of therapists uncomfortable with the orthodoxies of the IPA.

After the founding of the IPA in 1910, an international network of psychoanalytical societies, training institutes and clinics became well established and a regular schedule of biannual Congresses commenced after the end of World War I to coordinate their activities.

Abraham and Eitingon founded the Berlin Psychoanalytic Society in 1910 and then the Berlin Psychoanalytic Institute and the Poliklinik in 1920. The Poliklinik's innovations of free treatment, and child analysis and the Berlin Institute's standardisation of psychoanalytic training had a major influence on the wider psychoanalytic movement. In 1927 Ernst Simmel founded the Schloss Tegel Sanatorium on the outskirts of Berlin, the first such establishment to provide psychoanalytic treatment in an institutional framework. Freud organised a fund to help finance its activities and his architect son, Ernst, was commissioned to refurbish the building. It was forced to close in 1931 for economic reasons.

The 1910 Moscow Psychoanalytic Society became the Russian Psychoanalytic Society and Institute in 1922. Freud's Russian followers were the first to benefit from translations of his work, the 1904 Russian translation of "The Interpretation of Dreams" appearing nine years before Brill's English edition. The Russian Institute was unique in receiving state support for its activities, including publication of translations of Freud's works. Support was abruptly annulled in 1924, when Joseph Stalin came to power, after which psychoanalysis was denounced on ideological grounds.

After helping found the American Psychoanalytic Association in 1911, Ernest Jones returned to Britain from Canada in 1913 and founded the London Psychoanalytic Society the same year. In 1919, he dissolved this organisation and, with its core membership purged of Jungian adherents, founded the British Psychoanalytical Society, serving as its president until 1944. The Institute of Psychoanalysis was established 1924 and the London Clinic of Psychoanalysis established in 1926, both under Jones's directorship.

The Vienna Ambulatorium (Clinic) was established in 1922 and the Vienna Psychoanalytic Institute was founded in 1924 under the directorship of Helene Deutsch. Ferenczi founded the Budapest Psychoanalytic Institute in 1913 and a clinic in 1929.

Psychoanalytic societies and institutes were established in Switzerland (1919), France (1926), Italy (1932), the Netherlands (1933), Norway (1933) and in Palestine (Jerusalem, 1933) by Eitingon, who had fled Berlin after Adolf Hitler came to power. The New York Psychoanalytic Institute was founded in 1931.

The 1922 Berlin Congress was the last Freud attended. By this time his speech had become seriously impaired by the prosthetic device he needed as a result of a series of operations on his cancerous jaw. He kept abreast of developments through a regular correspondence with his principal followers and via the circular letters and meetings of the secret Committee which he continued to attend.

The Committee continued to function until 1927 by which time institutional developments within the IPA, such as the establishment of the International Training Commission, had addressed concerns about the transmission of psychoanalytic theory and practice. There remained, however, significant differences over the issue of lay analysis – i.e. the acceptance of non-medically qualified candidates for psychoanalytic training. Freud set out his case in favour in 1926 in his "The Question of Lay Analysis". He was resolutely opposed by the American societies who expressed concerns over professional standards and the risk of litigation (though child analysts were made exempt). These concerns were also shared by some of his European colleagues. Eventually an agreement was reached allowing societies autonomy in setting criteria for candidature.

In 1930 Freud was awarded the Goethe Prize in recognition of his contributions to psychology and to German literary culture.

Freud used pseudonyms in his case histories. Some patients known by pseudonyms were Cäcilie M. (Anna von Lieben); Dora (Ida Bauer, 1882–1945); Frau Emmy von N. (Fanny Moser); Fräulein Elisabeth von R. (Ilona Weiss); Fräulein Katharina (Aurelia Kronich); Fräulein Lucy R.; (Herbert Graf, 1903–1973); Rat Man (Ernst Lanzer, 1878–1914); Enos Fingy (Joshua Wild, 1878–1920); and Wolf Man (Sergei Pankejeff, 1887–1979). Other famous patients included Prince Pedro Augusto of Brazil (1866–1934); H.D. (1886–1961); Emma Eckstein (1865–1924); Gustav Mahler (1860–1911), with whom Freud had only a single, extended consultation; Princess Marie Bonaparte; Edith Banfield Jackson (1895–1977); and Albert Hirst (1887–1974).

In February 1923, Freud detected a leukoplakia, a benign growth associated with heavy smoking, on his mouth. He initially kept this secret, but in April 1923 he informed Ernest Jones, telling him that the growth had been removed. Freud consulted the dermatologist Maximilian Steiner, who advised him to quit smoking but lied about the growth's seriousness, minimizing its importance. Freud later saw Felix Deutsch, who saw that the growth was cancerous; he identified it to Freud using the euphemism "a bad leukoplakia" instead of the technical diagnosis epithelioma. Deutsch advised Freud to stop smoking and have the growth excised. Freud was treated by Marcus Hajek, a rhinologist whose competence he had previously questioned. Hajek performed an unnecessary cosmetic surgery in his clinic's outpatient department. Freud bled during and after the operation, and may narrowly have escaped death. Freud subsequently saw Deutsch again. Deutsch saw that further surgery would be required, but did not tell Freud he had cancer because he was worried that Freud might wish to commit suicide.

In January 1933, the Nazi Party took control of Germany, and Freud's books were prominent among those they burned and destroyed. Freud remarked to Ernest Jones: "What progress we are making. In the Middle Ages they would have burned me. Now, they are content with burning my books." Freud continued to underestimate the growing Nazi threat and remained determined to stay in Vienna, even following the Anschluss of 13 March 1938, in which Nazi Germany annexed Austria, and the outbreaks of violent antisemitism that ensued. Jones, the then president of the International Psychoanalytical Association (IPA), flew into Vienna from London via Prague on 15 March determined to get Freud to change his mind and seek exile in Britain. This prospect and the shock of the arrest and interrogation of Anna Freud by the Gestapo finally convinced Freud it was time to leave Austria. Jones left for London the following week with a list provided by Freud of the party of émigrés for whom immigration permits would be required. Back in London, Jones used his personal acquaintance with the Home Secretary, Sir Samuel Hoare, to expedite the granting of permits. There were seventeen in all and work permits were provided where relevant. Jones also used his influence in scientific circles, persuading the president of the Royal Society, Sir William Bragg, to write to the Foreign Secretary Lord Halifax, requesting to good effect that diplomatic pressure be applied in Berlin and Vienna on Freud's behalf. Freud also had support from American diplomats, notably his ex-patient and American ambassador to France, William Bullitt. Bullitt alerted U.S. President Roosevelt to the increased dangers facing the Freuds, resulting in the American consul-general in Vienna, John Cooper Wiley, arranging regular monitoring of Berggasse 19. He also intervened by phone call during the Gestapo interrogation of Anna Freud.

The departure from Vienna began in stages throughout April and May 1938. Freud's grandson Ernst Halberstadt and Freud's son Martin's wife and children left for Paris in April. Freud's sister-in-law, Minna Bernays, left for London on 5 May, Martin Freud the following week and Freud's daughter Mathilde and her husband, Robert Hollitscher, on 24 May.

By the end of the month, arrangements for Freud's own departure for London had become stalled, mired in a legally tortuous and financially extortionate process of negotiation with the Nazi authorities. Under regulations imposed on its Jewish population by the new Nazi regime, a Kommissar was appointed to manage Freud's assets and those of the IPA whose headquarters were nearby Freud's home. Freud was allocated to Dr. Anton Sauerwald, who had studied chemistry at Vienna University under Professor Josef Herzig, an old friend of Freud's. Sauerwald read Freud's books to further learn about him and became sympathetic towards his situation. Though required to disclose details of all Freud's bank accounts to his superiors and to arrange the destruction of the historic library of books housed in the offices of the IPA, Sauerwald did neither. Instead he removed evidence of Freud's foreign bank accounts to his own safe-keeping and arranged the storage of the IPA library in the Austrian National Library where it remained until the end of the war.

Though Sauerwald's intervention lessened the financial burden of the "flight" tax on Freud's declared assets, other substantial charges were levied in relation to the debts of the IPA and the valuable collection of antiquities Freud possessed. Unable to access his own accounts, Freud turned to Princess Marie Bonaparte, the most eminent and wealthy of his French followers, who had travelled to Vienna to offer her support and it was she who made the necessary funds available. This allowed Sauerwald to sign the necessary exit visas for Freud, his wife Martha and daughter Anna. They left Vienna on the Orient Express on 4 June, accompanied by their housekeeper and a doctor, arriving in Paris the following day where they stayed as guests of Marie Bonaparte before travelling overnight to London arriving at London Victoria station on 6 June.

Among those soon to call on Freud to pay their respects were Salvador Dalí, Stefan Zweig, Leonard Woolf, Virginia Woolf and H. G. Wells. Representatives of the Royal Society called with the Society's Charter for Freud, who had been elected a Foreign Member in 1936, to sign himself into membership. Marie Bonaparte arrived towards the end of June to discuss the fate of Freud's four elderly sisters left behind in Vienna. Her subsequent attempts to get them exit visas failed and they would all die in Nazi concentration camps.
In early 1939 Sauerwald arrived in London in mysterious circumstances where he met Freud's brother Alexander. He was tried and imprisoned in 1945 by an Austrian court for his activities as a Nazi Party official. Responding to a plea from his wife, Anna Freud wrote to confirm that Sauerwald "used his office as our appointed commissar in such a manner as to protect my father". Her intervention helped secure his release from jail in 1947.

In the Freuds' new home, 20 Maresfield Gardens, Hampstead, North London, Freud's Vienna consulting room was recreated in faithful detail. He continued to see patients there until the terminal stages of his illness. He also worked on his last books, "Moses and Monotheism", published in German in 1938 and in English the following year and the uncompleted "An Outline of Psychoanalysis" which was published posthumously.

By mid-September 1939, Freud's cancer of the jaw was causing him increasingly severe pain and had been declared to be inoperable. The last book he read, Balzac's "La Peau de chagrin", prompted reflections on his own increasing frailty and a few days later he turned to his doctor, friend and fellow refugee, Max Schur, reminding him that they had previously discussed the terminal stages of his illness: "Schur, you remember our 'contract' not to leave me in the lurch when the time had come. Now it is nothing but torture and makes no sense." When Schur replied that he had not forgotten, Freud said, "I thank you," and then "Talk it over with Anna, and if she thinks it's right, then make an end of it." Anna Freud wanted to postpone her father's death, but Schur convinced her it was pointless to keep him alive and on 21 and 22 September administered doses of morphine that resulted in Freud's death around 3 am on 23 September 1939. However, discrepancies in the various accounts Schur gave of his role in Freud's final hours, which have in turn led to inconsistencies between Freud's main biographers, has led to further research and a revised account. This proposes that Schur was absent from Freud's deathbed when a third and final dose of morphine was administered by Dr Josephine Stross, a colleague of Anna Freud, leading to Freud's death around midnight on 23 September 1939.

Three days after his death Freud's body was cremated at the Golders Green Crematorium in North London, with Harrods acting as funeral directors, on the instructions of his son, Ernst. Funeral orations were given by Ernest Jones and the Austrian author Stefan Zweig. Freud's ashes were later placed in the crematorium's "Ernest George Columbarium" (see "Freud Corner"). They rest on a plinth designed by his son, Ernst, in a sealed ancient Greek bell krater painted with Dionysian scenes that Freud had received as a gift from Marie Bonaparte and which he had kept in his study in Vienna for many years. After his wife, Martha, died in 1951, her ashes were also placed in the urn.

Freud began his study of medicine at the University of Vienna in 1873. He took almost nine years to complete his studies, due to his interest in neurophysiological research, specifically investigation of the sexual anatomy of eels and the physiology of the fish nervous system, and because of his interest in studying philosophy with Franz Brentano. He entered private practice in neurology for financial reasons, receiving his M.D. degree in 1881 at the age of 25. Amongst his principal concerns in the 1880s was the anatomy of the brain, specifically the medulla oblongata. He intervened in the important debates about aphasia with his monograph of 1891, "Zur Auffassung der Aphasien", in which he coined the term agnosia and counselled against a too locationist view of the explanation of neurological deficits. Like his contemporary Eugen Bleuler, he emphasized brain function rather than brain structure.

Freud was also an early researcher in the field of cerebral palsy, which was then known as "cerebral paralysis". He published several medical papers on the topic, and showed that the disease existed long before other researchers of the period began to notice and study it. He also suggested that William John Little, the man who first identified cerebral palsy, was wrong about lack of oxygen during birth being a cause. Instead, he suggested that complications in birth were only a symptom.

Freud hoped that his research would provide a solid scientific basis for his therapeutic technique. The goal of Freudian therapy, or psychoanalysis, was to bring repressed thoughts and feelings into consciousness in order to free the patient from suffering repetitive distorted emotions.

Classically, the bringing of unconscious thoughts and feelings to consciousness is brought about by encouraging a patient to talk about dreams and engage in free association, in which patients report their thoughts without reservation and make no attempt to concentrate while doing so. Another important element of psychoanalysis is transference, the process by which patients displace onto their analysts feelings and ideas which derive from previous figures in their lives. Transference was first seen as a regrettable phenomenon that interfered with the recovery of repressed memories and disturbed patients' objectivity, but by 1912, Freud had come to see it as an essential part of the therapeutic process.

The origin of Freud's early work with psychoanalysis can be linked to Josef Breuer. Freud credited Breuer with opening the way to the discovery of the psychoanalytical method by his treatment of the case of Anna O. In November 1880, Breuer was called in to treat a highly intelligent 21-year-old woman (Bertha Pappenheim) for a persistent cough that he diagnosed as hysterical. He found that while nursing her dying father, she had developed a number of transitory symptoms, including visual disorders and paralysis and contractures of limbs, which he also diagnosed as hysterical. Breuer began to see his patient almost every day as the symptoms increased and became more persistent, and observed that she entered states of "absence". He found that when, with his encouragement, she told fantasy stories in her evening states of "absence" her condition improved, and most of her symptoms had disappeared by April 1881. Following the death of her father in that month her condition deteriorated again. Breuer recorded that some of the symptoms eventually remitted spontaneously, and that full recovery was achieved by inducing her to recall events that had precipitated the occurrence of a specific symptom. In the years immediately following Breuer's treatment, Anna O. spent three short periods in sanatoria with the diagnosis "hysteria" with "somatic symptoms", and some authors have challenged Breuer's published account of a cure. Richard Skues rejects this interpretation, which he sees as stemming from both Freudian and anti-psychoanalytical revisionism, that regards both Breuer's narrative of the case as unreliable and his treatment of Anna O. as a failure. Psychologist Frank Sulloway contends that "Freud's case histories are rampant with censorship, distortions, highly dubious 'reconstructions,' and exaggerated claims."

In the early 1890s, Freud used a form of treatment based on the one that Breuer had described to him, modified by what he called his "pressure technique" and his newly developed analytic technique of interpretation and reconstruction. According to Freud's later accounts of this period, as a result of his use of this procedure most of his patients in the mid-1890s reported early childhood sexual abuse. He believed these stories, which he used as the basis for his seduction theory, but then he came to believe that they were fantasies. He explained these at first as having the function of "fending off" memories of infantile masturbation, but in later years he wrote that they represented Oedipal fantasies, stemming from innate drives that are sexual and destructive in nature.

Another version of events focuses on Freud's proposing that unconscious memories of infantile sexual abuse were at the root of the psychoneuroses in letters to Fliess in October 1895, before he reported that he had actually discovered such abuse among his patients. In the first half of 1896, Freud published three papers, which led to his seduction theory, stating that he had uncovered, in all of his current patients, deeply repressed memories of sexual abuse in early childhood. In these papers, Freud recorded that his patients were not consciously aware of these memories, and must therefore be present as "unconscious memories" if they were to result in hysterical symptoms or obsessional neurosis. The patients were subjected to considerable pressure to "reproduce" infantile sexual abuse "scenes" that Freud was convinced had been repressed into the unconscious. Patients were generally unconvinced that their experiences of Freud's clinical procedure indicated actual sexual abuse. He reported that even after a supposed "reproduction" of sexual scenes the patients assured him emphatically of their disbelief.

As well as his pressure technique, Freud's clinical procedures involved analytic inference and the symbolic interpretation of symptoms to trace back to memories of infantile sexual abuse. His claim of one hundred percent confirmation of his theory only served to reinforce previously expressed reservations from his colleagues about the validity of findings obtained through his suggestive techniques. Freud subsequently showed inconsistency as to whether his seduction theory was still compatible with his later findings. In an addendum to "The Aetiology of Hysteria" he stated: "All this is true [the sexual abuse of children]; but it must be remembered that at the time I wrote it I had not yet freed myself from my overvaluation of reality and my low valuation of phantasy". Some years later Freud explicitly rejected the claim of his colleague Ferenczi that his patients' reports of sexual molestation were actual memories instead of fantasies, and he tried to dissuade Ferenczi from making his views public. Karin Ahbel-Rappe concludes in her study "'I no longer believe': did Freud abandon the seduction theory?": "Freud marked out and started down a trail of investigation into the nature of the experience of infantile incest and its impact on the human psyche, and then abandoned this direction for the most part."

As a medical researcher, Freud was an early user and proponent of cocaine as a stimulant as well as analgesic. He believed that cocaine was a cure for many mental and physical problems, and in his 1884 paper "On Coca" he extolled its virtues. Between 1883 and 1887 he wrote several articles recommending medical applications, including its use as an antidepressant. He narrowly missed out on obtaining scientific priority for discovering its anesthetic properties of which he was aware but had mentioned only in passing. (Karl Koller, a colleague of Freud's in Vienna, received that distinction in 1884 after reporting to a medical society the ways cocaine could be used in delicate eye surgery.) Freud also recommended cocaine as a cure for morphine addiction. He had introduced cocaine to his friend Ernst von Fleischl-Marxow, who had become addicted to morphine taken to relieve years of excruciating nerve pain resulting from an infection acquired after injuring himself while performing an autopsy. His claim that Fleischl-Marxow was cured of his addiction was premature, though he never acknowledged that he had been at fault. Fleischl-Marxow developed an acute case of "cocaine psychosis", and soon returned to using morphine, dying a few years later still suffering from intolerable pain.

The application as an anesthetic turned out to be one of the few safe uses of cocaine, and as reports of addiction and overdose began to filter in from many places in the world, Freud's medical reputation became somewhat tarnished. After the "Cocaine Episode" Freud ceased to publicly recommend use of the drug, but continued to take it himself occasionally for depression, migraine and nasal inflammation during the early 1890s, before discontinuing its use in 1896.

The concept of the unconscious was central to Freud's account of the mind. Freud believed that while poets and thinkers had long known of the existence of the unconscious, he had ensured that it received scientific recognition in the field of psychology.

Freud states explicitly that his concept of the unconscious as he first formulated it was based on the theory of repression. He postulated a cycle in which ideas are repressed, but remain in the mind, removed from consciousness yet operative, then reappear in consciousness under certain circumstances. The postulate was based upon the investigation of cases of hysteria, which revealed instances of behaviour in patients that could not be explained without reference to ideas or thoughts of which they had no awareness and which analysis revealed were linked to the (real or imagined) repressed sexual scenarios of childhood. In his later re-formulations of the concept of repression in his 1915 paper 'Repression' ("Standard Edition" XIV) Freud introduced the distinction in the unconscious between primary repression linked to the universal taboo on incest ('innately present originally') and repression ('after expulsion') that was a product of an individual's life history ('acquired in the course of the ego's development') in which something that was at one point conscious is rejected or eliminated from consciousness.

In his account of the development and modification of his theory of unconscious mental processes he sets out in his 1915 paper 'The Unconscious' ("Standard Edition" XIV), Freud identifies the three perspectives he employs: the dynamic, the economic and the topographical.

The dynamic perspective concerns firstly the constitution of the unconscious by repression and secondly the process of "censorship" which maintains unwanted, anxiety inducing thoughts as such. Here Freud is drawing on observations from his earliest clinical work in the treatment of hysteria.

In the economic perspective the focus is upon the trajectories of the repressed contents "the vicissitudes of sexual impulses" as they undergo complex transformations in the process of both symptom formation and normal unconscious thought such as dreams and slips of the tongue. These were topics Freud explored in detail in "The Interpretation of Dreams" and "The Psychopathology of Everyday Life."

Whereas both these former perspectives focus on the unconscious as it is about to enter consciousness, the topographical perspective represents a shift in which the systemic properties of the unconscious, its characteristic processes and modes of operation such as condensation and displacement, are placed in the foreground.

This "first topography" presents a model of psychic structure comprising three systems:


In his later work, notably in "The Ego and the Id" (1923), a second topography is introduced comprising id, ego and super-ego, which is superimposed on the first without replacing it. In this later formulation of the concept of the unconscious the id comprises a reservoir of instincts or drives a portion of them being hereditary or innate, a portion repressed or acquired. As such, from the economic perspective, the id is the prime source of psychical energy and from the dynamic perspective it conflicts with the ego and the super-ego which, genetically speaking, are diversifications of the id.

Freud believed the function of dreams is to preserve sleep by representing as fulfilled wishes that would otherwise awaken the dreamer.

In Freud's theory dreams are instigated by the daily occurrences and thoughts of everyday life. In what Freud called the "dream-work", these "secondary process" thoughts ("word presentations"), governed by the rules of language and the reality principle, become subject to the "primary process" of unconscious thought ("thing presentations") governed by the pleasure principle, wish gratification and the repressed sexual scenarios of childhood. Because of the disturbing nature of the latter and other repressed thoughts and desires which may have become linked to them, the dream-work operates a censorship function, disguising by distortion, displacement and condensation the repressed thoughts so as to preserve sleep.

In the clinical setting Freud encouraged free association to the dream's manifest content, as recounted in the dream narrative, so as to facilitate interpretative work on its latent content – the repressed thoughts and fantasies – and also on the underlying mechanisms and structures operative in the dream-work. As Freud developed his theoretical work on dreams he went beyond his theory of dreams as wish-fulfillments to arrive at an emphasis on dreams as "nothing other than a particular form of thinking... It is the dream-work that creates that form, and it alone is the essence of dreaming".

Freud's theory of psychosexual development proposes that, following on from the initial polymorphous perversity of infantile sexuality, the sexual "drives" pass through the distinct developmental phases of the oral, the anal, and the phallic. Though these phases then give way to a latency stage of reduced sexual interest and activity (from the age of five to puberty, approximately), they leave, to a greater or lesser extent, a "perverse" and bisexual residue which persists during the formation of adult genital sexuality. Freud argued that neurosis or perversion could be explained in terms of fixation or regression to these phases whereas adult character and cultural creativity could achieve a sublimation of their perverse residue.

After Freud's later development of the theory of the Oedipus complex this normative developmental trajectory becomes formulated in terms of the child's renunciation of incestuous desires under the fantasised threat of (or phantasised fact of, in the case of the girl) castration. The "dissolution" of the Oedipus complex is then achieved when the child's rivalrous identification with the parental figure is transformed into the pacifying identifications of the Ego ideal which assume both similarity and difference and acknowledge the separateness and autonomy of the other.

Freud hoped to prove that his model was universally valid and turned to ancient mythology and contemporary ethnography for comparative material arguing that totemism reflected a ritualized enactment of a tribal Oedipal conflict.

Freud proposed that the human psyche could be divided into three parts: Id, ego and super-ego. Freud discussed this model in the 1920 essay "Beyond the Pleasure Principle", and fully elaborated upon it in "The Ego and the Id" (1923), in which he developed it as an alternative to his previous topographic schema (i.e., conscious, unconscious and preconscious). The id is the completely unconscious, impulsive, childlike portion of the psyche that operates on the "pleasure principle" and is the source of basic impulses and drives; it seeks immediate pleasure and gratification.

Freud acknowledged that his use of the term "Id" ("das Es", "the It") derives from the writings of Georg Groddeck. The super-ego is the moral component of the psyche, which takes into account no special circumstances in which the morally right thing may not be right for a given situation. The rational ego attempts to exact a balance between the impractical hedonism of the id and the equally impractical moralism of the super-ego; it is the part of the psyche that is usually reflected most directly in a person's actions. When overburdened or threatened by its tasks, it may employ defence mechanisms including denial, repression, undoing, rationalization, and displacement. This concept is usually represented by the "Iceberg Model". This model represents the roles the id, ego, and super- ego play in relation to conscious and unconscious thought.

Freud compared the relationship between the ego and the id to that between a charioteer and his horses: the horses provide the energy and drive, while the charioteer provides direction.

Freud believed that the human psyche is subject to two conflicting drives: the life drive or libido and the death drive. The life drive was also termed "Eros" and the death drive "Thanatos", although Freud did not use the latter term; "Thanatos" was introduced in this context by Paul Federn. Freud hypothesized that libido is a form of mental energy with which processes, structures and object-representations are invested.

In "Beyond the Pleasure Principle" (1920), Freud inferred the existence of a death drive. Its premise was a regulatory principle that has been described as "the principle of psychic inertia", "the Nirvana principle", and "the conservatism of instinct". Its background was Freud's earlier "Project for a Scientific Psychology", where he had defined the principle governing the mental apparatus as its tendency to divest itself of quantity or to reduce tension to zero. Freud had been obliged to abandon that definition, since it proved adequate only to the most rudimentary kinds of mental functioning, and replaced the idea that the apparatus tends toward a level of zero tension with the idea that it tends toward a minimum level of tension.

Freud in effect readopted the original definition in "Beyond the Pleasure Principle", this time applying it to a different principle. He asserted that on certain occasions the mind acts as though it could eliminate tension entirely, or in effect to reduce itself to a state of extinction; his key evidence for this was the existence of the compulsion to repeat. Examples of such repetition included the dream life of traumatic neurotics and children's play. In the phenomenon of repetition, Freud saw a psychic trend to work over earlier impressions, to master them and derive pleasure from them, a trend was prior to the pleasure principle but not opposed to it. In addition to that trend, there was also a principle at work that was opposed to, and thus "beyond" the pleasure principle. If repetition is a necessary element in the binding of energy or adaptation, when carried to inordinate lengths it becomes a means of abandoning adaptations and reinstating earlier or less evolved psychic positions. By combining this idea with the hypothesis that all repetition is a form of discharge, Freud reached the conclusion that the compulsion to repeat is an effort to restore a state that is both historically primitive and marked by the total draining of energy: death.

In his 1917 essay "Mourning and Melancholia", Freud drew a distinction between mourning, painful but an inevitable part of life, and "melancholia", his term for pathological refusal of a mourner to "decathect" from the lost one. Freud claimed that, in normal mourning, the ego was responsible for narcissistically detaching the libido from the lost one as a means of self-preservation, but that in "melancholia", prior ambivalence towards the lost one prevents this from occurring. Suicide, Freud hypothesized, could result in extreme cases, when unconscious feelings of conflict became directed against the mourner's own ego.

Initiating what became the first debate within psychoanalysis on femininity, Karen Horney of the Berlin Institute set out to challenge Freud's account of the development of feminine sexuality. Rejecting Freud's theories of the feminine castration complex and penis envy, Horney argued for a primary femininity and penis envy as a defensive formation rather than arising from the fact, or "injury", of biological asymmetry as Freud held. Horney had the influential support of Melanie Klein and Ernest Jones who coined the term "phallocentrism" in his critique of Freud's position.

In defending Freud against this critique, feminist scholar Jacqueline Rose has argued that it presupposes a more normative account of female sexual development than that given by Freud. She notes that Freud moved from a description of the little girl stuck with her 'inferiority' or 'injury' in the face of the anatomy of the little boy to an account in his later work which explicitly describes the process of becoming 'feminine' as an 'injury' or 'catastrophe' for the complexity of her earlier psychic and sexual life.

According to Freud, "Elimination of clitoral sexuality is a necessary precondition for the development of femininity, since it is immature and masculine in its nature." Freud postulated the concept of "vaginal orgasm" as separate from clitoral orgasm, achieved by external stimulation of the clitoris. In 1905, he stated that clitoral orgasms are purely an adolescent phenomenon and that, upon reaching puberty, the proper response of mature women is a change-over to vaginal orgasms, meaning orgasms without any clitoral stimulation. This theory has been criticized on the grounds that Freud provided no evidence for this basic assumption, and because it made many women feel inadequate when they could not achieve orgasm via vaginal intercourse alone.

Freud regarded the monotheistic God as an illusion based upon the infantile emotional need for a powerful, supernatural pater familias. He maintained that religion – once necessary to restrain man's violent nature in the early stages of civilization – in modern times, can be set aside in favor of reason and science. "Obsessive Actions and Religious Practices" (1907) notes the likeness between faith (religious belief) and neurotic obsession. "Totem and Taboo" (1913) proposes that society and religion begin with the patricide and eating of the powerful paternal figure, who then becomes a revered collective memory. These arguments were further developed in "The Future of an Illusion" (1927) in which Freud argued that religious belief serves the function of psychological consolation. Freud argues the belief of a supernatural protector serves as a buffer from man's "fear of nature" just as the belief in an afterlife serves as a buffer from man's fear of death. The core idea of the work is that all of religious belief can be explained through its function to society, not for its relation to the truth. This is why, according to Freud, religious beliefs are "illusions". In "Civilization and Its Discontents" (1930), he quotes his friend Romain Rolland, who described religion as an "oceanic sensation", but says he never experienced this feeling. "Moses and Monotheism" (1937) proposes that Moses was the tribal pater familias, killed by the Jews, who psychologically coped with the patricide with a reaction formation conducive to their establishing monotheist Judaism; analogously, he described the Roman Catholic rite of Holy Communion as cultural evidence of the killing and devouring of the sacred father.

Moreover, he perceived religion, with its suppression of violence, as mediator of the societal and personal, the public and the private, conflicts between Eros and Thanatos, the forces of life and death. Later works indicate Freud's pessimism about the future of civilization, which he noted in the 1931 edition of "Civilization and its Discontents".

In a footnote of his 1909 work, "Analysis of a Phobia in a Five year old Boy", Freud theorized that the universal fear of castration was provoked in the uncircumcised when they perceived circumcision and that this was "the deepest unconscious root of anti-Semitism".

Freud's legacy, though a highly contested area of controversy, was described by Stephen Frosh as "one of the strongest influences on twentieth-century thought, its impact comparable only to that of Darwinism and Marxism." Henri Ellenberger stated that its range of influence permeated "all the fields of culture ... so far as to change our way of life and concept of man." 

Though not the first methodology in the practice of individual verbal psychotherapy, Freud's psychoanalytic system came to dominate the field from early in the twentieth century, forming the basis for many later variants. While these systems have adopted different theories and techniques, all have followed Freud by attempting to achieve psychic and behavioral change through having patients talk about their difficulties. Psychoanalysis is not as influential as it once was in Europe and the United States, though in some parts of the world, notably Latin America, its influence in the later 20th century expanded substantially. Psychoanalysis also remains influential within many contemporary schools of psychotherapy and has led to innovative therapeutic work in schools and with families and groups. There is a substantial body of research which demonstrates the efficacy of the clinical methods of psychoanalysis and of related psychodynamic therapies in treating a wide range of psychological disorders.

The neo-Freudians, a group including Alfred Adler, Otto Rank, Karen Horney, Harry Stack Sullivan and Erich Fromm, rejected Freud's theory of instinctual drive, emphasized interpersonal relations and self-assertiveness, and made modifications to therapeutic practice that reflected these theoretical shifts. Adler originated the approach, although his influence was indirect due to his inability to systematically formulate his ideas. Neo-Freudian analysis places more emphasis on the patient's relationship with the analyst and less on exploration of the unconscious.

Carl Jung believed that the collective unconscious, which reflects the cosmic order and the history of the human species, is the most important part of the mind. It contains archetypes, which are manifested in symbols that appear in dreams, disturbed states of mind, and various products of culture. Jungians are less interested in infantile development and psychological conflict between wishes and the forces that frustrate them than in integration between different parts of the person. The object of Jungian therapy was to mend such splits. Jung focused in particular on problems of middle and later life. His objective was to allow people to experience the split-off aspects of themselves, such as the anima (a man's suppressed female self), the animus (a woman's suppressed male self), or the shadow (an inferior self-image), and thereby attain wisdom.

Jacques Lacan approached psychoanalysis through linguistics and literature. Lacan believed Freud's essential work had been done prior to 1905 and concerned the interpretation of dreams, neurotic symptoms, and slips, which had been based on a revolutionary way of understanding language and its relation to experience and subjectivity, and that ego psychology and object relations theory were based upon misreadings of Freud's work. For Lacan, the determinative dimension of human experience is neither the self (as in ego psychology) nor relations with others (as in object relations theory), but language. Lacan saw desire as more important than need and considered it necessarily ungratifiable.

Wilhelm Reich developed ideas that Freud had developed at the beginning of his psychoanalytic investigation but then superseded but never finally discarded. These were the concept of the Actualneurosis and a theory of anxiety based upon the idea of dammed-up libido. In Freud's original view, what really happened to a person (the "actual") determined the resulting neurotic disposition. Freud applied that idea both to infants and to adults. In the former case, seductions were sought as the causes of later neuroses and in the latter incomplete sexual release. Unlike Freud, Reich retained the idea that actual experience, especially sexual experience, was of key significance. By the 1920s, Reich had "taken Freud's original ideas about sexual release to the point of specifying the orgasm as the criteria of healthy function." Reich was also "developing his ideas about character into a form that would later take shape, first as "muscular armour", and eventually as a transducer of universal biological energy, the "orgone"."

Fritz Perls, who helped to develop Gestalt therapy, was influenced by Reich, Jung and Freud. The key idea of gestalt therapy is that Freud overlooked the structure of awareness, "an active process that moves toward the construction of organized meaningful wholes... between an organism and its environment." These wholes, called "gestalts", are "patterns involving all the layers of organismic function – thought, feeling, and activity." Neurosis is seen as splitting in the formation of gestalts, and anxiety as the organism sensing "the struggle towards its creative unification." Gestalt therapy attempts to cure patients through placing them in contact with "immediate organismic needs." Perls rejected the verbal approach of classical psychoanalysis; talking in gestalt therapy serves the purpose of self-expression rather than gaining self-knowledge. Gestalt therapy usually takes place in groups, and in concentrated "workshops" rather than being spread out over a long period of time; it has been extended into new forms of communal living.

Arthur Janov's primal therapy, which has been an influential post-Freudian psychotherapy, resembles psychoanalytic therapy in its emphasis on early childhood experience, but has also differences with it. While Janov's theory is akin to Freud's early idea of Actualneurosis, he does not have a dynamic psychology but a nature psychology like that of Reich or Perls, in which need is primary while wish is derivative and dispensable when need is met. Despite its surface similarity to Freud's ideas, Janov's theory lacks a strictly psychological account of the unconscious and belief in infantile sexuality. While for Freud there was a hierarchy of danger situations, for Janov the key event in the child's life is awareness that the parents do not love it. Janov writes in "The Primal Scream" (1970) that primal therapy has in some ways returned to Freud's early ideas and techniques.

Ellen Bass and Laura Davis, co-authors of "The Courage to Heal" (1988), are described as "champions of survivorship" by Frederick Crews, who considers Freud the key influence upon them, although in his view they are indebted not to classic psychoanalysis but to "the pre-psychoanalytic Freud... who supposedly took pity on his hysterical patients, found that they were all harboring memories of early abuse... and cured them by unknotting their repression." Crews sees Freud as having anticipated the recovered memory movement by emphasizing "mechanical cause-and-effect relations between symptomatology and the premature stimulation of one body zone or another", and with pioneering its "technique of thematically matching a patient's symptom with a sexually symmetrical 'memory.'" Crews believes that Freud's confidence in accurate recall of early memories anticipates the theories of recovered memory therapists such as Lenore Terr, which in his view have led to people being wrongfully imprisoned or involved in litigation.

Research projects designed to test Freud's theories empirically have led to a vast literature on the topic. American psychologists began to attempt to study repression in the experimental laboratory around 1930. In 1934, when the psychologist Saul Rosenzweig sent Freud reprints of his attempts to study repression, Freud responded with a dismissive letter stating that "the wealth of reliable observations" on which psychoanalytic assertions were based made them "independent of experimental verification." Seymour Fisher and Roger P. Greenberg concluded in 1977 that some of Freud's concepts were supported by empirical evidence. Their analysis of research literature supported Freud's concepts of oral and anal personality constellations, his account of the role of Oedipal factors in certain aspects of male personality functioning, his formulations about the relatively greater concern about loss of love in women's as compared to men's personality economy, and his views about the instigating effects of homosexual anxieties on the formation of paranoid delusions. They also found limited and equivocal support for Freud's theories about the development of homosexuality. They found that several of Freud's other theories, including his portrayal of dreams as primarily containers of secret, unconscious wishes, as well as some of his views about the psychodynamics of women, were either not supported or contradicted by research. Reviewing the issues again in 1996, they concluded that much experimental data relevant to Freud's work exists, and supports some of his major ideas and theories.

Other viewpoints include those of Hans Eysenck, who writes in "Decline and Fall of the Freudian Empire" (1985) that Freud set back the study of psychology and psychiatry "by something like fifty years or more", and Malcolm Macmillan, who concludes in "Freud Evaluated" (1991) that "Freud's method is not capable of yielding objective data about mental processes". Morris Eagle states that it has been "demonstrated quite conclusively that because of the epistemologically contaminated status of clinical data derived from the clinical situation, such data have questionable probative value in the testing of psychoanalytic hypotheses". Richard Webster, in "Why Freud Was Wrong" (1995), described psychoanalysis as perhaps the most complex and successful pseudoscience in history. Crews believes that psychoanalysis has no scientific or therapeutic merit.

I.B. Cohen regards Freud's "Interpretation of Dreams" as a revolutionary work of science, the last such work to be published in book form.
In contrast Allan Hobson believes that Freud, by rhetorically discrediting 19th century investigators of dreams such as Alfred Maury and the Marquis de Hervey de Saint-Denis at a time when study of the physiology of the brain was only beginning, interrupted the development of scientific dream theory for half a century. The dream researcher G. William Domhoff has disputed claims of Freudian dream theory being validated.

The philosopher Karl Popper, who argued that all proper scientific theories must be potentially falsifiable, claimed that Freud's psychoanalytic theories were presented in unfalsifiable form, meaning that no experiment could ever disprove them. The philosopher Adolf Grünbaum argues in "The Foundations of Psychoanalysis" (1984) that Popper was mistaken and that many of Freud's theories are empirically testable, a position with which others such as Eysenck agree. The philosopher Roger Scruton, writing in "Sexual Desire" (1986), also rejected Popper's arguments, pointing to the theory of repression as an example of a Freudian theory that does have testable consequences. Scruton nevertheless concluded that psychoanalysis is not genuinely scientific, on the grounds that it involves an unacceptable dependence on metaphor. The philosopher Donald Levy agrees with Grünbaum that Freud's theories are falsifiable but disputes Grünbaum's contention that therapeutic success is only the empirical basis on which they stand or fall, arguing that a much wider range of empirical evidence can be adduced if clinical case material is taken into consideration.

In a study of psychoanalysis in the United States, Nathan Hale reported on the "decline of psychoanalysis in psychiatry" during the years 1965–1985. The continuation of this trend was noted by Alan Stone: "As academic psychology becomes more 'scientific' and psychiatry more biological, psychoanalysis is being brushed aside." Paul Stepansky, while noting that psychoanalysis remains influential in the humanities, records the "vanishingly small number of psychiatric residents who choose to pursue psychoanalytic training" and the "nonanalytic backgrounds of psychiatric chairpersons at major universities" among the evidence he cites for his conclusion that "Such historical trends attest to the marginalisation of psychoanalysis within American psychiatry." Nonetheless Freud was ranked as the third most cited psychologist of the 20th century, according to a "Review of General Psychology" survey of American psychologists and psychology texts, published in 2002. It is also claimed that in moving beyond the "orthodoxy of the not so distant past... new ideas and new research has led to an intense reawakening of interest in psychoanalysis from neighbouring disciplines ranging from the humanities to neuroscience and including the non-analytic therapies".

Research in the emerging field of neuropsychoanalysis, founded by neuroscientist and psychoanalyst Mark Solms, has proved controversial with some psychoanalysts criticising the very concept itself. Solms and his colleagues have argued for neuro-scientific findings being "broadly consistent" with Freudian theories pointing out brain structures relating to Freudian concepts such as libido, drives, the unconscious, and repression. Neuroscientists who have endorsed Freud's work include David Eagleman who believes that Freud "transformed psychiatry" by providing " the first exploration of the way in which hidden states of the brain participate in driving thought and behavior" and Nobel laureate Eric Kandel who argues that "psychoanalysis still represents the most coherent and intellectually satisfying view of the mind."

Psychoanalysis has been interpreted as both radical and conservative. By the 1940s, it had come to be seen as conservative by the European and American intellectual community. Critics outside the psychoanalytic movement, whether on the political left or right, saw Freud as a conservative. Fromm had argued that several aspects of psychoanalytic theory served the interests of political reaction in his "The Fear of Freedom" (1942), an assessment confirmed by sympathetic writers on the right. In "" (1959), Philip Rieff portrayed Freud as a man who urged men to make the best of an inevitably unhappy fate, and admirable for that reason. In the 1950s, Herbert Marcuse challenged the then prevailing interpretation of Freud as a conservative in "Eros and Civilization" (1955), as did Lionel Trilling in "Freud and the Crisis of Our Culture" and Norman O. Brown in "Life Against Death" (1959). "Eros and Civilization" helped make the idea that Freud and Karl Marx were addressing similar questions from different perspectives credible to the left. Marcuse criticized neo-Freudian revisionism for discarding seemingly pessimistic theories such as the death instinct, arguing that they could be turned in a utopian direction. Freud's theories also influenced the Frankfurt School and critical theory as a whole.

Freud has been compared to Marx by Reich, who saw Freud's importance for psychiatry as parallel to that of Marx for economics, and by Paul Robinson, who sees Freud as a revolutionary whose contributions to twentieth century thought are comparable in importance to Marx's contributions to nineteenth century thought. Fromm calls Freud, Marx, and Einstein the "architects of the modern age", but rejects the idea that Marx and Freud were equally significant, arguing that Marx was both far more historically important and a finer thinker. Fromm nevertheless credits Freud with permanently changing the way human nature is understood. Gilles Deleuze and Félix Guattari write in "Anti-Oedipus" (1972) that psychoanalysis resembles the Russian Revolution in that it became corrupted almost from the beginning. They believe this began with Freud's development of the theory of the Oedipus complex, which they see as idealist.

Jean-Paul Sartre critiques Freud's theory of the unconscious in "Being and Nothingness" (1943), claiming that consciousness is essentially self-conscious. Sartre also attempts to adapt some of Freud's ideas to his own account of human life, and thereby develop an "existential psychoanalysis" in which causal categories are replaced by teleological categories. Maurice Merleau-Ponty considers Freud to be one of the anticipators of phenomenology, while Theodor W. Adorno considers Edmund Husserl, the founder of phenomenology, to be Freud's philosophical opposite, writing that Husserl's polemic against psychologism could have been directed against psychoanalysis. Paul Ricœur sees Freud as one of the three "masters of suspicion", alongside Marx and Nietzsche, for their unmasking 'the lies and illusions of consciousness'. Ricœur and Jürgen Habermas have helped create a "hermeneutic version of Freud", one which "claimed him as the most significant progenitor of the shift from an objectifying, empiricist understanding of the human realm to one stressing subjectivity and interpretation." Louis Althusser drew on Freud's concept of overdetermination for his reinterpretation of Marx's "Capital". Jean-François Lyotard developed a theory of the unconscious that reverses Freud's account of the dream-work: for Lyotard, the unconscious is a force whose intensity is manifest via disfiguration rather than condensation. Jacques Derrida finds Freud to be both a late figure in the history of western metaphysics and, with Nietzsche and Heidegger, a precursor of his own brand of radicalism.

Several scholars see Freud as parallel to Plato, writing that they hold nearly the same theory of dreams and have similar theories of the tripartite structure of the human soul or personality, even if the hierarchy between the parts of the soul is almost reversed. Ernest Gellner argues that Freud's theories are an inversion of Plato's. Whereas Plato saw a hierarchy inherent in the nature of reality, and relied upon it to validate norms, Freud was a naturalist who could not follow such an approach. Both men's theories drew a parallel between the structure of the human mind and that of society, but while Plato wanted to strengthen the super-ego, which corresponded to the aristocracy, Freud wanted to strengthen the ego, which corresponded to the middle class. Paul Vitz compares Freudian psychoanalysis to Thomism, noting St. Thomas's belief in the existence of an "unconscious consciousness" and his "frequent use of the word and concept 'libido' – sometimes in a more specific sense than Freud, but always in a manner in agreement with the Freudian use." Vitz suggests that Freud may have been unaware his theory of the unconscious was reminiscent of Aquinas.

The poem "In Memory of Sigmund Freud" was published by British poet W. H. Auden in his 1940 collection "Another Time". Auden describes Freud as having created "a whole climate of opinion / under whom we conduct our different lives."
Literary critic Harold Bloom has been influenced by Freud. Camille Paglia has also been influenced by Freud, whom she calls "Nietzsche's heir" and one of the greatest sexual psychologists in literature, but has rejected the scientific status of his work in her "Sexual Personae" (1990), writing, "Freud has no rivals among his successors because they think he wrote science, when in fact he wrote art."

The decline in Freud's reputation has been attributed partly to the revival of feminism. Simone de Beauvoir criticizes psychoanalysis from an existentialist standpoint in "The Second Sex" (1949), arguing that Freud saw an "original superiority" in the male that is in reality socially induced. Betty Friedan criticizes Freud and what she considered his Victorian view of women in "The Feminine Mystique" (1963). Freud's concept of penis envy was attacked by Kate Millett, who in "Sexual Politics" (1970) accused him of confusion and oversights. Naomi Weisstein writes that Freud and his followers erroneously thought his "years of intensive clinical experience" added up to scientific rigor.

Freud is also criticized by Shulamith Firestone and Eva Figes. In "The Dialectic of Sex" (1970), Firestone argues that Freud was a "poet" who produced metaphors rather than literal truths; in her view, Freud, like feminists, recognized that sexuality was the crucial problem of modern life, but ignored the social context and failed to question society itself. Firestone interprets Freud's "metaphors" in terms of the facts of power within the family. Figes tries in "Patriarchal Attitudes" (1970) to place Freud within a "history of ideas". Juliet Mitchell defends Freud against his feminist critics in "Psychoanalysis and Feminism" (1974), accusing them of misreading him and misunderstanding the implications of psychoanalytic theory for feminism. Mitchell helped introduce English-speaking feminists to Lacan. Mitchell is criticized by Jane Gallop in "The Daughter's Seduction" (1982). Gallop compliments Mitchell for her criticism of feminist discussions of Freud, but finds her treatment of Lacanian theory lacking.

Some French feminists, among them Julia Kristeva and Luce Irigaray, have been influenced by Freud as interpreted by Lacan. Irigaray has produced a theoretical challenge to Freud and Lacan, using their theories against them to put forward a "psychoanalytic explanation for theoretical bias". Irigaray, who claims that "the cultural unconscious only recognizes the male sex", describes how this affects "accounts of the psychology of women".

Psychologist Carol Gilligan writes that "The penchant of developmental theorists to project a masculine image, and one that appears frightening to women, goes back at least to Freud." She sees Freud's criticism of women's sense of justice reappearing in the work of Jean Piaget and Lawrence Kohlberg. Gilligan notes that Nancy Chodorow, in contrast to Freud, attributes sexual difference not to anatomy but to the fact that male and female children have different early social environments. Chodorow, writing against the masculine bias of psychoanalysis, "replaces Freud's negative and derivative description of female psychology with a positive and direct account of her own."

Toril Moi has developed a feminist perspective on psychoanalysis proposing that it is a discourse that "attempts to understand the psychic consequences of three universal traumas: the fact that there are others, the fact of sexual difference, and the fact of death". She replaces Freud's term of castration with Stanley Cavell's concept of "victimization" which is a more universal term that applies equally to both sexes. Moi regards this concept of human finitude as a suitable replacement for both castration and sexual difference as the traumatic "discovery of our separate, sexed, mortal existence" and how both men and women come to terms with it.





"The Standard Edition of the Complete Psychological Works of Sigmund Freud". Translated from the German under the general editorship of James Strachey, in collaboration with Anna Freud, assisted by Alix Strachey, Alan Tyson, and Angela Richards. 24 volumes, London: Hogarth Press and the Institute of Psycho-Analysis, 1953–1974.



</doc>
<doc id="26746" url="https://en.wikipedia.org/wiki?curid=26746" title="South Dakota">
South Dakota

South Dakota () is a U.S. state in the Midwestern region of the United States. It is named after the Lakota and Dakota Sioux Native American tribes, who compose a large portion of the population and historically dominated the territory. South Dakota is the seventeenth largest by area, but the fifth smallest by population and the 5th least densely populated of the 50 United States. As the southern part of the former Dakota Territory, South Dakota became a state on November 2, 1889, simultaneously with North Dakota. Pierre is the state capital and Sioux Falls, with a population of about 187,200, is South Dakota's largest city.

South Dakota is bordered by the states of North Dakota (to the north), Minnesota (to the east), Iowa (to the southeast), Nebraska (to the south), Wyoming (to the west), and Montana (to the northwest). The state is bisected by the Missouri River, dividing South Dakota into two geographically and socially distinct halves, known to residents as "East River" and "West River".

Eastern South Dakota is home to most of the state's population, and the area's fertile soil is used to grow a variety of crops. West of the Missouri River, ranching is the predominant agricultural activity, and the economy is more dependent on tourism and defense spending. Most of the Native American reservations are in West River. The Black Hills, a group of low pine-covered mountains sacred to the Sioux, are in the southwest part of the state. Mount Rushmore, a major tourist destination, is there. South Dakota has a temperate continental climate, with four distinct seasons and precipitation ranging from moderate in the east to semi-arid in the west. The state's ecology features species typical of a North American grassland biome.

Humans have inhabited the area for several millennia, with the Sioux becoming dominant by the early 19th century. In the late 19th century, European-American settlement intensified after a gold rush in the Black Hills and the construction of railroads from the east. Encroaching miners and settlers triggered a number of Indian wars, ending with the Wounded Knee Massacre in 1890. Key events in the 20th century included the Dust Bowl and Great Depression, increased federal spending during the 1940s and 1950s for agriculture and defense, and an industrialization of agriculture that has reduced family farming.

While several Democrats have represented South Dakota for multiple terms in both chambers of Congress, the state government is largely controlled by the Republican Party, whose nominees have carried South Dakota in each of the last 13 presidential elections. Historically dominated by an agricultural economy and a rural lifestyle, South Dakota has recently sought to diversify its economy in areas to attract and retain residents. South Dakota's history and rural character still strongly influence the state's culture.

South Dakota is in the north-central United States, and is considered a part of the Midwest by the U.S. Census Bureau; it is also part of the Great Plains region. The culture, economy, and geography of western South Dakota have more in common with the West than the Midwest. South Dakota has a total area of , making the state the 17th largest in the Union.

Black Elk Peak, formerly named Harney Peak, with an elevation of , is the state's highest point, while the shoreline of Big Stone Lake is the lowest, with an elevation of . South Dakota is bordered to the north by North Dakota; to the south by Nebraska; to the east by Iowa and Minnesota; and to the west by Wyoming and Montana. The geographical center of the U.S. is west of Castle Rock in Butte County. The North American continental pole of inaccessibility is between Allen and Kyle, from the nearest coastline.

The Missouri River is the largest and longest river in the state. Other major South Dakota rivers include the Cheyenne, James, Big Sioux, and White Rivers. Eastern South Dakota has many natural lakes, mostly created by periods of glaciation. Additionally, dams on the Missouri River create four large reservoirs: Lake Oahe, Lake Sharpe, Lake Francis Case, and Lewis and Clark Lake.

South Dakota can generally be divided into three regions: eastern South Dakota, western South Dakota, and the Black Hills. The Missouri River serves as a boundary in terms of geographic, social, and political differences between eastern and western South Dakota. The geography of the Black Hills, long considered sacred by Native Americans, differs from its surroundings to such an extent it can be considered separate from the rest of western South Dakota. At times the Black Hills are combined with the rest of western South Dakota, and people often refer to the resulting two regions divided by the Missouri River as West River and East River.
Eastern South Dakota generally features higher precipitation and lower topography than the western part of the state. Smaller geographic regions of this area include the Coteau des Prairies, the Dissected Till Plains, and the James River Valley. The Coteau des Prairies is a plateau bordered on the east by the Minnesota River Valley and on the west by the James River Basin. Further west, the James River Basin is mostly low, flat, highly eroded land, following the flow of the James River through South Dakota from north to south. The Dissected Till Plains, an area of rolling hills and fertile soil that covers much of Iowa and Nebraska, extends into the southeastern corner of South Dakota. Layers deposited during the Pleistocene epoch, starting around two million years ago, cover most of eastern South Dakota. These are the youngest rock and sediment layers in the state, the product of several successive periods of glaciation which deposited a large amount of rocks and soil, known as till, over the area.

The Great Plains cover most of the western two-thirds of South Dakota. West of the Missouri River the landscape becomes more arid and rugged, consisting of rolling hills, plains, ravines, and steep flat-topped hills called buttes. In the south, east of the Black Hills, lie the South Dakota Badlands. Erosion from the Black Hills, marine skeletons which fell to the bottom of a large shallow sea that once covered the area, and volcanic material all contribute to the geology of this area.
The Black Hills are in the southwestern part of South Dakota and extend into Wyoming. This range of low mountains covers , with peaks that rise from 2,000 to 4,000 feet (600 to 1,200 m) above their bases. The Black Hills are the location of Black Elk Peak (7,242 ft or 2,207 m above sea level), the highest point in South Dakota and also the highest point in the United States east of the Rocky Mountains. Two billion-year-old Precambrian formations, the oldest rocks in the state, form the central core of the Black Hills. Formations from the Paleozoic Era form the outer ring of the Black Hills; these were created between roughly 540 and 250 million years ago. This area features rocks such as limestone, which were deposited here when the area formed the shoreline of an ancient inland sea.

Much of South Dakota (except for the Black Hills area) is dominated by a temperate grasslands biome. Although grasses and crops cover most of this region, deciduous trees such as cottonwoods, elms, and willows are common near rivers and in shelter belts.
Mammals in this area include bison, deer, pronghorn, coyotes, and prairie dogs. The state bird, the ring-necked pheasant, has adapted well to the area after being introduced from China. Growing populations of bald eagles are spread throughout the state, especially near the Missouri River. Rivers and lakes of the grasslands support populations of walleye, carp, pike, bass, and other species. The Missouri River also contains the pre-historic paddlefish.

Due to a higher elevation and level of precipitation, the Black Hills ecology differs significantly from the plains. The mountains are thickly blanketed by various types of pines, including ponderosa and lodgepole pines, as well as spruces. Black Hills mammals include deer, elk (wapiti), bighorn sheep, mountain goats, pine marten, and mountain lions, while the streams and lakes contain several species of trout.

South Dakota has a continental climate with four distinct seasons, ranging from cold, dry winters to hot and semi-humid summers. During the summers, the state's average high temperature is often close to , although it cools to near at night. It is not unusual for South Dakota to have severe hot, dry spells in the summer with the temperature climbing above several times a year. Winters are cold with January high temperatures averaging below freezing and low temperatures averaging below in most of the state. The highest recorded temperature is at Usta on July 15, 2006 and the lowest recorded temperature is at McIntosh on February 17, 1936.

Average annual precipitation in South Dakota ranges from semi-arid conditions in the northwestern part of the state (around ) to semi-humid around the southeast portion of the state (around ), although a small area centered on Lead in the Black Hills has the highest precipitation at nearly per year.

South Dakota summers bring frequent, sometimes severe, thunderstorms with high winds, thunder, and hail. The state's eastern part is often considered part of Tornado Alley, and South Dakota experiences an average of 30 tornadoes each year. Severe blizzards and ice storms occur often during winter.

South Dakota has several sites administered by the National Park Service. Two national parks have been established in South Dakota, both in the state's southwestern part. Wind Cave National Park, established in 1903 in the Black Hills, has an extensive cave network as well as a large herd of bison. Badlands National Park was created in 1978. The park features an eroded, brightly colored landscape surrounded by semi-arid grasslands. Mount Rushmore National Memorial in the Black Hills was established in 1925. The sculpture of four U.S. Presidents was carved into the mountainside by sculptor Gutzon Borglum.

Other areas managed by the National Park Service include Jewel Cave National Monument near Custer, the Lewis and Clark National Historic Trail, the Minuteman Missile National Historic Site, which features a decommissioned nuclear missile silo and a separate missile control area several miles away, and the Missouri National Recreational River. The Crazy Horse Memorial is a large mountainside sculpture near Mt. Rushmore being built with private funds. The Mammoth Site near Hot Springs is another privately owned attraction in the Black Hills. A working paleontological dig, the site has one of the world's largest concentrations of mammoth remains.

Humans have lived in what is today South Dakota for several thousand years. The first inhabitants were Paleoindian hunter-gatherers, and disappeared from the area around 5000 BC. Between 500 AD and 800 AD, a semi-nomadic people known as the Mound Builders lived in central and eastern South Dakota. In the 14th century, the Crow Creek Massacre occurred, in which several hundred men, women, and children were killed near the Missouri River.

By 1500, the Arikara (or Ree) had settled in much of the Missouri River valley. European contact with the area began in 1743, when the LaVérendrye brothers explored the region. The LaVérendrye group buried a plate near the site of modern-day Pierre, claiming the region for France as part of greater Louisiana. In 1762 the entire region became part of the Spanish Louisiana until 1802. By the early 19th century, the Sioux had largely replaced the Arikara as the dominant group in the area.

In 1803, the United States purchased the Louisiana Territory, an area that included most of South Dakota, from Napoleon Bonaparte, and President Thomas Jefferson organized a group commonly referred to as the "Lewis and Clark Expedition" to explore the region. In 1817, an American fur trading post was set up at present-day Fort Pierre, beginning continuous American settlement of the area. In 1855, the U.S. Army bought Fort Pierre but abandoned it in 1857 in favor of Fort Randall to the south. Settlement by Americans and Europeans was by this time increasing rapidly, and in 1858 the Yankton Sioux signed the 1858 Treaty, ceding most of present-day eastern South Dakota to the United States.
Land speculators founded two of eastern South Dakota's largest present-day cities: Sioux Falls in 1856 and Yankton in 1859. In 1861, the Dakota Territory was established by the United States government (this initially included North Dakota, South Dakota, and parts of Montana and Wyoming). Settlement of the area, mostly by people from the eastern United States as well as western and northern Europe, increased rapidly, especially after the completion of an eastern railway link to Yankton in 1873.

In 1874, gold was discovered in the Black Hills during a military expedition led by George A. Custer and miners and explorers began illegally entering land promised to the Lakota. Custer's expedition took place despite the fact the US had granted the entire western half of present-day South Dakota (West River) to the Sioux in 1868 by the Treaty of Laramie as part of the Great Sioux Reservation. The Sioux declined to grant mining rights or land in the Black Hills, and war broke out after the U.S. failed to stop white miners and settlers from entering the region. Eventually the US defeated the Sioux and broke up the Great Sioux Reservation into five reservations, settling the Lakota in those areas. (In 1980, the US Supreme Court and Congress ordered payment to the Lakota for the illegal seizure of the Black Hills. The case remains unsettled, as the Lakota refuse to accept the money and instead insist on the return of the land.)

A growing population and political concerns (admitting two states meant having four new senators for the Republican Party) caused Dakota Territory to be divided in half and President Benjamin Harrison signed proclamations formally admitting South Dakota and North Dakota to the union on November 2, 1889. Harrison had the papers shuffled to obscure which one was signed first and the order went unrecorded.

On December 29, 1890, the Wounded Knee Massacre occurred on the Pine Ridge Indian Reservation. Commonly cited as the last major armed conflict between the United States and the Lakota Sioux Nation, the massacre resulted in the deaths of at least 146 Sioux, many of them women and children. 31 U.S. soldiers were also killed in the conflict.
During the 1930s, several economic and climatic conditions combined with disastrous results for South Dakota. A lack of rainfall, extremely high temperatures and inappropriate cultivation techniques produced what was known as the Dust Bowl in South Dakota and several other plains states. Fertile topsoil was blown away in massive dust storms, and several harvests were completely ruined. The experiences of the Dust Bowl, coupled with local bank foreclosures and the general economic effects of the Great Depression, resulted in many South Dakotans leaving the state. The population of South Dakota declined by more than 7% between 1930 and 1940.

Economic stability returned with the U.S. entry into World War II in 1941, when demand for the state's agricultural and industrial products grew as the nation mobilized for war. In 1944, the Pick–Sloan Plan was passed as part of the Flood Control Act of 1944 by the U.S. Congress, resulting in the construction of six large dams on the Missouri River, four of which are at least partially in South Dakota. Flood control, hydroelectricity, and recreational opportunities such as boating and fishing are provided by the dams and their reservoirs.

In recent decades, South Dakota has been transformed from a state dominated by agriculture to one with a more diversified economy. The tourism industry has grown considerably since the completion of the interstate system in the 1960s, with the Black Hills becoming more important as a destination. The financial service industry began to grow in the state as well, with Citibank moving its credit card operations from New York to Sioux Falls in 1981, a move that has been followed by several other financial companies. South Dakota was the first state to eliminate caps on interest rates.

In 2007, the site of the recently closed Homestake gold mine near Lead was chosen as the location of a new underground research facility, the Deep Underground Science and Engineering Laboratory. Despite a growing state population and recent economic development, many rural areas have been struggling over the past 50 years with locally declining populations and the emigration of educated young adults to larger South Dakota cities, such as Rapid City or Sioux Falls, or to other states. Mechanization and consolidation of agriculture has contributed greatly to the declining number of smaller family farms and the resulting economic and demographic challenges facing rural towns.

The United States Census Bureau estimates the population of South Dakota was 882,235 on July 1, 2018, an 8.36% increase since the 2010 United States Census, only North Dakota, Alaska, Vermont, and Wyoming have fewer residents.

As of 2018, South Dakota had an estimated population of 882,235, an increase of 68,055, or 8.36%, since the year 2010. 7.3% of South Dakota's population was reported as under 5, 24% under 18, and 14.3% were 65 or older. Females made up approximately 50.2% of the population. As of the 2000 census, South Dakota ranked fifth-lowest in the nation in population and population density.

Of the people residing in South Dakota, 65.7% were born in South Dakota, 31.4% were born in another U.S. state, 0.6% were born in Puerto Rico, U.S. Island areas, or born abroad to American parent(s), and 2.3% were born in another country.

The center of population of South Dakota is in Buffalo County, in the unincorporated county seat of Gann Valley.

According to the 2010 Census, the racial composition of the population was:

Ethnically, 2.7% of South Dakota's population was of Hispanic, Latino, or Spanish origin (they may be of any race).

As of 2011, 25.4% of South Dakota's population younger than age 1 were minorities, meaning they had at least one parent who was not non-Hispanic white.

As of 2000, the five largest ancestry groups in South Dakota are German (40.7%), Norwegian (15.3%), Irish (10.4%), Native American (8.3%), and English (7.1%).

German Americans are the largest ancestry group in most parts of the state, especially in East River (east of the Missouri River), although there are also large Scandinavian-descended populations in some counties. South Dakota has the nation's largest population of Hutterites, a communal Anabaptist group which emigrated in 1874 from Europe, primarily from German-speaking areas.
American Indians, largely Lakota, Dakota, and Nakota (Sioux), are predominant in several counties and constitute 20 per cent of the population in West River. The seven large Indian reservations in the state occupy an area much diminished from their former Great Sioux Reservation of West River, which the US government had once allocated to the Sioux tribes. South Dakota has the third-highest proportion of Native Americans of any state, behind Alaska and New Mexico.

Five of the state's counties are wholly within the boundaries of sovereign Indian reservations. Because of the limitations of climate and land, and isolation from urban areas with more employment opportunities, living standards on many South Dakota reservations are often far below the national average; Ziebach County ranked as the poorest county in the nation in 2009. The unemployment rate in Fort Thompson, on the Crow Creek Reservation, is 70%, and 21% of households lack plumbing or basic kitchen appliances. A 1995 study by the U.S. Census Bureau found 58% of homes on the Pine Ridge Indian Reservation did not have a telephone. The reservations' isolation also inhibits their ability to generate revenue from gaming casinos, an avenue that has proved profitable for many tribes closer to urban centers.

In 1995 the legislature passed a law to make English the "common language" of the state. Since 2019, ""the language of the Great Sioux Nation, three dialects, Dakota, Lakota, and Nakota"" is an official language. As of the 2000 census, 1.90% of the population aged 5 or older speak German at home, while 1.51% speak Lakota or Dakota, and 1.43% Spanish. As of 2010, 93.46% (692,504) of South Dakota residents aged 5 and older spoke English as their primary language. 6.54% of the population spoke a language other than English. 2.06% (15,292) of the population spoke Spanish, 1.39% (10,282) spoke Dakota, and 1.37% (10,140) spoke German. Other languages spoken included Vietnamese (0.16%), Chinese (0.12%), and Russian (0.10%).

Over the last several decades, the population in many rural areas has declined in South Dakota, in common with other Great Plains states. The change has been characterized as "rural flight" as family farming has declined. Young people have moved to cities for other employment. This trend has continued in recent years, with 30 of South Dakota's counties losing population between the 1990 and the 2000 census. During that time, nine counties had a population loss of greater than 10%, with Harding County, in the northwest corner of the state, losing nearly 19% of its population. Low birth rates and a lack of younger immigration has caused the median age of many of these counties to increase. In 24 counties, at least 20% of the population is over the age of 65, compared with a national rate of 12.8%.

The effect of rural flight has not been spread evenly through South Dakota, however. Although most rural counties and small towns have lost population, the Sioux Falls area, the larger counties along Interstate 29, the Black Hills, and many Indian reservations have all gained population. As the reservations have exercised more sovereignty, some Sioux have returned to them from urban areas. Lincoln County near Sioux Falls was the seventh fastest-growing county (by percentage) in the United States in 2010. The growth in these areas has compensated for losses in the rest of the state. South Dakota's total population continues to increase steadily, albeit at a slower rate than the national average.

The largest denominations by number of adherents in 2010 were the Roman Catholic Church with 148,883 members; the Evangelical Lutheran Church in America (ELCA) with 112,649 members; and the United Methodist Church (UMC) with 36,020 members. (The ELCA and UMC are specific denominations within the broader terms 'Lutheran' and 'Methodist', respectively.)
The results of a 2001 survey, in which South Dakotans were asked to identify their religion, include:

The current-dollar gross state product of South Dakota was US$39.8 billion as of 2010, the fifth smallest total state output in the US. The per capita personal income was $38,865 in 2010, ranked 25th in the U.S., and 12.5% of the population was below the poverty line in 2008.
CNBC's list of "Top States for Business for 2010" has recognized South Dakota as the seventh best state in the nation. In July 2011, the state's unemployment rate was 4.7%.

The service industry is the largest economic contributor in South Dakota. This sector includes the retail, finance, and health care industries. Citibank, which was the largest bank holding company in the United States at one time, established national banking operations in South Dakota in 1981 to take advantage of favorable banking regulations. Government spending is another important segment of the state's economy, providing over ten percent of the gross state product. Ellsworth Air Force Base, near Rapid City, is the second-largest single employer in the state.

Agriculture has historically been a key component of the South Dakota economy. Although other industries have expanded rapidly in recent decades, agricultural production is still very important to the state's economy, especially in rural areas. The five most valuable agricultural products in South Dakota are cattle, corn (maize), soybeans, wheat, and hogs. Agriculture-related industries such as meat packing and ethanol production also have a considerable economic impact on the state. South Dakota is the sixth leading ethanol-producing state in the nation.

Another important sector in South Dakota's economy is tourism. Many travel to view the attractions of the state, particularly those in the Black Hills region, such as historic Deadwood, Mount Rushmore, and the nearby state and national parks. One of the largest tourist events in the state is the annual Sturgis Motorcycle Rally. The five-day event drew over 739,000 attendees in 2015; significant considering the state has a total population of 850,000. In 2006, tourism provided an estimated 33,000 jobs in the state and contributed over two billion dollars to the economy of South Dakota.

South Dakota has of highways, roads, and streets, along with of interstate highways. Two major interstates pass through South Dakota: Interstate 90, which runs east and west through the southern half of the state; and Interstate 29, running north and south in the eastern portion of the state. The I-29 corridor features generally higher rates of population and economic growth than areas in eastern South Dakota further from the interstate.

Also in the state are the shorter Interstates 190, a spur into central Rapid City, and 229, a loop around southern and eastern Sioux Falls. Several major U.S. highways pass through the state. U.S. routes 12, 14, 16, 18 and 212 travel east and west, while U.S. routes 81, 83, 85 and 281 run north and south. South Dakota and Montana are the only states sharing a land border which is not traversed by a paved road.

South Dakota contains two National Scenic Byways. The Peter Norbeck National Scenic Byway is in the Black Hills, while the Native American Scenic Byway runs along the Missouri River in the north-central part of the state. Other scenic byways include the Badlands Loop Scenic Byway, the Spearfish Canyon Scenic Byway, and the Wildlife Loop Road Scenic Byway.

Railroads have played an important role in South Dakota transportation since the mid-19th century. Some of railroad track were built in South Dakota during the late 19th century and early 20th century, but only are active. BNSF Railway is the largest railroad in South Dakota; the Rapid City, Pierre and Eastern Railroad (formerly the Dakota, Minnesota and Eastern) is the state's other major carrier. Other state carriers include Dakota Southern Railway, Dakota and Iowa Railroad, Ellis and Eastern Railroad, Sunflour Railroad, Canadian Pacific Railway, and the Sisseton Milbank Railroad. Rail transportation in the state is mostly confined only to freight, but two passenger heritage railroads operate in the state, them being The Black Hills Central, and the Prairie Village, Herman, and Milwaukee Railroad. However, South Dakota is one of the two contiguous US states (Wyoming being the other) lacking Amtrak service although South Dakota is the only continental state that never had Amtrak since Wyoming used to be served by the San Francisco Zephyr and the Pioneer.

South Dakota's largest commercial airports in terms of passenger traffic are the Sioux Falls Regional Airport and Rapid City Regional Airport. Delta Air Lines, Frontier Airlines, and Allegiant Airlines, as well as commuter airlines using the brand affiliation with major airlines serve the two largest airports. Several other cities in the state also have commercial air service: Aberdeen Regional Airport, Pierre Regional Airport, and Watertown Regional Airport, some of which is subsidized by the Essential Air Service program.

Like other U.S. states, the structure of the government of South Dakota follows the same separation of powers as the federal government, with executive, legislative, and judicial branches. The structure of the state government is laid out in the Constitution of South Dakota, the highest law in the state. The constitution may be amended by a majority vote of both houses of the legislature, or by voter initiative.

The Governor of South Dakota occupies the executive branch of the state government. The current governor is Kristi Noem, a Republican. The state constitution gives the governor the power to sign into law or veto bills passed by the state legislature, to serve as commander-in-chief of the South Dakota National Guard, to appoint a cabinet, and to commute criminal sentences or to pardon those convicted of crimes. The governor serves for a four-year term, and may not serve more than two consecutive terms.

The state legislature is made up of two bodies, the Senate, which has 35 members, and the House of Representatives, with 70 members. South Dakota is divided into 35 legislative districts, with voters electing two representatives and one senator per district. The legislature meets for an annual session which begins on the second Tuesday in January and lasts for 30 days; it also meets if a special session is called by the governor.

The judicial branch is made up of several levels. The state supreme court, with four justices and a chief justice, is the highest court in the state. Below the supreme court are the circuit courts; 41 circuit judges serve in seven judicial circuits in the state. Below the circuit courts are the magistrate courts, which deal with more minor criminal and civil actions.

As of 2005, South Dakota has the lowest per capita total state tax rate in the United States. The state does not levy personal or corporate income taxes, inheritance taxes, or taxes on intangible personal property. The state sales tax rate is 4.5 percent. Various localities have local levies so in some areas the rate is 6 percent. The state sales tax does not apply to sales to Indians on Indian reservations, but many reservations have a compact with the state. Businesses on the reservation collect the tax and the state refunds to the Indian Tribes the percentage of sales tax collections relating to the ratio of Indian population to total population in the county or area affected. Ad valorem property taxes are local taxes and are a large source of funding for school systems, counties, municipalities and other local government units. The South Dakota Special Tax Division regulates some taxes including cigarette and alcohol-related taxes.

South Dakota is represented at the federal level by Senator John Thune, Senator Mike Rounds, and Representative Dusty Johnson. All three are Republicans. South Dakota is one of seven states with only one seat in the US House of Representatives. In United States presidential elections, South Dakota is allotted three of 538 votes in the Electoral College. As in all other states except Maine and neighboring Nebraska, South Dakota's electoral votes are granted in a winner-take-all system.

South Dakota politics are generally dominated by the Republican Party. Since statehood, Republicans have carried the state's electoral votes in all but five presidential elections: 1896, 1912 (By Theodore Roosevelt's Progressive Party), 1932, 1936 and 1964. Only Alaska has been carried fewer times by Democrat presidential candidates. Not even George McGovern, the Democratic nominee in 1972 as well as a native South Dakotan, was able to carry the state. Additionally, a Democrat has not won the governorship since 1974. As of 2016, Republicans hold a 15% voter registration advantage over Democrats and hold large majorities in both the state House of Representatives and Senate.

Despite the state's general Republican and conservative leanings, Democrats have found success in various statewide elections, most notably in those involving South Dakota's congressional representatives in Washington. American Indians have been becoming more active in state and county electoral politics. In the 2002 election, American Indian voting carried Tim Johnson as the Democratic candidate by a margin of 532 votes. Until his electoral defeat in 2004, Senator Tom Daschle was the Senate minority leader (and briefly its majority leader during Democratic control of the Senate in 2001–02).

In 2016, South Dakota voted for Republican nominee Donald Trump over Democratic nominee Hillary Clinton by a margin of 30%. In 2018, Republican congresswoman Kristi Noem defeated Democrat Billie Sutton in the gubernatorial election, and Republican Dusty Johnson defeated Democrat Tim Bjorkman for the state's at-large seat in the US House of Representatives.

Contemporary political issues in South Dakota include the costs and benefits of the state lottery, South Dakota's relatively low rankings in education spending (particularly teacher pay – recently the State Sales Tax was increased from 4% to 4.5% to finance an increase in teacher pay), and recent legislative and electoral attempts to ban abortion in the state.

A Republican bill passed in March 2019 requires that all public schools display "In God We Trust" in a prominent location.

South Dakota's culture reflects the state's American Indian, rural, Western, and European roots. A number of annual events celebrating the state's ethnic and historical heritage take place around the state, such as Days of '76 in Deadwood, Czech Days in Tabor, and the annual St. Patrick's Day and Cinco de Mayo festivities in Sioux Falls. The various tribes hold many annual pow wows at their reservations throughout the state, to which non-Native Americans are sometimes invited. Custer State Park holds an annual Buffalo Roundup, in which volunteers on horseback gather the park's herd of around 1,500 bison.

Black Elk (Lakota) was a medicine man and heyokha, whose life spanned the transition to reservations. His accounts of the 19th-century Indian Wars and Ghost Dance movement, and his deep thoughts on personal visions and Native American religion, form the basis of the book "Black Elk Speaks," first published in 1932. (Among several editions, a premier annotated edition was published in 2008.) Paul Goble, an award-winning children's book author and illustrator, has been based in the Black Hills since 1977.

Laura Ingalls Wilder, whose semi-autobiographical books are based on her experiences as a child and young adult on the frontier, is one of South Dakota's best-known writers. She drew from her life growing up on a homestead near De Smet as the basis for five of her novels: "By the Shores of Silver Lake," "The Long Winter," "Little Town on the Prairie," "These Happy Golden Years," and "The First Four Years". These gained renewed popularity in the United States when "Little House on the Prairie" was adapted and produced as a television series in 1974. Wilder's daughter, Rose Wilder Lane, who became a well-known writer in her own right, was born near De Smet in 1886.

South Dakota has also produced several notable artists. Harvey Dunn grew up on a homestead near Manchester in the late 19th century. While Dunn worked most of his career as a commercial illustrator, his most famous works showed various scenes of frontier life; he completed these near the end of his career. Oscar Howe (Crow) was born on the Crow Creek Indian Reservation and won fame for his watercolor paintings. Howe was one of the first Native American painters to adopt techniques and style heavily influenced by the mid-20th century abstraction movement, rather than relying on traditional Native American styles. Terry Redlin, originally from Watertown, is an accomplished painter of rural and wildlife scenes. Many of his works are on display at the Redlin Art Center in Watertown.

Sioux Falls is the largest city in South Dakota, with a 2010 population of 153,888,
and a metropolitan area population of 238,122.
The city, founded in 1856, is in the southeast corner of the state. Retail, finance, and healthcare have assumed greater importance in Sioux Falls, where the economy was originally centered on agri-business and quarrying.

Rapid City, with a 2010 population of 67,956, and a metropolitan area population of 124,766, is the second-largest city in the state. It is on the eastern edge of the Black Hills, and was founded in 1876. Rapid City's economy is largely based on tourism and defense spending, because of the proximity of many tourist attractions in the Black Hills and Ellsworth Air Force Base.

The next eight largest cities in the state, in order of descending 2010 population, are Aberdeen (26,091), Brookings (22,056), Watertown (21,482), Mitchell (15,254), Yankton (14,454), Pierre (13,646), Huron (12,592), and Vermillion (10,571). Pierre is the state capital, and Brookings and Vermillion are the locations of the state's two largest universities (South Dakota State University and University of South Dakota, respectively). With a population of about 14,000, Pierre is the second smallest state capital in the United States. Of the ten largest cities in the state, only Rapid City is west of the Missouri River.

South Dakota's first newspaper, the "Dakota Democrat", began publishing in Yankton in 1858. Today, the state's largest newspaper is the Sioux Falls "Argus Leader", with a Sunday circulation of 63,701 and a weekday circulation of 44,334. The "Rapid City Journal", with a Sunday circulation of 32,638 and a weekday circulation of 27,827, is South Dakota's second largest newspaper. The next four largest newspapers in the state are the Aberdeen "American News", the "Watertown Public Opinion", the "Huron Plainsman", and the "Brookings Register". In 1981, Tim Giago founded the "Lakota Times" as a newspaper for the local American Indian community on the Pine Ridge Indian Reservation. The newspaper, now published in New York and known as "Indian Country Today", is available in every state in the country. The "Sioux City Journal" also covers parts of South Dakota.

There are nine television stations broadcasting in South Dakota; South Dakota Public Television broadcasts from a number of locations around the state, while the other stations broadcast from Sioux Falls or Rapid City. The two largest television media markets in South Dakota are Sioux Falls-Mitchell, with a viewership of 246,020, and Rapid City, with a viewership of 91,070. The two markets rank as 114th and 177th largest in the United States, respectively. The state's first television station, KELO-TV, began airing in Sioux Falls in 1953. Among KELO's early programs was "Captain 11", an afternoon children's program. "Captain 11" ran from 1955 until 1996, making it the nation's longest continuously running children's television program.

A number of South Dakotans are famous for their work in television and publishing. Former NBC Nightly News anchor and author Tom Brokaw is from Webster and Yankton, "USA Today" founder Al Neuharth was from Eureka and Alpena, gameshow host Bob Barker spent much of his childhood in Mission, and entertainment news hosts Pat O'Brien and Mary Hart are from Sioux Falls.

As of 2006, South Dakota has a total primary and secondary school enrollment of 136,872, with 120,278 of these students being educated in the public school system. There are 703 public schools in 168 school districts, giving South Dakota the highest number of schools per capita in the United States. The current high school graduation rate is 89.9%, and the average ACT score is 21.8, slightly above the national average of 21.1. 89.8% of the adult population has earned at least a high school diploma, and 25.8% has earned a bachelor's degree or higher. South Dakota's 2008 average public school teacher salary of $36,674, compared to a national average of $52,308, was the lowest in the nation. In 2007 South Dakota passed legislation modeled after Montana's Indian Education for All Act (1999), mandating education about Native American tribal history, culture, and heritage in all the schools, from pre-school through college, in an effort to increase knowledge and appreciation about Indian culture among all residents of the state, as well as to reinforce Indian students' understanding of their own cultures' contributions.

The South Dakota Board of Regents, whose members are appointed by the governor, controls the six public universities in the state. South Dakota State University (SDSU), in Brookings, is the state's largest university, with an enrollment of 12,831. The University of South Dakota (USD), in Vermillion, is the state's oldest university, and has South Dakota's only law school and medical school. South Dakota also has several private universities, the largest of which is Augustana University in Sioux Falls.

Because of its low population, South Dakota does not host any major league professional sports franchises. The state has minor league and independent league teams, all of which play in Sioux Falls or Rapid City. Sioux Falls is home to four teams: the Sioux Falls Canaries (baseball), the Sioux Falls Skyforce (basketball), the Sioux Falls Stampede (hockey), and the Sioux Falls Storm (indoor American football). The Canaries play in the American Association, and their home field is Sioux Falls Stadium. The Skyforce play in the NBA G League, and are owned by the NBA's Miami Heat. They play at the Sanford Pentagon. The Stampede and Storm share the Denny Sanford Premier Center. The Stampede play in the USHL, and the Storm play in the IFL. Rapid City has a hockey team named the Rapid City Rush that plays in the ECHL. The Rush began their inaugural season in 2008 at the Rushmore Plaza Civic Center.

Universities in South Dakota host a variety of sports programs. For many years, South Dakota was one of the only states in the country without a NCAA Division I football or basketball team. However, several years ago SDSU decided to move their teams from Division II to Division I, a move followed by the University of South Dakota. Other universities in the state compete at the NCAA's Division II or III levels, or in the NAIA.

Famous South Dakota athletes include Billy Mills, Mike Miller, Mark Ellis, Becky Hammon, Brock Lesnar, Chad Greenway, and Adam Vinatieri. Mills is from the town of Pine Ridge and competed at the 1964 Summer Olympic Games in Tokyo, becoming the only American to win a gold medal in the 10,000-meter event. Miller, of Mitchell, is a two-time NBA champion who played college basketball at the University of Florida, leading them to the 2000 NCAA Championship game his sophomore year, and won the 2001 NBA rookie of the year award. Ellis, of Rapid City, played for the University of Florida and four MLB teams before retiring in 2015. Hammon, of Rapid City, played for the WNBA's New York Liberty and San Antonio Silver Stars before becoming an assistant coach for the NBA's San Antonio Spurs in 2014. Lesnar, of Webster, is a former heavy-weight champion in the UFC and WWE. Vinatieri is an NFL placekicker who grew up in Rapid City and attended SDSU.

Fishing and hunting are popular outdoor activities in South Dakota. Fishing contributes over $224 million to South Dakota's economy, and hunting contributes over $303 million. In 2007, over 275,000 hunting licences and 175,000 fishing licences were sold in the state; around half of the hunting licences and over two-thirds of the fishing licences were purchased by South Dakotans. Popular species of game include pheasants, white-tailed deer, mule deer, and turkeys, as well as waterfowl such as Canada geese, snow geese, and mallards. Targets of anglers include walleye in the eastern glacial lakes and Missouri River reservoirs, Chinook salmon in Lake Oahe, and trout in the Black Hills.

Other sports, such as cycling and running, are also popular in the state. In 1991, the state opened the George S. Mickelson Trail, a rail trail in the Black Hills. Besides being used by cyclists, the trail is also the site of a portion of the annual Mount Rushmore marathon; the marathon's entire course is at an elevation of over 4,000 feet (1,200 m). Other events in the state include the Tour de Kota, a , six-day cycling event that covers much of eastern and central South Dakota, and the annual Sturgis Motorcycle Rally, which draws hundreds of thousands of participants from around the United States.

Some of South Dakota's official state symbols include:





</doc>
<doc id="26748" url="https://en.wikipedia.org/wiki?curid=26748" title="Switzerland">
Switzerland

Switzerland, officially the Swiss Confederation, is a country situated in the confluence of western, central, and southern Europe. It is a federal republic composed of 26 cantons, with federal authorities seated in Bern. Switzerland is a landlocked country bordered by Italy to the south, France to the west, Germany to the north, and Austria and Liechtenstein to the east. It is geographically divided among the Swiss Plateau, the Alps, and the Jura, spanning a total area of , and land area of . While the Alps occupy the greater part of the territory, the Swiss population of approximately 8.5 million is concentrated mostly on the plateau, where the largest cities are located, among them the two global cities and economic centres of Zürich and Geneva.

The establishment of the Old Swiss Confederacy dates to the late medieval period, resulting from a series of military successes against Austria and Burgundy. Swiss independence from the Holy Roman Empire was formally recognized in the Peace of Westphalia in 1648. The Federal Charter of 1291 is considered the founding document of Switzerland which is celebrated on Swiss National Day. Since the Reformation of the 16th century, Switzerland has maintained a strong policy of armed neutrality; it has not fought an international war since 1815 and did not join the United Nations until 2002. Nevertheless, it pursues an active foreign policy and is frequently involved in peace-building processes around the world. Switzerland is the birthplace of the Red Cross, one of the world's oldest and best known humanitarian organisations, and is home to numerous international organisations, including the second largest UN office. It is a founding member of the European Free Trade Association, but notably not part of the European Union, the European Economic Area or the Eurozone. However, it participates in the Schengen Area and the European Single Market through bilateral treaties.

Switzerland occupies the crossroads of Germanic and Romance Europe, as reflected in its four main linguistic and cultural regions: German, French, Italian and Romansh. Although the majority of the population are German-speaking, Swiss national identity is rooted in a common historical background, shared values such as federalism and direct democracy, and Alpine symbolism. Due to its linguistic diversity, Switzerland is known by a variety of native names: "Schweiz" (German); "Suisse" (French); "Svizzera" (Italian); and "Svizra" (Romansh). On coins and stamps, the Latin name, "Confoederatio Helvetica" – frequently shortened to "Helvetia" – is used instead of the four national languages.

The sovereign state is one of the most developed countries in the world, with the highest nominal wealth per adult and the eighth-highest per capita gross domestic product. It ranks at or near the top in several international metrics, including economic competitiveness and human development. Zürich, Geneva and Basel have been ranked among the top ten cities in the world in terms of quality of life, with Zürich ranked second globally. In 2019, IMD place Switzerland first in the world in attracting skilled workers. World Economic Forum ranks it the 5th most competitive country globally.

The English name "Switzerland" is a compound containing "Switzer", an obsolete term for the Swiss, which was in use during the 16th to 19th centuries. The English adjective "Swiss" is a loan from French ', also in use since the 16th century. The name "Switzer" is from the Alemannic ', in origin an inhabitant of "Schwyz" and its associated territory, one of the Waldstätten cantons which formed the nucleus of the Old Swiss Confederacy. The Swiss began to adopt the name for themselves after the Swabian War of 1499, used alongside the term for "Confederates", "Eidgenossen" (literally: "comrades by oath"), used since the 14th century. The data code for Switzerland, CH, is derived from Latin "Confoederatio Helvetica" ().

The toponym "Schwyz" itself was first attested in 972, as Old High German ', ultimately perhaps related to ' ‘to burn’ (cf. Old Norse "svíða" ‘to singe, burn’), referring to the area of forest that was burned and cleared to build. The name was extended to the area dominated by the canton, and after the Swabian War of 1499 gradually came to be used for the entire Confederation.
The Swiss German name of the country, ', is homophonous to that of the canton and the settlement, but distinguished by the use of the definite article (' for the Confederation, but simply "" for the canton and the town).

The Latin name "Confoederatio Helvetica" was neologized and introduced gradually after the formation of the federal state in 1848, harking back to the Napoleonic Helvetic Republic, appearing on coins from 1879, inscribed on the Federal Palace in 1902 and after 1948 used in the official seal. (for example, the ISO banking code "CHF" for the Swiss franc, and the country top-level domain ".ch", are both taken from the state's Latin name). "Helvetica" is derived from the "Helvetii", a Gaulish tribe living on the Swiss plateau before the Roman era.

"Helvetia" appears as a national personification of the Swiss confederacy in the 17th century with a 1672 play by Johann Caspar Weissenbach.

Switzerland has existed as a state in its present form since the adoption of the Swiss Federal Constitution in 1848. The precursors of Switzerland established a protective alliance at the end of the 13th century (1291), forming a loose confederation of states which persisted for centuries.

The oldest traces of hominid existence in Switzerland date back about 150,000 years. The oldest known farming settlements in Switzerland, which were found at Gächlingen, have been dated to around 5300 BC.

The earliest known cultural tribes of the area were members of the Hallstatt and La Tène cultures, named after the archaeological site of La Tène on the north side of Lake Neuchâtel. La Tène culture developed and flourished during the late Iron Age from around 450 BC, possibly under some influence from the Greek and Etruscan civilisations. One of the most important tribal groups in the Swiss region was the Helvetii. Steadily harassed by the Germanic tribes, in 58 BC the Helvetii decided to abandon the Swiss plateau and migrate to western Gallia, but Julius Caesar's armies pursued and defeated them at the Battle of Bibracte, in today's eastern France, forcing the tribe to move back to its original homeland. In 15 BC, Tiberius, who would one day become the second Roman emperor, and his brother Drusus, conquered the Alps, integrating them into the Roman Empire. The area occupied by the Helvetii—the namesakes of the later "Confoederatio Helvetica"—first became part of Rome's Gallia Belgica province and then of its Germania Superior province, while the eastern portion of modern Switzerland was integrated into the Roman province of Raetia. Sometime around the start of the Common Era, the Romans maintained a large legionary camp called Vindonissa, now a ruin at the confluence of the Aare and Reuss rivers, near the town of Windisch, an outskirt of Brugg.

The first and second century AD was an age of prosperity for the population living on the Swiss plateau. Several towns, like Aventicum, Iulia Equestris and Augusta Raurica, reached a remarkable size, while hundreds of agricultural estates (Villae rusticae) were founded in the countryside.

Around 260 AD, the fall of the Agri Decumates territory north of the Rhine transformed today's Switzerland into a frontier land of the Empire. Repeated raids by the Alamanni tribes provoked the ruin of the Roman towns and economy, forcing the population to find shelter near Roman fortresses, like the Castrum Rauracense near Augusta Raurica. The Empire built another line of defence at the north border (the so-called Donau-Iller-Rhine-Limes), but at the end of the fourth century the increased Germanic pressure forced the Romans to abandon the linear defence concept, and the Swiss plateau was finally open to the settlement of Germanic tribes.

In the Early Middle Ages, from the end of the 4th century, the western extent of modern-day Switzerland was part of the territory of the Kings of the Burgundians. The Alemanni settled the Swiss plateau in the 5th century and the valleys of the Alps in the 8th century, forming Alemannia. Modern-day Switzerland was therefore then divided between the kingdoms of Alemannia and Burgundy. The entire region became part of the expanding Frankish Empire in the 6th century, following Clovis I's victory over the Alemanni at Tolbiac in 504 AD, and later Frankish domination of the Burgundians.

Throughout the rest of the 6th, 7th and 8th centuries the Swiss regions continued under Frankish hegemony (Merovingian and Carolingian dynasties). But after its extension under Charlemagne, the Frankish Empire was divided by the Treaty of Verdun in 843. The territories of present-day Switzerland became divided into Middle Francia and East Francia until they were reunified under the Holy Roman Empire around 1000 AD.

By 1200, the Swiss plateau comprised the dominions of the houses of Savoy, Zähringer, Habsburg, and Kyburg. Some regions (Uri, Schwyz, Unterwalden, later known as "Waldstätten") were accorded the Imperial immediacy to grant the empire direct control over the mountain passes. With the extinction of its male line in 1263 the Kyburg dynasty fell in AD 1264; then the Habsburgs under King Rudolph I (Holy Roman Emperor in 1273) laid claim to the Kyburg lands and annexed them extending their territory to the eastern Swiss plateau.

The Old Swiss Confederacy was an alliance among the valley communities of the central Alps. The Confederacy, governed by nobles and patricians of various cantons, facilitated management of common interests and ensured peace on the important mountain trade routes. The Federal Charter of 1291 agreed between the rural communes of Uri, Schwyz, and Unterwalden is considered the confederacy's founding document, even though similar alliances are likely to have existed decades earlier.

By 1353, the three original cantons had joined with the cantons of Glarus and Zug and the Lucerne, Zürich and Bern city states to form the "Old Confederacy" of eight states that existed until the end of the 15th century. The expansion led to increased power and wealth for the confederation. By 1460, the confederates controlled most of the territory south and west of the Rhine to the Alps and the Jura mountains, particularly after victories against the Habsburgs (Battle of Sempach, Battle of Näfels), over Charles the Bold of Burgundy during the 1470s, and the success of the Swiss mercenaries. The Swiss victory in the Swabian War against the Swabian League of Emperor Maximilian I in 1499 amounted to "de facto" independence within the Holy Roman Empire.

The Old Swiss Confederacy had acquired a reputation of invincibility during these earlier wars, but expansion of the confederation suffered a setback in 1515 with the Swiss defeat in the Battle of Marignano. This ended the so-called "heroic" epoch of Swiss history. The success of Zwingli's Reformation in some cantons led to inter-cantonal religious conflicts in 1529 and 1531 (Wars of Kappel). It was not until more than one hundred years after these internal wars that, in 1648, under the Peace of Westphalia, European countries recognised Switzerland's independence from the Holy Roman Empire and its neutrality.

During the Early Modern period of Swiss history, the growing authoritarianism of the patriciate families combined with a financial crisis in the wake of the Thirty Years' War led to the Swiss peasant war of 1653. In the background to this struggle, the conflict between Catholic and Protestant cantons persisted, erupting in further violence at the First War of Villmergen, in 1656, and the Toggenburg War (or Second War of Villmergen), in 1712.

In 1798, the revolutionary French government conquered Switzerland and imposed a new unified constitution. This centralised the government of the country, effectively abolishing the cantons: moreover, Mülhausen joined France and the Valtellina valley became part of the Cisalpine Republic, separating from Switzerland. The new regime, known as the Helvetic Republic, was highly unpopular. It had been imposed by a foreign invading army and destroyed centuries of tradition, making Switzerland nothing more than a French satellite state. The fierce French suppression of the Nidwalden Revolt in September 1798 was an example of the oppressive presence of the French Army and the local population's resistance to the occupation.

When war broke out between France and its rivals, Russian and Austrian forces invaded Switzerland. The Swiss refused to fight alongside the French in the name of the Helvetic Republic. In 1803 Napoleon organised a meeting of the leading Swiss politicians from both sides in Paris. The result was the Act of Mediation which largely restored Swiss autonomy and introduced a Confederation of 19 cantons. Henceforth, much of Swiss politics would concern balancing the cantons' tradition of self-rule with the need for a central government.

In 1815 the Congress of Vienna fully re-established Swiss independence and the European powers agreed to permanently recognise Swiss neutrality. Swiss troops still served foreign governments until 1860 when they fought in the Siege of Gaeta. The treaty also allowed Switzerland to increase its territory, with the admission of the cantons of Valais, Neuchâtel and Geneva. Switzerland's borders have not changed since, except for some minor adjustments.

The restoration of power to the patriciate was only temporary. After a period of unrest with repeated violent clashes, such as the Züriputsch of 1839, civil war (the "Sonderbundskrieg") broke out in 1847 when some Catholic cantons tried to set up a separate alliance (the "Sonderbund"). The war lasted for less than a month, causing fewer than 100 casualties, most of which were through friendly fire. Yet however minor the Sonderbundskrieg appears compared with other European riots and wars in the 19th century, it nevertheless had a major impact on both the psychology and the society of the Swiss and of Switzerland.

The war convinced most Swiss of the need for unity and strength towards its European neighbours. Swiss people from all strata of society, whether Catholic or Protestant, from the liberal or conservative current, realised that the cantons would profit more if their economic and religious interests were merged.

Thus, while the rest of Europe saw revolutionary uprisings, the Swiss drew up a constitution which provided for a federal layout, much of it inspired by the American example. This constitution provided for a central authority while leaving the cantons the right to self-government on local issues. Giving credit to those who favoured the power of the cantons (the Sonderbund Kantone), the national assembly was divided between an upper house (the Council of States, two representatives per canton) and a lower house (the National Council, with representatives elected from across the country). Referendums were made mandatory for any amendment of this constitution. This new constitution also brought a legal end to nobility in Switzerland.
A system of single weights and measures was introduced and in 1850 the Swiss franc became the Swiss single currency. Article 11 of the constitution forbade sending troops to serve abroad, with the exception of serving the Holy See, though the Swiss were still obliged to serve Francis II of the Two Sicilies with Swiss Guards present at the Siege of Gaeta in 1860, marking the end of foreign service.

An important clause of the constitution was that it could be re-written completely if this was deemed necessary, thus enabling it to evolve as a whole rather than being modified one amendment at a time.

This need soon proved itself when the rise in population and the Industrial Revolution that followed led to calls to modify the constitution accordingly. An early draft was rejected by the population in 1872 but modifications led to its acceptance in 1874. It introduced the facultative referendum for laws at the federal level. It also established federal responsibility for defence, trade, and legal matters.

In 1891, the constitution was revised with unusually strong elements of direct democracy, which remain unique even today.

Switzerland was not invaded during either of the world wars. During World War I, Switzerland was home to Vladimir Illych Ulyanov (Vladimir Lenin) and he remained there until 1917. Swiss neutrality was seriously questioned by the Grimm–Hoffmann Affair in 1917, but it was short-lived. In 1920, Switzerland joined the League of Nations, which was based in Geneva, on condition that it was exempt from any military requirements.

During World War II, detailed invasion plans were drawn up by the Germans, but Switzerland was never attacked. Switzerland was able to remain independent through a combination of military deterrence, concessions to Germany, and good fortune as larger events during the war delayed an invasion. Under General Henri Guisan, appointed the commander-in-chief for the duration of the war, a general mobilisation of the armed forces was ordered. The Swiss military strategy was changed from one of static defence at the borders to protect the economic heartland, to one of organised long-term attrition and withdrawal to strong, well-stockpiled positions high in the Alps known as the Reduit. Switzerland was an important base for espionage by both sides in the conflict and often mediated communications between the Axis and Allied powers.

Switzerland's trade was blockaded by both the Allies and by the Axis. Economic cooperation and extension of credit to the Third Reich varied according to the perceived likelihood of invasion and the availability of other trading partners. Concessions reached a peak after a crucial rail link through Vichy France was severed in 1942, leaving Switzerland (together with Liechtenstein) entirely isolated from the wider world by Axis controlled territory. Over the course of the war, Switzerland interned over 300,000 refugees and the International Red Cross, based in Geneva, played an important part during the conflict. Strict immigration and asylum policies as well as the financial relationships with Nazi Germany raised controversy, but not until the end of the 20th century.

During the war, the Swiss Air Force engaged aircraft of both sides, shooting down 11 intruding Luftwaffe planes in May and June 1940, then forcing down other intruders after a change of policy following threats from Germany. Over 100 Allied bombers and their crews were interned during the war. Between 1940 and 1945, Switzerland was bombed by the Allies causing fatalities and property damage. Among the cities and towns bombed were Basel, Brusio, Chiasso, Cornol, Geneva, Koblenz, Niederweningen, Rafz, Renens, Samedan, Schaffhausen, Stein am Rhein, Tägerwilen, Thayngen, Vals, and Zürich. Allied forces explained the bombings, which violated the 96th Article of War, resulted from navigation errors, equipment failure, weather conditions, and errors made by bomber pilots. The Swiss expressed fear and concern that the bombings were intended to put pressure on Switzerland to end economic cooperation and neutrality with Nazi Germany. Court-martial proceedings took place in England and the U.S. Government paid 62,176,433.06 in Swiss francs for reparations of the bombings.

Switzerland's attitude towards refugees was complicated and controversial; over the course of the war it admitted as many as 300,000 refugees while refusing tens of thousands more, including Jews who were severely persecuted by the Nazis.

After the war, the Swiss government exported credits through the charitable fund known as the Schweizerspende and also donated to the Marshall Plan to help Europe's recovery, efforts that ultimately benefited the Swiss economy.

During the Cold War, Swiss authorities considered the construction of a Swiss nuclear bomb. Leading nuclear physicists at the Federal Institute of Technology Zürich such as Paul Scherrer made this a realistic possibility. In 1988, the Paul Scherrer Institute was founded in his name to explore the therapeutic uses of neutron scattering technologies. Financial problems with the defence budget and ethical considerations prevented the substantial funds from being allocated, and the Nuclear Non-Proliferation Treaty of 1968 was seen as a valid alternative. All remaining plans for building nuclear weapons were dropped by 1988.

Switzerland was the last Western republic to grant women the right to vote. Some Swiss cantons approved this in 1959, while at the federal level it was achieved in 1971 and, after resistance, in the last canton Appenzell Innerrhoden (one of only two remaining "Landsgemeinde", along with Glarus) in 1990. After obtaining suffrage at the federal level, women quickly rose in political significance, with the first woman on the seven member Federal Council executive being Elisabeth Kopp, who served from 1984 to 1989, and the first female president being Ruth Dreifuss in 1999.

Switzerland joined the Council of Europe in 1963. In 1979 areas from the canton of Bern attained independence from the Bernese, forming the new canton of Jura. On 18 April 1999 the Swiss population and the cantons voted in favour of a completely revised federal constitution.

In 2002 Switzerland became a full member of the United Nations, leaving the Vatican City as the last widely recognised state without full UN membership. Switzerland is a founding member of the EFTA, but is not a member of the European Economic Area. An application for membership in the European Union was sent in May 1992, but not advanced since the EEA was rejected in December 1992 when Switzerland was the only country to launch a referendum on the EEA. There have since been several referendums on the EU issue; due to opposition from the citizens, the membership application has been withdrawn. Nonetheless, Swiss law is gradually being adjusted to conform with that of the EU, and the government has signed a number of bilateral agreements with the European Union. Switzerland, together with Liechtenstein, has been completely surrounded by the EU since Austria's entry in 1995. On 5 June 2005, Swiss voters agreed by a 55% majority to join the Schengen treaty, a result that was regarded by EU commentators as a sign of support by Switzerland, a country that is traditionally perceived as independent and reluctant to enter supranational bodies.

Extending across the north and south side of the Alps in west-central Europe, Switzerland encompasses a great diversity of landscapes and climates on a limited area of . The population is about 8 million, resulting in an average population density of around 195 people per square kilometre (500/sq mi). The more mountainous southern half of the country is far more sparsely populated than the northern half. In the largest Canton of Graubünden, lying entirely in the Alps, population density falls to 27 /km² (70 /sq mi).

Switzerland lies between latitudes 45° and 48° N, and longitudes 5° and 11° E. It contains three basic topographical areas: the Swiss Alps to the south, the Swiss Plateau or Central Plateau, and the Jura mountains on the west. The Alps are a high mountain range running across the central-south of the country, constituting about 60% of the country's total area. The majority of the Swiss population live in the Swiss Plateau. Among the high valleys of the Swiss Alps many glaciers are found, totalling an area of . From these originate the headwaters of several major rivers, such as the Rhine, Inn, Ticino and Rhône, which flow in the four cardinal directions into the whole of Europe. The hydrographic network includes several of the largest bodies of freshwater in Central and Western Europe, among which are included Lake Geneva (also called le Lac Léman in French), Lake Constance (known as Bodensee in German) and Lake Maggiore. Switzerland has more than 1500 lakes, and contains 6% of Europe's stock of fresh water. Lakes and glaciers cover about 6% of the national territory. The largest lake is Lake Geneva, in western Switzerland shared with France. The Rhône is both the main source and outflow of Lake Geneva. Lake Constance is the second largest Swiss lake and, like the Lake Geneva, an intermediate step by the Rhine at the border to Austria and Germany. While the Rhône flows into the Mediterranean Sea at the French Camargue region and the Rhine flows into the North Sea at Rotterdam in the Netherlands, about apart, both springs are only about apart from each other in the Swiss Alps.

Forty-eight of Switzerland's mountains are above sea in altitude or higher. At , Monte Rosa is the highest, although the Matterhorn () is often regarded as the most famous. Both are located within the Pennine Alps in the canton of Valais, on the border with Italy. The section of the Bernese Alps above the deep glacial Lauterbrunnen valley, containing 72 waterfalls, is well known for the Jungfrau () Eiger and Mönch, and the many picturesque valleys in the region. In the southeast the long Engadin Valley, encompassing the St. Moritz area in canton of Graubünden, is also well known; the highest peak in the neighbouring Bernina Alps is Piz Bernina ().

The more populous northern part of the country, constituting about 30% of the country's total area, is called the Swiss Plateau. It has greater open and hilly landscapes, partly forested, partly open pastures, usually with grazing herds, or vegetables and fruit fields, but it is still hilly. There are large lakes found here and the biggest Swiss cities are in this area of the country.

Within Switzerland there are two small enclaves: Büsingen belongs to Germany, Campione d'Italia belongs to Italy. Switzerland has no exclaves in other countries.

The Swiss climate is generally temperate, but can vary greatly between the localities, from glacial conditions on the mountaintops to the often pleasant near Mediterranean climate at Switzerland's southern tip. There are some valley areas in the southern part of Switzerland where some cold-hardy palm trees are found. Summers tend to be warm and humid at times with periodic rainfall so they are ideal for pastures and grazing. The less humid winters in the mountains may see long intervals of stable conditions for weeks, while the lower lands tend to suffer from inversion, during these periods, thus seeing no sun for weeks.

A weather phenomenon known as the föhn (with an identical effect to the chinook wind) can occur at all times of the year and is characterised by an unexpectedly warm wind, bringing air of very low relative humidity to the north of the Alps during rainfall periods on the southern face of the Alps. This works both ways across the alps but is more efficient if blowing from the south due to the steeper step for oncoming wind from the south. Valleys running south to north trigger the best effect.
The driest conditions persist in all inner alpine valleys that receive less rain because arriving clouds lose a lot of their content while crossing the mountains before reaching these areas. Large alpine areas such as Graubünden remain drier than pre-alpine areas and as in the main valley of the Valais wine grapes are grown there.

The wettest conditions persist in the high Alps and in the Ticino canton which has much sun yet heavy bursts of rain from time to time. Precipitation tends to be spread moderately throughout the year with a peak in summer. Autumn is the driest season, winter receives less precipitation than summer, yet the weather patterns in Switzerland are not in a stable climate system and can be variable from year to year with no strict and predictable periods.

Switzerland's ecosystems can be particularly fragile, because the many delicate valleys separated by high mountains often form unique ecologies. The mountainous regions themselves are also vulnerable, with a rich range of plants not found at other altitudes, and experience some pressure from visitors and grazing. The climatic, geological and topographical conditions of the alpine region make for a very fragile ecosystem that is particularly sensitive to climate change. Nevertheless, according to the 2014 Environmental Performance Index, Switzerland ranks first among 132 nations in safeguarding the environment, due to its high scores on environmental public health, its heavy reliance on renewable sources of energy (hydropower and geothermal energy), and its control of greenhouse gas emissions.

However, access to biocapacity in Switzerland is far lower than world average. In 2016, Switzerland had 1.0 global hectares of biocapacity per person within its territory, 40 percent less than world average of 1.6 global hectares per person. In contrast, in 2016, they used 4.6 global hectares of biocapacity - their ecological footprint of consumption. This means they used about 4.6 times as much biocapacity as Switzerland contains. The remainder comes from imports and overusing the global commons (such as the atmosphere through greenhouse gas emissions). As a result, Switzerland is running a biocapacity deficit.

The Federal Constitution adopted in 1848 is the legal foundation of the modern federal state. A new Swiss Constitution was adopted in 1999, but did not introduce notable changes to the federal structure. It outlines basic and political rights of individuals and citizen participation in public affairs, divides the powers between the Confederation and the cantons and defines federal jurisdiction and authority. There are three main governing bodies on the federal level: the bicameral parliament (legislative), the Federal Council (executive) and the Federal Court (judicial).

The Swiss Parliament consists of two houses: the Council of States which has 46 representatives (two from each canton and one from each half-canton) who are elected under a system determined by each canton, and the National Council, which consists of 200 members who are elected under a system of proportional representation, depending on the population of each canton. Members of both houses serve for 4 years and only serve as members of parliament part-time (so-called "Milizsystem" or citizen legislature). When both houses are in joint session, they are known collectively as the Federal Assembly. Through referendums, citizens may challenge any law passed by parliament and through initiatives, introduce amendments to the federal constitution, thus making Switzerland a direct democracy.

The Federal Council constitutes the federal government, directs the federal administration and serves as collective Head of State. It is a collegial body of seven members, elected for a four-year mandate by the Federal Assembly which also exercises oversight over the Council. The President of the Confederation is elected by the Assembly from among the seven members, traditionally in rotation and for a one-year term; the President chairs the government and assumes representative functions. However, the president is a "primus inter pares" with no additional powers, and remains the head of a department within the administration.

The Swiss government has been a coalition of the four major political parties since 1959, each party having a number of seats that roughly reflects its share of electorate and representation in the federal parliament.
The classic distribution of 2 CVP/PDC, 2 SPS/PSS, 2 FDP/PRD and 1 SVP/UDC as it stood from 1959 to 2003 was known as the "magic formula". Following the 2015 Federal Council elections, the seven seats in the Federal Council were distributed as follows:

The function of the Federal Supreme Court is to hear appeals against rulings of cantonal or federal courts. The judges are elected by the Federal Assembly for six-year terms.

Direct democracy and federalism are hallmarks of the Swiss political system. Swiss citizens are subject to three legal jurisdictions: the municipality, canton and federal levels. The 1848 and 1999 Swiss Constitutions define a system of direct democracy (sometimes called half-direct or representative direct democracy because it is aided by the more commonplace institutions of a representative democracy). The instruments of this system at the federal level, known as popular rights (, , ), include the right to submit a federal initiative and a referendum, both of which may overturn parliamentary decisions.

By calling a federal referendum, a group of citizens may challenge a law passed by parliament, if they gather 50,000 signatures against the law within 100 days. If so, a national vote is scheduled where voters decide by a simple majority whether to accept or reject the law. Any 8 cantons together can also call a constitutional referendum on a federal law.

Similarly, the federal "constitutional initiative" allows citizens to put a constitutional amendment to a national vote, if 100,000 voters sign the proposed amendment within 18 months. The Federal Council and the Federal Assembly can supplement the proposed amendment with a counter-proposal, and then voters must indicate a preference on the ballot in case both proposals are accepted. Constitutional amendments, whether introduced by initiative or in parliament, must be accepted by a double majority of the national popular vote and the cantonal popular votes.

The Swiss Confederation consists of 26 cantons:
<nowiki>*</nowiki>

The cantons are federated states, have a permanent constitutional status and, in comparison with the situation in other countries, a high degree of independence. Under the Federal Constitution, all 26 cantons are equal in status, except that 6 (referred to often as the half-cantons) are represented by only one councillor (instead of two) in the Council of States and have only half a cantonal vote with respect to the required cantonal majority in referendums on constitutional amendments. Each canton has its own constitution, and its own parliament, government, police and courts. However, there are considerable differences between the individual cantons, most particularly in terms of population and geographical area. Their populations vary between 16,003 (Appenzell Innerrhoden) and 1,487,969 (Zürich), and their area between (Basel-Stadt) and (Grisons).

The cantons comprise a total of 2,222 municipalities as of 2018.

Traditionally, Switzerland avoids alliances that might entail military, political, or direct economic action and has been neutral since the end of its expansion in 1515. Its policy of neutrality was internationally recognised at the Congress of Vienna in 1815. Only in 2002 did Switzerland become a full member of the United Nations and it was the first state to join it by referendum. Switzerland maintains diplomatic relations with almost all countries and historically has served as an intermediary between other states. Switzerland is not a member of the European Union; the Swiss people have consistently rejected membership since the early 1990s. However, Switzerland does participate in the Schengen Area.

Many international institutions have their seats in Switzerland, in part because of its policy of neutrality. Geneva is the birthplace of the Red Cross and Red Crescent Movement and the Geneva Conventions and, since 2006, hosts the United Nations Human Rights Council. Even though Switzerland is one of the most recent countries to have joined the United Nations, the Palace of Nations in Geneva is the second biggest centre for the United Nations after New York, and Switzerland was a founding member and home to the League of Nations.

Apart from the United Nations headquarters, the Swiss Confederation is host to many UN agencies, like the World Health Organization (WHO), the International Labour Organization (ILO), the International Telecommunication Union (ITU), the United Nations High Commissioner for Refugees (UNHCR) and about 200 other international organisations, including the World Trade Organization and the World Intellectual Property Organization. The annual meetings of the World Economic Forum in Davos bring together top international business and political leaders from Switzerland and foreign countries to discuss important issues facing the world, including health and the environment. Additionally the headquarters of the Bank for International Settlements (BIS) are located in Basel since 1930.

Furthermore, many sport federations and organisations are located throughout the country, such as the International Basketball Federation in Geneva, the Union of European Football Associations (UEFA) in Nyon, the International Federation of Association Football (FIFA) and the International Ice Hockey Federation both in Zürich, the International Cycling Union in Aigle, and the International Olympic Committee in Lausanne.

The Swiss Armed Forces, including the Land Forces and the Air Force, are composed mostly of conscripts, male citizens aged from 20 to 34 (in special cases up to 50) years. Being a landlocked country, Switzerland has no navy; however, on lakes bordering neighbouring countries, armed military patrol boats are used. Swiss citizens are prohibited from serving in foreign armies, except for the Swiss Guards of the Vatican, or if they are dual citizens of a foreign country and reside there.

The structure of the Swiss militia system stipulates that the soldiers keep their Army issued equipment, including all personal weapons, at home. Some organisations and political parties find this practice controversial. Women can serve voluntarily. Men usually receive military conscription orders for training at the age of 18. About two thirds of the young Swiss are found suited for service; for those found unsuited, various forms of alternative service exist. Annually, approximately 20,000 persons are trained in recruit centres for a duration from 18 to 21 weeks. The reform "Army XXI" was adopted by popular vote in 2003, it replaced the previous model "Army 95", reducing the effectives from 400,000 to about 200,000. Of those, 120,000 are active in periodic Army training and 80,000 are non-training reserves.
Overall, three general mobilisations have been declared to ensure the integrity and neutrality of Switzerland. The first one was held on the occasion of the Franco-Prussian War of 1870–71. The second was in response to the outbreak of the First World War in August 1914. The third mobilisation of the army took place in September 1939 in response to the German attack on Poland; Henri Guisan was elected as the General-in-Chief.

Because of its neutrality policy, the Swiss army does not currently take part in armed conflicts in other countries, but is part of some peacekeeping missions around the world. Since 2000 the armed force department has also maintained the Onyx intelligence gathering system to monitor satellite communications. Switzerland decided not to sign the Nuclear Weapon Ban Treaty.

Following the end of the Cold War there have been a number of attempts to curb military activity or even abolish the armed forces altogether. A notable referendum on the subject, launched by an anti-militarist group, was held on 26 November 1989. It was defeated with about two thirds of the voters against the proposal. A similar referendum, called for before, but held shortly after the 11 September attacks in the US, was defeated by over 78% of voters.

Gun politics in Switzerland are unique in Europe in that 29% of citizens are legally armed. The large majority of firearms kept at home are issued by the Swiss army, but ammunition is no longer issued.

Until 1848 the rather loosely coupled Confederation did not know a central political organisation, but representatives, mayors, and "Landammänner" met several times a year at the capital of the "Lieu" presiding the Confederal Diet for one year.
Until 1500 the legates met most of the time in Lucerne, but also in Zürich, Baden, Bern, Schwyz etc., but sometimes also at places outside of the confederation, such as Constance. From the Swabian War in 1499 onwards until Reformation, most conferences met in Zurich. Afterwards the town hall at Baden, where the annual accounts of the common people had been held regularly since 1426, became the most frequent, but not the sole place of assembly. After 1712 Frauenfeld gradually dissolved Baden. From 1526, the Catholic conferences were held mostly in Lucerne, the Protestant conferences from 1528 mostly in Aarau, the one for the legitimation of the French Ambassador in Solothurn. At the same time the syndicate for the "Ennetbirgischen Vogteien" located in the present Ticino met from 1513 in Lugano and Locarno.

After the Helvetic Republic and during the Mediation from 1803 until 1815 the Confederal Diet of the 19 "Lieus" met at the capitals of the "directoral cantons" Fribourg, Berne, Basel, Zurich, Lucerne and Solothurn.

After the Long Diet from 6 April 1814 to 31 August 1815 took place in Zurich to replace the constitution and the enhancement of the Confederation to 22 cantons by the admission of the cantons of Valais, Neuchâtel and Geneva to full members, the directoral cantons of Lucerne, Zurich and Berne took over the diet in two-year turns.

In 1848, the federal constitution provided that details concerning the federal institutions, such as their locations, should be taken care of by the Federal Assembly (BV 1848 Art. 108). Thus on 28 November 1848, the Federal Assembly voted in majority to locate the seat of government in Berne. And, as a prototypical federal compromise, to assign other federal institutions, such as the Federal Polytechnical School (1854, the later ETH) to Zurich, and other institutions to Lucerne, such as the later SUVA (1912) and the Federal Insurance Court (1917). In 1875, a law (RS 112) fixed the compensations owed by the city of Bern for the federal seat. According to these living fundamental federalistic feelings further federal institutions were subsequently attributed to Lausanne (Federal Supreme Court in 1872, and EPFL in 1969), Bellinzona (Federal Criminal Court, 2004), and St. Gallen (Federal Administrative Court and Federal Patent Court, 2012).

The 1999 new constitution, however, does not contain anything concerning any Federal City. In 2002 a tripartite committee has been asked by the Swiss Federal Council to prepare the "creation of a federal law on the status of Bern as a Federal City", and to evaluate the positive and negative aspects for the city and the canton of Bern if this status were awarded. After a first report the work of this committee was suspended in 2004 by the Swiss Federal Council, and work on this subject has not resumed since.

Thus as of today, no city in Switzerland has the official status either of capital or of Federal City, nevertheless Berne is commonly referred to as "Federal City" (, , ).

Switzerland has a stable, prosperous and high-tech economy and enjoys great wealth, being ranked as the wealthiest country in the world per capita in multiple rankings. In 2011 it was ranked as the wealthiest country in the world in per capita terms (with "wealth" being defined to include both financial and non-financial assets), while the 2013 Credit Suisse Global Wealth Report showed that Switzerland was the country with the highest average wealth per adult in 2013. It has the world's nineteenth largest economy by nominal GDP and the thirty-sixth largest by purchasing power parity. It is the twentieth largest exporter, despite its small size. Switzerland has the highest European rating in the Index of Economic Freedom 2010, while also providing large coverage through public services. The nominal per capita GDP is higher than those of the larger Western and Central European economies and Japan. If adjusted for purchasing power parity, Switzerland ranks 8th in the world in terms of GDP per capita, according to the World Bank and IMF (ranked 15th according to the CIA Worldfactbook).

The World Economic Forum's Global Competitiveness Report currently ranks Switzerland's economy as the most competitive in the world, while ranked by the European Union as Europe's most innovative country. For much of the 20th century, Switzerland was the wealthiest country in Europe by a considerable margin (by GDP – per capita). In 2017, average gross household income in Switzerland was 9,946 francs per month (equivalent to US$10,720 per month), though 61% of the population made less than the average income. Switzerland also has one of the world's largest account balances as a percentage of GDP.

Switzerland is home to several large multinational corporations. The largest Swiss companies by revenue are Glencore, Gunvor, Nestlé, Novartis, Hoffmann-La Roche, ABB, Mercuria Energy Group and Adecco. Also, notable are UBS AG, Zurich Financial Services, Credit Suisse, Barry Callebaut, Swiss Re, Tetra Pak, The Swatch Group and Swiss International Air Lines. Switzerland is ranked as having one of the most powerful economies in the world.

Switzerland's most important economic sector is manufacturing. Manufacturing consists largely of the production of specialist chemicals, health and pharmaceutical goods, scientific and precision measuring instruments and musical instruments. The largest exported goods are chemicals (34% of exported goods), machines/electronics (20.9%), and precision instruments/watches (16.9%). Exported services amount to a third of exports. The service sector – especially banking and insurance, tourism, and international organisations – is another important industry for Switzerland.

Slightly more than 5 million people work in Switzerland; about 25% of employees belonged to a trade union in 2004. Switzerland has a more flexible job market than neighbouring countries and the unemployment rate is very low. The unemployment rate increased from a low of 1.7% in June 2000 to a peak of 4.4% in December 2009. The unemployment rate decreased to 3.2% in 2014 without further decrease in 2015 and 2016. Population growth from net immigration is quite high, at 0.52% of population in 2004. The foreign citizen population was 21.8% in 2004, about the same as in Australia. GDP per hour worked is the world's 16th highest, at 49.46 international dollars in 2012.
Switzerland has an overwhelmingly private sector economy and low tax rates by Western World standards; overall taxation is one of the smallest of developed countries. Switzerland is a relatively easy place to do business, currently ranking 20th of 189 countries in the Ease of Doing Business Index. The slow growth Switzerland experienced in the 1990s and the early 2000s has brought greater support for economic reforms and harmonisation with the European Union. According to Credit Suisse, only about 37% of residents own their own homes, one of the lowest rates of home ownership in Europe. Housing and food price levels were 171% and 145% of the EU-25 index in 2007, compared to 113% and 104% in Germany.

The Swiss Federal budget had a size of 62.8 billion Swiss francs in 2010, which is an equivalent 11.35% of the country's GDP in that year; however, the regional (canton) budgets and the budgets of the municipalities are not counted as part of the federal budget and the total rate of government spending is closer to 33.8% of GDP. The main sources of income for the federal government are the value-added tax (33%) and the direct federal tax (29%) and the main expenditure is located in the areas of social welfare and finance & tax. The expenditures of the Swiss Confederation have been growing from 7% of GDP in 1960 to 9.7% in 1990 and to 10.7% in 2010. While the sectors social welfare and finance & tax have been growing from 35% in 1990 to 48.2% in 2010, a significant reduction of expenditures has been occurring in the sectors of agriculture and national defence; from 26.5% in to 12.4% (estimation for the year 2015).

Agricultural protectionism—a rare exception to Switzerland's free trade policies—has contributed to high food prices. Product market liberalisation is lagging behind many EU countries according to the OECD. Nevertheless, domestic purchasing power is one of the best in the world. Apart from agriculture, economic and trade barriers between the European Union and Switzerland are minimal and Switzerland has free trade agreements worldwide. Switzerland is a member of the European Free Trade Association (EFTA).

Education in Switzerland is very diverse because the constitution of Switzerland delegates the authority for the school system to the cantons. There are both public and private schools, including many private international schools. The minimum age for primary school is about six years in all cantons, but most cantons provide a free "children's school" starting at four or five years old. Primary school continues until grade four, five or six, depending on the school. Traditionally, the first foreign language in school was always one of the other national languages, although recently (2000) English was introduced first in a few cantons.

At the end of primary school (or at the beginning of secondary school), pupils are separated according to their capacities in several (often three) sections. The fastest learners are taught advanced classes to be prepared for further studies and the matura, while students who assimilate a little more slowly receive an education more adapted to their needs.
There are 12 universities in Switzerland, ten of which are maintained at cantonal level and usually offer a range of non-technical subjects. The first university in Switzerland was founded in 1460 in Basel (with a faculty of medicine) and has a tradition of chemical and medical research in Switzerland. The largest university in Switzerland is the University of Zurich with nearly 25,000 students.The Swiss Federal Institute of Technology Zurich (ETHZ) and the University of Zurich are listed 20th and 54th respectively, on the 2015 Academic Ranking of World Universities.

The two institutes sponsored by the federal government are the Swiss Federal Institute of Technology Zurich (ETHZ) in Zürich, founded 1855 and the EPFL in Lausanne, founded 1969 as such, which was formerly an institute associated with the University of Lausanne.

In addition, there are various Universities of Applied Sciences. In business and management studies, the University of St. Gallen, (HSG) is ranked 329th in the world according to QS World University Rankings and the International Institute for Management Development (IMD), was ranked first in open programmes worldwide by the "Financial Times." Switzerland has the second highest rate (almost 18% in 2003) of foreign students in tertiary education, after Australia (slightly over 18%).

As might befit a country that plays home to innumerable international organisations, the Graduate Institute of International and Development Studies, located in Geneva, is not only continental Europe's oldest graduate school of international and development studies, but also widely believed to be one of its most prestigious.

Many Nobel Prize laureates have been Swiss scientists. They include the world-famous physicist Albert Einstein in the field of physics, who developed his special relativity while working in Bern. More recently Vladimir Prelog, Heinrich Rohrer, Richard Ernst, Edmond Fischer, Rolf Zinkernagel, Kurt Wüthrich and Jacques Dubochet received Nobel Prizes in the sciences. In total, 114 Nobel Prize winners in all fields stand in relation to Switzerland and the Nobel Peace Prize has been awarded nine times to organisations residing in Switzerland.

Geneva and the nearby French department of Ain co-host the world's largest laboratory, CERN, dedicated to particle physics research. Another important research centre is the Paul Scherrer Institute. Notable inventions include lysergic acid diethylamide (LSD), diazepam (Valium), the scanning tunnelling microscope (Nobel prize) and Velcro. Some technologies enabled the exploration of new worlds such as the pressurised balloon of Auguste Piccard and the Bathyscaphe which permitted Jacques Piccard to reach the deepest point of the world's oceans.

Switzerland Space Agency, the Swiss Space Office, has been involved in various space technologies and programmes. In addition it was one of the 10 founders of the European Space Agency in 1975 and is the seventh largest contributor to the ESA budget. In the private sector, several companies are implicated in the space industry such as Oerlikon Space or Maxon Motors who provide spacecraft structures.

Switzerland voted against membership in the European Economic Area in a referendum in December 1992 and has since maintained and developed its relationships with the European Union (EU) and European countries through bilateral agreements. In March 2001, the Swiss people refused in a popular vote to start accession negotiations with the EU. In recent years, the Swiss have brought their economic practices largely into conformity with those of the EU in many ways, in an effort to enhance their international competitiveness. The economy grew at 3% in 2010, 1.9% in 2011, and 1% in 2012. EU membership was a long-term objective of the Swiss government, but there was and remains considerable popular sentiment against membership, which is opposed by the conservative SVP party, the largest party in the National Council, and not currently supported or proposed by several other political parties. The application for membership of the EU was formally withdrawn in 2016, having long been frozen. The western French-speaking areas and the urban regions of the rest of the country tend to be more pro-EU, nonetheless with far from a significant share of the population.
The government has established an Integration Office under the Department of Foreign Affairs and the Department of Economic Affairs. To minimise the negative consequences of Switzerland's isolation from the rest of Europe, Bern and Brussels signed seven bilateral agreements to further liberalise trade ties. These agreements were signed in 1999 and took effect in 2001. This first series of bilateral agreements included the free movement of persons. A second series covering nine areas was signed in 2004 and has since been ratified, which includes the Schengen Treaty and the Dublin Convention besides others. They continue to discuss further areas for cooperation.

In 2006, Switzerland approved 1 billion francs of supportive investment in the poorer Southern and Central European countries in support of cooperation and positive ties to the EU as a whole. A further referendum will be needed to approve 300 million francs to support Romania and Bulgaria and their recent admission. The Swiss have also been under EU and sometimes international pressure to reduce banking secrecy and to raise tax rates to parity with the EU. Preparatory discussions are being opened in four new areas: opening up the electricity market, participation in the European GNSS project Galileo, cooperating with the European centre for disease prevention and recognising certificates of origin for food products.

On 27 November 2008, the interior and justice ministers of European Union in Brussels announced Switzerland's accession to the Schengen passport-free zone from 12 December 2008. The land border checkpoints will remain in place only for goods movements, but should not run controls on people, though people entering the country had their passports checked until 29 March 2009 if they originated from a Schengen nation.

On 9 February 2014, Swiss voters narrowly approved by 50.3% a ballot initiative launched by the national conservative Swiss People's Party (SVP/UDC) to restrict immigration, and thus reintroducing a quota system on the influx of foreigners. This initiative was mostly backed by rural (57.6% approvals) and suburban agglomerations (51.2% approvals), and isolated towns (51.3% approvals) of Switzerland as well as by a strong majority (69.2% approval) in the canton of Ticino, while metropolitan centres (58.5% rejection) and the French-speaking part (58.5% rejection) of Switzerland rather rejected it. Some news commentators claim that this proposal "de facto" contradicts the bilateral agreements on the free movement of persons from these respective countries.

In December 2016, a compromise with the European Union was attained effectively canceling quotas on EU citizens but still allowing for favourable treatment of Swiss-based job applicants.

Electricity generated in Switzerland is 56% from hydroelectricity and 39% from nuclear power, resulting in a nearly CO-free electricity-generating network. On 18 May 2003, two anti-nuclear initiatives were turned down: "Moratorium Plus", aimed at forbidding the building of new nuclear power plants (41.6% supported and 58.4% opposed), and Electricity Without Nuclear (33.7% supported and 66.3% opposed) after a previous moratorium expired in 2000. However, as a reaction to the Fukushima nuclear disaster, the Swiss government announced in 2011 that it plans to end its use of nuclear energy in the next 2 or 3 decades. In November 2016, Swiss voters rejected a proposal by the Green Party to accelerate the phaseout of nuclear power (45.8% supported and 54.2% opposed). The Swiss Federal Office of Energy (SFOE) is the office responsible for all questions relating to energy supply and energy use within the Federal Department of Environment, Transport, Energy and Communications (DETEC). The agency is supporting the 2000-watt society initiative to cut the nation's energy use by more than half by the year 2050.
The most dense rail network in Europe of carries over 596 million passengers annually (as of 2015). In 2015, each Swiss resident travelled on average by rail, which makes them the keenest rail users. Virtually 100% of the network is electrified. The vast majority (60%) of the network is operated by the Swiss Federal Railways (SBB CFF FFS). Besides the second largest standard gauge railway company BLS AG two railways companies operating on narrow gauge networks are the Rhaetian Railway (RhB) in the southeastern canton of Graubünden, which includes some World Heritage lines, and the Matterhorn Gotthard Bahn (MGB), which co-operates together with RhB the Glacier Express between Zermatt and St. Moritz/Davos. On 31 May 2016 the world's longest and deepest railway tunnel and the first flat, low-level route through the Alps, the Gotthard Base Tunnel, opened as the largest part of the New Railway Link through the Alps (NRLA) project after 17 years of realization. It started its daily business for passenger transport on 11 December 2016 replacing the old, mountainous, scenic route over and through the St Gotthard Massif.

Switzerland has a publicly managed road network without road tolls that is financed by highway permits as well as vehicle and gasoline taxes. The Swiss autobahn/autoroute system requires the purchase of a vignette (toll sticker)—which costs 40 Swiss francs—for one calendar year in order to use its roadways, for both passenger cars and trucks. The Swiss autobahn/autoroute network has a total length of (as of 2000) and has, by an area of , also one of the highest motorway densities in the world. Zurich Airport is Switzerland's largest international flight gateway, which handled 22.8 million passengers in 2012. The other international airports are Geneva Airport (13.9 million passengers in 2012), EuroAirport Basel Mulhouse Freiburg which is located in France, Bern Airport, Lugano Airport, St. Gallen-Altenrhein Airport and Sion Airport. Swiss International Air Lines is the flag carrier of Switzerland. Its main hub is Zürich.

Switzerland has one of the best environmental records among nations in the developed world; it was one of the countries to sign the Kyoto Protocol in 1998 and ratified it in 2003. With Mexico and the Republic of Korea it forms the Environmental Integrity Group (EIG). The country is heavily active in recycling and anti-littering regulations and is one of the top recyclers in the world, with 66% to 96% of recyclable materials being recycled, depending on the area of the country. The 2014 Global Green Economy Index ranked Switzerland among the top 10 green economies in the world.

Switzerland developed an efficient system to recycle most recyclable materials. Publicly organised collection by volunteers and economical railway transport logistics started as early as 1865 under the leadership of the notable industrialist Hans Caspar Escher (Escher Wyss AG) when the first modern Swiss paper manufacturing plant was built in Biberist.

Switzerland also has an economic system for garbage disposal, which is based mostly on recycling and energy-producing incinerators due to a strong political will to protect the environment. As in other European countries, the illegal disposal of garbage is not tolerated at all and heavily fined. In almost all Swiss municipalities, stickers or dedicated garbage bags need to be purchased that allow for identification of disposable garbage.

In 2018, Switzerland's population slightly exceeded 8.5 million. In common with other developed countries, the Swiss population increased rapidly during the industrial era, quadrupling between 1800 and 1990 and has continued to grow. Like most of Europe, Switzerland faces an ageing population, albeit with consistent annual growth projected into 2035, due mostly to immigration and a fertility rate close to replacement level. Switzerland subsequently has one of the oldest populations in the world, with the average age of 42.5 years.

, resident foreigners make up 25.2% of the population, one of the largest proportions in the developed world. Most of these (64%) were from European Union or EFTA countries. Italians were the largest single group of foreigners, with 15.6% of total foreign population, followed closely by Germans (15.2%), immigrants from Portugal (12.7%), France (5.6%), Serbia (5.3%), Turkey (3.8%), Spain (3.7%), and Austria (2%). Immigrants from Sri Lanka, most of them former Tamil refugees, were the largest group among people of Asian origin (6.3%).

Additionally, the figures from 2012 show that 34.7% of the permanent resident population aged 15 or over in Switzerland (around 2.33 million), had an immigrant background. A third of this population (853,000) held Swiss citizenship. Four fifths of persons with an immigration background were themselves immigrants (first generation foreigners and native-born and naturalised Swiss citizens), whereas one fifth were born in Switzerland (second generation foreigners and native-born and naturalised Swiss citizens).

In the 2000s, domestic and international institutions expressed concern about what was perceived as an increase in xenophobia, particularly in some political campaigns. In reply to one critical report, the Federal Council noted that "racism unfortunately is present in Switzerland", but stated that the high proportion of foreign citizens in the country, as well as the generally unproblematic integration of foreigners, underlined Switzerland's openness.

Switzerland has four national languages: mainly German (spoken by 62.8% of the population in 2016); French (22.9%) in the west; and Italian (8.2%) in the south. The fourth national language, Romansh (0.5%), is a Romance language spoken locally in the southeastern trilingual canton of Grisons, and is designated by Article 4 of the Federal Constitution as a national language along with German, French, and Italian, and in Article 70 as an official language if the authorities communicate with persons who speak Romansh. However, federal laws and other official acts do not need to be decreed in Romansh.

In 2016, the languages most spoken at home among permanent residents aged 15 and older were Swiss German (59.4%), French (23.5%), Standard German (10.6%), and Italian (8.5%). Other languages spoken at home included English (5.0%), Portuguese (3.8%), Albanian (3.0%), Spanish (2.6%) and Serbian and Croatian (2.5%). 6.9% reported speaking another language at home. In 2014 almost two-thirds (64.4%) of the permanent resident population indicated speaking more than one language regularly.

The federal government is obliged to communicate in the official languages, and in the federal parliament simultaneous translation is provided from and into German, French and Italian.

Aside from the official forms of their respective languages, the four linguistic regions of Switzerland also have their local dialectal forms. The role played by dialects in each linguistic region varies dramatically: in the German-speaking regions, Swiss German dialects have become ever more prevalent since the second half of the 20th century, especially in the media, such as radio and television, and are used as an everyday language for many, while the Swiss variety of Standard German is almost always used instead of dialect for written communication (c.f. diglossic usage of a language). Conversely, in the French-speaking regions the local dialects have almost disappeared (only 6.3% of the population of Valais, 3.9% of Fribourg, and 3.1% of Jura still spoke dialects at the end of the 20th century), while in the Italian-speaking regions dialects are mostly limited to family settings and casual conversation.

The principal official languages (German, French, and Italian) have terms, not used outside of Switzerland, known as Helvetisms. German Helvetisms are, roughly speaking, a large group of words typical of Swiss Standard German, which do not appear either in Standard German, nor in other German dialects. These include terms from Switzerland's surrounding language cultures (German "Billett" from French), from similar terms in another language (Italian "azione" used not only as "act" but also as "discount" from German "Aktion"). The French spoken in Switzerland has similar terms, which are equally known as Helvetisms. The most frequent characteristics of Helvetisms are in vocabulary, phrases, and pronunciation, but certain Helvetisms denote themselves as special in syntax and orthography likewise. Duden, the comprehensive German dictionary, contains about 3000 Helvetisms. Current French dictionaries, such as the Petit Larousse, include several hundred Helvetisms.

Learning one of the other national languages at school is compulsory for all Swiss pupils, so many Swiss are supposed to be at least bilingual, especially those belonging to linguistic minority groups.

Swiss residents are universally required to buy health insurance from private insurance companies, which in turn are required to accept every applicant. While the cost of the system is among the highest, it compares well with other European countries in terms of health outcomes; patients have been reported as being, in general, highly satisfied with it. In 2012, life expectancy at birth was 80.4 years for men and 84.7 years for women — the highest in the world. However, spending on health is particularly high at 11.4% of GDP (2010), on par with Germany and France (11.6%) and other European countries, but notably less than spending in the USA (17.6%). From 1990, a steady increase can be observed, reflecting the high costs of the services provided. With an ageing population and new healthcare technologies, health spending will likely continue to rise.

Between two thirds and three quarters of the population live in urban areas. Switzerland has gone from a largely rural country to an urban one in just 70 years. Since 1935 urban development has claimed as much of the Swiss landscape as it did during the previous 2,000 years. This urban sprawl does not only affect the plateau but also the Jura and the Alpine foothills and there are growing concerns about land use. However, from the beginning of the 21st century, the population growth in urban areas is higher than in the countryside.

Switzerland has a dense network of towns, where large, medium and small towns are complementary. The plateau is very densely populated with about 450 people per km and the landscape continually shows signs of human presence. The weight of the largest metropolitan areas, which are Zürich, Geneva–Lausanne, Basel and Bern tend to increase. In international comparison the importance of these urban areas is stronger than their number of inhabitants suggests. In addition the two main centres of Zürich and Geneva are recognised for their particularly great quality of life.

Switzerland has no official state religion, though most of the cantons (except Geneva and Neuchâtel) recognise official churches, which are either the Roman Catholic Church or the Swiss Reformed Church. These churches, and in some cantons also the Old Catholic Church and Jewish congregations, are financed by official taxation of adherents.

Christianity is the predominant religion of Switzerland (about 68% of resident population in 2016 and 75% of Swiss citizens), divided between the Roman Catholic Church (37.2% of the population), the Swiss Reformed Church (25.0%), further Protestant churches (2.2%), Eastern Orthodoxy (around 2%), and other Christian denominations (1.3%). Immigration has established Islam (5.1%) as a sizeable minority religion.

24% of Swiss permanent residents are not affiliated with any church (Atheism, Agnosticism, and others).

As of the 2000 census other Christian minority communities included Neo-Pietism (0.44%), Pentecostalism (0.28%, mostly incorporated in the Schweizer Pfingstmission), Methodism (0.13%), the New Apostolic Church (0.45%), Jehovah's Witnesses (0.28%), other Protestant denominations (0.20%), the Old Catholic Church (0.18%), other Christian denominations (0.20%). Non-Christian religions are Hinduism (0.38%), Buddhism (0.29%), Judaism (0.25%) and others (0.11%); 4.3% did not make a statement.

The country was historically about evenly balanced between Catholic and Protestant, with a complex patchwork of majorities over most of the country. Switzerland played an exceptional role during the Reformation as it became home to many reformers. Geneva converted to Protestantism in 1536, just before John Calvin arrived there. In 1541, he founded the "Republic of Geneva" on his own ideals. It became known internationally as the "Protestant Rome", and housed such reformers as Theodore Beza, William Farel or Pierre Viret. Zürich became another stronghold around the same time, with Huldrych Zwingli and Heinrich Bullinger taking the lead there. Anabaptists Felix Manz and Conrad Grebel also operated there. They were later joined by the fleeing Peter Martyr Vermigli and Hans Denck. Other centres included Basel (Andreas Karlstadt and Johannes Oecolampadius), Berne (Berchtold Haller and Niklaus Manuel), and St. Gallen (Joachim Vadian). One canton, Appenzell, was officially divided into Catholic and Protestant sections in 1597. The larger cities and their cantons (Bern, Geneva, Lausanne, Zürich and Basel) used to be predominantly Protestant. Central Switzerland, the Valais, the Ticino, Appenzell Innerrhodes, the Jura, and Fribourg are traditionally Catholic. The Swiss Constitution of 1848, under the recent impression of the clashes of Catholic vs. Protestant cantons that culminated in the Sonderbundskrieg, consciously defines a consociational state, allowing the peaceful co-existence of Catholics and Protestants. A 1980 initiative calling for the complete separation of church and state was rejected by 78.9% of the voters. Some traditionally Protestant cantons and cities nowadays have a slight Catholic majority, not because they were growing in members, quite the contrary, but only because since about 1970 a steadily growing minority became not affiliated with any church or other religious body (21.4% in Switzerland, 2012) especially in traditionally Protestant regions, such as Basel-City (42%), canton of Neuchâtel (38%), canton of Geneva (35%), canton of Vaud (26%), or Zürich city (city: >25%; canton: 23%).

Three of Europe's major languages are official in Switzerland. Swiss culture is characterised by diversity, which is reflected in a wide range of traditional customs. A region may be in some ways strongly culturally connected to the neighbouring country that shares its language, the country itself being rooted in western European culture. The linguistically isolated Romansh culture in Graubünden in eastern Switzerland constitutes an exception, it survives only in the upper valleys of the Rhine and the Inn and strives to maintain its rare linguistic tradition.

Switzerland is home to many notable contributors to literature, art, architecture, music and sciences. In addition the country attracted a number of creative persons during time of unrest or war in Europe.
Some 1000 museums are distributed through the country; the number has more than tripled since 1950. Among the most important cultural performances held annually are the Paléo Festival, Lucerne Festival, the Montreux Jazz Festival, the Locarno International Film Festival and the Art Basel.

Alpine symbolism has played an essential role in shaping the history of the country and the Swiss national identity. Nowadays some concentrated mountain areas have a strong highly energetic ski resort culture in winter, and a hiking () or Mountain biking culture in summer. Other areas throughout the year have a recreational culture that caters to tourism, yet the quieter seasons are spring and autumn when there are fewer visitors. A traditional farmer and herder culture also predominates in many areas and small farms are omnipresent outside the towns. Folk art is kept alive in organisations all over the country. In Switzerland it is mostly expressed in music, dance, poetry, wood carving and embroidery. The alphorn, a trumpet-like musical instrument made of wood, has become alongside yodeling and the accordion an epitome of traditional Swiss music.

As the Confederation, from its foundation in 1291, was almost exclusively composed of German-speaking regions, the earliest forms of literature are in German. In the 18th century, French became the fashionable language in Bern and elsewhere, while the influence of the French-speaking allies and subject lands was more marked than before.

Among the classic authors of Swiss German literature are Jeremias Gotthelf (1797–1854) and Gottfried Keller (1819–1890). The undisputed giants of 20th-century Swiss literature are Max Frisch (1911–91) and Friedrich Dürrenmatt (1921–90), whose repertoire includes "Die Physiker" (The Physicists) and "Das Versprechen" (), released in 2001 as a Hollywood film.

Famous French-speaking writers were Jean-Jacques Rousseau (1712–1778) and Germaine de Staël (1766–1817). More recent authors include Charles Ferdinand Ramuz (1878–1947), whose novels describe the lives of peasants and mountain dwellers, set in a harsh environment and Blaise Cendrars (born Frédéric Sauser, 1887–1961). Italian and Romansh-speaking authors also contributed to the Swiss literary landscape, but generally in more modest ways given their small number.

Probably the most famous Swiss literary creation, "Heidi", the story of an orphan girl who lives with her grandfather in the Alps, is one of the most popular children's books ever and has come to be a symbol of Switzerland. Her creator, Johanna Spyri (1827–1901), wrote a number of other books on similar themes.

The freedom of the press and the right to free expression is guaranteed in the federal constitution of Switzerland. The Swiss News Agency (SNA) broadcasts information around-the-clock in three of the four national languages—on politics, economics, society and culture. The SNA supplies almost all Swiss media and a couple dozen foreign media services with its news.

Switzerland has historically boasted the greatest number of newspaper titles published in proportion to its population and size. The most influential newspapers are the German-language "Tages-Anzeiger" and "Neue Zürcher Zeitung" NZZ, and the French-language "Le Temps", but almost every city has at least one local newspaper. The cultural diversity accounts for a variety of newspapers.

The government exerts greater control over broadcast media than print media, especially due to finance and licensing. The Swiss Broadcasting Corporation, whose name was recently changed to SRG SSR, is charged with the production and broadcast of radio and television programmes. SRG SSR studios are distributed throughout the various language regions. Radio content is produced in six central and four regional studios while the television programmes are produced in Geneva, Zürich, and Lugano. An extensive cable network also allows most Swiss to access the programmes from neighbouring countries.

Skiing, snowboarding and mountaineering are among the most popular sports in Switzerland, the nature of the country being particularly suited for such activities. Winter sports are practised by the natives and tourists since the second half of the 19th century with the invention of bobsleigh in St. Moritz. The first world ski championships were held in Mürren (1931) and St. Moritz (1934). The latter town hosted the second Winter Olympic Games in 1928 and the fifth edition in 1948. Among the most successful skiers and world champions are Pirmin Zurbriggen and Didier Cuche.

The most prominently watched sports in Switzerland are football, ice hockey, Alpin skiing, "Schwingen", and tennis.

The headquarters of the international football's and ice hockey's governing bodies, the International Federation of Association Football (FIFA) and International Ice Hockey Federation (IIHF), are located in Zürich. Actually many other headquarters of international sports federations are located in Switzerland. For example, the International Olympic Committee (IOC), IOC's Olympic Museum and the Court of Arbitration for Sport (CAS) are located in Lausanne.

Switzerland hosted the 1954 FIFA World Cup, and was the joint host, with Austria, of the UEFA Euro 2008 tournament. The Swiss Super League is the nation's professional football club league. Europe's highest football pitch, at above sea level, is located in Switzerland and is named the "Ottmar Hitzfeld Stadium".
Many Swiss also follow ice hockey and support one of the 12 teams of the National League, which is the most attended league in Europe. In 2009, Switzerland hosted the IIHF World Championship for the 10th time. It also became World Vice-Champion in 2013 and 2018. The numerous lakes make Switzerland an attractive place for sailing. The largest, Lake Geneva, is the home of the sailing team Alinghi which was the first European team to win the America's Cup in 2003 and which successfully defended the title in 2007. Tennis has become an increasingly popular sport, and Swiss players such as Martina Hingis, Roger Federer, and Stanislas Wawrinka have won multiple Grand Slams.

Motorsport racecourses and events were banned in Switzerland following the 1955 Le Mans disaster with exception to events such as Hillclimbing. During this period, the country still produced successful racing drivers such as Clay Regazzoni, Sébastien Buemi, Jo Siffert, Dominique Aegerter, successful World Touring Car Championship driver Alain Menu, 2014 24 Hours of Le Mans winner Marcel Fässler and 2015 24 Hours Nürburgring winner Nico Müller. Switzerland also won the A1GP World Cup of Motorsport in 2007–08 with driver Neel Jani. Swiss motorcycle racer Thomas Lüthi won the 2005 MotoGP World Championship in the 125cc category. In June 2007 the Swiss National Council, one house of the Federal Assembly of Switzerland, voted to overturn the ban, however the other house, the Swiss Council of States rejected the change and the ban remains in place.

Traditional sports include Swiss wrestling or "Schwingen". It is an old tradition from the rural central cantons and considered the national sport by some. Hornussen is another indigenous Swiss sport, which is like a cross between baseball and golf. Steinstossen is the Swiss variant of stone put, a competition in throwing a heavy stone. Practised only among the alpine population since prehistoric times, it is recorded to have taken place in Basel in the 13th century. It is also central to the Unspunnenfest, first held in 1805, with its symbol the 83.5 stone named "Unspunnenstein".

The cuisine of Switzerland is multifaceted. While some dishes such as fondue, raclette or rösti are omnipresent through the country, each region developed its own gastronomy according to the differences of climate and languages. Traditional Swiss cuisine uses ingredients similar to those in other European countries, as well as unique dairy products and cheeses such as Gruyère or Emmental, produced in the valleys of Gruyères and Emmental. The number of fine-dining establishments is high, particularly in western Switzerland.

Chocolate has been made in Switzerland since the 18th century but it gained its reputation at the end of the 19th century with the invention of modern techniques such as conching and tempering which enabled its production on a high quality level. Also a breakthrough was the invention of solid milk chocolate in 1875 by Daniel Peter. The Swiss are the world's largest consumers of chocolate.

Due to the popularisation of processed foods at the end of the 19th century, Swiss health food pioneer Maximilian Bircher-Benner created the first nutrition-based therapy in form of the well-known rolled oats cereal dish, called Birchermüesli.

The most popular alcoholic drink in Switzerland is wine. Switzerland is notable for the variety of grapes grown because of the large variations in terroirs, with their specific mixes of soil, air, altitude and light. Swiss wine is produced mainly in Valais, Vaud (Lavaux), Geneva and Ticino, with a small majority of white wines. Vineyards have been cultivated in Switzerland since the Roman era, even though certain traces can be found of a more ancient origin. The most widespread varieties are the Chasselas (called Fendant in Valais) and Pinot noir. The Merlot is the main variety produced in Ticino.




</doc>
<doc id="26750" url="https://en.wikipedia.org/wiki?curid=26750" title="Sri Lanka">
Sri Lanka

Sri Lanka (, ; '; "Ilaṅkai"), officially the Democratic Socialist Republic of Sri Lanka, is an island country in South Asia, located in the Indian Ocean to the southwest of the Bay of Bengal and to the southeast of the Arabian Sea. The island is geographically separated from the Indian subcontinent by the Gulf of Mannar and the Palk Strait. The legislative capital, Sri Jayawardenepura Kotte, is a suburb of the commercial capital and largest city, Colombo.

Sri Lanka's documented history spans 3,000 years, with evidence of pre-historic human settlements dating back to at least 125,000 years. It has a rich cultural heritage, and the first known Buddhist writings of Sri Lanka, the Pāli Canon, date back to the Fourth Buddhist council in 29 BC. Its geographic location and deep harbours made it of great strategic importance from the time of the ancient Silk Road through to the modern Maritime Silk Road.

Sri Lanka was known in the west and from the beginning of British colonial rule as Ceylon (, ). A nationalist political movement arose in the country in the early 20th century to obtain political independence, which was granted in 1948; the country became a republic and adopted its current name in 1972. Sri Lanka's recent history has been marred by a 26-year civil war, which ended decisively when the Sri Lanka Armed Forces defeated the Liberation Tigers of Tamil Eelam (LTTE) in 2009.

The current constitution stipulates the political system as a republic and a unitary state governed by a semi-presidential system. It has had a long history of international engagement, as a founding member of the South Asian Association for Regional Cooperation (SAARC), and a member of the United Nations, the Commonwealth of Nations, the G77, and the Non-Aligned Movement. Along with the Maldives, Sri Lanka is one of only two South Asian countries rated "high" on the Human Development Index (HDI), with its HDI rating and per capita income the highest among South Asian nations. The Sri Lankan constitution accords Buddhism the "foremost place", although it does not identify it as a state religion. Buddhism is given special privileges in the Sri Lankan constitution.

The island is home to many cultures, languages, and ethnicities. The majority of the population are from the Sinhalese ethnicity, while a large minority of Tamils have also played an influential role in the island's history. Moors, Burghers, Malays, Chinese, and the indigenous Vedda are also established groups on the island.

In antiquity, Sri Lanka was known to travellers by a variety of names. According to the "Mahavamsa", the legendary Prince Vijaya named the land Tambapanni ('copper-red hands' or 'copper-red earth'), because his followers' hands were reddened by the red soil of the area. In Hindu mythology, such as the Ramayana, the island was referred to as "Lankā" ('Island'). The Tamil term Eelam (), was used to designate the whole island in Sangam literature. The island was known under Chola rule as "Mummudi Cholamandalam" ('realm of the three crowned Cholas').

Ancient Greek geographers called it "Taprobanā" () or "Taprobanē" () from the word "Tambapanni". The Persians and Arabs referred to it as "Sarandīb" (the origin of the word "serendipity") from Sanskrit "Siṃhaladvīpaḥ". "Ceilão", the name given to Sri Lanka by the Portuguese Empire when it arrived in 1505, was transliterated into English as "Ceylon". As a British crown colony, the island was known as Ceylon; it achieved independence as Ceylon in 1948.

The country is now known in Sinhala as ' () and in Tamil as ' (, ). In 1972, its formal name was changed to "Free, Sovereign and Independent Republic of Sri Lanka". Later, in 1978, it was changed to the "Democratic Socialist Republic of Sri Lanka". As the name Ceylon still appears in the names of a number of organisations, the Sri Lankan government announced in 2011 a plan to rename all those over which it has authority.

The pre-history of Sri Lanka goes back 125,000 years and possibly even as far back as 500,000 years. The era spans the Palaeolithic, Mesolithic, and early Iron Ages. Among the Paleolithic human settlements discovered in Sri Lanka, Pahiyangala (named after the Chinese traveller monk Faxian), which dates back to 37,000 BP, Batadombalena (28,500 BP) and Belilena (12,000 BP) are the most important. In these caves, archaeologists have found the remains of anatomically modern humans which they have named Balangoda Man, and other evidence suggesting that they may have engaged in agriculture and kept domestic dogs for driving game.

One of the first written references to the island is found in the Indian epic Ramayana, which provides details of a kingdom named "Lanka" that was created by the divine sculptor Vishwakarma for Kubera, the Lord of Wealth. It is said that Kubera was overthrown by his demon stepbrother Ravana, the powerful emperor who built a mythical flying machine named Dandu Monara. The modern city of Wariyapola is described as Ravana's airport.

Early inhabitants of Sri Lanka were probably ancestors of the Vedda people, an indigenous people numbering approximately 2,500 living in modern-day Sri Lanka. The 19th-century Irish historian James Emerson Tennent theorized that Galle, a city in southern Sri Lanka, was the ancient seaport of Tarshish from which King Solomon is said to have drawn ivory, peacocks, and other valuables.

According to the "Mahāvamsa", a Sinhalese chronicle written in Pāḷi, the original inhabitants of Sri Lanka are said to be the Yakshas and Nagas. Ancient cemeteries that were used before 600 BC and other signs of advanced civilisation have also been discovered in Sri Lanka. Sinhalese history traditionally starts in 543 BC with the arrival of Prince Vijaya, a semi-legendary prince who sailed with 700 followers to Sri Lanka, after being expelled from Vanga Kingdom (present-day Bengal). He established the Kingdom of Tambapanni, near modern-day Mannar. Vijaya (Singha) is the first of the approximately 189 monarchs of Sri Lanka described in chronicles such as the "Dipavamsa", "Mahāvaṃsa", "Cūḷavaṃsa", and "Rājāvaliya".

The Anuradhapura period (377 BC–1017 AD) began with the establishment of the Anuradhapura Kingdom in 380 BC during the reign of Pandukabhaya. Thereafter, Anuradhapura served as the capital city of the country for nearly 1,400 years. Ancient Sri Lankans excelled at building certain types of structures such as tanks, dagobas and palaces. Society underwent a major transformation during the reign of Devanampiya Tissa, with the arrival of Buddhism from India. In 250 BC, Mahinda, a bhikkhu and the son of the Mauryan Emperor Ashoka arrived in Mihintale carrying the message of Buddhism. His mission won over the monarch, who embraced the faith and propagated it throughout the Sinhalese population.

Succeeding kingdoms of Sri Lanka would maintain many Buddhist schools and monasteries and support the propagation of Buddhism into other countries in Southeast Asia. Sri Lankan Bhikkhus studied in India's famous ancient Buddhist University of Nalanda, which was destroyed by Bakhtiyar Khilji. It is probable that many of the scriptures from Nalanda are preserved in Sri Lanka's many monasteries and that the written form of the Tipitaka, including Sinhalese Buddhist literature, were part of the University of Nalanda. In 245 BC, bhikkhuni Sangamitta arrived with the Jaya Sri Maha Bodhi tree, which is considered to be a sapling from the historical Bodhi tree under which Gautama Buddha became enlightened. It is considered the oldest human-planted tree (with a continuous historical record) in the world. (Bodhivamsa)

Sri Lanka experienced the first of many foreign invasions during the reign of Suratissa, who was defeated by two horse traders named Sena and Guttika from South India. The next invasion came immediately in 205 BC by a Chola named Elara, who overthrew Asela and ruled the country for 44 years. Dutugemunu, the eldest son of the southern regional sub-king, Kavan Tissa, defeated Elara in the Battle of Vijithapura. During its two and a half millennia of existence, the Sinhala Kingdom was invaded at least eight times by neighbouring South Indian dynasties such as the Chola, Pandya, Chera, and Pallava. These invaders were all subsequently driven back. There also were incursions by the kingdoms of Kalinga (modern Odisha) and from the Malay Peninsula as well.

The Fourth Buddhist council of Theravada Buddhism was held at the Anuradhapura Maha Viharaya in Sri Lanka under the patronage of Valagamba of Anuradhapura in 25 BC. The council was held in response to a year in which the harvests in Sri Lanka were particularly poor and many Buddhist monks subsequently died of starvation. Because the Pāli Canon was at that time oral literature maintained in several recensions by "dhammabhāṇaka"s (dharma reciters), the surviving monks recognized the danger of not writing it down so that even if some of the monks whose duty it was to study and remember parts of the Canon for later generations died, the teachings would not be lost. After the Council, palm-leaf manuscripts containing the completed Canon were taken to other countries such as Burma, Thailand, Cambodia and Laos.

Sri Lanka was the first Asian country known to have a female ruler: Anula of Anuradhapura (r. 47–42 BC). Sri Lankan monarchs undertook some remarkable construction projects such as Sigiriya, the so-called "Fortress in the Sky", built during the reign of Kashyapa I of Anuradhapura, who ruled between 477 and 495. The Sigiriya rock fortress is surrounded by an extensive network of ramparts and moats. Inside this protective enclosure were gardens, ponds, pavilions, palaces and other structures.

In AD 993, the invasion of Chola emperor Rajaraja I forced the then Sinhalese ruler Mahinda V to flee to the southern part of Sri Lanka. Taking advantage of this situation, Rajendra I, son of Rajaraja I, launched a large invasion in 1017. Mahinda V was captured and taken to India, and the Cholas sacked the city of Anuradhapura causing the fall of Anuradhapura Kingdom. Subsequently, they moved the capital to Polonnaruwa.

Following a seventeen-year-long campaign, Vijayabahu I successfully drove the Chola out of Sri Lanka in 1070, reuniting the country for the first time in over a century. Upon his request, ordained monks were sent from Burma to Sri Lanka to re-establish Buddhism, which had almost disappeared from the country during the Chola reign. During the medieval period, Sri Lanka was divided into three sub-territories, namely Ruhunu, Pihiti and Maya.
Sri Lanka's irrigation system was extensively expanded during the reign of Parākramabāhu the Great (1153–1186). This period is considered as a time when Sri Lanka was at the height of its power. He built 1470 reservoirs – the highest number by any ruler in Sri Lanka's history – repaired 165 dams, 3910 canals, 163 major reservoirs, and 2376 mini-reservoirs. His most famous construction is the Parakrama Samudra, the largest irrigation project of medieval Sri Lanka. Parākramabāhu's reign is memorable for two major campaigns – in the south of India as part of a Pandyan war of succession, and a punitive strike against the kings of Ramanna (Burma) for various perceived insults to Sri Lanka.

After his demise, Sri Lanka gradually decayed in power. In 1215, Kalinga Magha, a South Indian with uncertain origins, identified as the founder of the Jaffna kingdom, invaded and captured the Kingdom of Polonnaruwa. He sailed from Kalinga 690 nautical miles on 100 large ships with a 24,000 strong army. Unlike previous invaders, he looted, ransacked, and destroyed everything in the ancient Anuradhapura and Polonnaruwa Kingdoms beyond recovery. His priorities in ruling were to extract as much as possible from the land and overturn as many of the traditions of Rajarata as possible. His reign saw the massive migration of native Sinhalese people to the south and west of Sri Lanka, and into the mountainous interior, in a bid to escape his power.

Sri Lanka never really recovered from the impact of Kalinga Magha's invasion. King Vijayabâhu III, who led the resistance, brought the kingdom to Dambadeniya. The north, in the meanwhile, eventually evolved into the Jaffna kingdom. The Jaffna kingdom never came under the rule of any kingdom of the south except on one occasion; in 1450, following the conquest led by king Parâkramabâhu VI's adopted son, Prince Sapumal. He ruled the North from AD 1450 to 1467.

The next three centuries starting from 1215 were marked by kaleidoscopically shifting collections of kingdoms in south and central Sri Lanka, including Dambadeniya, Yapahuwa, Gampola, Raigama, Kotte, Sitawaka, and finally, Kandy. Chinese admiral Zheng He and his naval expeditionary force landed at Galle, Sri Lanka in 1409 and got into battle with the local king Vira Alakesvara of Gampola. Zheng He captured King Vira Alakesvara and later released him. Zheng He erected the Galle Trilingual Inscription, a stone tablet at Galle written in three languages (Chinese, Tamil, and Persian), to commemorate his visit. The stele was discovered by S. H. Thomlin at Galle in 1911 and is now preserved in the Colombo National Museum.

The early modern period of Sri Lanka begins with the arrival of Portuguese soldier and explorer Lourenço de Almeida, the son of Francisco de Almeida, in 1505. In 1517, the Portuguese built a fort at the port city of Colombo and gradually extended their control over the coastal areas. In 1592, after decades of intermittent warfare with the Portuguese, Vimaladharmasuriya I moved his kingdom to the inland city of Kandy, a location he thought more secure from attack. In 1619, succumbing to attacks by the Portuguese, the independent existence of Jaffna kingdom came to an end.

During the reign of the Rajasinghe II, Dutch explorers arrived on the island. In 1638, the king signed a treaty with the Dutch East India Company to get rid of the Portuguese who ruled most of the coastal areas. The following Dutch–Portuguese War resulted in a Dutch victory, with Colombo falling into Dutch hands by 1656. The Dutch remained in the areas they had captured, thereby violating the treaty they had signed in 1638. An ethnic group named Burgher people emerged in Sri Lankan society as a result of Dutch rule.

The Kingdom of Kandy was the last independent monarchy of Sri Lanka. In 1595, Vimaladharmasurya brought the sacred Tooth Relic – the traditional symbol of royal and religious authority amongst the Sinhalese – to Kandy, and built the Temple of the Tooth. In spite of on-going intermittent warfare with Europeans, the kingdom survived. Later, a crisis of succession emerged in Kandy upon king Vira Narendrasinha's death in 1739. He was married to a Telugu-speaking Nayakkar princess from South India (Madurai) and was childless by her.

Eventually, with the support of bhikku Weliwita Sarankara, the crown passed to the brother of one of Narendrasinha's princesses, overlooking the right of ""Unambuwe Bandara"", Narendrasinha's own son by a Sinhalese concubine. The new king was crowned Sri Vijaya Rajasinha later that year. Kings of the Nayakkar dynasty launched several attacks on Dutch controlled areas, which proved to be unsuccessful.
During the Napoleonic Wars, fearing that French control of the Netherlands might deliver Sri Lanka to the French, Great Britain occupied the coastal areas of the island (which they called Ceylon) with little difficulty in 1796. Two years later, in 1798, Sri Rajadhi Rajasinha, third of the four Nayakkar kings of Sri Lanka, died of a fever. Following his death, a nephew of Rajadhi Rajasinha, eighteen-year-old Kannasamy, was crowned. The young king, now named Sri Vikrama Rajasinha, faced a British invasion in 1803 but successfully retaliated. The First Kandyan War ended in a stalemate.

By then the entire coastal area was under the British East India Company as a result of the Treaty of Amiens. On 14 February 1815, Kandy was occupied by the British in the second Kandyan War, ending Sri Lanka's independence. Sri Vikrama Rajasinha, the last native monarch of Sri Lanka, was exiled to India. The Kandyan Convention formally ceded the entire country to the British Empire. Attempts by Sri Lankan noblemen to undermine British power in 1818 during the Uva Rebellion were thwarted by Governor Robert Brownrigg.

The beginning of the modern period of Sri Lanka is marked by the Colebrooke-Cameron reforms of 1833. They introduced a utilitarian and liberal political culture to the country based on the rule of law and amalgamated the Kandyan and maritime provinces as a single unit of government. An executive council and a legislative council were established, later becoming the foundation of a representative legislature. By this time, experiments with coffee plantations were largely successful.

Soon coffee became the primary commodity export of Sri Lanka. Falling coffee prices as a result of the depression of 1847 stalled economic development and prompted the governor to introduce a series of taxes on firearms, dogs, shops, boats, etc., and to reintroduce a form of "rajakariya", requiring six days free labour on roads or payment of a cash equivalent. These harsh measures antagonised the locals, and another rebellion broke out in 1848. A devastating leaf disease, "Hemileia vastatrix", struck the coffee plantations in 1869, destroying the entire industry within fifteen years. The British quickly found a replacement: abandoning coffee, they began cultivating tea instead. Tea production in Sri Lanka thrived in the following decades. Large-scale rubber plantations began in the early 20th century.
By the end of the 19th century, a new educated social class transcending race and caste arose through British attempts to staff the Ceylon Civil Service and the legal, educational, and medical professions. New leaders represented the various ethnic groups of the population in the Ceylon Legislative Council on a communal basis. Buddhist and Hindu revivalism reacted against Christian missionary activities. The first two decades in the 20th century are noted by the unique harmony among Sinhalese and Tamil political leadership, which has since been lost.

In 1919, major Sinhalese and Tamil political organisations united to form the Ceylon National Congress, under the leadership of Ponnambalam Arunachalam, pressing colonial masters for more constitutional reforms. But without massive popular support, and with the governor's encouragement for "communal representation" by creating a "Colombo seat" that dangled between Sinhalese and Tamils, the Congress lost momentum towards the mid-1920s.

The Donoughmore reforms of 1931 repudiated the communal representation and introduced universal adult franchise (the franchise stood at 4% before the reforms). This step was strongly criticised by the Tamil political leadership, who realised that they would be reduced to a minority in the newly created State Council of Ceylon, which succeeded the legislative council. In 1937, Tamil leader G. G. Ponnambalam demanded a 50–50 representation (50% for the Sinhalese and 50% for other ethnic groups) in the State Council. However, this demand was not met by the Soulbury reforms of 1944–45.

The Soulbury constitution ushered in Dominion status, with independence proclaimed on 4 February 1948. D. S. Senanayake became the first Prime Minister of Ceylon. Prominent Tamil leaders including Ponnambalam and Arunachalam Mahadeva joined his cabinet. The British Royal Navy remained stationed at Trincomalee until 1956. A countrywide popular demonstration against withdrawal of the rice ration, known as Hartal 1953, resulted in the resignation of prime minister Dudley Senanayake.

S. W. R. D. Bandaranaike was elected prime minister in 1956. His three-year rule had a profound impact through his self-proclaimed role of "defender of the besieged Sinhalese culture". He introduced the controversial Sinhala Only Act, recognising Sinhala as the only official language of the government. Although partially reversed in 1958, the bill posed a grave concern for the Tamil community, which perceived in it a threat to their language and culture.

The Federal Party (FP) launched a movement of non-violent resistance (satyagraha) against the bill, which prompted Bandaranaike to reach an agreement (Bandaranaike–Chelvanayakam Pact) with S. J. V. Chelvanayakam, leader of the FP, to resolve the looming ethnic conflict. The pact proved ineffective in the face of ongoing protests by opposition and the Buddhist clergy. The bill, together with various government colonisation schemes, contributed much towards the political rancour between Sinhalese and Tamil political leaders. Bandaranaike was assassinated by an extremist Buddhist monk in 1959.

Sirimavo Bandaranaike, the widow of Bandaranaike, took office as prime minister in 1960, and withstood an attempted coup d'état in 1962. During her second term as prime minister, the government instituted socialist economic policies, strengthening ties with the Soviet Union and China, while promoting a policy of non-alignment. In 1971, Ceylon experienced a Marxist insurrection, which was quickly suppressed. In 1972, the country became a republic named Sri Lanka, repudiating its dominion status. Prolonged minority grievances and the use of communal emotionalism as an election campaign weapon by both Sinhalese and Tamil leaders abetted a fledgling Tamil militancy in the north during the 1970s. The policy of standardisation by the Sirimavo government to rectify disparities created in university enrolment, which was in essence an affirmative action to assist geographically disadvantaged students to obtain tertiary education, resulted in reducing the proportion of Tamil students at university level and acted as the immediate catalyst for the rise of militancy. The assassination of Jaffna Mayor Alfred Duraiyappah in 1975 by the Liberation Tigers of Tamil Eelam (LTTE) marked a crisis point.

The government of J. R. Jayawardene swept to power in 1977, defeating the largely unpopular United Front government. Jayawardene introduced a new constitution, together with a free-market economy and a powerful executive presidency modelled after that of France. It made Sri Lanka the first South Asian country to liberalise its economy. Beginning in 1983, ethnic tensions were manifested in an on-and-off insurgency against the government by the LTTE. An LTTE attack on 13 soldiers resulted in the anti-Tamil race riots in July 1983, allegedly backed by Sinhalese hard-line ministers, which resulted in more than 150,000 Tamil civilians fleeing the island, seeking asylum in other countries.
Lapses in foreign policy resulted in India strengthening the Tigers by providing arms and training. In 1987, the Indo-Sri Lanka Accord was signed and the Indian Peace Keeping Force (IPKF) was deployed in northern Sri Lanka to stabilise the region by neutralising the LTTE. The same year, the JVP launched its second insurrection in Southern Sri Lanka, necessitating redeployment of the IPKF in 1990. In October 1990, the LTTE expelled Sri Lankan Moors (Muslims by religion) from northern Sri Lanka. In 2002, the Sri Lankan government and LTTE signed a Norwegian-mediated ceasefire agreement.

The 2004 Asian tsunami killed over 35,000 in Sri Lanka. From 1985 to 2006, the Sri Lankan government and Tamil insurgents held four rounds of peace talks without success. Both LTTE and the government resumed fighting in 2006, and the government officially backed out of the ceasefire in 2008. In 2009, under the Presidency of Mahinda Rajapaksa, the Sri Lanka Armed Forces defeated the LTTE and re-established control of the entire country by the Sri Lankan Government. Overall, between 60,000 and 100,000 people were killed during the 26 years of conflict.

Forty thousand Tamil civilians may have been killed in the final phases of the Sri Lankan civil war, according to an Expert Panel convened by UN Secretary General Ban Ki-moon. The exact number of Tamils killed is still a speculation that needs further study. Following the LTTE's defeat, the Tamil National Alliance, the largest Tamil political party in Sri Lanka, dropped its demand for a separate state in favour of a federal solution. The final stages of the war left some 294,000 people displaced. The UN Human Rights Council has documented over 12,000 named individuals who have undergone disappearance after detention by security forces in Sri Lanka, the second highest figure in the world since the Working Group came into being in 1980. In March 2009, 378 people had been killed in one day and at least another 1,212 injured. The report was based only on those casualties brought to the hospital. The UN described the situation as a "bloodbath", and one that its Colombo office had been warning against for some time. Their spokesperson Gordon Weiss said that over 100 children had been killed over the weekend in the "large-scale killing of civilians".

According to the Ministry of Resettlement, most of the displaced persons had been released or returned to their places of origin, leaving only 6,651 in the camps as of December 2011. In May 2010, President Rajapaksa appointed the Lessons Learnt and Reconciliation Commission (LLRC) to assess the conflict between the time of the ceasefire agreement in 2002 and the defeat of the LTTE in 2009. Sri Lanka has emerged from its 26-year war to become one of the fastest growing economies of the world.

Sri Lanka lies on the Indian Plate, a major tectonic plate that was formerly part of the Indo-Australian Plate. It is in the Indian Ocean southwest of the Bay of Bengal, between latitudes 5° and 10°N, and longitudes 79° and 82°E. Sri Lanka is separated from the mainland portion of the Indian subcontinent by the Gulf of Mannar and Palk Strait. According to Hindu mythology, a land bridge existed between the Indian mainland and Sri Lanka. It now amounts to only a chain of limestone shoals remaining above sea level. Legends claim that it was passable on foot up to 1480 AD, until cyclones deepened the channel. Portions are still as shallow as , hindering navigation. The island consists mostly of flat to rolling coastal plains, with mountains rising only in the south-central part. The highest point is Pidurutalagala, reaching above sea level. 

Sri Lanka has 103 rivers. The longest of these is the Mahaweli River, extending . These waterways give rise to 51 natural waterfalls of 10 meters or more. The highest is Bambarakanda Falls, with a height of . Sri Lanka's coastline is 1,585 km long. Sri Lanka claims an Exclusive Economic Zone (EEZ) extending 200 nautical miles, which is approximately 6.7 times Sri Lanka's land area. The coastline and adjacent waters support highly productive marine ecosystems such as fringing coral reefs and shallow beds of coastal and estuarine seagrasses.

Sri Lanka has 45 estuaries and 40 lagoons. Sri Lanka's mangrove ecosystem spans over 7,000 hectares and played a vital role in buffering the force of the waves in the 2004 Indian Ocean tsunami. The island is rich in minerals such as ilmenite, feldspar, graphite, silica, kaolin, mica and thorium. Existence of petroleum and gas in the Gulf of Mannar has also been confirmed and the extraction of recoverable quantities is underway.

The climate is tropical and warm, due to the moderating effects of ocean winds. Mean temperatures range from in the central highlands, where frost may occur for several days in the winter, to a maximum of in other low-altitude areas. Average yearly temperatures range from to nearly . Day and night temperatures may vary by to .

Rainfall pattern is influenced by monsoon winds from the Indian Ocean and Bay of Bengal. The "wet zone" and some of the windward slopes of the central highlands receive up to of rain each year, but the leeward slopes in the east and northeast receive little rain. Most of the east, southeast, and northern parts of Sri Lanka comprise the "dry zone", which receives between of rain annually.

The arid northwest and southeast coasts receive the least amount of rain at per year. Periodic squalls occur and sometimes tropical cyclones bring overcast skies and rains to the southwest, northeast, and eastern parts of the island. Humidity is typically higher in the southwest and mountainous areas and depends on the seasonal patterns of rainfall.

An increase in average rainfall coupled with heavier rainfall events has resulted in recurrent flooding and related damages to infrastructure, utility supply and the urban economy.

Lying within the Indomalaya ecozone, Sri Lanka is one of 25 biodiversity hotspots in the world. Although the country is relatively small in size, it has the highest biodiversity density in Asia. A remarkably high proportion of the species among its flora and fauna, 27% of the 3,210 flowering plants and 22% of the mammals ("see List"), are endemic. Sri Lanka has declared 24 wildlife reserves, which are home to a wide range of native species such as Asian elephants, leopards, sloth bears, the unique small loris, a variety of deer, the purple-faced langur, the endangered wild boar, porcupines and Indian pangolins.

Flowering acacias flourish on the arid Jaffna Peninsula. Among the trees of the dry-land forests are valuable species such as satinwood, ebony, ironwood, mahogany and teak. The wet zone is a tropical evergreen forest with tall trees, broad foliage, and a dense undergrowth of vines and creepers. Subtropical evergreen forests resembling those of temperate climates flourish in the higher altitudes.
Yala National Park in the southeast protects herds of elephant, deer, and peacocks. The Wilpattu National Park in the northwest, the largest national park, preserves the habitats of many water birds such as storks, pelicans, ibis, and spoonbills. The island has four biosphere reserves: Bundala, Hurulu Forest Reserve, the Kanneliya-Dediyagala-Nakiyadeniya, and Sinharaja. Of these, Sinharaja forest reserve is home to 26 endemic birds and 20 rainforest species, including the elusive red-faced malkoha, the green-billed coucal and the Sri Lanka blue magpie.
The untapped genetic potential of Sinharaja flora is enormous. Of the 211 woody trees and lianas within the reserve, 139 (66%) are endemic. The total vegetation density, including trees, shrubs, herbs and seedlings, has been estimated at 240,000 individuals per hectare. The Minneriya National Park borders the Minneriya tank, which is an important source of water for numerous elephants (Elephus maximus) inhabiting the surrounding forests. Dubbed "The Gathering", the congregation of elephants can be seen on the tank-bed in the late dry season (August to October) as the surrounding water sources steadily disappear. The park also encompasses a range of micro-habitats which include classic dry zone tropical monsoonal evergreen forest, thick stands of giant bamboo, hilly pastures (patanas). and grasslands (talawas).

Sri Lanka is home to over 250 types of resident birds ("see List"). It has declared several bird sanctuaries including Kumana. During the Mahaweli Program of the 1970s and 1980s in northern Sri Lanka, the government set aside four areas of land totalling as national parks. Sri Lanka's forest cover, which was around 49% in 1920, had fallen to approximately 24% by 2009.

Sri Lanka is the oldest democracy in Asia. The Donoughmore Constitution, drafted by the Donoughmore Commission in 1931, enabled general elections with adult universal suffrage (universal adult voting) in the country. The first election under the universal adult franchise, held in June 1931, was for the Ceylon State Council. Sir Don Baron Jayatilaka was elected as Leader of the House.

In 1944, the Soulbury Commission was appointed to draft a new constitution. During this time, struggle for independence was fought on "constitutionalist" lines under the leadership of D. S. Senanayake. The draft constitution was enacted in the same year, and Senanayake was appointed Prime Minister following the parliamentary election in 1947. The Soulbury constitution ushered in Dominion status and granted independence to Sri Lanka in 1948.

The current political culture in Sri Lanka is a contest between two rival coalitions led by the centre-leftist and progressivist United People's Freedom Alliance (UPFA), an offspring of Sri Lanka Freedom Party (SLFP), and the comparatively right-wing and pro-capitalist United National Party (UNP). Sri Lanka is essentially a multi-party democracy with many smaller Buddhist, socialist and Tamil nationalist political parties. As of July 2011, the number of registered political parties in the country is 67. Of these, the Lanka Sama Samaja Party (LSSP), established in 1935, is the oldest.

The UNP, established by D. S. Senanayake in 1946, was until recently the largest single political party. It is the only political group which had representation in all parliaments since independence. SLFP was founded by S. W. R. D. Bandaranaike, who was the Cabinet minister of Local Administration before he left the UNP in July 1951. SLFP registered its first victory in 1956, defeating the ruling UNP in 1956 Parliamentary election. Following the parliamentary election in July 1960, Sirimavo Bandaranaike became the prime minister and the world's first elected female head of government.

G. G. Ponnambalam, the Tamil nationalist counterpart of S. W. R. D. Bandaranaike, founded the All Ceylon Tamil Congress (ACTC) in 1944. Objecting to Ponnambalam's cooperation with D. S. Senanayake, a dissident group led by S.J.V. Chelvanayakam broke away in 1949 and formed the Illankai Tamil Arasu Kachchi (ITAK), also known as the Federal Party, becoming the main Tamil political party in Sri Lanka for next two decades. The Federal Party advocated a more aggressive stance toward the Sinhalese.

With the constitutional reforms of 1972, the All Ceylon Tamil Congress (ACTC) and Illankai Tamil Arasu Kachchi (ITAK) created a common front called the Tamil United Front (later Tamil United Liberation Front). Following a period of turbulence as Tamil militants rose to power in the late 1970s, these Tamil political parties were succeeded in October 2001 by the Tamil National Alliance. Janatha Vimukthi Peramuna, a Marxist–Leninist political party founded by Rohana Wijeweera in 1965, serves as a third force in the current political context. It endorses leftist policies which are more radical than the traditionalist leftist politics of the LSSP and the Communist Party. Founded in 1981, the Sri Lanka Muslim Congress is the largest Muslim political party in Sri Lanka.

Sri Lanka is a democratic republic and a unitary state which is governed by a semi-presidential system, with a mixture of a presidential system and a parliamentary system. Most provisions of the constitution can be amended by a two-thirds majority in parliament. The amendment of certain basic features such as the clauses on language, religion, and reference to Sri Lanka as a unitary state require both a two-thirds majority and approval in a nationwide referendum.

In common with many democracies, the Sri Lankan government has three branches:

For administrative purposes, Sri Lanka is divided into nine provinces and twenty-five districts.

Provinces
There have been provinces in Sri Lanka since the 19th century, but they had no legal status until 1987 when the 13th Amendment to the 1978 constitution established provincial councils after several decades of increasing demand for a decentralisation of the Government of Sri Lanka. Each provincial council is an autonomous body not under the authority of any Ministry. Some of its functions had been undertaken by central government ministries, departments, corporations, and statutory authorities, but authority over land and police is not as a rule given to provincial councils. Between 1989 and 2006, the Northern and Eastern provinces were temporarily merged to form the North-East Province. Prior to 1987, all administrative tasks for the provinces were handled by a district-based civil service which had been in place since colonial times. Now each province is administered by a directly elected provincial council:

Districts and local authorities
Sri Lanka is also divided into 25 districts. Each district is administered under a District Secretariat. The districts are further subdivided into 256 divisional secretariats, and these, in turn, to approximately 14,008 Grama Niladhari divisions. The Districts are known in Sinhala as "Disa" and in Tamil as "Māwaddam". Originally, a Disa (usually rendered into English as Dissavony) was a duchy, notably Matale and Uva. A government agent, who is known as "District Secretary", administers a district.

There are three other types of local authorities: Municipal Councils (18), Urban councils (13) and Pradeshiya Sabha, also called Pradesha Sabhai (256). Local authorities were originally based on feudal counties named "korale" and "rata", and were formerly known as 'D.R.O. divisions' after the 'Divisional Revenue Officer'. Later the D.R.O.s became 'Assistant Government Agents' and the divisions were known as 'A.G.A. divisions'. These Divisional Secretariats are currently administered by a 'Divisional Secretary'.

Sri Lanka is a founding member of the Non-Aligned Movement (NAM). While ensuring that it maintains its independence, Sri Lanka has cultivated relations with India. Sri Lanka became a member of the United Nations in 1955. Today, it is also a member of the Commonwealth, the SAARC, the World Bank, the International Monetary Fund, the Asian Development Bank, and the Colombo Plan.

One of the two parties that have governed Sri Lanka since its independence, the United National Party, has traditionally favoured links with the West, while its left-leaning counterpart, the Sri Lanka Freedom Party, has favoured links with the East. Sri Lankan Finance Minister J. R. Jayewardene, together with then Australian Foreign Minister Sir Percy Spencer, proposed the Colombo Plan at the Commonwealth Foreign Minister's Conference held in Colombo in 1950. At the San Francisco Peace Conference in 1951, while many countries were reluctant, Sri Lanka argued for a free Japan and refused to accept payment of reparations for World War II damage because it believed it would harm Japan's economy. Sri Lanka-China relations started as soon as the PRC was formed in 1949. The two countries signed an important Rice-Rubber Pact in 1952. Sri Lanka played a vital role at the Asian–African Conference in 1955, which was an important step in the crystallisation of the NAM.

The Bandaranaike government of 1956 significantly changed the pro-western policies set by the previous UNP government. It recognised Cuba under Fidel Castro in 1959. Shortly afterward, Cuba's revolutionary Ernesto Che Guevara paid a visit to Sri Lanka. The "Sirima-Shastri Pact" of 1964 and "Sirima-Gandhi Pact" of 1974 were signed between Sri Lankan and Indian leaders in an attempt to solve the long-standing dispute over the status of plantation workers of Indian origin. In 1974, Kachchatheevu, a small island in Palk Strait, was formally ceded to Sri Lanka. By this time, Sri Lanka was strongly involved in the NAM and Colombo held the fifth NAM summit in 1976. The relationship between Sri Lanka and India became tense under the government of J. R. Jayawardene. As a result, India intervened in the Sri Lankan Civil War and subsequently deployed an Indian Peace Keeping Force in 1987. In the present, Sri Lanka enjoys extensive relations with China, Russia, and Pakistan.

The Sri Lanka Armed Forces, comprising the Sri Lanka Army, the Sri Lanka Navy, and the Sri Lanka Air Force, come under the purview of the Ministry of Defence (MoD). The total strength of the three services is around 346,000 personnel, with nearly 36,000 reserves. Sri Lanka has not enforced military conscription. Paramilitary units include the Special Task Force, the Civil Security Force, and the Sri Lanka Coast Guard.

Since independence in 1948, the primary focus of the armed forces has been internal security, crushing three major insurgencies, two by Marxist militants of the JVP and a 26-year-long conflict with the LTTE which has been proscribed as a terrorist organisation by 32 countries. The armed forces have been in a continuous mobilised state for the last 30 years. Marking a rare occurrence in modern military history, the Sri Lankan military was able to bring a decisive end to the Sri Lankan Civil War in May 2009. Sri Lanka has claimed to be the first country in the modern world to eradicate terrorism on its own soil. The Sri Lankan Armed Forces have engaged in United Nations peacekeeping operations since the early 1960s, contributing forces to permanent contingents deployed in several UN peacekeeping missions in Chad, Lebanon, and Haiti.

]

According to the International Monetary Fund, Sri Lanka's GDP in terms of purchasing power parity is second only to the Maldives in the South Asian region in terms of per capita income.

In the 19th and 20th centuries, Sri Lanka became a plantation economy famous for its production and export of cinnamon, rubber, and Ceylon tea, which remains a trademark national export. The development of modern ports under British rule raised the strategic importance of the island as a centre of trade. From 1948 to 1977, socialism strongly influenced the government's economic policies. Colonial plantations were dismantled, industries were nationalised, and a welfare state established. In 1977, the free market economy was introduced to the country incorporating privatisation, deregulation, and the promotion of private enterprise.

While the production and export of tea, rubber, coffee, sugar, and other commodities remain important, industrialisation has increased the importance of food processing, textiles, telecommunications, and finance. The country's main economic sectors are tourism, tea export, clothing, rice production, and other agricultural products. In addition to these economic sectors, overseas employment, especially in the Middle East, contributes substantially in foreign exchange.

, the service sector makes up 60% of GDP, the industrial sector 28%, and the agriculture sector 12%. The private sector accounts for 85% of the economy. India is Sri Lanka's largest trading partner. Economic disparities exist between the provinces with the Western Province contributing 45.1% of the GDP and the Southern Province and the Central Province contributing 10.7% and 10%, respectively. With the end of the war, the Northern Province reported a record 22.9% GDP growth in 2010.
The per capita income of Sri Lanka has doubled since 2005. During the same period, poverty has dropped from 15.2% to 7.6%, unemployment rate has dropped from 7.2% to 4.9%, market capitalisation of the Colombo Stock Exchange has quadrupled and the budget deficit has doubled. Over 90% of the households in Sri Lanka are electrified. 87.3% of the population have access to safe drinking water and 39% have access to pipe-borne water. Income inequality has also dropped in recent years, indicated by a Gini coefficient of 0.36 in 2010. Sri Lanka's cellular subscriber base has shown a staggering 550% growth from 2005 to 2010. Sri Lanka was the first country in the South Asian region to introduce 3G, 3.5G (HSDPA), 3.75G (HSUPA) and 4G (LTE) mobile telecommunication technologies.

The Global Competitiveness Report, published by the World Economic Forum, has described Sri Lanka's economy as transitioning from the factor-driven stage to the efficiency-driven stage and that it ranks 52nd in global competitiveness. Also, out of the 142 countries surveyed, Sri Lanka ranked 45th in health and primary education, 32nd in business sophistication, 42nd in innovation, and 41st in goods market efficiency. Sri Lanka ranks 5th in the World Giving Index, registering high levels of contentment and charitable behaviour in its society. In 2010, "The New York Times" placed Sri Lanka at the top of its list of 31 places to visit. S&P Dow Jones Indices classifies Sri Lanka as a frontier market as of 2018, and Citigroup classified it as a 3G country in February 2011. Sri Lanka ranks well above other South Asian countries in the Human Development Index (HDI) with an index of 0.750.

Sri Lanka's road network consists of 35 A-Grade highways and two controlled-access highways (E01 and E03). The railway network, operated by the state-run national railway operator Sri Lanka Railways, spans . Sri Lanka also has three deep-water ports at Colombo, Galle, and Trincomalee, in addition to the newest port being built at Hambantota. The port at Trincomalee is the fifth largest natural harbour in the world; during World War II, the British stated that they could place their entire navy in the harbour with room to spare. Sri Lanka's flag carrier airline is SriLankan Airlines. Fitch Ratings has affirmed Sri Lanka's Foreign- and Local-Currency Issuer Default Ratings (IDRs) at 'BB-' with a "stable" outlook. With a grant of 20 million dollars from the US and help from China, a space academy has been set up for the purpose of developing an indigenous space sector to launch satellites of other nations as well as of Sri Lanka. This dual use of launching technology will also serve to develop missile technology. On 26 September 2012 China launched Sri Lanka's first satellite, with plans for more launches in the coming years.

During the past few years, the country's debt has soared as it was developing its infrastructure to the point of near bankruptcy which required a bailout from the International Monetary Fund (IMF) The IMF had agreed to provide a US$1.5 billion bailout loan in April 2016 after Sri Lanka provided a set of criteria intended to improve its economy. By the fourth quarter of 2016, the debt was estimated to be $64.9 billion. Additional debt had been incurred in the past by state-owned organizations and this was said to be at least $9.5 billion. Since early 2015, domestic debt increased by 12 percent and external debt by 25 percent.

In November 2016, the International Monetary Fund reported that the initial disbursement was larger than US$150 million originally planned, a full US$162.6 million (SDR 119.894 million), to Sri Lanka. The agency's evaluation for the first tranche was cautiously optimistic about the future. Under the program Sri Lankan government implemented a new Inland Revenue Act and an automatic fuel pricing formula which were noted by the IMF in its fourth review. In 2018 China agreed to bail out Sri Lanka with a loan of $1.25 billion to deal with foreign debt repayment spikes in 2019 to 2021.

Sri Lanka is the 57th most populated nation in the world, with roughly 21,670,000 people, and an annual population growth rate of 1.14%. Sri Lanka has a birth rate of 17.6 births per 1,000 people and a death rate of 6.2 deaths per 1,000 people. Population density is highest in western Sri Lanka, especially in and around the capital. Sinhalese constitute the largest ethnic group in the country, with 74.8% of the total population.

Sri Lankan Tamils are the second major ethnic group in the island, with a percentage of 11.2%. Sri Lankan Moors comprise 9.2%. Tamils of Indian origin were brought into the country as indentured labourers by British colonists to work on estate plantations. Nearly 50% of them were repatriated following independence in 1948. They are distinguished from the native Tamil population that has resided in Sri Lanka since ancient times. There are also small ethnic groups such as the Burghers (of mixed European descent) and Malays from Southeast Asia. Moreover, there is a small population of Vedda people who are believed to be the original indigenous group to inhabit the island.

Sinhala and Tamil are the two official languages of Sri Lanka. The Constitution defines English as the link language. English is widely used for education, scientific and commercial purposes. Members of the Burgher community speak variant forms of Portuguese Creole and Dutch with varying proficiency, while members of the Malay community speak a form of Creole Malay that is unique to the island.

Sri Lanka is a multi-religious country. Buddhists comprise 70% of the population, with the Theravada school being predominant. Most Buddhists are of the Sinhalese ethnic group. Buddhism was introduced to Sri Lanka in the 2nd century BCE by Venerable Mahinda. A sapling of the Bodhi Tree under which the Buddha attained enlightenment was brought to Sri Lanka during the same time. The Pāli Canon ("Thripitakaya"), having previously been preserved as an oral tradition, was first committed to writing in Sri Lanka around 30 BCE.

Sri Lanka has the longest continuous history of Buddhism of any predominantly Buddhist nation, with the Sangha having existed in a largely unbroken lineage since its introduction in the 2nd century BCE. During periods of decline, the Sri Lankan monastic lineage was revived through contact with Thailand and Burma. Buddhism is given special recognition in the Constitution which requires Sri Lankans to "protect and foster the Buddha Sasana".

Hinduism is the second most prevalent religion in Sri Lanka and predates Buddhism. Today, Hinduism is dominant in Northern, Eastern and Central Sri Lanka.

Islam is the third most prevalent religion in the country, having first been brought to the island by Arab traders over the course of many centuries, starting around the 7th century CE. Most Muslims are Sunni who follow the Shafi'i school. Most followers of Islam in Sri Lanka today are believed to be descendants of those Arab traders and the local women they married.

Christianity reached the country through Western colonists in the early 16th century. Around 7.4% of the Sri Lankan population are Christians, of whom 82% are Roman Catholics who trace their religious heritage directly to the Portuguese. Sri Lankan Tamil Catholics attribute their religious heritage to St.Francis Xavier as well as Portuguese missionaries. The remaining Christians are evenly split between the Anglican Church of Ceylon and other Protestant denominations.

There is also a small population of Zoroastrian immigrants from India (Parsis) who settled in Ceylon during the period of British rule, but this community has steadily dwindled in recent years. Religion plays a prominent role in the life and culture of Sri Lankans. The Buddhist majority observe Poya Days each month according to the Lunar calendar, and Hindus and Muslims also observe their own holidays. In a 2008 Gallup poll, Sri Lanka was ranked the third most religious country in the world, with 99% of Sri Lankans saying religion was an important part of their daily life.

Sri Lankans have a life expectancy of 77.9 years at birth, which is 10% higher than the world average. The infant mortality rate stands at 8.5 per 1,000 births and the maternal mortality rate at 0.39 per 1,000 births, which is on par with figures from the developed countries. The universal "pro-poor" health care system adopted by the country has contributed much towards these figures.

Sri Lanka ranks first among southeast Asian countries with respect to commitment of suicide, with 33 deaths per 100,000 persons. According to Department of Census and Statistics, poverty, destructive pastimes and inability to cope with stressful situations, are the main causes behind the high suicide rates.

With a literacy rate of 92.5%, Sri Lanka has one of the most literate populations amongst developing nations. Its youth literacy rate stands at 98%, computer literacy rate at 35%, and primary school enrollment rate at over 99%. An education system which dictates 9 years of compulsory schooling for every child is in place. The free education system established in 1945, is a result of the initiative of C. W. W. Kannangara and A. Ratnayake. It is one of the few countries in the world that provide universal free education from primary to tertiary stage.

Kannangara led the establishment of the Madhya Maha Vidyalayas (Central Schools) in different parts of the country in order to provide education to Sri Lanka's rural children. In 1942 a special education committee proposed extensive reforms to establish an efficient and quality education system for the people. However, in the 1980s changes to this system saw the separation of the administration of schools between the central government and the provincial government. Thus the elite National Schools are controlled directly by the Ministry of Education and the provincial schools by the provincial government. Sri Lanka has approximately 9675 government schools, 817 private schools and Pirivenas.

Sri Lanka has 15 public universities. A lack of responsiveness of the education system to labour market requirements, disparities in access to quality education, lack of an effective linkage between secondary and tertiary education remain major challenges for the education sector. A number of private, degree awarding institutions have emerged in recent times to fill in these gaps, yet the participation at tertiary level education remains at 5.1%. The proposed private university bill has been withdrawn by the Higher Education Ministry after university students' heavy demonstrations and resistance.

The late British science fiction author Arthur C. Clarke served as Chancellor of Moratuwa University in Sri Lanka from 1979 to 2002.

Sri Lanka has an extensive road network for inland transportation. With more than 100,000 km of paved roads, it has one of the highest road densities in the world (1.5 km of paved roads per every 1sq.km. of land). E-grade highways are the latest addition to Sri Lanka's road network. These are access-controlled, high-mobility roads with permitted speeds up to 100 km/h. These highways connect local communities together, by-passing busy and congested town centers.

A and B grade roads are national (arterial) highways administered by Road Development Authority. C and D grade roads are provincial roads coming under the purview of the Provincial Road Development Authority of the respective province. The other roads are local roads falling under local government authorities.

The rail network of Sri Lanka consists of main lines, coastal lines, and up-country lines. In addition, air- and water-based transportation modalities augment the inland transport of the country.

The Sri Lanka Broadcasting Corporation (formerly Radio Ceylon) is the oldest-running radio station in Asia, established in 1923 by Edward Harper just three years after broadcasting began in Europe. The station broadcasts services in Sinhala, Tamil, English and Hindi. Since the 1980s, many private radio stations have also been introduced. Broadcast television was introduced to the country in 1979 when the Independent Television Network was launched. Initially, all Television stations were state-controlled, but private television networks began broadcasts in 1992.

, 51 newspapers (30 Sinhala, 10 Tamil, 11 English) are published and 34 TV stations and 52 radio stations are in operation. In recent years, freedom of the press in Sri Lanka has been alleged by media freedom groups to be amongst the poorest in democratic countries. Alleged abuse of a newspaper editor by a senior government minister achieved international notoriety because of the unsolved murder of the editor's predecessor, Lasantha Wickrematunge, who had also been a critic of the government and had presaged his own death in a posthumously published article.

Officially, the constitution of Sri Lanka guarantees human rights as ratified by the United Nations. However, human rights in Sri Lanka have come under criticism by Amnesty International, Freedom from Torture, Human Rights Watch, and the United States Department of State. British colonial rulers, the separatist Liberation Tigers of Tamil Eelam (LTTE), and the government of Sri Lanka are accused of violating human rights. A report by an advisory panel to the UN secretary-general has accused both the LTTE and the Sri Lankan government of alleged war crimes during final stages of the civil war. Corruption remains a problem in Sri Lanka, and there is currently very little protection for those who stand up against corruption. The 135-year-old Article 365 of the Sri Lankan Penal Code criminalizes gay sex and provides for a penalty of up to ten years in prison.

The UN Human Rights Council has documented over 12,000 named individuals who have undergone disappearance after detention by security forces in Sri Lanka, the second highest figure in the world since the Working Group came into being in 1980. The Sri Lankan government has confirmed that 6,445 of these are dead. Allegations of human rights abuses have not ended with the close of the ethnic conflict.

UN Human Rights Commissioner Navanethem Pillay visited Sri Lanka in May 2013. After her visit, she said: "The war may have ended [in Sri Lanka], but in the meantime democracy has been undermined and the rule of law eroded." Pillay spoke about the military's increasing involvement in civilian life and reports of military land grabbing. She also said that, while in Sri Lanka, she had been allowed to go wherever she wanted, but that Sri Lankans who came to meet her were harassed and intimidated by security forces.

In 2012, the UK charity Freedom from Torture reported that it had received 233 referrals of torture survivors from Sri Lanka for clinical treatment or other services provided by the charity. In the same year, Freedom from Torture published "Out of the Silence", which documents evidence of torture in Sri Lanka and demonstrates that the practice has continued long after the end of the civil war in May 2009.

The culture of Sri Lanka dates back over 2500 years. It is influenced primarily by Buddhism and Hinduism. Sri Lanka is the home to two main traditional cultures: the Sinhalese (centred in the ancient cities of Kandy and Anuradhapura) and the Tamil (centred in the city of Jaffna). In more recent times, the British colonial culture has also influenced the locals. Sri Lanka claims a democratic tradition matched by few other developing countries.

The first Tamil immigration was probably around the 3rd century BC. Tamils co-existed with the Sinhalese people since then, and the early mixing rendered the two ethnic groups almost physically indistinct. Ancient Sri Lanka is marked for its genius in hydraulic engineering and architecture. The rich cultural traditions shared by all Sri Lankan cultures is the basis of the country's long life expectancy, advanced health standards and high literacy rate.

Dishes include rice and curry, pittu, kiribath, wholemeal roti, string hoppers, wattalapam (a rich pudding of Malay origin made of coconut milk, jaggery, cashew nuts, eggs, and spices including cinnamon and nutmeg), kottu, and hoppers. Jackfruit may sometimes replace rice. Traditionally food is served on a plantain leaf or lotus leaf.

Middle Eastern influences and practices are found in traditional Moor dishes, while Dutch and Portuguese influences are found with the island's Burgher community preserving their culture through traditional dishes such as Lamprais (rice cooked in stock and baked in a banana leaf), Breudher (Dutch Holiday Biscuit), and Bolo Fiado (Portuguese-style layer cake).

In April, Sri Lankans celebrate the Buddhist and Hindu new year festival. Esala Perahera is a symbolic Buddhist festival consisting of dances and decorated elephants held in Kandy in July and August. Fire-dances, whip-dances, Kandian dances and various other cultural dances are integral parts of the festival. Christians celebrate Christmas on 25 December to celebrate the birth of Jesus Christ and Easter to celebrate the resurrection of Jesus. Tamils celebrate Thai Pongal and Maha Shivaratri, and Muslims celebrate Hajj and Ramadan.

The movie "Kadawunu Poronduwa" (The broken promise), produced by S. M. Nayagam of Chitra Kala Movietone, heralded the coming of Sri Lankan cinema in 1947. "Ranmuthu Duwa" (Island of treasures, 1962) marked the transition cinema from black-and-white to colour. It in the recent years has featured subjects such as family melodrama, social transformation and the years of conflict between the military and the LTTE. The Sri Lankan cinematic style is similar to Bollywood movies. In 1979, movie attendance rose to an all-time high, but has been in steady decline since then.

An influential filmmaker is Lester James Peiris, who has directed a number of movies which led to global acclaim, including "Rekava" (Line of destiny, 1956), "Gamperaliya" (The changing village, 1964), "Nidhanaya" (The treasure, 1970) and "Golu Hadawatha" (Cold heart, 1968). Sri Lankan-Canadian poet Rienzi Crusz, is the subject of a documentary on his life in Sri Lanka. His work is published in Sinhala and English. Similarly, naturalized-Canadian Michael Ondaatje, is well known for his English-language novels and three films.

The earliest music in Sri Lanka came from theatrical performances such as "Kolam", "Sokari" and "Nadagam". Traditional music instruments such as "Béra", "Thammátama", "Daŭla" and "Răbān" were performed at these dramas. The first music album, "Nurthi", recorded in 1903, was released through Radio Ceylon (founded in 1925). Songwriters like Mahagama Sekara and Ananda Samarakoon and musicians such as W. D. Amaradeva, Victor Ratnayake, Nanda Malini and Clarence Wijewardene have contributed much towards the upliftment of Sri Lankan music. Baila is another popular music genre in the country, originated among Kaffirs or the Afro-Sinhalese community.
There are three main styles of Sri Lankan classical dance. They are, the Kandyan dances, low country dances and Sabaragamuwa dances. Of these, the Kandyan style, which flourished under kings of the Kingdom of Kandy, is more prominent. It is a sophisticated form of dance, that consists of five sub-categories: "Ves dance", "Naiyandi dance", "Udekki dance", "Pantheru dance" and "18 Vannam". An elaborate headdress is worn by the male dancers and a drum called "Geta Béraya" is used to assist the dancer to keep on rhythm. In addition, four folk drama variants named "Sokri", Kolam "Nadagam", "Pasu", and several devil dance variants such as Sanni Yakuma and "Kohomba Kankariya" can be also observed.

The history of Sri Lankan painting and sculpture can be traced as far back as to the 2nd or 3rd century BC. The earliest mention about the art of painting on Mahavamsa, is to the drawing of a palace on cloth using cinnabar in the 2nd century BC. The chronicles have description of various paintings in relic-chambers of Buddhist stupas, and in monastic residence.

Theatre moved into the country when a Parsi theatre company from Mumbai introduced "Nurti", a blend of European and Indian theatrical conventions to the Colombo audience in the 19th century. The golden age of Sri Lankan drama and theatre began with the staging of "Maname", a play written by Ediriweera Sarachchandra in 1956. It was followed by a series of popular dramas like "Sinhabāhu", "Pabāvatī", "Mahāsāra", "Muudu Puththu" and "Subha saha Yasa".

Sri Lankan literature spans at least two millennia, and is heir to the Aryan literary tradition as embodied in the hymns of the Rigveda. The Pāli Canon, the standard collection of scriptures in the Theravada Buddhist tradition, was written down in Sri Lanka during the Fourth Buddhist council, at the Alulena cave temple, Kegalle, as early as 29 BC. Ancient chronicles such as the Mahāvamsa, written in the 6th century, provide vivid descriptions of Sri Lankan dynasties. According to the German philosopher Wilhelm Geiger, the chronicles are based on Sinhala Atthakatha (commentary), that dates few more centuries back. The oldest surviving prose work is the "Dhampiya-Atuva-Getapadaya", compiled in the 9th century.

The greatest literary feats of medieval Sri Lanka include "Sandesha Kāvya" (poetic messages) such as "Girā Sandeshaya" (Parrot message), "Hansa Sandeshaya" (Swan message) and "Salalihini Sandeshaya" (Myna message). Poetry including "Kavsilumina", "Kavya-Sekharaya" (diadem of poetry) and proses such as "Saddharma-Ratnāvaliya", "Amāvatura" (Flood of nectar) and "Pujāvaliya" are also notable works of this period, which is considered to be the golden age of Sri Lankan literature. The first modern-day novel, "Meena", a work of Simon de Silva appeared in 1905, and was followed by a number of revolutionary literary works. Martin Wickramasinghe, the author of "Madol Doova" is considered the iconic figure of Sri Lankan literature.

While the national sport in Sri Lanka is volleyball, by far the most popular sport in the country is cricket. Rugby union also enjoys extensive popularity, as do athletics, football (soccer), netball and tennis. Sri Lanka's schools and colleges regularly organise sports and athletics teams, competing on provincial and national levels.

The Sri Lanka national cricket team achieved considerable success beginning in the 1990s, rising from underdog status to winning the 1996 Cricket World Cup. They also won the 2014 ICC World Twenty20 played in Bangladesh, beating India in the final. In addition, Sri Lanka became the runners-up of the Cricket World Cup in 2007 and 2011, and of the ICC World Twenty20 in 2009 and 2012.

Former Sri Lankan off-spinner Muttiah Muralitharan has been rated as the greatest Test match bowler ever by "Wisden Cricketers' Almanack", and four Sri Lankan cricketers ranked 2nd (Sangakkara), 4th (Jayasuriya), 5th (Jayawardene) and 11th (Dilshan) highest ODI run scorers of all time, which is the second best by a team. Sri Lanka has won the Asia Cup in 1986, 1997, 2004, 2008 and 2014. Sri Lanka once held highest team score in all three formats of cricket, where currently holds Test team total. The country co-hosted the Cricket World Cup in 1996 and 2011, and hosted the 2012 ICC World Twenty20.

Sri Lankans have won two medals at Olympic Games, one silver, by Duncan White at 1948 London Olympics for men's 400 metres hurdles and one silver by Susanthika Jayasinghe at 2000 Sydney Olympics for women's 200 metres. In 1973, Muhammad Lafir won the World Billiards Championship, the highest feat by a Sri Lankan in a Cue sport. Sri Lanka has also won the Carrom World Championship titles twice in 2012, 2016 and 2018, men's team becoming champions and women's team won second place. Aquatic sports such as boating, surfing, swimming, kitesurfing and scuba diving on the coast, the beaches and backwaters attract many Sri Lankans and foreign tourists. There are two styles of martial arts native to Sri Lanka, Cheena di and Angampora.

The Sri Lanka national netball team has won the Asian Netball Championship five times and are the current Asian Champions as well.


Government

Overviews and data

History

Maps

Trade


</doc>
<doc id="26751" url="https://en.wikipedia.org/wiki?curid=26751" title="Sun">
Sun

The Sun is the star at the center of the Solar System. It is a nearly perfect sphere of hot plasma, with internal convective motion that generates a magnetic field via a dynamo process.<ref name="doi10.1146/annurev-astro-081913-040012"></ref> It is by far the most important source of energy for life on Earth. Its diameter is about 1.39 million kilometers (864,000 miles), or 109 times that of Earth, and its mass is about 330,000 times that of Earth. It accounts for about 99.86% of the total mass of the Solar System.
Roughly three quarters of the Sun's mass consists of hydrogen (~73%); the rest is mostly helium (~25%), with much smaller quantities of heavier elements, including oxygen, carbon, neon, and iron.

The Sun is a G-type main-sequence star (G2V) based on its spectral class. As such, it is informally and not completely accurately referred to as a yellow dwarf (its light is closer to white than yellow). It formed approximately 4.6 billion years ago from the gravitational collapse of matter within a region of a large molecular cloud. Most of this matter gathered in the center, whereas the rest flattened into an orbiting disk that became the Solar System. The central mass became so hot and dense that it eventually initiated nuclear fusion in its core. It is thought that almost all stars form by this process.

The Sun currently fuses about 600 million tons of hydrogen into helium every second, converting 4 million tons of matter into energy every second as a result. This energy, which can take between 10,000 and 170,000 years to escape from its core, is the source of the Sun's light and heat. When hydrogen fusion in its core has diminished to the point at which the Sun is no longer in hydrostatic equilibrium, its core will undergo a marked increase in density and temperature while its outer layers expand, eventually transforming the Sun into a red giant. It is calculated that the Sun will become sufficiently large to engulf the current orbits of Mercury and Venus, and render Earth uninhabitable – but not for about five billion years. After this, it will shed its outer layers and become a dense type of cooling star known as a white dwarf, and no longer produce energy by fusion, but still glow and give off heat from its previous fusion.

The enormous effect of the Sun on Earth has been recognized since prehistoric times, and the Sun has been regarded by some cultures as a deity. The synodic rotation of Earth and its orbit around the Sun are the basis of solar calendars, one of which is the predominant calendar in use today.

The English proper name "Sun" developed from Old English "sunne" and may be related to "south". Cognates to English "sun" appear in other Germanic languages, including Old Frisian "sunne", "sonne", Old Saxon "sunna", Middle Dutch "sonne", modern Dutch "zon", Old High German "sunna", modern German "Sonne", Old Norse "sunna", and Gothic "sunnō". All Germanic terms for the Sun stem from Proto-Germanic *"sunnōn".

The Latin name for the Sun, "Sol", is not commonly used in everyday English. "Sol" is also used by planetary astronomers to refer to the duration of a solar day on another planet, such as Mars. The related word "solar" is the usual adjectival term used, in terms such as solar day, solar eclipse, and Solar System.

The English weekday name "Sunday" stems from Old English ("Sunnandæg"; "Sun's day", from before 700) and is ultimately a result of a Germanic interpretation of Latin "dies solis", itself a translation of the Greek ἡμέρα ἡλίου ("hēméra hēlíou").

The Sun is a G-type main-sequence star that comprises about 99.86% of the mass of the Solar System. The Sun has an absolute magnitude of +4.83, estimated to be brighter than about 85% of the stars in the Milky Way, most of which are red dwarfs.
The Sun is a Population I, or heavy-element-rich, star. The formation of the Sun may have been triggered by shockwaves from one or more nearby supernovae. This is suggested by a high abundance of heavy elements in the Solar System, such as gold and uranium, relative to the abundances of these elements in so-called Population II, heavy-element-poor, stars. The heavy elements could most plausibly have been produced by endothermic nuclear reactions during a supernova, or by transmutation through neutron absorption within a massive second-generation star.

The Sun is by far the brightest object in the Earth's sky, with an apparent magnitude of −26.74. This is about 13 billion times brighter than the next brightest star, Sirius, which has an apparent magnitude of −1.46. is defined as the mean distance of the Sun's center to Earth's center, though the distance varies as Earth moves from perihelion in January to aphelion in July. At this average distance, light travels from the Sun's horizon to Earth's horizon in about 8 minutes and 19 seconds, while light from the closest points of the Sun and Earth takes about two seconds less. The energy of this sunlight supports almost all life on Earth by photosynthesis, and drives Earth's climate and weather.

The Sun does not have a definite boundary, but its density decreases exponentially with increasing height above the photosphere. For the purpose of measurement, the Sun's radius is considered to be the distance from its center to the edge of the photosphere, the apparent visible surface of the Sun. By this measure, the Sun is a near-perfect sphere with an oblateness estimated at about 9 millionths, which means that its polar diameter differs from its equatorial diameter by only .
The tidal effect of the planets is weak and does not significantly affect the shape of the Sun. The Sun rotates faster at its equator than at its poles. This differential rotation is caused by convective motion due to heat transport and the Coriolis force due to the Sun's rotation. In a frame of reference defined by the stars, the rotational period is approximately 25.6 days at the equator and 33.5 days at the poles. Viewed from Earth as it orbits the Sun, the "apparent rotational period" of the Sun at its equator is about 28 days.

The solar constant is the amount of power that the Sun deposits per unit area that is directly exposed to sunlight. The solar constant is equal to approximately (watts per square meter) at a distance of one astronomical unit (AU) from the Sun (that is, on or near Earth). Sunlight on the surface of Earth is attenuated by Earth's atmosphere, so that less power arrives at the surface (closer to ) in clear conditions when the Sun is near the zenith. Sunlight at the top of Earth's atmosphere is composed (by total energy) of about 50% infrared light, 40% visible light, and 10% ultraviolet light. The atmosphere in particular filters out over 70% of solar ultraviolet, especially at the shorter wavelengths. Solar ultraviolet radiation ionizes Earth's dayside upper atmosphere, creating the electrically conducting ionosphere.

The Sun's color is white, with a CIE color-space index near (0.3, 0.3), when viewed from space or when the Sun is high in the sky. When measuring all the photons emitted, the Sun is emitting more photons in the green portion of the spectrum than any other. When the Sun is low in the sky, atmospheric scattering renders the Sun yellow, red, orange, or magenta. Despite its typical whiteness, most people mentally picture the Sun as yellow; the reasons for this are the subject of debate.
The Sun is a G2V star, with "G2" indicating its surface temperature of approximately 5,778 K (5,505 °C, 9,941 °F), and "V" that it, like most stars, is a main-sequence star. The average luminance of the Sun is about 1.88 giga candela per square metre, but as viewed through Earth's atmosphere, this is lowered to about 1.44 Gcd/m. However, the luminance is not constant across the disk of the Sun (limb darkening).

The Sun is composed primarily of the chemical elements hydrogen and helium. At this time in the Sun's life, they account for 74.9% and 23.8% of the mass of the Sun in the photosphere, respectively. All heavier elements, called "metals" in astronomy, account for less than 2% of the mass, with oxygen (roughly 1% of the Sun's mass), carbon (0.3%), neon (0.2%), and iron (0.2%) being the most abundant.

The Sun's original chemical composition was inherited from the interstellar medium out of which it formed. Originally it would have contained about 71.1% hydrogen, 27.4% helium, and 1.5% heavier elements. The hydrogen and most of the helium in the Sun would have been produced by Big Bang nucleosynthesis in the first 20 minutes of the universe, and the heavier elements were produced by previous generations of stars before the Sun was formed, and spread into the interstellar medium during the final stages of stellar life and by events such as supernovae.

Since the Sun formed, the main fusion process has involved fusing hydrogen into helium. Over the past 4.6 billion years, the amount of helium and its location within the Sun has gradually changed. Within the core, the proportion of helium has increased from about 24% to about 60% due to fusion, and some of the helium and heavy elements have settled from the photosphere towards the center of the Sun because of gravity. The proportions of metals (heavier elements) is unchanged. Heat is transferred outward from the Sun's core by radiation rather than by convection (see Radiative zone below), so the fusion products are not lifted outward by heat; they remain in the core and gradually an inner core of helium has begun to form that cannot be fused because presently the Sun's core is not hot or dense enough to fuse helium. In the current photosphere the helium fraction is reduced, and the metallicity is only 84% of what it was in the protostellar phase (before nuclear fusion in the core started). In the future, helium will continue to accumulate in the core, and in about 5 billion years this gradual build-up will eventually cause the Sun to exit the main sequence and become a red giant.

The chemical composition of the photosphere is normally considered representative of the composition of the primordial Solar System. The solar heavy-element abundances described above are typically measured both using spectroscopy of the Sun's photosphere and by measuring abundances in meteorites that have never been heated to melting temperatures. These meteorites are thought to retain the composition of the protostellar Sun and are thus not affected by settling of heavy elements. The two methods generally agree well.

In the 1970s, much research focused on the abundances of iron-group elements in the Sun. Although significant research was done, until 1978 it was difficult to determine the abundances of some iron-group elements (e.g. cobalt and manganese) via spectrography because of their hyperfine structures.

The first largely complete set of oscillator strengths of singly ionized iron-group elements were made available in the 1960s, and these were subsequently improved. In 1978, the abundances of singly ionized elements of the iron group were derived.

Various authors have considered the existence of a gradient in the isotopic compositions of solar and planetary noble gases, e.g. correlations between isotopic compositions of neon and xenon in the Sun and on the planets.

Prior to 1983, it was thought that the whole Sun has the same composition as the solar atmosphere. In 1983, it was claimed that it was fractionation in the Sun itself that caused the isotopic-composition relationship between the planetary and solar-wind-implanted noble gases.

The structure of the Sun contains the following layers:

The core of the Sun extends from the center to about 20–25% of the solar radius. It has a density of up to (about 150 times the density of water) and a temperature of close to 15.7 million kelvins (K). By contrast, the Sun's surface temperature is approximately 5,800 K. Recent analysis of SOHO mission data favors a faster rotation rate in the core than in the radiative zone above. Through most of the Sun's life, energy has been produced by nuclear fusion in the core region through a series of nuclear reactions called the p–p (proton–proton) chain; this process converts hydrogen into helium. Only 0.8% of the energy generated in the Sun comes from another sequence of fusion reactions called the CNO cycle, though this proportion is expected to increase as the Sun becomes older.

The core is the only region in the Sun that produces an appreciable amount of thermal energy through fusion; 99% of the power is generated within 24% of the Sun's radius, and by 30% of the radius, fusion has stopped nearly entirely. The remainder of the Sun is heated by this energy as it is transferred outwards through many successive layers, finally to the solar photosphere where it escapes into space through radiation (photons) or advection (massive particles).

The proton–proton chain occurs around times each second in the core, converting about 3.7 protons into alpha particles (helium nuclei) every second (out of a total of ~8.9 free protons in the Sun), or about 6.2 kg/s. Fusing four free protons (hydrogen nuclei) into a single alpha particle (helium nucleus) releases around 0.7% of the fused mass as energy, so the Sun releases energy at the mass–energy conversion rate of 4.26 million metric tons per second (which requires 600 metric megatons of hydrogen ), for 384.6 yottawatts (), or 9.192 megatons of TNT per second. The large power output of the Sun is mainly due to the huge size and density of its core (compared to Earth and objects on Earth), with only a fairly small amount of power being generated per cubic metre. Theoretical models of the Sun's interior indicate a maximum power density, or energy production, of approximately 276.5 watts per cubic metre at the center of the core, which is about the same rate of power production as takes place in reptile metabolism or a compost pile.

The fusion rate in the core is in a self-correcting equilibrium: a slightly higher rate of fusion would cause the core to heat up more and expand slightly against the weight of the outer layers, reducing the density and hence the fusion rate and correcting the perturbation; and a slightly lower rate would cause the core to cool and shrink slightly, increasing the density and increasing the fusion rate and again reverting it to its present rate.

From the core out to about 0.7 solar radii, thermal radiation is the primary means of energy transfer. The temperature drops from approximately 7 million to 2 million kelvins with increasing distance from the core. This temperature gradient is less than the value of the adiabatic lapse rate and hence cannot drive convection, which explains why the transfer of energy through this zone is by radiation instead of thermal convection. Ions of hydrogen and helium emit photons, which travel only a brief distance before being reabsorbed by other ions. The density drops a hundredfold (from 20 g/cm to 0.2 g/cm) from 0.25 solar radii to the 0.7 radii, the top of the radiative zone.

The radiative zone and the convective zone are separated by a transition layer, the tachocline. This is a region where the sharp regime change between the uniform rotation of the radiative zone and the differential rotation of the convection zone results in a large shear between the two—a condition where successive horizontal layers slide past one another. Presently, it is hypothesized (see Solar dynamo) that a magnetic dynamo within this layer generates the Sun's magnetic field.

The Sun's convection zone extends from 0.7 solar radii (500,000 km) to near the surface. In this layer, the solar plasma is not dense enough or hot enough to transfer the heat energy of the interior outward via radiation. Instead, the density of the plasma is low enough to allow convective currents to develop and move the Sun's energy outward towards its surface. Material heated at the tachocline picks up heat and expands, thereby reducing its density and allowing it to rise. As a result, an orderly motion of the mass develops into thermal cells that carry the majority of the heat outward to the Sun's photosphere above. Once the material diffusively and radiatively cools just beneath the photospheric surface, its density increases, and it sinks to the base of the convection zone, where it again picks up heat from the top of the radiative zone and the convective cycle continues. At the photosphere, the temperature has dropped to 5,700 K and the density to only 0.2 g/m (about 1/6,000 the density of air at sea level).

The thermal columns of the convection zone form an imprint on the surface of the Sun giving it a granular appearance called the solar granulation at the smallest scale and supergranulation at larger scales. Turbulent convection in this outer part of the solar interior sustains "small-scale" dynamo action over the near-surface volume of the Sun. The Sun's thermal columns are Bénard cells and take the shape of hexagonal prisms.

The visible surface of the Sun, the photosphere, is the layer below which the Sun becomes opaque to visible light. Photons produced in this layer escape the Sun through the transparent solar atmosphere above it and become solar radiation, sunlight. The change in opacity is due to the decreasing amount of H ions, which absorb visible light easily. Conversely, the visible light we see is produced as electrons react with hydrogen atoms to produce H ions.
The photosphere is tens to hundreds of kilometers thick, and is slightly less opaque than air on Earth. Because the upper part of the photosphere is cooler than the lower part, an image of the Sun appears brighter in the center than on the edge or "limb" of the solar disk, in a phenomenon known as limb darkening. The spectrum of sunlight has approximately the spectrum of a black-body radiating at 5,777 K, interspersed with atomic absorption lines from the tenuous layers above the photosphere. The photosphere has a particle density of ~10 m (about 0.37% of the particle number per volume of Earth's atmosphere at sea level). The photosphere is not fully ionized—the extent of ionization is about 3%, leaving almost all of the hydrogen in atomic form.

During early studies of the optical spectrum of the photosphere, some absorption lines were found that did not correspond to any chemical elements then known on Earth. In 1868, Norman Lockyer hypothesized that these absorption lines were caused by a new element that he dubbed "helium", after the Greek Sun god Helios. Twenty-five years later, helium was isolated on Earth.

During a total solar eclipse, when the disk of the Sun is covered by that of the Moon, parts of the Sun's surrounding atmosphere can be seen. It is composed of four distinct parts: the chromosphere, the transition region, the corona and the heliosphere.

The coolest layer of the Sun is a temperature minimum region extending to about above the photosphere, and has a temperature of about . This part of the Sun is cool enough to allow the existence of simple molecules such as carbon monoxide and water, which can be detected via their absorption spectra.

The chromosphere, transition region, and corona are much hotter than the surface of the Sun. The reason is not well understood, but evidence suggests that Alfvén waves may have enough energy to heat the corona.

Above the temperature minimum layer is a layer about thick, dominated by a spectrum of emission and absorption lines. It is called the "chromosphere" from the Greek root "chroma", meaning color, because the chromosphere is visible as a colored flash at the beginning and end of total solar eclipses. The temperature of the chromosphere increases gradually with altitude, ranging up to around near the top. In the upper part of the chromosphere helium becomes partially ionized.

Above the chromosphere, in a thin (about 200 km) transition region, the temperature rises rapidly from around 20,000 K in the upper chromosphere to coronal temperatures closer to 1,000,000 K. The temperature increase is facilitated by the full ionization of helium in the transition region, which significantly reduces radiative cooling of the plasma. The transition region does not occur at a well-defined altitude. Rather, it forms a kind of nimbus around chromospheric features such as spicules and filaments, and is in constant, chaotic motion. The transition region is not easily visible from Earth's surface, but is readily observable from space by instruments sensitive to the extreme ultraviolet portion of the spectrum.

The corona is the next layer of the Sun. The low corona, near the surface of the Sun, has a particle density around 10 m to 10 m. The average temperature of the corona and solar wind is about 1,000,000–2,000,000 K; however, in the hottest regions it is 8,000,000–20,000,000 K. Although no complete theory yet exists to account for the temperature of the corona, at least some of its heat is known to be from magnetic reconnection.
The corona is the extended atmosphere of the Sun, which has a volume much larger than the volume enclosed by the Sun's photosphere. A flow of plasma outward from the Sun into interplanetary space is the solar wind.

The heliosphere, the tenuous outermost atmosphere of the Sun, is filled with the solar wind plasma. This outermost layer of the Sun is defined to begin at the distance where the flow of the solar wind becomes "superalfvénic"—that is, where the flow becomes faster than the speed of Alfvén waves, at approximately 20 solar radii (0.1 AU).
Turbulence and dynamic forces in the heliosphere cannot affect the shape of the solar corona within, because the information can only travel at the speed of Alfvén waves. The solar wind travels outward continuously through the heliosphere, forming the solar magnetic field into a spiral shape, until it impacts the heliopause more than 50 AU from the Sun. In December 2004, the Voyager 1 probe passed through a shock front that is thought to be part of the heliopause. In late 2012 Voyager 1 recorded a marked increase in cosmic ray collisions and a sharp drop in lower energy particles from the solar wind, which suggested that the probe had passed through the heliopause and entered the interstellar medium.

High-energy gamma-ray photons initially released with fusion reactions in the core are almost immediately absorbed by the solar plasma of the radiative zone, usually after traveling only a few millimeters. Re-emission happens in a random direction and usually at a slightly lower energy. With this sequence of emissions and absorptions, it takes a long time for radiation to reach the Sun's surface. Estimates of the photon travel time range between 10,000 and 170,000 years. In contrast, it takes only 2.3 seconds for the neutrinos, which account for about 2% of the total energy production of the Sun, to reach the surface. Because energy transport in the Sun is a process that involves photons in thermodynamic equilibrium with matter, the time scale of energy transport in the Sun is longer, on the order of 30,000,000 years. This is the time it would take the Sun to return to a stable state, if the rate of energy generation in its core were suddenly changed.

Neutrinos are also released by the fusion reactions in the core, but, unlike photons, they rarely interact with matter, so almost all are able to escape the Sun immediately. For many years measurements of the number of neutrinos produced in the Sun were lower than theories predicted by a factor of 3. This discrepancy was resolved in 2001 through the discovery of the effects of neutrino oscillation: the Sun emits the number of neutrinos predicted by the theory, but neutrino detectors were missing of them because the neutrinos had changed flavor by the time they were detected.

The Sun has a magnetic field that varies across the surface of the Sun. Its polar field is , whereas the field is typically in features on the Sun called sunspots and in solar prominences.

The magnetic field also varies in time and location. The quasi-periodic 11-year solar cycle is the most prominent variation in which the number and size of sunspots waxes and wanes.

Sunspots are visible as dark patches on the Sun's photosphere, and correspond to concentrations of magnetic field where the convective transport of heat is inhibited from the solar interior to the surface. As a result, sunspots are slightly cooler than the surrounding photosphere, so they appear dark. At a typical solar minimum, few sunspots are visible, and occasionally none can be seen at all. Those that do appear are at high solar latitudes. As the solar cycle progresses towards its maximum, sunspots tend form closer to the solar equator, a phenomenon known as Spörer's law. The largest sunspots can be tens of thousands of kilometers across.

An 11-year sunspot cycle is half of a 22-year Babcock–Leighton dynamo cycle, which corresponds to an oscillatory exchange of energy between toroidal and poloidal solar magnetic fields. At solar-cycle maximum, the external poloidal dipolar magnetic field is near its dynamo-cycle minimum strength, but an internal toroidal quadrupolar field, generated through differential rotation within the tachocline, is near its maximum strength. At this point in the dynamo cycle, buoyant upwelling within the convective zone forces emergence of toroidal magnetic field through the photosphere, giving rise to pairs of sunspots, roughly aligned east–west and having footprints with opposite magnetic polarities. The magnetic polarity of sunspot pairs alternates every solar cycle, a phenomenon known as the Hale cycle.

During the solar cycle's declining phase, energy shifts from the internal toroidal magnetic field to the external poloidal field, and sunspots diminish in number and size. At solar-cycle minimum, the toroidal field is, correspondingly, at minimum strength, sunspots are relatively rare, and the poloidal field is at its maximum strength. With the rise of the next 11-year sunspot cycle, differential rotation shifts magnetic energy back from the poloidal to the toroidal field, but with a polarity that is opposite to the previous cycle. The process carries on continuously, and in an idealized, simplified scenario, each 11-year sunspot cycle corresponds to a change, then, in the overall polarity of the Sun's large-scale magnetic field.

The solar magnetic field extends well beyond the Sun itself. The electrically conducting solar wind plasma carries the Sun's magnetic field into space, forming what is called the interplanetary magnetic field. In an approximation known as ideal magnetohydrodynamics, plasma particles only move along the magnetic field lines. As a result, the outward-flowing solar wind stretches the interplanetary magnetic field outward, forcing it into a roughly radial structure. For a simple dipolar solar magnetic field, with opposite hemispherical polarities on either side of the solar magnetic equator, a thin current sheet is formed in the solar wind. At great distances, the rotation of the Sun twists the dipolar magnetic field and corresponding current sheet into an Archimedean spiral structure called the Parker spiral. The interplanetary magnetic field is much stronger than the dipole component of the solar magnetic field. The Sun's dipole magnetic field of 50–400 μT (at the photosphere) reduces with the inverse-cube of the distance to about 0.1 nT at the distance of Earth. However, according to spacecraft observations the interplanetary field at Earth's location is around 5 nT, about a hundred times greater. The difference is due to magnetic fields generated by electrical currents in the plasma surrounding the Sun.

The Sun's magnetic field leads to many effects that are collectively called solar activity. Solar flares and coronal-mass ejections tend to occur at sunspot groups. Slowly changing high-speed streams of solar wind are emitted from coronal holes at the photospheric surface. Both coronal-mass ejections and high-speed streams of solar wind carry plasma and interplanetary magnetic field outward into the Solar System. The effects of solar activity on Earth include auroras at moderate to high latitudes and the disruption of radio communications and electric power. Solar activity is thought to have played a large role in the formation and evolution of the Solar System.

With solar-cycle modulation of sunspot number comes a corresponding modulation of space weather conditions, including those surrounding Earth where technological systems can be affected.

In December 2019, a new type of solar magnetic explosion was observed, known as forced magnetic reconnection. Previously, in a process called spontaneous magnetic reconnection, it was observed that the solar magnetic field lines diverge explosively and then converge again instantaneously. Forced Magnetic Reconnection was similar, but it was triggered by an explosion in the corona.

Long-term secular change in sunspot number is thought, by some scientists, to be correlated with long-term change in solar irradiance, which, in turn, might influence Earth's long-term climate.
For example, in the 17th century, the solar cycle appeared to have stopped entirely for several decades; few sunspots were observed during a period known as the Maunder minimum. This coincided in time with the era of the Little Ice Age, when Europe experienced unusually cold temperatures. Earlier extended minima have been discovered through analysis of tree rings and appear to have coincided with lower-than-average global temperatures.

A recent theory claims that there are magnetic instabilities in the core of the Sun that cause fluctuations with periods of either 41,000 or 100,000 years. These could provide a better explanation of the ice ages than the Milankovitch cycles.

The Sun today is roughly halfway through the most stable part of its life. It has not changed dramatically for over four billion years, and will remain fairly stable for more than five billion more. However, after hydrogen fusion in its core has stopped, the Sun will undergo dramatic changes, both internally and externally.

The Sun formed about 4.6 billion years ago from the collapse of part of a giant molecular cloud that consisted mostly of hydrogen and helium and that probably gave birth to many other stars. This age is estimated using computer models of stellar evolution and through nucleocosmochronology. The result is consistent with the radiometric date of the oldest Solar System material, at 4.567 billion years ago. Studies of ancient meteorites reveal traces of stable daughter nuclei of short-lived isotopes, such as iron-60, that form only in exploding, short-lived stars. This indicates that one or more supernovae must have occurred near the location where the Sun formed. A shock wave from a nearby supernova would have triggered the formation of the Sun by compressing the matter within the molecular cloud and causing certain regions to collapse under their own gravity. As one fragment of the cloud collapsed it also began to rotate because of conservation of angular momentum and heat up with the increasing pressure. Much of the mass became concentrated in the center, whereas the rest flattened out into a disk that would become the planets and other Solar System bodies. Gravity and pressure within the core of the cloud generated a lot of heat as it accreted more matter from the surrounding disk, eventually triggering nuclear fusion.

HD 162826 and HD 186302 are hypothesized stellar siblings of the Sun, having formed in the same molecular cloud.

The Sun is about halfway through its main-sequence stage, during which nuclear fusion reactions in its core fuse hydrogen into helium. Each second, more than four million tonnes of matter are converted into energy within the Sun's core, producing neutrinos and solar radiation. At this rate, the Sun has so far converted around 100 times the mass of Earth into energy, about 0.03% of the total mass of the Sun. The Sun will spend a total of approximately 10 billion years as a main-sequence star. The Sun is gradually becoming hotter during its time on the main sequence, because the helium atoms in the core occupy less volume than the hydrogen atoms that were fused. The core is therefore shrinking, allowing the outer layers of the Sun to move closer to the center and experience a stronger gravitational force, according to the inverse-square law. This stronger force increases the pressure on the core, which is resisted by a gradual increase in the rate at which fusion occurs. This process speeds up as the core gradually becomes denser. It is estimated that the Sun has become 30% brighter in the last 4.5 billion years. At present, it is increasing in brightness by about 1% every 100 million years.

The Sun does not have enough mass to explode as a supernova. Instead it will exit the main sequence in approximately 5 billion years and start to turn into a red giant. As a red giant, the Sun will grow so large that it will engulf Mercury, Venus, and probably Earth.

Even before it becomes a red giant, the luminosity of the Sun will have nearly doubled, and Earth will receive as much sunlight as Venus receives today. Once the core hydrogen is exhausted in 5.4 billion years, the Sun will expand into a subgiant phase and slowly double in size over about half a billion years. It will then expand more rapidly over about half a billion years until it is over two hundred times larger than today and a couple of thousand times more luminous. This then starts the red-giant-branch phase where the Sun will spend around a billion years and lose around a third of its mass.
After the red-giant branch the Sun has approximately 120 million years of active life left, but much happens. First, the core, full of degenerate helium ignites violently in the helium flash, where it is estimated that 6% of the core, itself 40% of the Sun's mass, will be converted into carbon within a matter of minutes through the triple-alpha process. The Sun then shrinks to around 10 times its current size and 50 times the luminosity, with a temperature a little lower than today. It will then have reached the red clump or horizontal branch, but a star of the Sun's mass does not evolve blueward along the horizontal branch. Instead, it just becomes moderately larger and more luminous over about 100 million years as it continues to react helium in the core.

When the helium is exhausted, the Sun will repeat the expansion it followed when the hydrogen in the core was exhausted, except that this time it all happens faster, and the Sun becomes larger and more luminous. This is the asymptotic-giant-branch phase, and the Sun is alternately reacting hydrogen in a shell or helium in a deeper shell. After about 20 million years on the early asymptotic giant branch, the Sun becomes increasingly unstable, with rapid mass loss and thermal pulses that increase the size and luminosity for a few hundred years every 100,000 years or so. The thermal pulses become larger each time, with the later pulses pushing the luminosity to as much as 5,000 times the current level and the radius to over 1 AU. According to a 2008 model, Earth's orbit is shrinking due to tidal forces (and, eventually, drag from the lower chromosphere), so that it will be engulfed by the Sun near the tip of the red giant branch phase, 3.8 and 1 million years after Mercury and Venus have respectively had the same fate. Models vary depending on the rate and timing of mass loss. Models that have higher mass loss on the red-giant branch produce smaller, less luminous stars at the tip of the asymptotic giant branch, perhaps only 2,000 times the luminosity and less than 200 times the radius. For the Sun, four thermal pulses are predicted before it completely loses its outer envelope and starts to make a planetary nebula. By the end of that phase—lasting approximately 500,000 years—the Sun will only have about half of its current mass.

The post-asymptotic-giant-branch evolution is even faster. The luminosity stays approximately constant as the temperature increases, with the ejected half of the Sun's mass becoming ionized into a planetary nebula as the exposed core reaches 30,000 K. The final naked core, a white dwarf, will have a temperature of over 100,000 K, and contain an estimated 54.05% of the Sun's present day mass. The planetary nebula will disperse in about 10,000 years, but the white dwarf will survive for trillions of years before fading to a hypothetical black dwarf.

The Sun lies close to the inner rim of the Milky Way's Orion Arm, in the Local Interstellar Cloud or the Gould Belt, at a distance of 7.5–8.5 kpc (25,000–28,000 light-years) from the Galactic Center.

The Sun is contained within the Local Bubble, a space of rarefied hot gas, possibly produced by the supernova remnant Geminga, or multiple supernovae in subgroup B1 of the Pleiades moving group. The distance between the local arm and the next arm out, the Perseus Arm, is about 6,500 light-years. The Sun, and thus the Solar System, is found in what scientists call the galactic habitable zone.
The "Apex of the Sun's Way", or the solar apex, is the direction that the Sun travels relative to other nearby stars. This motion is towards a point in the constellation Hercules, near the star Vega.

Within of the Sun there are 315 known stars in 227 systems, as of 2000, including 163 single stars. It is estimated that a further 130 systems within this range have not yet been identified. Out to , there may be up to 7,500 stars, of which around 2,600 are known. The number of substellar objects in that volume are expected to be comparable to the number of stars. Of the 50 nearest stellar systems within 17 light-years from Earth (the closest being the red dwarf Proxima Centauri at approximately 4.2 light-years), the Sun ranks fourth in mass.

The Sun orbits the center of the Milky Way, and it is presently moving in the direction of the constellation of Cygnus. A simple model of the motion of a star in the galaxy gives the galactic coordinates , , and as:
where , , and are the respective velocities with respect to the local standard of rest, and are the Oort constants, formula_4 is the angular velocity of galactic rotation for the local standard of rest, formula_5 is the "epicyclic frequency", and ν is the vertical oscillation frequency. For the sun, the present values of , , and are estimated as formula_6 km/s, and estimates for the other constants are  = 15.5 km/s/kpc,  = −12.2 km/s/kpc, κ = 37 km/s/kpc, and ν=74 km/s/kpc. We take and to be zero and is estimated to be 17 parsecs. This model implies that the Sun circulates around a point that is itself going around the galaxy. The period of the Sun's circulation around the point is formula_7. which, using the equivalence that a parsec equals 1 km/s times 0.978 million years, comes to 166 million years, shorter than the time it takes for the point to go around the galaxy. In the () coordinates, the Sun describes an ellipse around the point, whose length in the direction is

and whose width in the direction is

The ratio of length to width of this ellipse, the same for all stars in our neighborhood, is formula_10
The moving point is presently at

The oscillation in the direction takes the Sun

above the galactic plane and the same distance below it, with a period of formula_14 or 83 million years, approximately 2.7 times per orbit. Although formula_15 is 222 million years, the value of formula_16 at the point around which the Sun circulates is

(see Oort constants), corresponding to 235 million years, and this is the time that the point takes to go once around the galaxy. Other stars with the same value of formula_18 have to take the same amount of time to go around the galaxy as the sun and thus remain in the same general vicinity as the Sun.

The Sun's orbit around the Milky Way is perturbed due to the non-uniform mass distribution in Milky Way, such as that in and between the galactic spiral arms. It has been argued that the Sun's passage through the higher density spiral arms often coincides with mass extinctions on Earth, perhaps due to increased impact events. It takes the Solar System about 225–250 million years to complete one orbit through the Milky Way (a "galactic year"), so it is thought to have completed 20–25 orbits during the lifetime of the Sun. The orbital speed of the Solar System about the center of the Milky Way is approximately 251 km/s (156 mi/s). At this speed, it takes around 1,190 years for the Solar System to travel a distance of 1 light-year, or 7 days to travel 1 AU.

The Milky Way is moving with respect to the cosmic microwave background radiation (CMB) in the direction of the constellation Hydra with a speed of 550 km/s, and the Sun's resultant velocity with respect to the CMB is about 370 km/s in the direction of Crater or Leo.

The temperature of the photosphere is approximately 6,000 K, whereas the temperature of the corona reaches 1,000,000–2,000,000 K. The high temperature of the corona shows that it is heated by something other than direct heat conduction from the photosphere.

It is thought that the energy necessary to heat the corona is provided by turbulent motion in the convection zone below the photosphere, and two main mechanisms have been proposed to explain coronal heating. The first is wave heating, in which sound, gravitational or magnetohydrodynamic waves are produced by turbulence in the convection zone. These waves travel upward and dissipate in the corona, depositing their energy in the ambient matter in the form of heat. The other is magnetic heating, in which magnetic energy is continuously built up by photospheric motion and released through magnetic reconnection in the form of large solar flares and myriad similar but smaller events—nanoflares.

Currently, it is unclear whether waves are an efficient heating mechanism. All waves except Alfvén waves have been found to dissipate or refract before reaching the corona. In addition, Alfvén waves do not easily dissipate in the corona. Current research focus has therefore shifted towards flare heating mechanisms.

Theoretical models of the Sun's development suggest that 3.8 to 2.5 billion years ago, during the Archean eon, the Sun was only about 75% as bright as it is today. Such a weak star would not have been able to sustain liquid water on Earth's surface, and thus life should not have been able to develop. However, the geological record demonstrates that Earth has remained at a fairly constant temperature throughout its history, and that the young Earth was somewhat warmer than it is today. One theory among scientists is that the atmosphere of the young Earth contained much larger quantities of greenhouse gases (such as carbon dioxide, methane) than are present today, which trapped enough heat to compensate for the smaller amount of solar energy reaching it.

However, examination of Archaean sediments appears inconsistent with the hypothesis of high greenhouse concentrations. Instead, the moderate temperature range may be explained by a lower surface albedo brought about by less continental area and the "lack of biologically induced cloud condensation nuclei". This would have led to increased absorption of solar energy, thereby compensating for the lower solar output.

The enormous effect of the Sun on Earth has been recognized since prehistoric times, and the Sun has been regarded by some cultures as a deity.

The Sun has been an object of veneration in many cultures throughout human history. Humanity's most fundamental understanding of the Sun is as the luminous disk in the sky, whose presence above the horizon creates day and whose absence causes night. In many prehistoric and ancient cultures, the Sun was thought to be a solar deity or other supernatural entity. Worship of the Sun was central to civilizations such as the ancient Egyptians, the Inca of South America and the Aztecs of what is now Mexico. In religions such as Hinduism, the Sun is still considered a god. Many ancient monuments were constructed with solar phenomena in mind; for example, stone megaliths accurately mark the summer or winter solstice (some of the most prominent megaliths are located in Nabta Playa, Egypt; Mnajdra, Malta and at Stonehenge, England); Newgrange, a prehistoric human-built mount in Ireland, was designed to detect the winter solstice; the pyramid of El Castillo at Chichén Itzá in Mexico is designed to cast shadows in the shape of serpents climbing the pyramid at the vernal and autumnal equinoxes.

The Egyptians portrayed the god Ra as being carried across the sky in a solar barque, accompanied by lesser gods, and to the Greeks, he was Helios, carried by a chariot drawn by fiery horses. From the reign of Elagabalus in the late Roman Empire the Sun's birthday was a holiday celebrated as Sol Invictus (literally "Unconquered Sun") soon after the winter solstice, which may have been either an antecedent to or attempted replacement for Christmas. Regarding the fixed stars, the Sun appears from Earth to revolve once a year along the ecliptic through the zodiac, and so Greek astronomers categorized it as one of the seven planets (Greek "planetes", "wanderer"); the naming of the days of the weeks after the seven planets dates to the Roman era.

In the early first millennium BC, Babylonian astronomers observed that the Sun's motion along the ecliptic is not uniform, though they did not know why; it is today known that this is due to the movement of Earth in an elliptic orbit around the Sun, with Earth moving faster when it is nearer to the Sun at perihelion and moving slower when it is farther away at aphelion.

One of the first people to offer a scientific or philosophical explanation for the Sun was the Greek philosopher Anaxagoras. He reasoned that it was not the chariot of Helios, but instead a giant flaming ball of metal even larger than the land of the Peloponnesus and that the Moon reflected the light of the Sun. For teaching this heresy, he was imprisoned by the authorities and sentenced to death, though he was later released through the intervention of Pericles. Eratosthenes estimated the distance between Earth and the Sun in the 3rd century BC as "of stadia myriads 400 and 80000", the translation of which is ambiguous, implying either 4,080,000 stadia (755,000 km) or 804,000,000 stadia (148 to 153 million kilometers or 0.99 to 1.02 AU); the latter value is correct to within a few percent. In the 1st century AD, Ptolemy estimated the distance as 1,210 times the radius of Earth, approximately .

The theory that the Sun is the center around which the planets orbit was first proposed by the ancient Greek Aristarchus of Samos in the 3rd century BC, and later adopted by Seleucus of Seleucia (see Heliocentrism). This view was developed in a more detailed mathematical model of a heliocentric system in the 16th century by Nicolaus Copernicus.

Observations of sunspots were recorded during the Han Dynasty (206 BC–AD 220) by Chinese astronomers, who maintained records of these observations for centuries. Averroes also provided a description of sunspots in the 12th century. The invention of the telescope in the early 17th century permitted detailed observations of sunspots by Thomas Harriot, Galileo Galilei and other astronomers. Galileo posited that sunspots were on the surface of the Sun rather than small objects passing between Earth and the Sun.

Arabic astronomical contributions include Al-Battani's discovery that the direction of the Sun's apogee (the place in the Sun's orbit against the fixed stars where it seems to be moving slowest) is changing. (In modern heliocentric terms, this is caused by a gradual motion of the aphelion of the "Earth's" orbit). Ibn Yunus observed more than 10,000 entries for the Sun's position for many years using a large astrolabe.
From an observation of a transit of Venus in 1032, the Persian astronomer and polymath Ibn Sina concluded that Venus is closer to Earth than the Sun. In 1672 Giovanni Cassini and Jean Richer determined the distance to Mars and were thereby able to calculate the distance to the Sun.

In 1666, Isaac Newton observed the Sun's light using a prism, and showed that it is made up of light of many colors. In 1800, William Herschel discovered infrared radiation beyond the red part of the solar spectrum. The 19th century saw advancement in spectroscopic studies of the Sun; Joseph von Fraunhofer recorded more than 600 absorption lines in the spectrum, the strongest of which are still often referred to as Fraunhofer lines. In the early years of the modern scientific era, the source of the Sun's energy was a significant puzzle. Lord Kelvin suggested that the Sun is a gradually cooling liquid body that is radiating an internal store of heat. Kelvin and Hermann von Helmholtz then proposed a gravitational contraction mechanism to explain the energy output, but the resulting age estimate was only 20 million years, well short of the time span of at least 300 million years suggested by some geological discoveries of that time. In 1890 Joseph Lockyer, who discovered helium in the solar spectrum, proposed a meteoritic hypothesis for the formation and evolution of the Sun.

Not until 1904 was a documented solution offered. Ernest Rutherford suggested that the Sun's output could be maintained by an internal source of heat, and suggested radioactive decay as the source. However, it would be Albert Einstein who would provide the essential clue to the source of the Sun's energy output with his mass–energy equivalence relation . In 1920, Sir Arthur Eddington proposed that the pressures and temperatures at the core of the Sun could produce a nuclear fusion reaction that merged hydrogen (protons) into helium nuclei, resulting in a production of energy from the net change in mass. The preponderance of hydrogen in the Sun was confirmed in 1925 by Cecilia Payne using the ionization theory developed by Meghnad Saha. The theoretical concept of fusion was developed in the 1930s by the astrophysicists Subrahmanyan Chandrasekhar and Hans Bethe. Hans Bethe calculated the details of the two main energy-producing nuclear reactions that power the Sun. In 1957, Margaret Burbidge, Geoffrey Burbidge, William Fowler and Fred Hoyle showed that most of the elements in the universe have been synthesized by nuclear reactions inside stars, some like the Sun.

The first satellites designed for long term observation of the Sun from interplanetary space were NASA's Pioneers 6, 7, 8 and 9, which were launched between 1959 and 1968. These probes orbited the Sun at a distance similar to that of Earth, and made the first detailed measurements of the solar wind and the solar magnetic field. Pioneer 9 operated for a particularly long time, transmitting data until May 1983.

In the 1970s, two Helios spacecraft and the Skylab Apollo Telescope Mount provided scientists with significant new data on solar wind and the solar corona. The Helios 1 and 2 probes were U.S.–German collaborations that studied the solar wind from an orbit carrying the spacecraft inside Mercury's orbit at perihelion. The Skylab space station, launched by NASA in 1973, included a solar observatory module called the Apollo Telescope Mount that was operated by astronauts resident on the station. Skylab made the first time-resolved observations of the solar transition region and of ultraviolet emissions from the solar corona. Discoveries included the first observations of coronal mass ejections, then called "coronal transients", and of coronal holes, now known to be intimately associated with the solar wind.

In 1980, the Solar Maximum Mission was launched by NASA. This spacecraft was designed to observe gamma rays, X-rays and UV radiation from solar flares during a time of high solar activity and solar luminosity. Just a few months after launch, however, an electronics failure caused the probe to go into standby mode, and it spent the next three years in this inactive state. In 1984 Space Shuttle "Challenger" mission STS-41C retrieved the satellite and repaired its electronics before re-releasing it into orbit. The Solar Maximum Mission subsequently acquired thousands of images of the solar corona before re-entering Earth's atmosphere in June 1989.

Launched in 1991, Japan's Yohkoh ("Sunbeam") satellite observed solar flares at X-ray wavelengths. Mission data allowed scientists to identify several different types of flares, and demonstrated that the corona away from regions of peak activity was much more dynamic and active than had previously been supposed. Yohkoh observed an entire solar cycle but went into standby mode when an annular eclipse in 2001 caused it to lose its lock on the Sun. It was destroyed by atmospheric re-entry in 2005.

One of the most important solar missions to date has been the Solar and Heliospheric Observatory, jointly built by the European Space Agency and NASA and launched on 2 December 1995. Originally intended to serve a two-year mission, a mission extension through 2012 was approved in October 2009. It has proven so useful that a follow-on mission, the Solar Dynamics Observatory (SDO), was launched in February 2010. Situated at the Lagrangian point between Earth and the Sun (at which the gravitational pull from both is equal), SOHO has provided a constant view of the Sun at many wavelengths since its launch. Besides its direct solar observation, SOHO has enabled the discovery of a large number of comets, mostly tiny sungrazing comets that incinerate as they pass the Sun.
All these satellites have observed the Sun from the plane of the ecliptic, and so have only observed its equatorial regions in detail. The "Ulysses" probe was launched in 1990 to study the Sun's polar regions. It first traveled to Jupiter, to "slingshot" into an orbit that would take it far above the plane of the ecliptic. Once "Ulysses" was in its scheduled orbit, it began observing the solar wind and magnetic field strength at high solar latitudes, finding that the solar wind from high latitudes was moving at about 750 km/s, which was slower than expected, and that there were large magnetic waves emerging from high latitudes that scattered galactic cosmic rays.

Elemental abundances in the photosphere are well known from spectroscopic studies, but the composition of the interior of the Sun is more poorly understood. A solar wind sample return mission, "Genesis", was designed to allow astronomers to directly measure the composition of solar material.

The Solar Terrestrial Relations Observatory (STEREO) mission was launched in October 2006. Two identical spacecraft were launched into orbits that cause them to (respectively) pull further ahead of and fall gradually behind Earth. This enables stereoscopic imaging of the Sun and solar phenomena, such as coronal mass ejections.

The Indian Space Research Organisation has scheduled the launch of a 100 kg satellite named "Aditya" for mid 2020. Its main instrument will be a coronagraph for studying the dynamics of the Solar corona.

The brightness of the Sun can cause pain from looking at it with the naked eye; however, doing so for brief periods is not hazardous for normal non-dilated eyes. Looking directly at the Sun causes phosphene visual artifacts and temporary partial blindness. It also delivers about 4 milliwatts of sunlight to the retina, slightly heating it and potentially causing damage in eyes that cannot respond properly to the brightness. UV exposure gradually yellows the lens of the eye over a period of years, and is thought to contribute to the formation of cataracts, but this depends on general exposure to solar UV, and not whether one looks directly at the Sun. Long-duration viewing of the direct Sun with the naked eye can begin to cause UV-induced, sunburn-like lesions on the retina after about 100 seconds, particularly under conditions where the UV light from the Sun is intense and well focused; conditions are worsened by young eyes or new lens implants (which admit more UV than aging natural eyes), Sun angles near the zenith, and observing locations at high altitude.

Viewing the Sun through light-concentrating optics such as binoculars may result in permanent damage to the retina without an appropriate filter that blocks UV and substantially dims the sunlight. When using an attenuating filter to view the Sun, the viewer is cautioned to use a filter specifically designed for that use. Some improvised filters that pass UV or IR rays, can actually harm the eye at high brightness levels.
Herschel wedges, also called Solar Diagonals, are effective and inexpensive for small telescopes. The sunlight that is destined for the eyepiece is reflected from an unsilvered surface of a piece of glass. Only a very small fraction of the incident light is reflected. The rest passes through the glass and leaves the instrument. If the glass breaks because of the heat, no light at all is reflected, making the device fail-safe. Simple filters made of darkened glass allow the full intensity of sunlight to pass through if they break, endangering the observer's eyesight. Unfiltered binoculars can deliver hundreds of times as much energy as using the naked eye, possibly causing immediate damage. It is claimed that even brief glances at the midday Sun through an unfiltered telescope can cause permanent damage.

Partial solar eclipses are hazardous to view because the eye's pupil is not adapted to the unusually high visual contrast: the pupil dilates according to the total amount of light in the field of view, "not" by the brightest object in the field. During partial eclipses most sunlight is blocked by the Moon passing in front of the Sun, but the uncovered parts of the photosphere have the same surface brightness as during a normal day. In the overall gloom, the pupil expands from ~2 mm to ~6 mm, and each retinal cell exposed to the solar image receives up to ten times more light than it would looking at the non-eclipsed Sun. This can damage or kill those cells, resulting in small permanent blind spots for the viewer. The hazard is insidious for inexperienced observers and for children, because there is no perception of pain: it is not immediately obvious that one's vision is being destroyed.

During sunrise and sunset, sunlight is attenuated because of Rayleigh scattering and Mie scattering from a particularly long passage through Earth's atmosphere, and the Sun is sometimes faint enough to be viewed comfortably with the naked eye or safely with optics (provided there is no risk of bright sunlight suddenly appearing through a break between clouds). Hazy conditions, atmospheric dust, and high humidity contribute to this atmospheric attenuation.

An optical phenomenon, known as a green flash, can sometimes be seen shortly after sunset or before sunrise. The flash is caused by light from the Sun just below the horizon being bent (usually through a temperature inversion) towards the observer. Light of shorter wavelengths (violet, blue, green) is bent more than that of longer wavelengths (yellow, orange, red) but the violet and blue light is scattered more, leaving light that is perceived as green.

Ultraviolet light from the Sun has antiseptic properties and can be used to sanitize tools and water. It also causes sunburn, and has other biological effects such as the production of vitamin D and sun tanning. It is also the main cause of skin cancer. Ultraviolet light is strongly attenuated by Earth's ozone layer, so that the amount of UV varies greatly with latitude and has been partially responsible for many biological adaptations, including variations in human skin color in different regions of the Earth.

The Sun has eight known planets. This includes four terrestrial planets (Mercury, Venus, Earth, and Mars), two gas giants (Jupiter and Saturn), and two ice giants (Uranus and Neptune). The Solar System also has at least five dwarf planets, an asteroid belt, numerous comets, and a large number of icy bodies which lie beyond the orbit of Neptune.

Solar deities play a major role in many world religions and mythologies. The ancient Sumerians believed that the sun was Utu, the god of justice and twin brother of Inanna, the Queen of Heaven, who was identified as the planet Venus. Later, Utu was identified with the East Semitic god Shamash. Utu was regarded as a helper-deity, who aided those in distress, and, in iconography, he is usually portrayed with a long beard and clutching a saw, which represented his role as the dispenser of justice.

From at least the 4th Dynasty of Ancient Egypt, the Sun was worshipped as the god Ra, portrayed as a falcon-headed divinity surmounted by the solar disk, and surrounded by a serpent. In the New Empire period, the Sun became identified with the dung beetle, whose spherical ball of dung was identified with the Sun. In the form of the sun disc Aten, the Sun had a brief resurgence during the Amarna Period when it again became the preeminent, if not only, divinity for the Pharaoh Akhenaton.

In Proto-Indo-European religion, the Sun was personified as the goddess "*Sehul". Derivatives of this goddess in Indo-European languages include the Old Norse "Sól", Sanskrit "Surya", Gaulish "Sulis", Lithuanian "Saulė", and Slavic "Solntse". In ancient Greek religion, the sun deity was the male god Helios, but traces of an earlier female solar deity are preserved in Helen of Troy. In later times, Helios was syncretized with Apollo.

In the Bible, mentions the "Sun of Righteousness" (sometimes translated as the "Sun of Justice"), which some Christians have interpreted as a reference to the Messiah (Christ). In ancient Roman culture, Sunday was the day of the sun god. It was adopted as the Sabbath day by Christians who did not have a Jewish background. The symbol of light was a pagan device adopted by Christians, and perhaps the most important one that did not come from Jewish traditions. In paganism, the Sun was a source of life, giving warmth and illumination to mankind. It was the center of a popular cult among Romans, who would stand at dawn to catch the first rays of sunshine as they prayed. The celebration of the winter solstice (which influenced Christmas) was part of the Roman cult of the unconquered Sun (Sol Invictus). Christian churches were built with an orientation so that the congregation faced toward the sunrise in the East.

Tonatiuh, the Aztec god of the sun, was usually depicted holding arrows and a shield and was closely associated with the practice of human sacrifice. The sun goddess Amaterasu is the most important deity in the Shinto religion, and she is believed to be the direct ancestor of all Japanese emperors.




</doc>
<doc id="26752" url="https://en.wikipedia.org/wiki?curid=26752" title="Smiley">
Smiley

A smiley (sometimes called a happy face or smiley face) is a stylized representation of a smiling humanoid face that is a part of popular culture worldwide. The classic form designed by Harvey Ball in 1963 comprises a yellow circle with two black dots representing eyes and a black arc representing the mouth () On the Internet and in other plain text communication channels, the emoticon form (sometimes also called the smiley-face emoticon) has traditionally been most popular, typically employing a colon and a right parenthesis to form sequences such as codice_1, codice_2, codice_3, codice_4, or codice_5 that resemble a smiling face when viewed after rotation through 90 degrees. "Smiley" is also sometimes used as a generic term for any emoticon (see Emoji.) The smiley has been referenced in nearly all areas of Western culture including music, movies, and art. The smiley has also been associated with late 1980s and early 1990s rave culture.

The plural form "smilies" is commonly used, but the variant spelling "smilie" is not as common as the "y" spelling.

In 2017, a team of archaeologists led by Nicolò Marchetti of the University of Bologna pieced together the fragments of a Hittite pot from approximately 1700 BC that had been found in Karkamış, Turkey. After it was pieced together, the team saw that it had what appeared to be a large smiley face painted on it.

The Danish poet and author Johannes V. Jensen was amongst other things famous for experimenting with the form of his writing. In a letter sent to publisher Ernst Bojesen in December 1900, he includes both a happy face and a sad face, resembling the modern smiley.

A commercial version of a smiley face with the word "THANKS" above it was available in 1919 and applied as a sticker on receipts issued by the Buffalo Steam Roller Company in Buffalo New York. The round face was much more detailed than the one depicted above, having eyebrows, nose, teeth, chin, facial creases and shading, and is reminiscent of "man-in-the-moon" style characterizations.

Ingmar Bergman's 1948 film "Port of Call" includes a scene where the unhappy Berit draws a "sad" face closely resembling the modern "frowny", but including a dot for the nose in lipstick on her mirror, before being interrupted. In 1953 and 1958, similar happy faces were used in promotional campaigns for the films "Lili" (1953) and "Gigi" (1958).

A smiley was used in a promotion by New York radio station WMCA beginning in 1962. Listeners who answered their phone "WMCA Good Guys!" were rewarded with a "WMCA good guys" sweatshirt that incorporated a happy face into its design. Thousands of these sweatshirts were given away. The WMCA smiley was yellow with black dots as eyes, had a slightly crooked smile, and had no creases at the sides of the mouth.

According to the Smithsonian Institution, the smiley face as we know it today was created by Harvey Ross Ball, an American graphic artist. In 1963, Ball was employed by State Mutual Life Assurance Company of Worcester, Massachusetts (now known as Hanover Insurance) to create a happy face to raise the morale of the employees. Ball created the design in ten minutes and was paid $45 (). His rendition, with a bright yellow background, dark oval eyes, full smile, and creases at the sides of the mouth, was imprinted on more than fifty million buttons and became familiar around the world. The design is so simple that it is certain that similar versions were produced before 1963, including those cited above. However, Ball’s rendition, as described here, has become the most iconic version. In 1967, Seattle graphic artist George Tenagi drew his own version at the request of advertising agent, David Stern. Tenagi's design was used in an advertising campaign for Seattle-based University Federal Savings & Loan. The ad campaign was inspired by Lee Adams's lyrics in "Put on a Happy Face" from the musical "Bye Bye Birdie". Stern, the man behind this campaign, also later incorporated the Happy Face in his run for Seattle mayor in 1993.

The graphic was further popularized in the early 1970s by Philadelphia brothers Bernard and Murray Spain, who seized upon it in September 1970 in a campaign to sell novelty items. The two produced buttons as well as coffee mugs, t-shirts, bumper stickers and many other items emblazoned with the symbol and the phrase "Have a happy day", which mutated into "Have a nice day". Working with New York button manufacturer NG Slater, some 50 million happy face badges were produced by 1972.

In 1972, Frenchman Franklin Loufrani became the first person to legally trademark use of a smiley face. He used it to highlight the good news parts of the newspaper "France Soir". He simply called the design "Smiley" and launched The Smiley Company. In 1996 Loufrani's son Nicolas Loufrani took over the family business and built it into a multinational corporation. Nicolas Loufrani was outwardly skeptical of Harvey Ball's claim to creating the first smiley face. While noting that the design that his father came up with and Ball's design were nearly identical, Loufrani argued that the design is so simple that no one person can lay claim to having created it. As evidence for this, Loufrani's website points to early cave paintings found in France (2500 BC) that he claims are the first depictions of a smiley face. Loufrani also points to a 1960 radio ad campaign that reportedly made use of a similar design.

In the UK, the happy face has been associated with psychedelic culture since Ubi Dwyer and the Windsor Free Festival in the 1970s and the electronic dance music culture, particularly with acid house, that emerged during the Second Summer of Love in the late 1980s. The association was cemented when the band Bomb the Bass used an extracted smiley from the comic book series "Watchmen" on the center of its "Beat Dis" hit single.

The earliest known smiley-like image in a written document was drawn by a Slovak notary to indicate his satisfaction with the state of his town's municipal financial records in 1635. A disputed early use of the smiley in a printed text may have been in Robert Herrick's poem "To Fortune" (1648), which contains the line "Upon my ruins (smiling yet :)". Journalist Levi Stahl has suggested that this may have been an intentional "orthographic joke", while this occurrence is likely merely the colon placed inside parentheses rather than outside of them as is standard typographic practice today -- (smiling yet):. There are citations of similar punctuation in a non-humorous context, even within Herrick's own work. It is likely that the parenthesis was added later by modern editors.

On the Internet, the smiley has become a visual means of conveyance that uses images. The first known mention on the Internet was on September 19, 1982, when Scott Fahlman from Carnegie Mellon University wrote:

Yellow graphical smileys have been used for many different purposes, including use in early 1980s video games. Yahoo! Messenger (from 1998) used smiley symbols in the user list next to each user, and also as an icon for the application. 
In 2001, SmileyWorld launched the website "The official Smiley dictionary", with smileys proposed to replace ASCII emoticons (i.e. emojis). In November 2001, and later, smiley emojis inside the actual chat text was adopted by several chat systems, including Yahoo Messenger.

The smiley is the printable version of characters 1 and 2 of (black-and-white versions of) codepage 437 (1981) of the first IBM PC and all subsequent PC compatible computers. For modern computers, all versions of Microsoft Windows after Windows 95 can use the smiley as part of Windows Glyph List 4, although some computer fonts miss some characters.

The smiley face was included in Unicode's Miscellaneous Symbols from version 1.1 (1993).

Later additions to Unicode included a large number of variants expressing a range of human emotions, in particlar with the addition of the "Emoticons" and "Supplemental Symbols and Pictographs blocks in Unicode versions 6.0 (2010) and 8.0 (2015), respectively.
These were introduced for compatibility with the ad-hoc implementation of emoticons by Japanese telephone carriers in unused ranges of the Shift JIS standard.
This resulted in a de-facto standard in the range with lead bytes 0xF5 to 0xF9. 
KDDI has gone much further than this, and has introduced hundreds more in the space with lead bytes 0xF3 and 0xF4.

Since the 1960s, the generic yellow smiley face has become an international icon used for various purposes. Its origins in popular culture began during the 1970s when it was frequently used across multiple platforms. The smiley was also used frequently in cartoons and comics. The smiley's first use in print was in 1971, where Franklin Loufrani designed a smiley face for the newspaper, "France-Soir". The newspaper used Loufrani's smiley to highlight stories that they defined as ""feel-good news."" This particular smiley went onto form The Smiley Company.

Mad magazine notably used the smiley a year later in 1972 across their entire front page for the April edition of the magazine. This was one of the first instances that the smiling face had been adapted, with one of the twenty visible smileys pulling a face.

In the late 1970s, the American band Dead Kennedys launched their first recording, California Über Alles. The single cover was a collage aimed to look like that of a Nazi rally prior to World War II. The usual swastika banners used at rallies, was replaced on the single cover with three large smileys.

In 1980, Namco released the now famous Pac-man, a yellow faced cartoon character. In the late-1980s, the smiley again became a prominent image within the music industry. It was adopted during the growth of acid house across Europe and the UK in the late 1980s. According to many, this began when DJ, Danny Rampling, used the smiley to celebrate Paul Oakenfold's birthday. This sparked a movement where the smiley moved into various dance genres, becoming a symbol of 1980s dance music.

The logo for the "Watchmen" comic book series includes a smiley with blood on top of it. 

The rights to the Smiley trademark in one hundred countries are owned by the Smiley Company. Its subsidiary SmileyWorld Ltd, in London, headed by Nicolas Loufrani, creates or approves all the Smiley products sold in countries where it holds the trademark. The Smiley brand and logo have significant exposure through licensees in sectors such as clothing, home decoration, perfumery, plush, stationery, publishing, and through promotional campaigns. The Smiley Company is one of the 100 biggest licensing companies in the world, with a turnover of US$167 million in 2012. The first Smiley shop opened in London in the Boxpark shopping centre in December 2011.

In 1997, Franklin Loufrani and Smiley World attempted to acquire trademark rights to the symbol (and even to the word "smiley" itself) in the United States. This brought Loufrani into conflict with Wal-Mart, which had begun prominently featuring a happy face in its "Rolling Back Prices" campaign over a year earlier. Wal-Mart responded first by trying to block Loufrani's application, then later by trying to register the smiley face itself; Loufrani, in turn, sued to stop Wal-Mart's application, and in 2002 after the issue went to court, where it would languish for seven years before a decision.

Wal-Mart began phasing out the smiley face on its vests and its website in 2006. Despite that, Wal-Mart sued an online parodist for alleged "trademark infringement" after he used the symbol (as well as various portmanteaus of "Wal-", such as "Walocaust"). The District Court found in favor of the parodist when in March 2008, the judge concluded that Wal-Mart's smiley face logo was not shown to be "inherently distinctive" and that it "has failed to establish that the smiley face has acquired secondary meaning or that it is otherwise a protectable trademark" under U.S. law.

In June 2010, Wal-Mart and the Smiley Company founded by Loufrani settled their 10-year-old dispute in front of the Chicago federal court. The terms remain confidential. In 2016, Wal-Mart brought back the smiley face on its website, social media profiles, and in selected stores.



</doc>
<doc id="26753" url="https://en.wikipedia.org/wiki?curid=26753" title="Signature">
Signature

A signature (; from , "to sign") is a handwritten (and often stylized) depiction of someone's name, nickname, or even a simple "X" or other mark that a person writes on documents as a proof of identity and intent. The writer of a signature is a signatory or signer. Similar to a handwritten signature, a signature work describes the work as readily identifying its creator. A signature may be confused with an autograph, which is chiefly an artistic signature. This can lead to confusion when people have both an autograph and signature and as such some people in the public eye keep their signatures private whilst fully publishing their autograph.

The traditional function of a signature is to permanently affix to a document a person’s uniquely personal, undeniable self-identification as physical evidence of that person's personal witness and certification of the content of all, or a specified part, of the document. For example, the role of a signature in many consumer contracts is not solely to provide evidence of the identity of the contracting party, but also to provide evidence of deliberation and informed consent. In many countries, signatures may be witnessed and recorded in the presence of a notary public to carry additional legal force. On legal documents, an illiterate signatory can make a "mark" (often an "X" but occasionally a personalized symbol), so long as the document is countersigned by a literate witness. In some countries, illiterate people place a thumbprint on legal documents in lieu of a written signature.

In the United States, signatures encompass marks and actions of all sorts that are indicative of identity and intent. The legal rule is that unless a statute specifically prescribes a particular method of making a signature it may be made in any number of ways. These include by a mechanical or rubber stamp facsimile. A signature may be made by the purported signatory; alternatively someone else duly authorized by the signatory, acting in the signer's presence and at the signatory's direction, may make the signature.

Many individuals have much more fanciful signatures than their normal cursive writing, including elaborate ascenders, descenders and exotic flourishes, much as one would find in calligraphic writing. As an example, the final "k" in John Hancock's famous signature on the US Declaration of Independence loops back to underline his name. This kind of flourish is also known as a "paraph".

Paraphe is a term meaning flourish, initial or signature in French. The paraph is used in graphology analyses.

Several cultures whose languages use writing systems other than alphabets do not share the Western notion of signatures per se: the "signing" of one's name results in a written product no different from the result of "writing" one's name in the standard way. For these languages, to write or to sign involves the same written characters. Also see Calligraphy.

Special signature machines, called autopens, are capable of automatically reproducing an individual's signature. These are typically used by people required to sign a lot of printed matter, such as celebrities, heads of state or CEOs. More recently, Members of Congress in the United States have begun having their signature made into a TrueType font file. This allows staff members in the Congressman's office to easily reproduce it on correspondence, legislation, and official documents. In the East Asian languages of Chinese, Japanese, and Korean, people traditionally use stamp-like objects known as "name-seals" with the name carved in "tensho" script ("seal script") in lieu of a handwritten signature. 

Some government agencies require that professional persons or official reviewers sign originals and all copies of originals to authenticate that they personally viewed the content. In the United States this is prevalent with architectural and construction plans. Its intent is to prevent mistakes or fraud but the practice is not known to be effective.

In e-mail and newsgroup usage, another type of signature exists which is independent of one's language. Users can set one or more lines of custom text known as a signature block to be automatically appended to their messages. This text usually includes a name, contact information, and sometimes quotations and ASCII art. A shortened form of a signature block, only including one's name, often with some distinguishing prefix, can be used to simply indicate the end of a post or response. Some web sites also allow graphics to be used. Note, however, that this type of signature is not related to electronic signatures or digital signatures, which are more technical in nature and not directly understandable by humans. On Wikipedia, an online wiki-based encyclopedia edited by volunteers, the contributors "sign" their comments on talk pages with their username (only the username holder has the right to digitally affix their signature).

The signature on a painting or other work of art has always been an important item in the assessment of art. Fake signatures are sometimes added to enhance the value of a painting, or are added to a fake painting to support its authenticity. A notorious case was the signature of Johannes Vermeer on the fake "Supper at Emmaus" made by the art-forger Han van Meegeren. However, the fact that painters' signatures often vary over time (particularly in the modern and contemporary periods) might complicate the issue. The signatures of some painters take on an artistic form that may be of less value in determining forgeries.

The term "signature" is also used to mean the characteristics that give an object, or a piece of information, its identity—for example, the shape of a Coca-Cola bottle. In rock music and heavy metal music, electric guitarists develop a unique tone and sound using particular settings on their guitar amp, effects units and modifications to their guitar pickups that is called their "signature sound". In wrestling such as WWE, wrestlers are known for distinctive "signature" finishing moves. In golf courses, a "signature hole" is the most aesthetically pleasing and photogenic hole, which makes a particular course unique. By analogy, the word "signature" may be used to refer to the characteristic expression of a process or thing. For example, the climate phenomenon known as ENSO or El Niño has characteristic modes in different ocean basins which are often referred to as the "signature" of Icie collier. A signatory indicates a party to an agreement, especially an international treaty or convention, "e.g." Brazil is a signatory to the Convention on Biological Diversity.

Under British law, the appearance of signatures (not the names themselves) may be protected under copyright law.

Under United States copyright law, "titles, names [I c...]; mere variations of typographic ornamentation, lettering, or coloring" are not eligible for copyright; however, the appearance of signatures (not the names themselves) may be protected under copyright law.

Uniform Commercial Code §1-201(37) of the United States generally defines signed as "using any symbol executed or adopted with present intention to adopt or accept a writing." The Uniform Commercial Code §3-401(b) for negotiable instruments states "A signature may be made (i) manually or by means of a device or machine, and (ii) by the use of any name, including a trade or assumed name, or by a word, mark, or symbol executed or adopted by a person with present intention to authenticate a writing."




</doc>
<doc id="26754" url="https://en.wikipedia.org/wiki?curid=26754" title="Seal">
Seal

Seal may refer to any of the following:











</doc>
<doc id="26756" url="https://en.wikipedia.org/wiki?curid=26756" title="Sino-Tibetan languages">
Sino-Tibetan languages

Sino-Tibetan, in a few sources also known as Trans-Himalayan, is a family of more than 400 languages, second only to Indo-European in number of native speakers. The Sino-Tibetan language with the most native speakers is Mandarin Chinese (920 million), although since not all forms of Mandarin are mutually-intelligible, it may be regarded as a complex series of dialect continua. Other Sino-Tibetan languages with large numbers of speakers include Burmese (33 million) and the Tibetic languages (six million). Other languages of the family are spoken in the Himalayas, the Southeast Asian Massif, and the eastern edge of the Tibetan Plateau; the latter group, in most cases, have only small speech communities, in remote mountain areas, and as such are poorly documented. While most linguists do not include Kra–Dai and Hmong–Mien languages within Sino-Tibetan, Chinese linguists generally do include them.

Several low-level subgroups have been securely reconstructed, but reconstruction of a proto-language for the family as a whole is still at an early stage, so the higher-level structure of Sino-Tibetan remains unclear. Although the family is traditionally presented as divided into Sinitic (i.e. Chinese) and Tibeto-Burman branches, a common origin of the non-Sinitic languages has never been demonstrated.
Several links to other language families have been proposed, but none has broad acceptance.

A genetic relationship between Chinese, Tibetan, Burmese and other languages was first proposed in the early 19th century and is now broadly accepted. The initial focus on languages of civilizations with long literary traditions has been broadened to include less widely spoken languages, some of which have only recently, or never, been written. However, the reconstruction of the family is much less developed than for families such as Indo-European or Austroasiatic. Difficulties have included the great diversity of the languages, the lack of inflection in many of them, and the effects of language contact. In addition, many of the smaller languages are spoken in mountainous areas that are difficult to access, and are often also sensitive border zones.

During the 18th century, several scholars had noticed parallels between Tibetan and Burmese, both languages with extensive literary traditions.
Early in the following century, Brian Houghton Hodgson and others noted that many non-literary languages of the highlands of northeast India and Southeast Asia were also related to these.
The name "Tibeto-Burman" was first applied to this group in 1856 by James Richardson Logan, who added Karen in 1858.
The third volume of the "Linguistic Survey of India", edited by Sten Konow, was devoted to the Tibeto-Burman languages of British India.

Studies of the "Indo-Chinese" languages of Southeast Asia from the mid-19th century by Logan and others revealed that they comprised four families: Tibeto-Burman, Tai, Mon–Khmer and Malayo-Polynesian.
Julius Klaproth had noted in 1823 that Burmese, Tibetan and Chinese all shared common basic vocabulary but that Thai, Mon, and Vietnamese were quite different.
Ernst Kuhn envisaged a group with two branches, Chinese-Siamese and Tibeto-Burman.
August Conrady called this group Indo-Chinese in his influential 1896 classification, though he had doubts about Karen. Conrady's terminology was widely used, but there was uncertainty regarding his exclusion of Vietnamese. Franz Nikolaus Finck in 1909 placed Karen as a third branch of Chinese-Siamese.

Jean Przyluski introduced the French term "sino-tibétain" as the title of his chapter on the group in Meillet and Cohen's "Les langues du monde" in 1924. He divided them into three groups: Tibeto-Burman, Chinese and Tai, and was uncertain about the affinity of Karen and Hmong–Mien. The English translation "Sino-Tibetan" first appeared in a short note by Przyluski and Luce in 1931.

In 1935, the anthropologist Alfred Kroeber started the Sino-Tibetan Philology Project, funded by the Works Project Administration and based at the University of California, Berkeley.
The project was supervised by Robert Shafer until late 1938, and then by Paul K. Benedict.
Under their direction, the staff of 30 non-linguists collated all the available documentation of Sino-Tibetan languages.
The result was eight copies of a 15-volume typescript entitled "Sino-Tibetan Linguistics".
This work was never published, but furnished the data for a series of papers by Shafer, as well as Shafer's five-volume "Introduction to Sino-Tibetan" and Benedict's "Sino-Tibetan, a Conspectus".

Benedict completed the manuscript of his work in 1941, but it was not published until 1972. Instead of building the entire family tree, he set out to reconstruct a Proto-Tibeto-Burman language by comparing five major languages, with occasional comparisons with other languages. He reconstructed a two-way distinction on initial consonants based on voicing, with aspiration conditioned by pre-initial consonants that had been retained in Tibetic but lost in many other languages. Thus, Benedict reconstructed the following initials:

Although the initial consonants of cognates tend to have the same place and manner of articulation, voicing and aspiration is often unpredictable.
This irregularity was attacked by Roy Andrew Miller, though Benedict's supporters attribute it to the effects of prefixes that have been lost and are often unrecoverable.
The issue remains unsolved today.
It was cited together with the lack of reconstructable shared morphology, and evidence that much shared lexical material has been borrowed from Chinese into Tibeto-Burman, by Christopher Beckwith, one of the few scholars still arguing that Chinese is not related to Tibeto-Burman.

Benedict also reconstructed, at least for Tibeto-Burman, prefixes such as the causative "s-", the intransitive "m-", and "r-", "b-" "g-" and "d-" of uncertain function, as well as suffixes "-s", "-t" and "-n".

Old Chinese is by far the oldest recorded Sino-Tibetan language, with inscriptions dating from 1200 BC and a huge body of literature from the first millennium BC, but the Chinese script is not alphabetic. Scholars have sought to reconstruct the phonology of Old Chinese by comparing the obscure descriptions of the sounds of Middle Chinese in medieval dictionaries with phonetic elements in Chinese characters and the rhyming patterns of early poetry. The first complete reconstruction, the "Grammata Serica Recensa" of Bernard Karlgren, was used by Benedict and Shafer.

Karlgren's reconstruction was somewhat unwieldy, with many sounds having a highly non-uniform distribution. Later scholars have revised it by drawing on a range of other sources. Some proposals were based on cognates in other Sino-Tibetan languages, though workers have also found solely Chinese evidence for them. For example, recent reconstructions of Old Chinese have reduced Karlgren's 15 vowels to a six-vowel system originally suggested by Nicholas Bodman. Similarly, Karlgren's *l has been recast as *r, with a different initial interpreted as *l, matching Tibeto-Burman cognates, but also supported by Chinese transcriptions of foreign names. A growing number of scholars believe that Old Chinese did not use tones, and that the tones of Middle Chinese developed from final consonants. One of these, *-s, is believed to be a suffix, with cognates in other Sino-Tibetan languages.
Tibetic has extensive written records from the adoption of writing by the Tibetan Empire in the mid-7th century. The earliest records of Burmese (such as the 12th-century Myazedi inscription) are more limited, but later an extensive literature developed. Both languages are recorded in alphabetic scripts ultimately derived from the Brahmi script of Ancient India. Most comparative work has used the conservative written forms of these languages, following the dictionaries of Jäschke (Tibetan) and Judson (Burmese), though both contain entries from a wide range of periods.

There are also extensive records in Tangut, the language of the Western Xia (1038–1227). Tangut is recorded in a Chinese-inspired logographic script, whose interpretation presents many difficulties, even though multilingual dictionaries have been found.

Gong Hwang-cherng has compared Old Chinese, Tibetic, Burmese and Tangut in an effort to establish sound correspondences between those languages. He found that Tibetic and Burmese correspond to two Old Chinese vowels, *a and *ə. While this has been considered evidence for a separate Tibeto-Burman subgroup, Hill (2014) finds that Burmese has distinct correspondences for Old Chinese rhymes "-ay" : *-aj and "-i" : *-əj, and hence argues that the development *ə > *a occurred independently in Tibetan and Burmese.

The descriptions of non-literary languages used by Shafer and Benedict were often produced by missionaries and colonial administrators of varying linguistic skill.
Most of the smaller Sino-Tibetan languages are spoken in inaccessible mountainous areas, many of which are politically or militarily sensitive and thus closed to investigators.
Until the 1980s, the best-studied areas were Nepal and northern Thailand.
In the 1980s and 1990s, new surveys were published from the Himalayas and southwestern China.
Of particular interest was the discovery of a new branch of the family, the Qiangic languages of western Sichuan and adjacent areas.

Most of the current spread of Sino-Tibetan languages is the result of historical expansions of the three groups with the most speakers – Chinese, Burmese and Tibetic – replacing an unknown number of earlier languages.
These groups also have the longest literary traditions of the family.
The remaining languages are spoken in mountainous areas, along the southern slopes of the Himalayas, the Southeast Asian Massif and the eastern edge of the Tibetan Plateau.

By far the largest branch are the Sinitic languages, with 1.3 billion speakers, most of whom live in the eastern half of China.
The first records of Chinese are oracle bone inscriptions from c. 1200 BC, when Old Chinese was spoken around the middle reaches of the Yellow River.
Chinese has since expanded throughout China, forming a family whose diversity has been compared with the Romance languages.
Diversity is greater in the rugged terrain of southeast China than in the North China Plain.

Burmese is the national language of Myanmar, and the first language of some 33 million people.
Burmese speakers first entered the northern Irrawaddy basin from what is now western Yunnan in the early 9th century, when the Pyu city-states had been weakened by an invasion by Nanzhao.
Other Burmish languages are still spoken in Dehong Prefecture in the far west of Yunnan.
By the 11th century their Pagan Kingdom had expanded over the whole basin.
The oldest texts, such as the Myazedi inscription, date from the early 12th century.

The Tibetic languages are spoken by some 6 million people on the Tibetan Plateau and neighbouring areas in the Himalayas and western Sichuan.
They are descended from Old Tibetan, which was originally spoken in the Yarlung Valley before it was spread by the expansion of the Tibetan Empire in the 7th century.
Although the empire collapsed in the 9th century, Classical Tibetan remained influential as the liturgical language of Tibetan Buddhism.

The remaining languages are spoken in upland areas.
Southernmost are the Karen languages, spoken by 4 million people in the hill country along the Myanmar–Thailand border, with the greatest diversity in the Karen Hills, which are believed to be the homeland of the group.
The highlands stretching from northeast India to northern Myanmar contain over 100 high-diverse Sino-Tibetan languages.
Other Sino-Tibetan languages are found along the southern slopes of the Himalayas, southwest China and northern Thailand.

There have been a range of proposals for the Sino-Tibetan urheimat, reflecting the uncertainty about the classification of the family and its time depth.
James Matisoff (1991) places it in the eastern part of the Tibetan plateau around 4000 BC, with the various groups migrating out down the Yellow, Yangtze, Mekong, Salween and Brahmaputra rivers.
George van Driem (2005) proposes that Sino-Tibetan originated in the Sichuan Basin before 7000 BC, with an early migration into northeast India, and a later migration north of the predecessors of Chinese and Tibetic. Roger Blench and Mark Post (2014) have proposed that the Sino-Tibetan homeland is Northeast India, the area of greatest diversity, around 7000 BC.
Roger Blench (2009) argues that agriculture cannot be reconstructed for Proto-Sino-Tibetan, and that the earliest speakers of Sino-Tibetan were not farmers but highly diverse foragers.

Zhang et al. (2019) performed a computational phylogenetic analysis of 109 Sino-Tibetan languages to suggest a Sino-Tibetan homeland in northern China near the Yellow River basin. The study further suggests that there was an initial major split between the Sinitic languages and the Tibeto-Burman languages approximately 4,200 - 7,800 years ago (with an average of 5,900 years ago), associating this expansion with the Yangshao culture and/or the later Majiayao culture. Sagart et al. (2019) also performed another phylogenetic analysis based on different data and method to arrive at the same conclusions with respect to the homeland and divergence model, but proposed an earlier root age of approximately 7,200 years ago, associating its origin with the late Cishan and early Yangshao culture.

Several low-level branches of the family, particularly Lolo-Burmese, have been securely reconstructed, but in the absence of a secure reconstruction of a Sino-Tibetan proto-language, the higher-level structure of the family remains unclear.
Thus, a conservative classification of Sino-Tibetan/Tibeto-Burman would posit several dozen small coordinate families and isolates; attempts at subgrouping are either geographic conveniences or hypotheses for further research.

In a survey in the 1937 "Chinese Yearbook", Li Fang-Kuei described the family as consisting of four branches:

Tai and Miao–Yao were included because they shared isolating typology, tone systems and some vocabulary with Chinese. At the time, tone was considered so fundamental to language that tonal typology could be used as the basis for classification. In the Western scholarly community, these languages are no longer included in Sino-Tibetan, with the similarities attributed to diffusion across the Mainland Southeast Asia linguistic area, especially since .
The exclusions of Vietnamese by Kuhn and of Tai and Miao–Yao by Benedict were vindicated in 1954 when André-Georges Haudricourt demonstrated that the tones of Vietnamese were reflexes of final consonants from Proto-Mon–Khmer.

Many Chinese linguists continue to follow Li's classification. However, this arrangement remains problematic. For example, there is disagreement over whether to include the entire Kra–Dai family or just Kam–Tai (Zhuang–Dong excludes the Kra languages), because the Chinese cognates that form the basis of the putative relationship are not found in all branches of the family and have not been reconstructed for the family as a whole. In addition, Kam–Tai itself no longer appears to be a valid node within Kra–Dai.

Benedict overtly excluded Vietnamese (placing it in Mon–Khmer) as well as Hmong–Mien and Kra–Dai (placing them in Austro-Tai).
He otherwise retained the outlines of Conrady's Indo-Chinese classification, though putting Karen in an intermediate position:

Shafer criticized the division of the family into Tibeto-Burman and Sino-Daic branches, which he attributed to the different groups of languages studied by Konow and other scholars in British India on the one hand and by Henri Maspero and other French linguists on the other.
He proposed a detailed classification, with six top-level divisions:

Shafer was sceptical of the inclusion of Daic, but after meeting Maspero in Paris decided to retain it pending a definitive resolution of the question.

James Matisoff abandoned Benedict's Tibeto-Karen hypothesis:

Some more-recent Western scholars, such as Bradley (1997) and La Polla (2003), have retained Matisoff's two primary branches, though differing in the details of Tibeto-Burman. However, Jacques (2006) notes, "comparative work has never been able to put forth evidence for common innovations to all the Tibeto-Burman languages (the Sino-Tibetan languages to the exclusion of Chinese)" and that "it no longer seems justified to treat Chinese as the first branching of the Sino-Tibetan family," because the morphological divide between Chinese and Tibeto-Burman has been bridged by recent reconstructions of Old Chinese.

The internal structure of Sino-Tibetan has been tentatively revised as the following Stammbaum by Matisoff (2015: xxxii, 1123-1127) in the final print release of the "Sino-Tibetan Etymological Dictionary and Thesaurus" (STEDT). Matisoff (2015: xxxi) acknowledges that the position of Chinese as either a sister branch of Tibeto-Burman or a branch within Tibeto-Burman remains an open question.


Sergei Starostin proposed that both the Kiranti languages and Chinese are divergent from a "core" Tibeto-Burman of at least Bodish, Lolo-Burmese, Tamangic, Jinghpaw, Kukish, and Karen (other families were not analysed) in a hypothesis called "Sino-Kiranti". The proposal takes two forms: that Sinitic and Kiranti are themselves a valid node or that the two are not demonstrably close, so that Sino-Tibetan has three primary branches:

van Driem, like Shafer, rejects a primary split between Chinese and the rest, suggesting that Chinese owes its traditional privileged place in Sino-Tibetan to historical, typological, and cultural, rather than linguistic, criteria. He calls the entire family "Tibeto-Burman", a name he says has historical primacy, but other linguists who reject a privileged position for Chinese nevertheless continue to call the resulting family "Sino-Tibetan", including Roger Blench.

Like Matisoff, van Driem acknowledges that the relationships of the "Kuki–Naga" languages (Kuki, Mizo, Meitei, etc.), both amongst each other and to the other languages of the family, remain unclear. However, rather than placing them in a geographic grouping, as Matisoff does, van Driem leaves them unclassified.
He has proposed several hypotheses, including the reclassification of Chinese to a Sino-Bodic subgroup:

Van Driem points to two main pieces of evidence establishing a special relationship between Sinitic and Bodic and thus placing Chinese within the Tibeto-Burman family. First, there are a number of parallels between the morphology of Old Chinese and the modern Bodic languages. Second, there is an impressive body of lexical cognates between the Chinese and Bodic languages, represented by the Kirantic language Limbu.

In response, Matisoff notes that the existence of shared lexical material only serves to establish an absolute relationship between two language families, not their relative relationship to one another. Although some cognate sets presented by van Driem are confined to Chinese and Bodic, many others are found in Sino-Tibetan languages generally and thus do not serve as evidence for a special relationship between Chinese and Bodic.

George van Driem (2001) has also proposed a "fallen leaves" model that lists dozens of well-established low-level groups while remaining agnostic about intermediate groupings of these.
In the most recent version (van Driem 2014), 42 groups are identified (with individual languages highlighted in "italics"):
van Driem (2007) also suggested that the Sino-Tibetan language family be renamed "Trans-Himalayan", which he considers to be more neutral.

Roger Blench and Mark W. Post have criticized the applicability of conventional Sino-Tibetan classification schemes to minor languages lacking an extensive written history (unlike Chinese, Tibetic, and Burmese). They find that the evidence for the subclassification or even ST affiliation at all of several minor languages of northeastern India, in particular, is either poor or absent altogether.

In their view, many such languages would for now be best considered unclassified, or "internal isolates" within the family. They propose a provisional classification of the remaining languages:
Following that, because they propose that the three best-known branches may actually be much closer related to each other than they are to "minor" Sino-Tibetan languages, Blench and Post argue that "Sino-Tibetan" or "Tibeto-Burman" are inappropriate names for a family whose earliest divergences led to different languages altogether. They support the proposed name "Trans-Himalayan".

This is the classification scheme proposed in Menghan Zhang, Shi Yan, et al. (2019).

Except for the Chinese, Bai, Karenic, and Mruic languages, the usual word order in Sino-Tibetan languages is object–verb. Most scholars believe this to be the original order, with Chinese, Karen and Bai having acquired subject–verb–object order due to the influence of neighbouring languages in the Mainland Southeast Asia linguistic area.
However, Chinese and Bai differ from almost all other SVO languages in the world in placing relative clauses before the nouns they modify.

Hodgson had in 1849 noted a dichotomy between "pronominalized" (inflecting) languages, stretching across the Himalayas from Himachal Pradesh to eastern Nepal, and "non-pronominalized" (isolating) languages. Konow (1909) explained the pronominalized languages as due to a Munda substratum, with the idea that Indo-Chinese languages were essentially isolating as well as tonal. Maspero later attributed the putative substratum to Indo-Aryan. It was not until Benedict that the inflectional systems of these languages were recognized as (partially) native to the family.
Scholars disagree over the extent to which the agreement system in the various languages can be reconstructed for the proto-language.

In morphosyntactic alignment, many Tibeto-Burman languages have ergative and/or anti-ergative (an argument that is not an actor) case marking. However, the anti-ergative case markings can not be reconstructed at higher levels in the family and are thought to be innovations.

Beyond the traditionally recognized families of Southeast Asia, a number of possible broader relationships have been suggested:

One of these is the "Sino-Caucasian" hypothesis of Sergei Starostin, which posits that the Yeniseian languages and North Caucasian languages form a clade with Sino-Tibetan. The Sino-Caucasian hypothesis has been expanded by others to "Dené–Caucasian" to include the Na-Dené languages of North America, Burushaski, Basque and, occasionally, Etruscan. Edward Sapir had commented on a connection between Na-Dené and Sino-Tibetan. A narrower binary Dené–Yeniseian family has recently been well-received. The validity of the rest of the family, however, is viewed as doubtful or rejected by nearly all historical linguists.

Geoffrey Caveney (2014) suggest that the Sino-Tibetan and Na-Dene languages are related but say that his analysis does not support the Sino-Caucasian or Dene-Caucasian hypothese.

In contrast, Laurent Sagart proposes a Sino-Austronesian family with Sino-Tibetan and Austronesian (including Kra–Dai as a subbranch) as primary branches. Stanley Starosta has extended this proposal with a further branch called "Yangzian" joining Hmong–Mien and Austroasiatic.





</doc>
<doc id="26757" url="https://en.wikipedia.org/wiki?curid=26757" title="Slavic languages">
Slavic languages

The Slavic languages (also called Slavonic languages) are the Indo-European languages spoken by the Slavic peoples. They are thought to descend from a proto-language called Proto-Slavic, spoken during the Early Middle Ages, which in turn is thought to have descended from the earlier Proto-Balto-Slavic language, linking the Slavic languages to the Baltic languages in a Balto-Slavic group within the Indo-European family.

The Slavic languages are conventionally (that is, also on the basis of extralinguistic features) divided intro three subgroups: East, West, and South, which together constitute more than 20 languages. Of these, 10 have at least one million speakers and official status as the national languages of the countries in which they are predominantly spoken: Russian, Belarusian and Ukrainian (of the East group), Polish, Czech and Slovak (of the West group) and Bulgarian and Macedonian (eastern dialects of the South group), and Serbo-Croatian and Slovene (western dialects of the South group).

The current geographic distribution of natively spoken Slavic languages covers Eastern Europe, the Balkans, Central Europe and all of the territory of Russia, which includes northern and north-central Asia. Furthermore, the diasporas of many Slavic peoples have established isolated minorities of speakers of their languages all over the world. The number of speakers of all Slavic languages together was estimated to be 315 million at the turn of the twenty-first century. Despite the large extent, the individual Slavic languages are considerably less differentiated than Germanic and Romance languages.

Since the interwar period scholars have traditionally divided Slavic languages, on the basis of geographical and genealogical principle, and with the use of the extralinguistic feature of script, into three main branches, that is, East, West and South. (From the vantage of linguistic features alone, there are only two branches of the Slavic languages, namely North and South. These three conventional branches feature some of the following subbranches:




Some linguists speculate that a North Slavic branch has existed as well. The Old Novgorod dialect may have reflected some idiosyncrasies of this group.
Mutual intelligibility also plays a role in determining the West, East, and South branches. Speakers of languages within the same branch will in most cases be able to understand each other at least partially, but they are generally unable to across branches (which would be comparable to a native English speaker trying to understand any other Germanic language).

The most obvious differences between the East, West and South Slavic branches are in the orthography of the standard languages: West Slavic languages (and Western South Slavic languages – Croatian and Slovene) are written in the Latin script, and have had more Western European influence due to their proximity and speakers being historically Roman Catholic, whereas the East Slavic and Eastern South Slavic languages are written in Cyrillic and, with Eastern Orthodox or Uniate faith, have had more Greek influence. East Slavic languages such as Russian have, however, during and after Peter the Great's Europeanization campaign, absorbed many words of Latin, French, German, and Italian origin.

The tripartite division of the Slavic languages does not take into account the spoken dialects of each language. Of these, certain so-called transitional dialects and hybrid dialects often bridge the gaps between different languages, showing similarities that do not stand out when comparing Slavic literary (i.e. standard) languages. For example, Slovak (West Slavic) and Ukrainian (East Slavic) are bridged by the Rusyn language/dialect of Eastern Slovakia and Western Ukraine. Similarly, the Croatian Kajkavian dialect is more similar to Slovene than to the standard Croatian language.

Although the Slavic languages diverged from a common proto-language later than any other group of the Indo-European language family, enough differences exist between the various Slavic dialects and languages to make communication between speakers of different Slavic languages difficult. Within the individual Slavic languages, dialects may vary to a lesser degree, as those of Russian, or to a much greater degree, as those of Slovene.

Slavic languages descend from Proto-Slavic, their immediate parent language, ultimately deriving from Proto-Indo-European, the ancestor language of all Indo-European languages, via a Proto-Balto-Slavic stage. During the Proto-Balto-Slavic period a number of exclusive isoglosses in phonology, morphology, lexis, and syntax developed, which makes Slavic and Baltic the closest related of all the Indo-European branches. The secession of the Balto-Slavic dialect ancestral to Proto-Slavic is estimated on archaeological and glottochronological criteria to have occurred sometime in the period 1500–1000 BCE.

A minority of Baltists maintain the view that the Slavic group of languages differs so radically from the neighboring Baltic group (Lithuanian, Latvian, and the now-extinct Old Prussian), that they could not have shared a parent language after the breakup of the Proto-Indo-European continuum about five millennia ago. Substantial advances in Balto-Slavic accentology that occurred in the last three decades, however, make this view very hard to maintain nowadays, especially when one considers that there was most likely no "Proto-Baltic" language and that West Baltic and East Baltic differ from each other as much as each of them does from Proto-Slavic.

The imposition of Old Church Slavonic on Orthodox Slavs was often at the expense of the vernacular. Says WB Lockwood, a prominent Indo-European linguist, "It (O.C.S) remained in use to modern times but was more and more influenced by the living, evolving languages, so that one distinguishes Bulgarian, Serbian, and Russian varieties. The use of such media hampered the development of the local languages for literary purposes, and when they do appear the first attempts are usually in an artificially mixed style." (148)

Lockwood also notes that these languages have "enriched" themselves by drawing on Church Slavonic for the vocabulary of abstract concepts. The situation in the Catholic countries, where Latin was more important, was different. The Polish Renaissance poet Jan Kochanowski and the Croatian Baroque writers of the 16th century all wrote in their respective vernaculars (though Polish itself had drawn amply on Latin in the same way Russian would eventually draw on Church Slavonic).
Although Church Slavonic hampered vernacular literatures, it fostered Slavonic literary activity and abetted linguistic independence from external influences. Only the Croatian vernacular literary tradition nearly matches Church Slavonic in age. It began with the Vinodol Codex and continued through the Renaissance until the codifications of Croatian in 1830, though much of the literature between 1300 and 1500 was written in much the same mixture of the vernacular and Church Slavonic as prevailed in Russia and elsewhere.

The most important early monument of Croatian literacy is the Baška tablet from the late 11th century. It is a large stone tablet found in the small Church of St. Lucy, Jurandvor on the Croatian island of Krk, containing text written mostly in Čakavian dialect in angular Croatian Glagolitic script. The independence of Dubrovnik facilitated the continuity of the tradition.
More recent foreign influences follow the same general pattern in Slavic languages as elsewhere and are governed by the political relationships of the Slavs. In the 17th century, bourgeois Russian ("delovoi jazyk") absorbed German words through direct contacts between Russians and communities of German settlers in Russia. In the era of Peter the Great, close contacts with France invited countless loan words and calques from French, a significant fraction of which not only survived but also replaced older Slavonic loans. In the 19th century, Russian influenced most literary Slavic languages by one means or another.

The Proto-Slavic language existed until around AD 500. By the 7th century, it had broken apart into large dialectal zones.

There are no reliable hypotheses about the nature of the subsequent breakups of West and South Slavic. East Slavic is generally thought to converge to one Old Russian or Old East Slavonic language, which existed until at least the 12th century.

Linguistic differentiation was accelerated by the dispersion of the Slavic peoples over a large territory, which in Central Europe exceeded the current extent of Slavic-speaking majorities. Written documents of the 9th, 10th, and 11th centuries already display some local linguistic features. For example, the Freising manuscripts show a language that contains some phonetic and lexical elements peculiar to Slovene dialects (e.g. rhotacism, the word "krilatec"). The Freising manuscripts are the first Latin-script continuous text in a Slavic language.

The migration of Slavic speakers into the Balkans in the declining centuries of the Byzantine Empire expanded the area of Slavic speech, but the pre-existing writing (notably Greek) survived in this area. The arrival of the Hungarians in Pannonia in the 9th century interposed non-Slavic speakers between South and West Slavs. Frankish conquests completed the geographical separation between these two groups, also severing the connection between Slavs in Moravia and Lower Austria (Moravians) and those in present-day Styria, Carinthia, East Tyrol in Austria, and in the provinces of modern Slovenia, where the ancestors of the Slovenes settled during first colonisation.
In September 2015, Alexei Kassian and Anna Dybo published, as a part of interdisciplinary study of Slavic ethnogenesis, a lexicostatistical classification of Slavic languages. It was built using qualitative 110-word Swadesh lists that were compiled according to the standards of the Global Lexicostatistical Database project and processed using modern phylogenetic algorithms.

The resulting dated tree complies with the traditional expert views on the Slavic group structure. Kassian-Dybo's tree suggests that Proto-Slavic first diverged into three branches: Eastern, Western and Southern. The Proto-Slavic break-up is dated to around 100 A.D., which correlates with the archaeological assessment of Slavic population in the early 1st millennium A.D. being spread on a large territory and already not being monolithic. Then, in the 5th and 6th centuries A.D., these three Slavic branches almost simultaneously divided into sub-branches, which corresponds to the fast spread of the Slavs through Eastern Europe and the Balkans during the second half of the 1st millennium A.D. (the so-called Slavicization of Europe).

The Slovenian language was excluded from the analysis, as both Ljubljana koine and Literary Slovenian show mixed lexical features of Southern and Western Slavic languages (which could possibly indicate the Western Slavic origin of Slovenian, which for a long time was being influenced on the part of the neighboring Serbo-Croatian dialects), and the quality Swadesh lists were not yet collected for Slovenian dialects. Because of scarcity or unreliability of data, the study also did not cover the so-called Old Novgordian dialect, the Polabian language and some other Slavic lects.

The following is a summary of the main changes from Proto-Indo-European (PIE) leading up to the Common Slavic (CS) period immediately following the Proto-Slavic language (PS).


The Slavic languages are a relatively homogeneous family, compared with other families of Indo-European languages (e.g. Germanic, Romance, and Indo-Iranian). As late as the 10th century AD, the entire Slavic-speaking area still functioned as a single, dialectally differentiated language, termed "Common Slavic". Compared with most other Indo-European languages, the Slavic languages are quite conservative, particularly in terms of morphology (the means of inflecting nouns and verbs to indicate grammatical differences). Most Slavic languages have a rich, fusional morphology that conserves much of the inflectional morphology of Proto-Indo-European.

The following table shows the inventory of consonants of Late Common Slavic:

The sound did not occur in West Slavic, where it had developed to .

This inventory of sounds is quite similar to what is found in most modern Slavic languages. The extensive series of palatal consonants, along with the affricates *ts and *dz, developed through a series of palatalizations that happened during the Proto-Slavic period, from earlier sequences either of velar consonants followed by front vowels (e.g. *ke, *ki, *ge, *gi, *xe, and *xi), or of various consonants followed by *j (e.g. *tj, *dj, *sj, *zj, *rj, *lj, *kj, and *gj, where *j is the palatal approximant (, the sound of the English letter "y" in "yes" or "you").

The biggest change in this inventory results from a further general palatalization occurring near the end of the Common Slavic period, where "all" consonants became palatalized before front vowels. This produced a large number of new palatalized (or "soft") sounds, which formed pairs with the corresponding non-palatalized (or "hard") consonants and absorbed the existing palatalized sounds . These sounds were best preserved in Russian but were lost to varying degrees in other languages (particularly Czech and Slovak). The following table shows the inventory of modern Russian:

This general process of palatalization did not occur in Serbo-Croatian and Slovenian. As a result, the modern consonant inventory of these languages is nearly identical to the Late Common Slavic inventory.

Late Common Slavic tolerated relatively few consonant clusters. However, as a result of the loss of certain formerly present vowels (the weak yers), the modern Slavic languages allow quite complex clusters, as in the Russian word взблеск ("flash"). Also present in many Slavic languages are clusters rarely found cross-linguistically, as in Russian ртуть ("mercury") or Polish mchu ("moss", gen. sg.). The word for "mercury" with the initial "rt-" cluster, for example, is also found in the other East and West Slavic languages, although Slovak retains an epenthetic vowel ("ortuť").

A typical vowel inventory is as follows:

The sound occurs only in some languages (Russian and Belarusian), and even in these languages, it is unclear whether it is its own phoneme or an allophone of /i/. Nonetheless, it is a quite prominent and noticeable characteristic of the languages in which it is present.

Common Slavic also had two nasal vowels: *ę and *ǫ . However, these are preserved only in modern Polish (along with a few lesser-known dialects and microlanguages; see Yus for more details).

Other phonemic vowels are found in certain languages (e.g. the schwa in Bulgarian and Slovenian, distinct high-mid and low-mid vowels in Slovenian, and the lax front vowel in Ukrainian).

An area of great difference among Slavic languages is that of prosody (i.e. syllabic distinctions such as vowel length, accent, and tone). Common Slavic had a complex system of prosody, inherited with little change from Proto-Indo-European. This consisted of phonemic vowel length and a free, mobile pitch accent:

The modern languages vary greatly in the extent to which they preserve this system. On one extreme, Serbo-Croatian preserves the system nearly unchanged (even more so in the conservative Chakavian dialect); on the other, Macedonian has basically lost the system in its entirety. Between them are found numerous variations:

Similarly, Slavic languages have extensive morphophonemic alternations in their derivational and inflectional morphology, including between velar and postalveolar consonants, front and back vowels, and between a vowel and no vowel.

The following is a very brief selection of cognates in basic vocabulary across the Slavic language family, which may serve to give an idea of the sound changes involved. This is not a list of translations: cognates have a common origin, but their meaning may be shifted and loanwords may have replaced them.

Most languages of the former Soviet Union and of some neighbouring countries (for example, Mongolian) are significantly influenced by Russian, especially in vocabulary. The Romanian, Albanian, and Hungarian languages show the influence of the neighboring Slavic nations, especially in vocabulary pertaining to urban life, agriculture, and crafts and trade—the major cultural innovations at times of limited long-range cultural contact. In each one of these languages, Slavic lexical borrowings represent at least 15% of the total vocabulary. However, Romanian has much lower influence from Slavic than Albanian or Hungarian. This is potentially because Slavic tribes crossed and partially settled the territories inhabited by ancient Illyrians and Vlachs on their way to the Balkans.

Although also spoken in neighbouring lands, the Germanic languages show less significant Slavic influence, partly because Slavic migrations were mostly headed south rather than west. Slavic tribes did push westwards into Germanic territory, but borrowing for the most part seems to have been from Germanic to Slavic rather than the other way: for instance, the now-extinct Polabian language was heavily influenced by German, far more than any living Slavic language today. The Slavic contributions to Germanic languages remains a moot question, though Max Vasmer, a specialist in Slavic etymology, has claimed that there were no Slavic loans into Proto-Germanic. The only Germanic languages that shows significant Slavic influence are Yiddish and the historical colonial dialects of German that were spoken East of the Oder–Neisse line, such as Silesian German (formerly spoken in Silesia and South of East Prussia) and the Eastern varieties of East Low German, with the exception of Low Prussian, which had a strong Baltic substratum. Modern Dutch slang, especially the Amsterdam dialect, borrowed much from Yiddish in turn. However, there are isolated Slavic loans (mostly recent) into other Germanic languages. For example, the word for "border" (in modern German "Grenze", Dutch "grens") was borrowed from the Common Slavic "granica". There are, however, many cities and villages of Slavic origin in Eastern Germany, the largest of which are Berlin, Leipzig and Dresden. English derives "quark" (a kind of cheese, not the subatomic particle) from the German "Quark", which in turn is derived from the Slavic "tvarog", which means "curd". Many German surnames, particularly in Eastern Germany and Austria, are Slavic in origin. Swedish also has "torg" (market place) from Old Russian "tъrgъ" or Polish "targ", "tolk" (interpreter) from Old Slavic "tlŭkŭ", and "pråm" (barge) from West Slavonic "pramŭ".

The Czech word is now found in most languages worldwide, and the word , probably also from Czech, is found in many European languages, such as Greek "".

A well-known Slavic word in almost all European languages is vodka, a borrowing from Russian "водка" ("vodka") – which itself was borrowed from Polish "wódka" (lit. "little water"), from common Slavic "voda" ("water", cognate to the English word) with the diminutive ending "-ka". Owing to the medieval fur trade with Northern Russia, Pan-European loans from Russian include such familiar words as "sable". The English word "vampire" was borrowed (perhaps via French "vampire") from German "Vampir", in turn derived from Serbian "vampir", continuing Proto-Slavic "*ǫpyrь", although Polish scholar K. Stachowski has argued that the origin of the word is early Slavic "*vąpěrь", going back to Turkic "oobyr". Several European languages, including English, have borrowed the word "polje" (meaning "large, flat plain") directly from the former Yugoslav languages (i.e. Slovene, Croatian, and Serbian). During the heyday of the USSR in the 20th century, many more Russian words became known worldwide: "da", "Soviet", "sputnik", "perestroika", "glasnost", "kolkhoz", etc. Also in the English language borrowed from Russian is "samovar" (lit. "self-boiling") to refer to the specific Russian tea urn.

The following tree for the Slavic languages derives from the Ethnologue report for Slavic languages. It includes the ISO 639-1 and ISO 639-3 codes where available.
East Slavic languages:

West Slavic languages:

South Slavic languages:

Para- and supranational languages





</doc>
<doc id="26758" url="https://en.wikipedia.org/wiki?curid=26758" title="SGI">
SGI

SGI may refer to:





</doc>
<doc id="26764" url="https://en.wikipedia.org/wiki?curid=26764" title="International System of Units">
International System of Units

The International System of Units (SI, abbreviated from the French ") is the modern form of the metric system and is the most widely used system of measurement, based on the International System of Quantities. It comprises a coherent system of units of measurement built on seven base units, which are the second, metre, kilogram, ampere, kelvin, mole, candela, and a set of twenty prefixes to the unit names and unit symbols that may be used when specifying multiples and fractions of the units. The system also specifies names for 22 derived units, such as lumen and watt, for other common physical quantities.

The units chosen for the base quantities have been defined in terms of invariant constants of nature, such as the speed of light in vacuum and the charge of the electron, which can be observed and measured with great accuracy. Seven constants are used in various combinations to define the seven base units. Prior to 2019, artefacts were used instead of some of these constants, the last being the International Prototype of the Kilogram, a cylinder of platinum-iridium. Concern regarding its stability led to a revision of the definition of the base units entirely in terms of constants of nature, which was put into effect on 20 May 2019.

Derived units may be defined in terms of base units or other derived units. They are adopted to facilitate measurement of diverse quantities. The SI is intended to be an evolving system; units and prefixes are created and unit definitions are modified through international agreement as the technology of measurement progresses and the precision of measurements improves. The most recently named derived unit, the katal, was defined in 1999.

The reliability of the SI depends not only on the precise measurement of standards for the base units in terms of various physical constants of nature, but also on precise definition of those constants. The set of underlying constants is modified as more stable constants are found, or may be more precisely measured. For example, in 1983 the metre was redefined as the distance that light propagates in vacuum in a given fraction of a second, thus making the value of the speed of light in terms of the defined units exact.
The motivation for the development of the SI was the diversity of units that had sprung up within the centimetre–gram–second (CGS) systems (specifically the inconsistency between the systems of electrostatic units and electromagnetic units) and the lack of coordination between the various disciplines that used them. The General Conference on Weights and Measures (French: " – CGPM), which was established by the Metre Convention of 1875, brought together many international organisations to establish the definitions and standards of a new system and to standardise the rules for writing and presenting measurements. The system was published in 1960 as a result of an initiative that began in 1948. It is based on the metre–kilogram–second system of units (MKS) rather than any variant of the CGS.

The International System of Units consists of a set of base units, derived units, and a set of decimal-based multipliers that are used as prefixes. The units, excluding prefixed units, form a coherent system of units, which is based on a system of quantities in such a way that the equations between the numerical values expressed in coherent units have exactly the same form, including numerical factors, as the corresponding equations between the quantities. For example, 1 N = 1 kg × 1 m/s says that "one" newton is the force required to accelerate a mass of "one" kilogram at "one" metre per second squared, as related through the principle of coherence to the equation relating the corresponding quantities: .

Derived units apply to derived quantities, which may by definition be expressed in terms of base quantities, and thus are not independent; for example, electrical conductance is the inverse of electrical resistance, with the consequence that the siemens is the inverse of the ohm, and similarly, the ohm and siemens can be replaced with a ratio of an ampere and a volt, because those quantities bear a defined relationship to each other. Other useful derived quantities can be specified in terms of the SI base and derived units that have no named units in the SI system, such as acceleration, which is defined in SI units as m/s.

The SI base units are the building blocks of the system and all the other units are derived from them.

The derived units in the SI are formed by powers, products, or quotients of the base units and are potentially unlimited in number. Derived units are associated with derived quantities; for example, velocity is a quantity that is derived from the base quantities of time and length, and thus the SI derived unit is metre per second (symbol m/s). The dimensions of derived units can be expressed in terms of the dimensions of the base units.

Combinations of base and derived units may be used to express other derived units. For example, the SI unit of force is the newton (N), the SI unit of pressure is the pascal (Pa)—and the pascal can be defined as one newton per square metre (N/m).

Prefixes are added to unit names to produce multiples and submultiples of the original unit. All of these are integer powers of ten, and above a hundred or below a hundredth all are integer powers of a thousand. For example, "kilo-" denotes a multiple of a thousand and "milli-" denotes a multiple of a thousandth, so there are one thousand millimetres to the metre and one thousand metres to the kilometre. The prefixes are never combined, so for example a millionth of a metre is a "micrometre", not a millimillimetre. Multiples of the kilogram are named as if the gram were the base unit, so a millionth of a kilogram is a "milligram", not a microkilogram. When prefixes are used to form multiples and submultiples of SI base and derived units, the resulting units are no longer coherent.

The BIPM specifies 20 prefixes for the International System of Units (SI):

Many non-SI units continue to be used in the scientific, technical, and commercial literature. Some units are deeply embedded in history and culture, and their use has not been entirely replaced by their SI alternatives. The CIPM recognised and acknowledged such traditions by compiling a list of non-SI units accepted for use with SI:

Some units of time, angle, and legacy non-SI units have a long history of use. Most societies have used the solar day and its non-decimal subdivisions as a basis of time and, unlike the foot or the pound, these were the same regardless of where they were being measured. The radian, being of a revolution, has mathematical advantages but is rarely used for navigation. Further, the units used in navigation around the world are similar. The tonne, litre, and hectare were adopted by the CGPM in 1879 and have been retained as units that may be used alongside SI units, having been given unique symbols. The catalogued units are given below:

These units are used in combination with SI units in common units such as the kilowatt-hour (1 kW⋅h = 3.6 MJ).

The basic units of the metric system, as originally defined, represented common quantities or relationships in nature. They still do – the modern precisely defined quantities are refinements of definition and methodology, but still with the same magnitudes. In cases where laboratory precision may not be required or available, or where approximations are good enough, the original definitions may suffice.


The symbols for the SI units are intended to be identical, regardless of the language used, but names are ordinary nouns and use the character set and follow the grammatical rules of the language concerned. Names of units follow the grammatical rules associated with common nouns: in English and in French they start with a lowercase letter (e.g., newton, hertz, pascal), even when the unit is named after a person and its symbol begins with a capital letter. This also applies to "degrees Celsius", since "degree" is the beginning of the unit. The only exceptions are in the beginning of sentences and in headings and publication titles. The English spelling for certain SI units differs: US English uses the spelling "deka-", "meter", and "liter", whilst International English more commonly uses "deca-", "metre", and "litre".

Although the writing of unit names is language-specific, the writing of unit symbols and the values of quantities is consistent across all languages and therefore the SI Brochure has specific rules in respect of writing them. The guideline produced by the National Institute of Standards and Technology (NIST) clarifies language-specific areas in respect of American English that were left open by the SI Brochure, but is otherwise identical to the SI Brochure.

General rules for writing SI units and quantities apply to text that is either handwritten or produced using an automated process:


The rules covering printing of quantities and units are part of ISO 80000-1:2009.

Further rules are specified in respect of production of text using printing presses, word processors, typewriters, and the like.

The quantities and equations that provide the context in which the SI units are defined are now referred to as the "International System of Quantities" (ISQ).
The system is based on the quantities underlying each of the seven base units of the SI. Other quantities, such as area, pressure, and electrical resistance, are derived from these base quantities by clear non-contradictory equations. The ISQ defines the quantities that are measured with the SI units. The ISQ is defined in the international standard ISO/IEC 80000, and was finalised in 2009 with the publication of ISO 80000-1.

Metrologists carefully distinguish between the definition of a unit and its realisation. The definition of each base unit of the SI is drawn up so that it is unique and provides a sound theoretical basis on which the most accurate and reproducible measurements can be made. The realisation of the definition of a unit is the procedure by which the definition may be used to establish the value and associated uncertainty of a quantity of the same kind as the unit. A description of the "mise en pratique" of the base units is given in an electronic appendix to the SI Brochure.

The published "mise en pratique" is not the only way in which a base unit can be determined: the SI Brochure states that "any method consistent with the laws of physics could be used to realise any SI unit." In the current (2016) exercise to overhaul the definitions of the base units, various consultative committees of the CIPM have required that more than one "mise en pratique" shall be developed for determining the value of each unit. In particular:

The International Bureau of Weights and Measures (BIPM) has described SI as "the modern form of metric system". Changing technology has led to an evolution of the definitions and standards that has followed two principal strands – changes to SI itself, and clarification of how to use units of measure that are not part of SI but are still nevertheless used on a worldwide basis.

Since 1960 the CGPM has made a number of changes to the SI to meet the needs of specific fields, notably chemistry and radiometry. These are mostly additions to the list of named derived units, and include the "mole" (symbol mol) for an amount of substance, the "pascal" (symbol Pa) for pressure, the "siemens" (symbol S) for electrical conductance, the "becquerel" (symbol Bq) for "activity referred to a radionuclide", the "gray" (symbol Gy) for ionising radiation, the "sievert" (symbol Sv) as the unit of dose equivalent radiation, and the "katal" (symbol kat) for catalytic activity.

Acknowledging the advancement of precision science at both large and small scales, the range of defined prefixes pico- (10) to tera- (10) was extended to 10 to 10.

The 1960 definition of the standard metre in terms of wavelengths of a specific emission of the krypton 86 atom was replaced with the distance that light travels in a vacuum in exactly second, so that the speed of light is now an exactly specified constant of nature.

A few changes to notation conventions have also been made to alleviate lexicographic ambiguities. An analysis under the aegis of CSIRO, published in 2009 by the Royal Society, has pointed out the opportunities to finish the realisation of that goal, to the point of universal zero-ambiguity machine readability.

After the metre was redefined in 1960, the kilogram remained the only SI base unit directly based on a specific physical artefact, the International Prototype of the Kilogram (IPK), for its definition and thus the only unit that was still subject to periodic comparisons of national standard kilograms with the IPK. During the 2nd and 3rd Periodic Verification of National Prototypes of the Kilogram, a significant divergence had occurred between the mass of the IPK and all of its official copies stored around the world: the copies had all noticeably increased in mass with respect to the IPK. During "extraordinary verifications" carried out in 2014 preparatory to redefinition of metric standards, continuing divergence was not confirmed. Nonetheless, the residual and irreducible instability of a physical IPK undermined the reliability of the entire metric system to precision measurement from small (atomic) to large (astrophysical) scales.

A proposal was made that:

In 2015, the CODATA task group on fundamental constants announced special submission deadlines for data to compute the final values for the new definitions.

The new definitions were adopted at the 26th CGPM on 16 November 2018, and came into effect on 20 May 2019. The change was adopted by the European Union through Directive (EU) 2019/1258.

The units and unit magnitudes of the metric system which became the SI were improvised piecemeal from everyday physical quantities starting in the mid-18th century. Only later were they moulded into an orthogonal coherent decimal system of measurement.

The degree centigrade as a unit of temperature resulted from the scale devised by Swedish astronomer Anders Celsius in 1742. His scale counter-intuitively designated 100 as the freezing point of water and 0 as the boiling point. Independently, in 1743, the French physicist Jean-Pierre Christin described a scale with 0 as the freezing point of water and 100 the boiling point. The scale became known as the centi-grade, or 100 gradations of temperature, scale.

The metric system was developed from 1791 onwards by a committee of the French Academy of Sciences, commissioned to create a unified and rational system of measures. The group, which included preeminent French men of science, used the same principles for relating length, volume, and mass that had been proposed by the English clergyman John Wilkins in 1668 and the concept of using the Earth's meridian as the basis of the definition of length, originally proposed in 1670 by the French abbot Mouton.

In March 1791, the Assembly adopted the committee's proposed principles for the new decimal system of measure including the metre defined to be 1/10,000,000 of the length of the quadrant of earth's meridian passing through Paris, and authorised a survey to precisely establish the length of the meridian. In July 1792, the committee proposed the names "metre", "are", "litre" and "grave" for the units of length, area, capacity, and mass, respectively. The committee also proposed that multiples and submultiples of these units were to be denoted by decimal-based prefixes such as "centi" for a hundredth and "kilo" for a thousand.
Later, during the process of adoption of the metric system, the Latin "gramme" and "kilogramme", replaced the former provincial terms "gravet" (1/1000 "grave") and "grave". In June 1799, based on the results of the meridian survey, the standard "mètre des Archives" and "kilogramme des Archives" were deposited in the French National Archives. Subsequently, that year, the metric system was adopted by law in France. 

During the first half of the 19th century there was little consistency in the choice of preferred multiples of the base units: typically the myriametre ( metres) was in widespread use in both France and parts of Germany, while the kilogram ( grams) rather than the myriagram was used for mass.

In 1832, the German mathematician Carl Friedrich Gauss, assisted by Wilhelm Weber, implicitly defined the second as a base unit when he quoted the Earth's magnetic field in terms of millimetres, grams, and seconds. Prior to this, the strength of the Earth's magnetic field had only been described in relative terms. The technique used by Gauss was to equate the torque induced on a suspended magnet of known mass by the Earth's magnetic field with the torque induced on an equivalent system under gravity. The resultant calculations enabled him to assign dimensions based on mass, length and time to the magnetic field.

A candlepower as a unit of illuminance was originally defined by an 1860 English law as the light produced by a pure spermaceti candle weighing pound (76 grams) and burning at a specified rate. Spermaceti, a waxy substance found in the heads of sperm whales, was once used to make high-quality candles. At this time the French standard of light was based upon the illumination from a Carcel oil lamp. The unit was defined as that illumination emanating from a lamp burning pure rapeseed oil at a defined rate. It was accepted that ten standard candles were about equal to one Carcel lamp.

A French-inspired initiative for international cooperation in metrology led to the signing in 1875 of the Metre Convention, also called Treaty of the Metre, by 17 nations. Initially the convention only covered standards for the metre and the kilogram. In 1921, the Metre Convention was extended to include all physical units, including the ampere and others thereby enabling the CGPM to address inconsistencies in the way that the metric system had been used.

A set of 30 prototypes of the metre and 40 prototypes of the kilogram, in each case made of a 90% platinum-10% iridium alloy, were manufactured by British metallurgy specialty firm and accepted by the CGPM in 1889. One of each was selected at random to become the International prototype metre and International prototype kilogram that replaced the "mètre des Archives" and "kilogramme des Archives" respectively. Each member state was entitled to one of each of the remaining prototypes to serve as the national prototype for that country.

The treaty also established a number of international organisations to oversee the keeping of international standards of measurement:
In the 1860s, James Clerk Maxwell, William Thomson (later Lord Kelvin) and others working under the auspices of the British Association for the Advancement of Science, built on Gauss's work and formalised the concept of a coherent system of units with base units and derived units christened the centimetre–gram–second system of units in 1874. The principle of coherence was successfully used to define a number of units of measure based on the CGS, including the erg for energy, the dyne for force, the barye for pressure, the poise for dynamic viscosity and the stokes for kinematic viscosity.

In 1879, the CIPM published recommendations for writing the symbols for length, area, volume and mass, but it was outside its domain to publish recommendations for other quantities. Beginning in about 1900, physicists who had been using the symbol "μ" (mu) for "micrometre" or "micron", "λ" (lambda) for "microlitre", and "γ" (gamma) for "microgram" started to use the symbols "μm", "μL" and "μg".

At the close of the 19th century three different systems of units of measure existed for electrical measurements: a CGS-based system for electrostatic units, also known as the Gaussian or ESU system, a CGS-based system for electromechanical units (EMU) and an International system based on units defined by the Metre Convention. for electrical distribution systems. 
Attempts to resolve the electrical units in terms of length, mass, and time using dimensional analysis was beset with difficulties—the dimensions depended on whether one used the ESU or EMU systems. This anomaly was resolved in 1901 when Giovanni Giorgi published a paper in which he advocated using a fourth base unit alongside the existing three base units. The fourth unit could be chosen to be electric current, voltage, or electrical resistance. Electric current with named unit 'ampere' was chosen as the base unit, and the other electrical quantities derived from it according to the laws of physics. This became the foundation of the MKS system of units.

In the late 19th and early 20th centuries, a number of non-coherent units of measure based on the gram/kilogram, centimetre/metre, and second, such as the "Pferdestärke" (metric horsepower) for power, the darcy for permeability and "millimetres of mercury" for barometric and blood pressure were developed or propagated, some of which incorporated standard gravity in their definitions.

At the end of the Second World War, a number of different systems of measurement were in use throughout the world. Some of these systems were metric system variations; others were based on customary systems of measure, like the U.S customary system and Imperial system of the UK and British Empire.

In 1948, the 9th CGPM commissioned a study to assess the measurement needs of the scientific, technical, and educational communities and "to make recommendations for a single practical system of units of measurement, suitable for adoption by all countries adhering to the Metre Convention". This working document was "Practical system of units of measurement". Based on this study, the 10th CGPM in 1954 defined an international system derived from six base units including units of temperature and optical radiation in addition to those for the MKS system mass, length, and time units and Giorgi's current unit. Six base units were recommended: the metre, kilogram, second, ampere, degree Kelvin, and candela.

The 9th CGPM also approved the first formal recommendation for the writing of symbols in the metric system when the basis of the rules as they are now known was laid down. These rules were subsequently extended and now cover unit symbols and names, prefix symbols and names, how quantity symbols should be written and used, and how the values of quantities should be expressed.

In 1960, the 11th CGPM synthesised the results of the 12-year study into a set of 16 resolutions. The system was named the "International System of Units", abbreviated SI from the French name, .

When Maxwell first introduced the concept of a coherent system, he identified three quantities that could be used as base units: mass, length, and time. Giorgi later identified the need for an electrical base unit, for which the unit of electric current was chosen for SI. Another three base units (for temperature, amount of substance, and luminous intensity) were added later.

The early metric systems defined a unit of weight as a base unit, while the SI defines an analogous unit of mass. In everyday use, these are mostly interchangeable, but in scientific contexts the difference matters. Mass, strictly the inertial mass, represents a quantity of matter. It relates the acceleration of a body to the applied force via Newton's law, : force equals mass times acceleration. A force of 1 N (newton) applied to a mass of 1 kg will accelerate it at 1 m/s. This is true whether the object is floating in space or in a gravity field e.g. at the Earth's surface. Weight is the force exerted on a body by a gravitational field, and hence its weight depends on the strength of the gravitational field. Weight of a 1 kg mass at the Earth's surface is ; mass times the acceleration due to gravity, which is 9.81 newtons at the Earth's surface and is about 3.5 newtons at the surface of Mars. Since the acceleration due to gravity is local and varies by location and altitude on the Earth, weight is unsuitable for precision measurements of a property of a body, and this makes a unit of weight unsuitable as a base unit.






</doc>
<doc id="26766" url="https://en.wikipedia.org/wiki?curid=26766" title="Sapiens">
Sapiens

Sapiens, a Latin word meaning wise, may refer to :




</doc>
<doc id="26768" url="https://en.wikipedia.org/wiki?curid=26768" title="Sirenia">
Sirenia

The Sirenia, commonly referred to as sea-cows or sirenians, are an order of fully aquatic, herbivorous mammals that inhabit swamps, rivers, estuaries, marine wetlands, and coastal marine waters. The Sirenia currently comprise the families Dugongidae (the dugong and, historically, Steller's sea cow) and Trichechidae (manatees) with a total of four species. The Protosirenidae (Eocene sirenians) and Prorastomidae (terrestrial sirenians) families are extinct. Sirenians are classified in the clade Paenungulata, alongside the elephants and the hyraxes, and evolved in the Eocene 50 million years ago. The Dugongidae diverged from the Trichechidae in the late Eocene or early Oligocene.

Sirenians grow to between in length and in weight. The now extinct Steller's sea cow was the largest sirenian to have lived, and could reach lengths of and weights of . Sirenians have a large, fusiform body to reduce drag through the water. They have heavy bones that act as ballasts to counteract the buoyancy of their blubber. They have a thin layer of blubber and consequently are sensitive to temperature fluctuations, which cause migrations when water temperatures dip too low. Sirenians are slow-moving, typically coasting at , but they can reach in short bursts. They use their strong lips to pull out seagrasses, consuming 10–15% of their body weight per day.

While breathing, they hold just their nostrils above the surface, sometimes standing on their tails to do so. Sirenians typically inhabit warm, shallow, coastal waters, or rivers. They are mainly herbivorous, but have been known to consume animals such as birds and jellyfish. Males typically mate with more than one female (polygyny), and may participate in lek mating. Sirenians are K-selected, and display parental care.

The meat, oil, bones, and skins are valuable items sold in markets. Mortality is often caused by direct hunting by humans or other human-induced causes, such as habitat destruction, entanglement in fishing gear, and watercraft collisions. Steller's sea cow went extinct due to overhunting in 1768.

Sirenia, commonly sirenians, are also referred to by the common name sirens, deriving from the sirens of Greek mythology. This comes from a legend about their discovery, involving lonely sailors mistaking them for mermaids. "Seekoei" (sea cow) is also the name for a hippopotamus in Afrikaans.

Sirenians are classified within the cohort Afrotheria in the clade Paenungulata, alongside Proboscidea (elephants), Hyracoidea (hyraxes), Embrithopoda, Desmostylia, and Afroinsectiphilia. This clade was first established by George Gaylord Simpson in 1945 based on anatomical evidence, such as testicondy and similar fetal development. The Paenungulata, along with the Afrotheria, are one of the most well-supported mammalian clades in molecular phylogeny. Sirenia, Proboscidae, and Desmotylia are grouped together in the clade Tethytheria. Based on morphological similarities, Tethytheria, Perissodactyla, and Hyracoidea were considered to be grouped together as the Altungulata, but this has been invalidated by molecular data.

The evolution of sirenians is characterized by the appearance of several traits, which are found in all sirenians (monophyly). The nostrils are large and retracted, the upper-jaw bone contacts the frontal bone, the sagittal crest is missing, the mastoid fills the supratemporal fenestra (an opening on the top of the skull), a drop-like ectotympanic (a bony ring that holds the ear drum), and pachyosteosclerotic (dense and bulky) bones.

Sirenians first appeared in the fossil record in the Early Eocene and significantly diversified throughout the epoch. They inhabited rivers, estuaries, and nearshore marine waters. Sirenians, unlike other marine mammals such as cetaceans, lived in the New World. One of the earliest aquatic sirenians discovered is "Prorastomus" which dates back to 40 million years ago, and the first known sirenian, the quadruped "Pezosiren", lived 50 million years ago. An ancient sirenian fossil of a petrosal bone was found in Tunisia, dating back to approximately the same time as "Prorastomus". This is the oldest sirenian fossil to be found in Africa and supports molecular data suggesting that sirenians may have originated in Africa. Prorastomidae and Protosirenidae, the earliest sirenian families, consisted of pig-like amphibious creatures who died out at the end of the Eocene. When the Dugongidae appeared at this time, sirenians had evolved the characteristics of modern variety, including an aquatic streamlined body with flipper-like front legs with no hind limbs, and a powerful tail with horizontal caudal fins which uses an up-and-down motion to move them through the water.

The last of the sirenian families to appear, Trichechidae, apparently arose from early dugongids in the late Eocene or early Oligocene. It is a monophyletic taxon. In 1994, the family was expanded to include not only the subfamily Trichechinae ("Potamosiren", "Ribodon", and "Trichechus"), but also Miosireninae ("Anomotherium" and "Miosiren"). The African manatee and the West Indian manatee are more closely related to each other than to the Amazonian manatee.

Dugongidae comprises the subfamilies Dugonginae and Hydrodamalinae (which are both monophyletic) and the paraphyletic Halitheriinae. The tusks of modern-day dugongs may have originally been used for digging, but they are now used for social interaction. The genus "Dugong" probably originated in the Indo-Pacific area.

The tail fluke of a dugong is notched and similar to those of dolphins, whereas the tail fluke of manatees is paddle-shaped. The fluke is raised up and down in long strokes to move the animal forward, or twisted to turn. The forelimbs are paddle-like flippers which aid in turning and slowing. Unlike manatees, the dugong lacks nails on its flippers, which are only 15% of a dugong's body length. Manatees generally glide at speeds of , but can reach speeds of in short bursts. The body is fusiform to reduce drag in the water. Like cetaceans, the hind limbs are internal and vestigial. The snout is angled downwards to aid in bottom-feeding. Sirenians typically make two- to three-minute dives, but manatees can hold their breath for up to 15 minutes while resting and dugongs up to six minutes. They may stand on their tail to hold their head above water.
Much like elephants, manatees are polyphyodonts, and continuously replace their teeth from the back of the jaw. Adults lack incisors, canines, and premolars, and instead have 8 to 10 cheek teeth in their mouth. Manatees have an infinite supply of teeth moving in from the back and shedding in the front, which are continuously formed by a dental capsule behind the tooth-row. These teeth are constantly worn down by the abrasive vascular plants they forage, particularly aquatic grasses. Unlike in manatees, the dugong's teeth do not continually grow back via horizontal tooth replacement. The dugong has two tusks which emerge in males during puberty, and sometime later in life for females after reaching the base of the premaxilla. The number of growth layer groups in a tusk indicates the age of a dugong.

Sirenians exhibit pachyostosis, a condition in which the ribs and other long bones are solid and contain little or no bone marrow. They have among the densest bones in the animal kingdom, which may be used as ballast, counteracting the buoyancy effect of their blubber and help keep sirenians suspended slightly below the water's surface. Manatees do not possess blubber, per se, but rather have thick skin, and, consequently, are sensitive to temperature changes. Likewise, they often migrate to warmer waters whenever the water temperature dips below . The lungs of sirenians are unlobed; they, along with the diaphragm, extend the entire length of the vertebral column, which help them control their buoyancy and reduce tipping in the water.

Extant sirenians grow to between in length and can weigh up to . Steller's sea cow was the largest sirenian to have lived, and could reach lengths of , and could weigh in at . A dugong's brain weighs a maximum of , about 0.1% of the animal's body weight. The body of sirenians is sparsely covered in short hair (vibrissae), except for on the muzzle, which may allow for tactile interpretation of their environment. Manatees are the only creatures to exhibit corneal avascularity, and lack blood vessels in the cornea, which prevents optical clarity and vision. This may be the result of irritations from or protection against their hypotonic freshwater environment.

Sirenians are referred to as "sea cows" because their diet consists mainly of seagrass. Dugongs sift through the seafloor in search of seagrasses. Dugongs use their sense of smell to find the seagrass because their eyesight is poor making it difficult to use their sense of sight to find food. They ingest the whole plant, including the roots, although they will feed on just the leaves if this is not possible. Manatees, in particular the West Indian manatee, are known to consume over 60 different freshwater and saltwater plants, such as shoalweed, water lettuce, muskgrass, manatee grass, and turtle grass. Using their divided upper lip, an adult manatee will commonly eat up to 10–15% of their body weight, or , per day, which requires the manatee to graze for several hours per day. However, 10% of the diet of the African manatee is fish and mollusks. Manatees have been known to eat small amounts of fish from nets. As opposed to bulk feeding, dugongs target high-nitrogen grasses to maximize nutrient intake, and, although almost completely herbivorous, dugongs will occasionally eat invertebrates such as jellyfish, sea squirts, and shellfish. Some populations of dugongs, such as the one in Moreton Bay, Australia, are omnivorous, feeding on invertebrates such as polychaetes or marine algae when their supply of seagrasses decrease. In other dugong populations in western and eastern Australia, there is evidence that dugongs actively seek out large invertebrates. Populations of Amazonian manatees become restricted to lakes during the July–August dry season when water levels begin to fall, and are thought to fast during this period. Their large fat reserves and low metabolic rates—only 36% of the usual placental mammal metabolic rate—allow them to survive for up to seven months with little or no food.

Despite being mostly solitary, sirenians congregate in groups while females are in estrus. These groups usually include one female with multiple males. Sirenians are K-selectors, so, despite the longevity, females give birth only a few times during their life and invest considerable parental care in their young. Dugongs generally gather in groups of less than a dozen individuals for one to two days. Since they congregate in turbid waters, little is known about their reproductive behavior. The males are often seen with scars, and the tusks on dugongs grow in first for males, suggesting they are important in lekking. They have also been known to lunge at each other. The age when a female first gives birth is disputed, ranging anywhere from six to seventeen years. The time between births is unclear, with estimates ranging from 2 to 7 years. Though, in Sarasota, Florida 113 manatee were observed of known gender. Of these 113, there were 53 females that produced at least 55 calves during a five-year period of observation.

Manatees can reach sexual maturity as early as two to five years of age. Manatee gestation length is around one year, and then they lactate for one to two years. West Indian manatees and African manatees can breed year-round, and a female will mate with multiple male partners. Amazonian manatees have a breeding season, usually mating when the river levels begin to rise, which varies from place to place.

Manatees, in comparison to other marine species, tend to do well in a captive environment. In 1875, there started to be success in keeping manatees thriving in captivity. It is not only about keeping them alive but making sure they get everything they need without having to be in their natural environment. It is extremely difficult to replicate a natural environment in an enclosed setting but places that work in conservation, rescue, attraction, and rehabilitation do their best to get as close as they can to a natural environment. It is not only difficult for these places to make an enclosure like their natural environment but also provide the food they would have there. Manatees in a captive environment, due to what they eat and the quantities of which they eat, cannot consume the diet they would in a natural environment. The amount of food and what they specifically eat is not cost effective for places to provide for them.
Manatees that are in captivity for rehabilitate purposes or simply can not survive in the wild due to injury or illness benefit from being in these programs. When captivity is not beneficial is when healthy manatees are being pulled out of their natural environments to be put on display for human enjoyment. In some cases, the entertainment industry does not provide the best care for them like a conservation rescue would. Their main priority is the amount of money they make from people wanting to see the manatees. Manatee population numbers are lower than they should be, so it is not helping the conservation efforts when manatees are being taken out of nature for monetary reasons. Rehabilitation places for manatees are beneficial for the conservation of manatees despite them being in captivity. The rescue injured or sick manatees and bring them back to health. During this time, they collect as much information they can before releasing them back into the wild. They use this information to learn about manatees in the wild and better protect them injuries or illness. 

The three extant manatee species (family Trichechidae) and the dugong (family Dugongidae) are rated as vulnerable on the IUCN Red List of endangered species. All four are vulnerable to extinction from habitat loss and other negative impacts related to human population growth and coastal development. Steller's sea cow, extinct since 1786, was hunted to extinction by humans.

The meat, oil, bones, and skin of manatees are valuable items. In some countries, such as Nigeria and Cameroon, African manatees are sold to zoos, aquariums, and online as pets, sometimes being shipped internationally. Though illegal, lack of law enforcement in these areas induce poaching. Some residents of West African countries, such as Mali and Chad, depend on the oil of the African manatee to cure ailments such as ear infections, rheumatism, and skin conditions. Hunting is the largest source of mortality in Amazonian manatees, and there are no management plans except for in Colombia. Amazonian manatees, especially calves, are sometimes illegally sold as pets, but there are several institutions that care for and rescue these orphans, with the possibility of their releasing into the wild. The body parts of dugongs are used as medicinal remedies across the Indian Ocean.

Manatees have also faced threats in Cuba, an area that is not always known for its manatee population. Manatees in Cuba have faced poaching, entanglement and pollution. The area has some of the most extensive and best manatee habitat in the Caribbean, but the population has been unable to thrive there for numerous reasons. Existing information about manatees in Cuba is limited and scarce, this makes it difficult to spread awareness which therefore enhances the situation of illegal poaching and entanglement within fishing nets in a majority of the coastal communities. Poaching of the manatees has been a significant issue since the 1970s when it was initially reported that the hunting was taking its toll on the manatee population in Cuba. In 1975 it was recorded that the manatees' status in Cuba was rare and declining at an alarming rate due to pollution and hunting. In 1996 manatees were placed under protection through the Fishery Decree law 164. This law provided penalties against those that manipulated, harm, or injured manatees. However, it was seen that the hunting of manatees in Cuba in the 1990s may have been the result of economic hardships in this country and the manatees were an alternative source of protein. Although there have been efforts made to protect the population of manatees in Cuba, it has not proven to be helpful or as impactful as those working to protect the population had hoped. Many of these areas are seen as "paper parks" or parks or protected areas that only exist due to their name and nothing else, and they do not have a significant impact on conversation and protections.
Environmental hazards induced by humans also puts sirenians at risk. Sirenians, especially the West Indian manatee, face high mortality from watercraft collision, and about half of all West Indian manatee deaths are caused by watercraft collisions. An increased usage of hydroelectric power and subsequent damming of rivers increase waterway traffic, which can lead to vessel collisions, and manatees may become entangled in navigational locks. The urbanized coastline of areas such as the Caribbean and Australia can result in the decline of seagrass populations. Reliable areas of warm water in Florida are generally the result of discharge from power plants, but newer plants with more efficient cooling systems may disrupt the pattern of warm water refuges, and an increased demand for artesian springs for water, the natural source of warm water, decreases the number of warm water refuges. Sirenians can be caught as bycatch from fisheries, and they can be seen as pests with the interference of local fishermen and the destruction of their nets. African manatees have also been known to venture into rice paddies and destroy the crops during the rainy season, and these confrontations with locals may lead to intentional culling of the manatees.
Red tide, scientifically known as Karenia brevis is a harmful algae bloom that releases toxins into the water killing many marine species. In 1982, numerous sick manatees were accounted for and researchers believe this was due to the accumulation brevetoxins in filter-feeding organisms attached to seagrass blades, which are a popular diet for manatees. Manatee die-offs from exposure to red tide toxins were recorded by the Florida Fish and Wildlife Conservation Commission in southwest Florida in 2002, 2003, 2005, 2007, and most recently in 2013. As of June 20, 2018 the current red tide bloom spreads from Pasco county to Collier County off the West coast of Florida. As of January 2018 there have been a total of 472 manatee deaths caused by this red tide along with water crafts, cold stress, and other factors.

Weather disasters and other natural occurrences are also sources of mortality. The West Indian manatee and Dugong face risks from hurricanes and cyclones, which are predicted to increase in the future. These storms may also damage seagrass populations. Exposure to brevetoxin from "Karenia brevis" during a red tide event are also sources of mortality; they may be able to be exposed to brevetoxin after a red tide has subsided, as it could settle on seagrasses. African manatees can become stranded during the dry season when rivers and lakes become too small or dry up completely.

All sirenians are protected by the Marine Mammal Protection Act of 1972 (MMPA), the Endangered Species Act of 1973 (ESA), and the Convention on the International Trade in Endangered Species of Wild Fauna and Flora (CITES). In addition to this, the four species are further protected by various specialty organizations. The Dugong is listed in the Convention on Biological Diversity (CBD), the Convention on Migratory Species, and the Coral Triangle Initiative. In Florida, manatees are protected by the Florida Manatee Sanctuary Act of 1978, which implements actions such as the limitation or prohibition of watercraft speeds where manatees exist. Marine mammal rehabilitation programs have been underway and regulated in the United States for more than 40 years. In 1973 injured and distressed manatees were rescued or aided within Florida. Eventually, the program was formalized into the Manatee Rescue, Rehabilitation, and Release Program managed by the USFWS. In 2012 the program became the Manatee Rescue/Rehabilitation Partnership (MRP) with permitting and oversight by the USFWS. From 1973 through 2014, this program rescued 1,619 manatees and 526 Florida manatees have been released.





</doc>
<doc id="26769" url="https://en.wikipedia.org/wiki?curid=26769" title="South America">
South America

South America is a continent in the Western Hemisphere, mostly in the Southern Hemisphere, with a relatively small portion in the Northern Hemisphere. It may also be considered a subcontinent of the Americas, which is how it is viewed in the Spanish and Portuguese-speaking regions of the Americas where Dutch and English are spoken officially. The reference to South America instead of other regions (like Latin America or the Southern Cone) has increased in the last decades due to changing geopolitical dynamics (in particular, the rise of Brazil).

It is bordered on the west by the Pacific Ocean and on the north and east by the Atlantic Ocean; North America and the Caribbean Sea lie to the northwest. It includes twelve sovereign states (Argentina, Bolivia, Brazil, Chile, Colombia, Ecuador, Guyana, Paraguay, Peru, Suriname, Uruguay, and Venezuela), a part of France (French Guiana), and a non-sovereign area (the Falkland Islands, a British Overseas Territory though this is disputed by Argentina). In addition to this, the ABC islands of the Kingdom of the Netherlands, Trinidad and Tobago, and Panama may also be considered part of South America.

South America has an area of 17,840,000 square kilometers (6,890,000 sq mi). Its population has been estimated at more than floor(/1e6) million. South America ranks fourth in area (after Asia, Africa, and North America) and fifth in population (after Asia, Africa, Europe, and North America). Brazil is by far the most populous South American country, with more than half of the continent's population, followed by Colombia, Argentina, Venezuela and Peru. In recent decades Brazil has also concentrated half of the region's GDP and has become a first regional power.

Most of the population lives near the continent's western or eastern coasts while the interior and the far south are sparsely populated. The geography of western South America is dominated by the Andes mountains; in contrast, the eastern part contains both highland regions and vast lowlands where rivers such as the Amazon, Orinoco, and Paraná flow. Most of the continent lies in the tropics.

The continent's cultural and ethnic outlook has its origin with the interaction of indigenous peoples with European conquerors and immigrants and, more locally, with African slaves. Given a long history of colonialism, the overwhelming majority of South Americans speak Portuguese or Spanish, and societies and states reflect Western traditions.

South America occupies the southern portion of the Americas. The continent is generally delimited on the northwest by the Darién watershed along the Colombia–Panama border, although some may consider the border instead to be the Panama Canal. Geopolitically and geographically all of Panama – including the segment east of the Panama Canal in the isthmus – is typically included in North America alone and among the countries of Central America. Almost all of mainland South America sits on the South American Plate.

South America is home to the world's highest uninterrupted waterfall, Angel Falls in Venezuela; the highest single drop waterfall Kaieteur Falls in Guyana; the largest river by volume, the Amazon River; the longest mountain range, the Andes (whose highest mountain is Aconcagua at ); the driest non-polar place on earth, the Atacama Desert; the largest rainforest, the Amazon Rainforest; the highest capital city, La Paz, Bolivia; the highest commercially navigable lake in the world, Lake Titicaca; and, excluding research stations in Antarctica, the world's southernmost permanently inhabited community, Puerto Toro, Chile.

South America's major mineral resources are gold, silver, copper, iron ore, tin, and petroleum. These resources found in South America have brought high income to its countries especially in times of war or of rapid economic growth by industrialized countries elsewhere. However, the concentration in producing one major export commodity often has hindered the development of diversified economies. The fluctuation in the price of commodities in the international markets has led historically to major highs and lows in the economies of South American states, often causing extreme political instability. This is leading to efforts to diversify production to drive away from staying as economies dedicated to one major export.

South America is one of the most biodiverse continents on earth. South America is home to many interesting and unique species of animals including the llama, anaconda, piranha, jaguar, vicuña, and tapir. The Amazon rainforests possess high biodiversity, containing a major proportion of the Earth's species.

Brazil is the largest country in South America, encompassing around half of the continent's land area and population. The remaining countries and territories are divided among three regions: The Andean States, the Guianas and the Southern Cone.

Traditionally, South America also includes some of the nearby islands. Aruba, Bonaire, Curaçao, Trinidad, Tobago, and the federal dependencies of Venezuela sit on the northerly South American continental shelf and are often considered part of the continent. Geo-politically, the island states and overseas territories of the Caribbean are generally grouped as a part or subregion of North America, since they are more distant on the Caribbean Plate, even though San Andres and Providencia are politically part of Colombia and Aves Island is controlled by Venezuela.

Other islands that are included with South America are the Galápagos Islands that belong to Ecuador and Easter Island (in Oceania but belonging to Chile), Robinson Crusoe Island, Chiloé (both Chilean) and Tierra del Fuego (split in between Chile and Argentina). In the Atlantic, Brazil owns Fernando de Noronha, Trindade and Martim Vaz, and the Saint Peter and Saint Paul Archipelago, while the Falkland Islands are governed by the United Kingdom, whose sovereignty over the islands is disputed by Argentina. South Georgia and the South Sandwich Islands may be associated with either South America or Antarctica.
The distribution of the average temperatures in the region presents a constant regularity from the 30° of latitude south, when the isotherms tend, more and more, to be confused with the degrees of latitude.

In temperate latitudes, winters are milder and summers warmer than in North America. Because its most extensive part of the continent is in the equatorial zone, the region has more areas of equatorial plains than any other region.

The average annual temperatures in the Amazon basin oscillate around , with low thermal amplitudes and high rainfall indices. Between the Maracaibo Lake and the mouth of the Orinoco, predominates an equatorial climate of the type Congolese, that also includes parts of the Brazilian territory.

The east-central Brazilian plateau has a humid and warm tropical climate. The northern and eastern parts of the Argentine pampas have a humid subtropical climate with dry winters and humid summers of the Chinese type, while the western and eastern ranges have a subtropical climate of the dinaric type. At the highest points of the Andean region, climates are colder than the ones occurring at the highest point of the Norwegian fjords. In the Andean plateaus, the warm climate prevails, although it is tempered by the altitude, while in the coastal strip, there is an equatorial climate of the Guinean type. From this point until the north of the Chilean coast appear, successively, Mediterranean oceanic climate, temperate of the Breton type and, already in Tierra del Fuego, cold climate of the Siberian type.

The distribution of rainfall is related to the regime of winds and air masses. In most of the tropical region east of the Andes, winds blowing from the northeast, east and southeast carry moisture from the Atlantic, causing abundant rainfall. However, due to a consistently strong wind shear and a weak Intertropical Convergence Zone, South Atlantic tropical cyclones are rare. In the Orinoco Llanos and in the Guianas plateau, the precipitation levels go from moderate to high. The Pacific coast of Colombia and northern Ecuador are rainy regions, with Chocó in Colombia being the most rainy place in the world along with the northern slopes of Indian Himalayas. The Atacama Desert, along this stretch of coast, is one of the driest regions in the world. The central and southern parts of Chile are subject to extratropical cyclones, and most of the Argentine Patagonia is desert. In the pampas of Argentina, Uruguay and South of Brazil the rainfall is moderate, with rains well distributed during the year. The moderately dry conditions of the Chaco oppose the intense rainfall of the eastern region of Paraguay. In the semiarid coast of the Brazilian Northeast the rains are linked to a monsoon regime.

Important factors in the determination of climates are sea currents, such as the current Humboldt and Falklands. The equatorial current of the South Atlantic strikes the coast of the Northeast and there is divided into two others: the current of Brazil and a coastal current that flows to the northwest towards the Antilles, where there it moves towards northeast course thus forming the most Important and famous ocean current in the world, the Gulf Stream.

South America is believed to have been joined with Africa from the late Paleozoic Era to the early Mesozoic Era, until the supercontinent Pangaea began to rift and break apart about 225 million years ago. Therefore, South America and Africa share similar fossils and rock layers.

South America is thought to have been first inhabited by humans when people were crossing the Bering Land Bridge (now the Bering Strait) at least 15,000 years ago from the territory that is present-day Russia. They migrated south through North America, and eventually reached South America through the Isthmus of Panama.

The first evidence for the existence of the human race in South America dates back to about 9000 BC, when squashes, chili peppers and beans began to be cultivated for food in the highlands of the Amazon Basin. Pottery evidence further suggests that manioc, which remains a staple food today, was being cultivated as early as 2000 BC.

By 2000 BC, many agrarian communities had been settled throughout the Andes and the surrounding regions. Fishing became a widespread practice along the coast, helping establish fish as a primary source of food. Irrigation systems were also developed at this time, which aided in the rise of an agrarian society.

South American cultures began domesticating llamas, vicuñas, guanacos, and alpacas in the highlands of the Andes circa 3500 BC. Besides their use as sources of meat and wool, these animals were used for transportation of goods.

The rise of plant growing and the subsequent appearance of permanent human settlements allowed for the multiple and overlapping beginnings of civilizations in South America.

One of the earliest known South American civilizations was at Norte Chico, on the central Peruvian coast. Though a pre-ceramic culture, the monumental architecture of Norte Chico is contemporaneous with the pyramids of Ancient Egypt. Norte Chico governing class established a trade network and developed agriculture then followed by Chavín by 900 BC, according to some estimates and archaeological finds. Artifacts were found at a site called Chavín de Huantar in modern Peru at an elevation of . Chavín civilization spanned 900 BC to 300 BC.

In the central coast of Peru, around the beginning of the 1st millennium AD, Moche (100 BC – 700 AD, at the northern coast of Peru), Paracas and Nazca (400 BC – 800 AD, Peru) cultures flourished with centralized states with permanent militia improving agriculture through irrigation and new styles of ceramic art. At the Altiplano, Tiahuanaco or Tiwanaku (100 BC – 1200 AD, Bolivia) managed a large commercial network based on religion.

Around the 7th century, both Tiahuanaco and Wari or Huari Empire (600–1200, Central and northern Peru) expanded its influence to all the Andean region, imposing the Huari urbanism and Tiahuanaco religious iconography.

The Muisca were the main indigenous civilization in what is now Colombia. They established the Muisca Confederation of many clans, or "cacicazgos", that had a free trade network among themselves. They were goldsmiths and farmers.

Other important Pre-Columbian cultures include: the Cañaris (in south central Ecuador), Chimú Empire (1300–1470, Peruvian northern coast), Chachapoyas, and the Aymaran kingdoms (1000–1450, Western Bolivia and southern Peru).
Holding their capital at the great city of Cusco, the Inca civilization dominated the Andes region from 1438 to 1533. Known as "Tawantin suyu", and "the land of the four regions," in Quechua, the Inca Empire was highly distinct and developed. Inca rule extended to nearly a hundred linguistic or ethnic communities, some nine to fourteen million people connected by a 25,000 kilometer road system. Cities were built with precise, unmatched stonework, constructed over many levels of mountain terrain. Terrace farming was a useful form of agriculture.

The Mapuche in Central and Southern Chile resisted the European and Chilean settlers, waging the Arauco War for more than 300 years.

In 1494, Portugal and Spain, the two great maritime European powers of that time, on the expectation of new lands being discovered in the west, signed the Treaty of Tordesillas, by which they agreed, with the support of the Pope, that all the land outside Europe should be an exclusive duopoly between the two countries.

The treaty established an imaginary line along a north–south meridian 370 leagues west of the Cape Verde Islands, roughly 46° 37' W. In terms of the treaty, all land to the west of the line (known to comprise most of the South American soil) would belong to Spain, and all land to the east, to Portugal. As accurate measurements of longitude were impossible at that time, the line was not strictly enforced, resulting in a Portuguese expansion of Brazil across the meridian.

Beginning in the 1530s, the people and natural resources of South America were repeatedly exploited by foreign conquistadors, first from Spain and later from Portugal. These competing colonial nations claimed the land and resources as their own and divided it in colonies.

European infectious diseases (smallpox, influenza, measles, and typhus) – to which the native populations had no immune resistance – caused large-scale depopulation of the native population under Spanish control. Systems of forced labor, such as the haciendas and mining industry's mit'a also contributed to the depopulation. After this, African slaves, who had developed immunities to these diseases, were quickly brought in to replace them.

The Spaniards were committed to converting their native subjects to Christianity and were quick to purge any native cultural practices that hindered this end; however, many initial attempts at this were only partially successful, as native groups simply blended Catholicism with their established beliefs and practices. Furthermore, the Spaniards brought their language to the degree they did with their religion, although the Roman Catholic Church's evangelization in Quechua, Aymara, and Guaraní actually contributed to the continuous use of these native languages albeit only in the oral form.

Eventually, the natives and the Spaniards interbred, forming a mestizo class. At the beginning, many mestizos of the Andean region were offspring of Amerindian mothers and Spanish fathers. After independence, most mestizos had native fathers and European or mestizo mothers.

Many native artworks were considered pagan idols and destroyed by Spanish explorers; this included many gold and silver sculptures and other artifacts found in South America, which were melted down before their transport to Spain or Portugal. Spaniards and Portuguese brought the western European architectural style to the continent, and helped to improve infrastructures like bridges, roads, and the sewer system of the cities they discovered or conquered. They also significantly increased economic and trade relations, not just between the old and new world but between the different South American regions and peoples. Finally, with the expansion of the Portuguese and Spanish languages, many cultures that were previously separated became united through that of Latin American.

Guyana was first a Dutch, and then a British colony, though there was a brief period during the Napoleonic Wars when it was colonized by the French. The country was once partitioned into three parts, each being controlled by one of the colonial powers until the country was finally taken over fully by the British.

Indigenous peoples of the Americas in various European colonies were forced to work in European plantations and mines; along with African slaves who were also introduced in the proceeding centuries. The colonists were heavily dependent on indigenous labor during the initial phases of European settlement to maintain the subsistence economy, and natives were often captured by expeditions. The importation of African slaves began midway through the 16th century, but the enslavement of indigenous peoples continued well into the 17th and 18th centuries. The Atlantic slave trade brought African slaves primarily to South American colonies, beginning with the Portuguese since 1502. The main destinations of this phase were the Caribbean colonies and Brazil, as European nations built up economically slave-dependent colonies in the New World. Nearly 40% of all African slaves trafficked to the Americas went to Brazil. An estimated 4.9 million slaves from Africa came to Brazil during the period from 1501 to 1866.

While the Portuguese, English, French and Dutch settlers enslaved mainly African blacks, the Spaniards became very disposed of the natives. In 1750 Portugal abolished native slavery in the colonies because they considered them unfit for labour and began to import even more African slaves. Slaves were brought to the mainland on slave ships, under inhuman conditions and ill-treatment, and those who survived were sold into the slave markets.

After independence, all South American countries maintained slavery for some time. The first South American country to abolish slavery was Chile in 1823, Uruguay in 1830, Bolivia in 1831, Colombia and Ecuador in 1851, Argentina in 1853, Peru and Venezuela in 1854, Suriname in 1863, Paraguay in 1869, and in 1888 Brazil was the last South American nation and the last country in western world to abolish slavery.

The European Peninsular War (1807–1814), a theater of the Napoleonic Wars, changed the political situation of both the Spanish and Portuguese colonies. First, Napoleon invaded Portugal, but the House of Braganza avoided capture by escaping to Brazil. Napoleon also captured King Ferdinand VII of Spain, and appointed his own brother instead. This appointment provoked severe popular resistance, which created Juntas to rule in the name of the captured king.

Many cities in the Spanish colonies, however, considered themselves equally authorized to appoint local Juntas like those of Spain. This began the Spanish American wars of independence between the patriots, who promoted such autonomy, and the royalists, who supported Spanish authority over the Americas. The Juntas, in both Spain and the Americas, promoted the ideas of the Enlightenment. Five years after the beginning of the war, Ferdinand VII returned to the throne and began the Absolutist Restoration as the royalists got the upper hand in the conflict.

The independence of South America was secured by Simón Bolívar (Venezuela) and José de San Martín (Argentina), the two most important "Libertadores". Bolívar led a great uprising in the north, then led his army southward towards Lima, the capital of the Viceroyalty of Peru. Meanwhile, San Martín led an army across the Andes Mountains, along with Chilean expatriates, and liberated Chile. He organized a fleet to reach Peru by sea, and sought the military support of various rebels from the Viceroyalty of Peru. The two armies finally met in Guayaquil, Ecuador, where they cornered the Royal Army of the Spanish Crown and forced its surrender.

In the Portuguese Kingdom of Brazil, Dom Pedro I (also Pedro IV of Portugal), son of the Portuguese King Dom João VI, proclaimed the independent Kingdom of Brazil in 1822, which later became the Empire of Brazil. Despite the Portuguese loyalties of garrisons in Bahia, Cisplatina and Pará, independence was diplomatically accepted by the crown in Portugal in 1825, on condition of a high compensation paid by Brazil mediatized by the United Kingdom.

The newly independent nations began a process of fragmentation, with several civil and international wars. However, it was not as strong as in Central America. Some countries created from provinces of larger countries stayed as such up to modern times (such as Paraguay or Uruguay), while others were reconquered and reincorporated into their former countries (such as the Republic of Entre Ríos and the Riograndense Republic).

The first separatist attempt was in 1820 by the Argentine province of Entre Ríos, led by a caudillo. In spite of the "Republic" in its title, General Ramírez, its caudillo, never really intended to declare an independent Entre Rios. Rather, he was making a political statement in opposition to the monarchist and centralist ideas that back then permeated Buenos Aires politics. The "country" was reincorporated at the United Provinces in 1821.

In 1825 the Cisplatine Province declared its independence from the Empire of Brazil, which led to the Cisplatine War between the imperials and the Argentine from the United Provinces of the Río de la Plata to control the region. Three years later, the United Kingdom intervened in the question by proclaiming a tie and creating in the former Cisplatina a new independent country: The Oriental Republic of Uruguay.

Later in 1836, while Brazil was experiencing the chaos of the regency, Rio Grande do Sul proclaimed its independence motivated by a tax crisis. With the anticipation of the coronation of Pedro II to the throne of Brazil, the country could stabilize and fight the separatists, which the province of Santa Catarina had joined in 1839. The Conflict came to an end by a process of compromise by which both Riograndense Republic and Juliana Republic were reincorporated as provinces in 1845.

The Peru–Bolivian Confederation, a short-lived union of Peru and Bolivia, was blocked by Chile in the War of the Confederation (1836–1839) and again during the War of the Pacific (1879–1883). Paraguay was virtually destroyed by Argentina, Brazil and Uruguay in the Paraguayan War.

South American history in early 19th century was built almost exclusively on wars. Despite the Spanish American wars of independence and the Brazilian War of Independence, the new nations quickly began to suffer with internal conflicts and wars among themselves.

In 1825 the proclamation of independence of Cisplatina led to the Cisplatine War between historical rivals the Empire of Brazil and the United Provinces of the Río de la Plata, Argentina's predecessor. The result was a stalemate, ending with the British arranging for the independence of Uruguay. Soon after, another Brazilian province proclaimed its independence leading to the Ragamuffin War which Brazil won.

Between 1836 and 1839 the War of the Confederation broke out between the short-lived Peru-Bolivian Confederation and Chile, with the support of the Argentine Confederation. The war was fought mostly in the actual territory of Peru and ended with a Confederate defeat and the dissolution of the Confederacy and annexation of many territories by Argentina.

Meanwhile, the Argentine Civil Wars plagued Argentina since its independence. The conflict was mainly between those who defended the centralization of power in Buenos Aires and those who defended a confederation. During this period it can be said that "there were two Argentines": the Argentine Confederation and the Argentine Republic. At the same time the political instability in Uruguay led to the Uruguayan Civil War among the main political factions of the country. All this instability in the platine region interfered with the goals of other countries such as Brazil, which was soon forced to take sides. In 1851 the Brazilian Empire, supporting the centralizing unitarians, and the Uruguayan government invaded Argentina and deposed the caudillo, Juan Manuel Rosas, who ruled the confederation with an iron hand. Although the Platine War did not put an end to the political chaos and civil war in Argentina, it brought temporary peace to Uruguay where the Colorados faction won, supported by the Brazilian Empire, British Empire, French Empire and the Unitarian Party of Argentina.

Peace lasted only a short time: in 1864 the Uruguayan factions faced each other again in the Uruguayan War. The Blancos supported by Paraguay started to attack Brazilian and Argentine farmers near the borders. The Empire made an initial attempt to settle the dispute between Blancos and Colorados without success. In 1864, after a Brazilian ultimatum was refused, the imperial government declared that Brazil's military would begin reprisals. Brazil declined to acknowledge a formal state of war, and, for most of its duration, the Uruguayan–Brazilian armed conflict was an undeclared war which led to the deposition of the "Blancos" and the rise of the pro-Brazilian "Colorados" to power again. This angered the Paraguayan government, which even before the end of the war invaded Brazil, beginning the biggest and deadliest war in both South American and Latin American histories: the Paraguayan War.

The Paraguayan War began when the Paraguayan dictator Francisco Solano López ordered the invasion of the Brazilian provinces of Mato Grosso and Rio Grande do Sul. His attempt to cross Argentinian territory without Argentinian approval led the pro-Brazilian Argentine government into the war. The pro-Brazilian Uruguayan government showed its support by sending troops. In 1865 the three countries signed the Treaty of the Triple Alliance against Paraguay. At the beginning of the war, the Paraguayans took the lead with several victories, until the Triple Alliance organized to repel the invaders and fight effectively. This was the second total war experience in the world after the American Civil War. It was deemed the greatest war effort in the history of all participating countries, taking almost 6 years and ending with the complete devastation of Paraguay. The country lost 40% of its territory to Brazil and Argentina and lost 60% of its population, including 90% of the men. The dictator Lopez was killed in battle and a new government was instituted in alliance with Brazil, which maintained occupation forces in the country until 1876.

The last South American war in the 19th century was the War of the Pacific with Bolivia and Peru on one side and Chile on the other. In 1879 the war began with Chilean troops occupying Bolivian ports, followed by Bolivia declaring war on Chile which activated an alliance treaty with Peru. The Bolivians were completely defeated in 1880 and Lima was occupied in 1881. The peace was signed with Peru in 1883 while a truce was signed with Bolivia in 1884. Chile annexed territories of both countries leaving Bolivia with no path to the sea.

In the new century, as wars became less violent and less frequent, Brazil entered into a small conflict with Bolivia for the possession of the Acre, which was acquired by Brazil in 1902. In 1917 Brazil declared war on the Central Powers and join the allied side in the World War I, sending a small fleet to the Mediterranean Sea and some troops to be integrated with the British and French troops. Brazil was the only South American country that fought in WWI. Later in 1932 Colombia and Peru entered a short armed conflict for territory in the Amazon. In the same year Paraguay declared war on Bolivia for possession of the Chaco, in a conflict that ended three years later with Paraguay's victory. Between 1941 and 1942 Peru and Ecuador fought decisively for territories claimed by both that were annexed by Peru, usurping Ecuador's frontier with Brazil.

Also in this period the first naval battle of World War II was fought on the continent, in the River Plate, between British forces and German submarines. The Germans still made numerous attacks on Brazilian ships on the coast, causing Brazil to declare war on the Axis powers in 1942, being the only South American country to fight in this war (and in both World Wars). Brazil sent naval and air forces to combat German and Italian submarines off the continent and throughout the South Atlantic, in addition to sending an expeditionary force to fight in the Italian Campaign.

A brief war was fought between Argentina and the UK in 1982, following an Argentine invasion of the Falkland Islands, which ended with an Argentine defeat. The last international war to be fought on South American soil was the 1995 Cenepa War between Ecuador and the Peru along their mutual border.

Wars became less frequent in the 20th century, with Bolivia-Paraguay and Peru-Ecuador fighting the last inter-state wars. Early in the 20th century, the three wealthiest South American countries engaged in a vastly expensive naval arms race which was catalyzed by the introduction of a new warship type, the "dreadnought". At one point, the Argentine government was spending a fifth of its entire yearly budget for just two dreadnoughts, a price that did not include later in-service costs, which for the Brazilian dreadnoughts was sixty percent of the initial purchase.

The continent became a battlefield of the Cold War in the late 20th century. Some democratically elected governments of Argentina, Brazil, Chile, Uruguay and Paraguay were overthrown or displaced by military dictatorships in the 1960s and 1970s. To curtail opposition, their governments detained tens of thousands of political prisoners, many of whom were tortured and/or killed on inter-state collaboration. Economically, they began a transition to neoliberal economic policies. They placed their own actions within the US Cold War doctrine of "National Security" against internal subversion. Throughout the 1980s and 1990s, Peru suffered from an internal conflict.

Argentina and Britain fought the Falklands War in 1982.

Colombia has had an ongoing, though diminished internal conflict, which started in 1964 with the creation of Marxist guerrillas (FARC-EP) and then involved several illegal armed groups of leftist-leaning ideology as well as the private armies of powerful drug lords. Many of these are now defunct, and only a small portion of the ELN remains, along with the stronger, though also greatly reduced, FARC.

Revolutionary movements and right-wing military dictatorships became common after World War II, but since the 1980s, a wave of democratization passed through the continent, and democratic rule is widespread now. Nonetheless, allegations of corruption are still very common, and several countries have developed crises which have forced the resignation of their governments, although, on most occasions, regular civilian succession has continued.
International indebtedness turned into a severe problem in the late 1980s, and some countries, despite having strong democracies, have not yet developed political institutions capable of handling such crises without resorting to unorthodox economic policies, as most recently illustrated by Argentina's default in the early 21st century. The last twenty years have seen an increased push towards regional integration, with the creation of uniquely South American institutions such as the Andean Community, Mercosur and Unasur. Notably, starting with the election of Hugo Chávez in Venezuela in 1998, the region experienced what has been termed a pink tide – the election of several leftist and center-left administrations to most countries of the area, except for the Guianas and Colombia.

Historically, the Hispanic countries were founded as Republican dictatorships led by caudillos. Brazil was the only exception, being a constitutional monarchy for its first 67 years of independence, until a coup d'état proclaimed a republic. In the late 19th century, the most democratic countries were Brazil, Chile, Argentina and Uruguay.

In the interwar period, nationalism grew stronger on the continent, influenced by countries like Nazi Germany and Fascist Italy. A series of authoritarian rules broke out in South American countries with views bringing them closer to the Axis Powers, like Vargas's Brazil. In the late 20th century, during the Cold War, many countries became military dictatorships under American tutelage in attempts to avoid the influence of the Soviet Union. After the fall of the authoritarian regimes, these countries became democratic republics.

During the first decade of the 21st century, South American governments have drifted to the political left, with leftist leaders being elected in Chile, Uruguay, Brazil, Argentina, Ecuador, Bolivia, Paraguay, Peru and Venezuela. Most South American countries are making increasing use of protectionist policies, helping local development.

All South American countries are presidential republics with the exceptions of Peru, which is a semi-presidential republic, and Suriname, a parliamentary republic. French Guiana is a French overseas department, while the Falkland Islands and South Georgia and the South Sandwich Islands are British overseas territories. It is currently the only inhabited continent in the world without monarchies; the Empire of Brazil existed during the 19th century and there was an unsuccessful attempt to establish a Kingdom of Araucanía and Patagonia in southern Argentina and Chile. Also in the twentieth century, Suriname was established as a constituent kingdom of the Kingdom of the Netherlands and Guyana retained the British monarch as head of state for 4 years after its independence.

Recently, an intergovernmental entity has been formed which aims to merge the two existing customs unions: Mercosur and the Andean Community, thus forming the third-largest trade bloc in the world.
This new political organization, known as Union of South American Nations, seeks to establish free movement of people, economic development, a common defense policy and the elimination of tariffs.

South America has over floor(/1e6) million inhabitants and a population growth rate of about 0.6% per year. There are several areas of sparse demographics such as tropical forests, the Atacama Desert and the icy portions of Patagonia. On the other hand, the continent presents regions of high population density, such as the great urban centers. The population is formed by descendants of Europeans (mainly Spaniards, Portuguese and Italians), Africans and indigenous peoples. There is a high percentage of mestizos that vary greatly in composition by place. There is also a minor population of Asians, especially in Brazil. The two main languages are by far Spanish and Portuguese, followed by French, English and Dutch in smaller numbers.

Spanish and Portuguese are the most spoken languages in South America, with approximately 200 million speakers each. Spanish is the official language of most countries, along with other native languages in some countries. Portuguese is the official language of Brazil. Dutch is the official language of Suriname; English is the official language of Guyana, although there are at least twelve other languages spoken in the country, including Portuguese, Chinese, Hindustani and several native languages. English is also spoken in the Falkland Islands. French is the official language of French Guiana and the second language in Amapá, Brazil.

Indigenous languages of South America include Quechua in Peru, Bolivia, Ecuador, Chile and Colombia; Wayuunaiki in northern Colombia (La Guajira) and northwestern Venezuela (Zulia); Guaraní in Paraguay and, to a much lesser extent, in Bolivia; Aymara in Bolivia, Peru, and less often in Chile; and Mapudungun is spoken in certain pockets of southern Chile. At least three South American indigenous languages (Quechua, Aymara, and Guarani) are recognized along with Spanish as national languages.

Other languages found in South America include Hindustani and Javanese in Suriname; Italian in Argentina, Brazil, Uruguay and Venezuela; and German in certain pockets of Argentina and Brazil. German is also spoken in many regions of the southern states of Brazil, Riograndenser Hunsrückisch being the most widely spoken German dialect in the country; among other Germanic dialects, a Brazilian form of East Pomeranian is also well represented and is experiencing a revival. Welsh remains spoken and written in the historic towns of Trelew and Rawson in the Argentine Patagonia. There are also small clusters of Japanese-speakers in Brazil, Colombia and Peru. Arabic speakers, often of Lebanese, Syrian, or Palestinian descent, can be found in Arab communities in Argentina, Colombia, Brazil, Venezuela and in Paraguay.

An estimated 90% of South Americans are Christians (82% Roman Catholic, 8% other Christian denominations mainly traditional Protestants and Evangelicals but also Orthodox), accounting for c. 19% of Christians worldwide.

African descendent religions and Indigenous religions are also common throughout all South America, some examples of are Santo Daime, Candomblé, Umbanda and Encantados.

Crypto-Jews or Marranos, conversos, and Anusim were an important part of colonial life in Latin America.

Both Buenos Aires, Argentina and São Paulo, Brazil figure among the largest Jewish populations by urban area.

Japanese Buddhism, Shintoism, and Shinto-derived Japanese New Religions are common in Brazil and Peru. Korean Confucianism is especially found in Brazil while Chinese Buddhism and Chinese Confucianism have spread throughout the continent.

Kardecist Spiritism can be found in several countries.

Part of Religions in South America (2013):

Genetic admixture occurs at very high levels in South America. In Argentina, the European influence accounts for 65–79% of the genetic background, Amerindian for 17–31% and sub-Saharan African for 2–4%. In Colombia, the sub-Saharan African genetic background varied from 1% to 89%, while the European genetic background varied from 20% to 79%, depending on the region.
In Peru, European ancestries ranged from 1% to 31%, while the African contribution was only 1% to 3%. The Genographic Project determined the average Peruvian from Lima had about 28% European ancestry, 68% Native American, 2% Asian ancestry and 2% sub-Saharan African.

Descendants of indigenous peoples, such as the Quechua and Aymara, or the Urarina of Amazonia make up the majority of the population in Bolivia (56%) and, per some sources, in Peru (44%). In Ecuador, Amerindians are a large minority that comprises two-fifths of the population. The native European population is also a significant element in most other former Portuguese colonies.

People who identify as of primarily or totally European descent, or identify their phenotype as corresponding to such group, are more of a majority in Argentina, and Uruguay and more than half of the population of Chile (64.7%) and (48.4%) in Brazil. In Venezuela, according to the national census 42% of the population is primarily native Spanish, Italian and Portuguese descendants. In Colombia, people who identify as European descendant are about 37%. In Peru, European descendants are the third group in number (15%).

Mestizos (mixed European and Amerindian) are the largest ethnic group in Paraguay, Venezuela, Colombia and Ecuador and the second group in Peru and Chile.

South America is also home to one of the largest populations of Africans. This group is significantly present in Brazil, Colombia, Guyana, Suriname, French Guiana, Venezuela and Ecuador.

Brazil followed by Peru have the largest Japanese, Korean and Chinese communities in South America, Lima has the largest ethnic Chinese community in Latin America. East Indians form the largest ethnic group in Guyana and Suriname.

In many places indigenous people still practice a traditional lifestyle based on subsistence agriculture or as hunter-gatherers. There are still some uncontacted tribes residing in the Amazon Rainforest.
The most populous country in South America is Brazil with /1e6 round 1 million people. The second largest country is Colombia with a population of . Argentina is the third most populous country with .

While Brazil, Argentina, and Colombia maintain the largest populations, large city populations are not restricted to those nations. The largest cities in South America, by far, are São Paulo, Lima, and Bogotá. These cities are the only cities on the continent to exceed eight million, and three of five in the Americas. Next in size are Rio de Janeiro, Santiago, Caracas, Buenos Aires and Salvador.

Five of the top ten metropolitan areas are in Brazil. These metropolitan areas all have a population of above 4 million and include the São Paulo metropolitan area, Rio de Janeiro metropolitan area, and Belo Horizonte metropolitan area. Whilst the majority of the largest metropolitan areas are within Brazil, Argentina is host to the second largest metropolitan area by population in South America: the Buenos Aires metropolitan region is above 13 million inhabitants.

South America has also been witness to the growth of megapolitan areas. In Brazil four megaregions exist including the Expanded Metropolitan Complex of São Paulo with more than 32 million inhabitants. The others are the Greater Rio, Greater Belo Horizonte and Greater Porto Alegre. Colombia also has four megaregions which comprise 72% of its population, followed by Venezuela, Argentina and Peru which are also homes of megaregions.

The top ten largest South American metropolitan areas by population as of 2015, based on national census numbers from each country:

South America relies less on the export of both manufactured goods and natural resources than the world average; merchandise exports from the continent were 16% of GDP on an exchange rate basis, compared to 25% for the world as a whole. Brazil (the seventh largest economy in the world and the largest in South America) leads in terms of merchandise exports at $251 billion, followed by Venezuela at $93 billion, Chile at $86 billion, and Argentina at $84 billion.

Since 1930, the continent has experienced remarkable growth and diversification in most economic sectors. Most agricultural and livestock products are destined for the domestic market and local consumption. However, the export of agricultural products is essential for the balance of trade in most countries.

The main agrarian crops are export crops, such as soy and wheat. The production of staple foods such as vegetables, corn or beans is large, but focused on domestic consumption. Livestock raising for meat exports is important in Argentina, Paraguay, Uruguay and Colombia. In tropical regions the most important crops are coffee, cocoa and bananas, mainly in Brazil, Colombia and Ecuador. Traditionally, the countries producing sugar for export are Peru, Guyana and Suriname, and in Brazil, sugar cane is also used to make ethanol. On the coast of Peru, northeast and south of Brazil, cotton is grown. Fifty percent of the South American surface is covered by forests, but timber industries are small and directed to domestic markets. In recent years, however, transnational companies have been settling in the Amazon to exploit noble timber destined for export. The Pacific coastal waters of South America are the most important for commercial fishing. The anchovy catch reaches thousands of tons, and tuna is also abundant (Peru is a major exporter). The capture of crustaceans is remarkable, particularly in northeastern Brazil and Chile.

Only Brazil and Argentina are part of the G20 (industrial countries), while only Brazil is part of the G8+5 (the most powerful and influential nations in the world). In the tourism sector, a series of negotiations began in 2005 to promote tourism and increase air connections within the region. Punta del Este, Florianópolis and Mar del Plata are among the most important resorts in South America.

The most industrialized countries in South America are Brazil, Argentina, Chile, Colombia, Venezuela and Uruguay respectively. These countries alone account for more than 75 percent of the region's economy and add up to a GDP of more than US$3.0 trillion. Industries in South America began to take on the economies of the region from the 1930s when the Great Depression in the United States and other countries of the world boosted industrial production in the continent. From that period the region left the agricultural side behind and began to achieve high rates of economic growth that remained until the early 1990s when they slowed due to political instabilities, economic crises and neoliberal policies.

Since the end of the economic crisis in Brazil and Argentina that occurred in the period from 1998 to 2002, which has led to economic recession, rising unemployment and falling population income, the industrial and service sectors have been recovering rapidly. Chile, Argentina and Brazil have recovered fastest, growing at an average of 5% per year. All of South America after this period has been recovering and showing good signs of economic stability, with controlled inflation and exchange rates, continuous growth, a decrease in social inequality and unemployment–factors that favor industry.

The main industries are: electronics, textiles, food, automotive, metallurgy, aviation, naval, clothing, beverage, steel, tobacco, timber, chemical, among others. Exports reach almost US$400 billion annually, with Brazil accounting for half of this.

The economic gap between the rich and poor in most South American nations is larger than on most other continents. The richest 10% receive over 40% of the nation's income in Bolivia, Brazil, Chile, Colombia, and Paraguay, while the poorest 20% receive 4% or less in Bolivia, Brazil, and Colombia. This wide gap can be seen in many large South American cities where makeshift shacks and slums lie in the vicinity of skyscrapers and upper-class luxury apartments; nearly one in nine South Americans live on less than $2 per day (on a purchasing power parity basis).

Tourism has increasingly become a significant source of income for many South American countries.

Historical relics, architectural and natural wonders, a diverse range of foods and culture, vibrant and colorful cities, and stunning landscapes attract millions of tourists every year to South America. Some of the most visited places in the region are Iguazu Falls, Recife, Olinda, Machu Picchu, Bariloche, the Amazon rainforest, Rio de Janeiro, São Luís, Salvador, Fortaleza, Maceió, Buenos Aires, Florianópolis, San Ignacio Miní, Isla Margarita, Natal, Lima, São Paulo, Angel Falls, Brasília, Nazca Lines, Cuzco, Belo Horizonte, Lake Titicaca, Salar de Uyuni, Jesuit Missions of Chiquitos, Los Roques archipelago, Gran Sabana, Patagonia, Tayrona National Natural Park, Santa Marta, Bogotá, Cali, Medellín, Cartagena, Perito Moreno Glacier and the Galápagos Islands. In 2016 Brazil hosted the 2016 Summer Olympics.

South Americans are culturally influenced by their indigenous peoples, the historic connection with the Iberian Peninsula and Africa, and waves of immigrants from around the globe.

South American nations have a rich variety of music. Some of the most famous genres include vallenato and cumbia from Colombia, pasillo from Colombia and Ecuador, samba, bossa nova and música sertaneja from Brazil, and tango from Argentina and Uruguay. Also well known is the non-commercial folk genre Nueva Canción movement which was founded in Argentina and Chile and quickly spread to the rest of the Latin America. People on the Peruvian coast created the fine guitar and cajon duos or trios in the most mestizo (mixed) of South American rhythms such as the Marinera (from Lima), the Tondero (from Piura), the 19th century popular Creole Valse or Peruvian Valse, the soulful Arequipan Yaravi, and the early 20th century Paraguayan Guarania. In the late 20th century, Spanish rock emerged by young hipsters influenced by British pop and American rock. Brazil has a Portuguese-language pop rock industry as well a great variety of other music genres.

The literature of South America has attracted considerable critical and popular acclaim, especially with the Latin American Boom of the 1960s and 1970s, and the rise of authors such as Mario Vargas Llosa, Gabriel García Márquez in novels and Jorge Luis Borges and Pablo Neruda in other genres. The Brazilians Machado de Assis and João Guimarães Rosa are widely regarded as the greatest Brazilian writers.

Because of South America's broad ethnic mix, South American cuisine has African, South American Indian, South Asian, East Asian, and European influences. Bahia, Brazil, is especially well known for its West African–influenced cuisine. Argentines, Chileans, Uruguayans, Brazilians, Bolivians, and Venezuelans regularly consume wine. People in Argentina, Paraguay, Uruguay, southern Chile, Bolivia and Brazil drink mate, an herb which is brewed. The Paraguayan version, terere, differs from other forms of mate in that it is served cold. Pisco is a liquor distilled from grapes in Peru and Chile. Peruvian cuisine mixes elements from Chinese, Japanese, Spanish, Italian, African, Arab, Andean, and Amazonic food.

The artist Oswaldo Guayasamín (1919–1999) from Ecuador, represented with his painting style the feeling of the peoples of Latin America highlighting social injustices in various parts of the world. The Colombian Fernando Botero (1932) is one of the greatest exponents of painting and sculpture that continues still active and has been able to develop a recognizable style of his own. For his part, the Venezuelan Carlos Cruz-Diez has contributed significantly to contemporary art, with the presence of works around the world.

Currently several emerging South American artists are recognized by international art critics: Guillermo Lorca – Chilean painter, Teddy Cobeña – Ecuadorian sculptor and recipient of international sculpture award in France) and Argentine artist Adrián Villar Rojas – winner of the Zurich Museum Art Award among many others.

A wide range of sports are played in the continent of South America, with football being the most popular overall, while baseball is the most popular in Venezuela.

Other sports include basketball, cycling, polo, volleyball, futsal, motorsports, rugby (mostly in Argentina and Uruguay), handball, tennis, golf, field hockey, boxing and cricket.

South America hosted its first Olympic Games in Rio de Janeiro, Brazil in 2016 and will host the Youth Olympic Games in Buenos Aires, Argentina in 2018.

South America shares with Europe supremacy over the sport of football as all winners in FIFA World Cup history and all winning teams in the FIFA Club World Cup have come from these two continents. Brazil holds the record at the FIFA World Cup with five titles in total. Argentina and Uruguay have two titles each. So far four South American nations have hosted the tournament including the first edition in Uruguay (1930). The other three were Brazil (1950, 2014), Chile (1962), and Argentina (1978).

South America is home to the longest running international football tournament; the Copa América, which has been regularly contested since 1916. Uruguay won the Copa América a record 15 times, surpassing hosts Argentina in 2011 to reach 15 titles (they were previously equal at 14 titles each during the 2011 Copa América).

Also, in South America, a multi-sport event, the South American Games, are held every four years. The first edition was held in La Paz in 1978 and the most recent took place in Santiago in 2014.

South American Cricket Championship is an international limited-overs cricket tournament played since 1995 featuring national teams from South America and certain other invited sides including teams from North America, currently played annually but until 2013 was usually played every two seasons.

Due to the diversity of topography and pluviometric precipitation conditions, the region's water resources vary enormously in different areas. In the Andes, navigation possibilities are limited, except for the Magdalena River, Lake Titicaca and the lakes of the southern regions of Chile and Argentina. Irrigation is an important factor for agriculture from northwestern Peru to Patagonia. Less than 10% of the known electrical potential of the Andes had been used until the mid-1960s.

The Brazilian Highlands has a much higher hydroelectric potential than the Andean region and its possibilities of exploitation are greater due to the existence of several large rivers with high margins and the occurrence of great differences forming huge cataracts, such as those of Paulo Afonso, Iguaçu and others. The Amazon River system has about 13,000 km of waterways, but its possibilities for hydroelectric use are still unknown.

Most of the continent's energy is generated through hydroelectric power plants, but there is also an important share of thermoelectric and wind energy. Brazil and Argentina are the only South American countries that generate nuclear power, each with two nuclear power plants. In 1991 these countries signed a peaceful nuclear cooperation agreement.

South American transportation systems are still deficient, with low kilometric densities. The region has about 1,700,000 km of highways and 100,000 km of railways, which are concentrated in the coastal strip, and the interior is still devoid of communication.

Only two railroads are continental: the Transandina, which connects Buenos Aires, in Argentina to Valparaíso, in Chile, and the Brazil–Bolivia Railroad, which makes it the connection between the port of Santos in Brazil and the city of Santa Cruz de la Sierra, in Bolivia. In addition, there is the Pan-American Highway, which crosses the Andean countries from north to south, although some stretches are unfinished.

Two areas of greater density occur in the railway sector: the platinum network, which develops around the Platine region, largely belonging to Argentina, with more than 45,000 km in length; And the Southeast Brazil network, which mainly serves the state of São Paulo, state of Rio de Janeiro and Minas Gerais. Brazil and Argentina also stand out in the road sector. In addition to the modern roads that extend through northern Argentina and south-east and south of Brazil, a vast road complex aims to link Brasília, the federal capital, to the South, Southeast, Northeast and Northern regions of Brazil.

The Port of Callao is the main port of Peru.
South America has one of the largest bays of navigable inland waterways in the world, represented mainly by the Amazon basin, the Platine basin, the São Francisco and the Orinoco basins, Brazil having about 54,000 km navigable, while Argentina has 6,500 km and Venezuela, 1,200 km.

The two main merchant fleets also belong to Brazil and Argentina. The following are those of Chile, Venezuela, Peru and Colombia. The largest ports in commercial movement are those of Buenos Aires, Santos, Rio de Janeiro, Bahía Blanca, Rosario, Valparaíso, Recife, Salvador, Montevideo, Paranaguá, Rio Grande, Fortaleza, Belém and Maracaibo.

In South America, commercial aviation has a magnificent expansion field, which has one of the largest traffic density lines in the world, Rio de Janeiro–São Paulo, and large airports, such as Congonhas, São Paulo–Guarulhos International and Viracopos (São Paulo), Rio de Janeiro International and Santos Dumont (Rio de Janeiro), El Dorado (Bogotá), Ezeiza (Buenos Aires), Tancredo Neves International Airport (Belo Horizonte), Curitiba International Airport (Curitiba), Brasilia, Caracas, Montevideo, Lima, Recife, Salvador, Salgado Filho International Airport (Porto Alegre), Fortaleza, Manaus and Belém.

The main public transport in major cities is the bus. Many cities also have a diverse system of metro and subway trains, the first of which was the Buenos Aires subte, opened 1913. The Santiago subway is the largest network in South America, with 103 km, while the São Paulo subway is the largest in transportation, with more than 4.6 million passengers per day and was voted the best in the Americas. In Rio de Janeiro was installed the first railroad of the continent, in 1854. Today the city has a vast and diversified system of metropolitan trains, integrated with buses and subway. Recently it was also inaugurated in the city a Light Rail System called VLT, a small electrical trams at low speed, while São Paulo inaugurated its monorail, the first of South America. In Brazil, an express bus system called Bus Rapid Transit (BRT), which operates in several cities, has also been developed.




</doc>
<doc id="26771" url="https://en.wikipedia.org/wiki?curid=26771" title="Spindletop">
Spindletop

Spindletop is an oil field located in the southern portion of Beaumont, Texas, in the United States. The Spindletop dome was derived from the Louann Salt evaporite layer of the Jurassic geologic period. On January 10, 1901, a well at Spindletop struck oil ("came in"). The Spindletop gusher blew for 9 days at a rate estimated at of oil per day. Gulf Oil and Texaco, now part of Chevron Corporation, were formed to develop production at Spindletop. According to Daniel Yergin, the Spindletop discovery led the United States into the oil age. Prior to Spindletop, oil was primarily used for lighting and as a lubricant. Because of the quantity of oil discovered, burning petroleum as a fuel for mass consumption suddenly became economically feasible.

The frenzy of oil exploration and the economic development it generated in the state became known as the Texas oil boom. The United States soon became the world's leading oil producer.

There had long been suspicions that oil might be under "Spindletop Hill." The area was known for its vast sulfur springs and bubbling gas seepages that would ignite if lit. In August 1892, George W. O'Brien, George W. Carroll, Pattillo Higgins, and others formed the Gladys City Oil, Gas, and Manufacturing Company to do exploratory drilling on Spindletop Hill. The company drilled many dry holes and ran into trouble, as investors began to balk at pouring more money into drilling with no oil to show for it.

Pattillo Higgins left the company and teamed with Captain Anthony F. Lucas, the leading expert in the U.S. on salt-dome formations. Lucas made a lease agreement in 1899 with the Gladys City Company and a subsequent agreement with Higgins. Lucas drilled to before running out of money. He secured additional funding from John H. Galey and James M. Guffey of Pittsburgh, but the deal left Lucas with only an eighth share of the lease and Higgins with nothing.

Lucas continued drilling, and on January 10, 1901, at a depth of 1,139 ft (347 m), what is known as the Lucas Gusher or the Lucas Geyser blew oil over in the air at a rate of (4,200,000 gallons). Nine days passed before the well was brought under control.

Spindletop was the largest gusher the world had seen and catapulted Beaumont into an oil-fueled boomtown. Beaumont's population of 10,000 tripled in 3 months and eventually rose to 50,000. Speculation led land prices to increase rapidly. By the end of 1902, more than 500 companies had been formed and 285 wells were in operation.

Spindletop was the first oilfield found on the US Gulf Coast, and prompted further drilling, and further oil-field discoveries. Oil drillers looking for another Spindletop particularly sought out other salt domes, and were often successful. The Gulf Coast turned into a major oil region.

Standard Oil, which then had a monopoly or near-monopoly on the petroleum industry in the eastern states, was prevented from moving aggressively into the new oilfield by state antitrust laws. Populist sentiment against Standard Oil was particularly strong at the time of the Spindletop discovery. In 1900, an oil-products marketing company affiliated with Standard Oil had been banned from the state for its cutthroat business practices. Although Standard built refineries in the area, it was unable to dominate the new Gulf Coast oil fields the way it had in the eastern states. As a result, a number of startup oil companies at Spindletop, such as Texaco and Gulf Oil, grew into formidable competitors to Standard Oil.

Among those drilling at Spindletop was W. Scott Heywood, a native of Cleveland, Ohio, who in 1901 made the first oil discovery in nearby Jeff Davis Parish in southwestern Louisiana. In 1932, Heywood was elected to a single term in the Louisiana State Senate.

Production at Spindletop began to decline rapidly after 1902, and the wells produced only by 1904. Unfortunately the developers had signed a 20-year contract to sell 25,000 barrels per day at $0.25 per barrel to Shell Oil. When the price climbed above $0.35 per barrel, the operation was stressed and Mellon who had lent money for Spindle Top’s development took control of the company, won a law suit allowing Mellon to renege on the contract, and created Gulf Oil. On November 14, 1925, the Yount-Lee Oil Company brought in its McFaddin No. 2 at a depth around , sparking a second boom, which culminated in the field's peak production year of 1927, during which 21 million barrels (3.3 GL) were produced. Over the 10 years following the McFaddin discovery, more than 72 million barrels (11.4 GL) of oil were produced, mostly from the newer areas of the field. Spindletop continued as a productive source of oil until about 1936. It was then mined for sulfur from the 1950s to about 1975.

In 1976, Lamar University dedicated the Spindletop-Gladys City Boomtown Museum to preserve the history of the Spindletop oil gusher era in Beaumont. The museum features an oil derrick and many reconstructed Gladys City building interiors furnished with authentic artifacts from the Spindletop boomtown period.

The Lucas Gusher Monument is located at the museum. The monument, erected at the wellhead in July, 1941, was moved to the Spindletop-Gladys City Museum after it became unstable due to ground subsidence. According to an article by Nedra Foster, LS in the July/August, 2000 issue of the "Professional Surveyor Magazine," the monument was originally located within 4 ft of the site of the Spindletop well.

Today, the wellhead is marked at Spindletop Park by a flagpole flying the Texas flag. It is located about 1.5 miles southwest of the museum, off West Port Arthur Road/Spur 93. The site includes a viewing platform with information placards, about a quarter mile from the flagpole. The wellhead site is in the middle of swampland on private land and is not accessible. Directions to the park and viewing platform are available at the museum.

On December 4, 1955, the Spindletop story was dramatized in "Spindletop – The First Great Texas Oil Strike (January 10, 1901)" on the CBS history series, "You Are There". Robert Bray was cast as Pattillo Higgins, Mike Ragan as Marion Fletcher, Parley Baer as Captain Lucas, Jean Byron as Caroline Lucas, DeForest Kelley as Al Hammill, Tyler McVey as Mayor Wheat, and William Fawcett as a farmer.





</doc>
<doc id="26773" url="https://en.wikipedia.org/wiki?curid=26773" title="Stendhal">
Stendhal

Marie-Henri Beyle (; 23 January 1783 – 23 March 1842), better known by his pen name Stendhal (, ; ), was a 19th-century French writer. Best known for the novels "Le Rouge et le Noir" ("The Red and the Black", 1830) and "La Chartreuse de Parme" ("The Charterhouse of Parma", 1839), he is highly regarded for the acute analysis of his characters' psychology and considered one of the early and foremost practitioners of realism.

Born in Grenoble, Isère, he was an unhappy child, disliking his "unimaginative" father and mourning his mother, whom he passionately loved, and who died when he was seven. His closest friend was his younger sister, Pauline, with whom he maintained a steady correspondence throughout the first decade of the 19th century.
The military and theatrical worlds of the First French Empire were a revelation to Beyle. He was named an auditor with the Conseil d'État on 3 August 1810, and thereafter took part in the French administration and in the Napoleonic wars in Italy. He travelled extensively in Germany and was part of Napoleon's army in the 1812 invasion of Russia.

Stendhal witnessed the burning of Moscow from just outside the city. He was appointed Commissioner of War Supplies and sent to Smolensk to prepare provisions for the returning army. He crossed the Berezina River by finding a usable ford rather than the overwhelmed pontoon bridge, which probably saved his life and those of his companions. He arrived in Paris in 1813, largely unaware of the general fiasco that the retreat had become. Stendhal became known, during the Russian campaign, for keeping his wits about him, and maintaining his "sang-froid and clear-headedness." He also maintained his daily routine, shaving each day during the retreat from Moscow.

After the 1814 Treaty of Fontainebleau, he left for Italy, where he settled in Milan. He formed a particular attachment to Italy, where he spent much of the remainder of his career, serving as French consul at Trieste and Civitavecchia. His novel "The Charterhouse of Parma", written in 52 days, is set in Italy, which he considered a more sincere and passionate country than Restoration France. An aside in that novel, referring to a character who contemplates suicide after being jilted, speaks about his attitude towards his home country: "To make this course of action clear to my French readers, I must explain that in Italy, a country very far away from us, people are still driven to despair by love."

Stendhal identified with the nascent liberalism and his sojourn in Italy convinced him that Romanticism was essentially the literary counterpart of liberalism in politics. When Stendhal was appointed to a consular post in Trieste in 1830, Metternich refused his "exequatur" on account of Stendhal's liberalism and anti-clericalism.

Stendhal was a dandy and wit about town in Paris, as well as an obsessive womaniser. His genuine empathy towards women is evident in his books; Simone de Beauvoir spoke highly of him in "The Second Sex". One of his early works is "On Love," a rational analysis of romantic passion that was based on his unrequited love for Mathilde, Countess Dembowska, whom he met while living at Milan. This fusion of, and tension between, clear-headed analysis and romantic feeling is typical of Stendhal's great novels; he could be considered a Romantic realist.

Stendhal suffered miserable physical disabilities in his final years as he continued to produce some of his most famous work. As he noted in his journal, he was taking iodide of potassium and quicksilver to treat his syphilis, resulting in swollen armpits, difficulty swallowing, pains in his shrunken testicles, sleeplessness, giddiness, roaring in the ears, racing pulse and "tremors so bad he could scarcely hold a fork or a pen". Modern medicine has shown that his health problems were more attributable to his treatment than to his syphilis.

Stendhal died on 23 March 1842, a few hours after collapsing with a seizure on the streets of Paris. He is interred in the Cimetière de Montmartre.

Before settling on the pen name Stendhal, he published under many pen names, including "Louis Alexandre Bombet" and "Anastasius Serpière". The only book that Stendhal published under his own name was "The History of Painting" (1817). From the publication of "Rome, Naples, Florence" (September 1817) onwards, he published his works under the pseudonym "M. de Stendhal, officier de cavalerie". He borrowed this "nom de plume" from the German city of Stendal, birthplace of Johann Joachim Winckelmann, an art historian and archaeologist famous at the time.

In 1807 Stendhal stayed near Stendal, where he fell in love with a woman named Wilhelmine, whom he called Minette, and for whose sake he remained in the city. "I have no inclination, now, except for Minette, for this blonde and charming Minette, this soul of the north, such as I have never seen in France or Italy." Stendhal added an additional "H" to make more clear the Germanic pronunciation.

Stendhal used many aliases in his autobiographical writings and correspondence, and often assigned pseudonyms to friends, some of whom adopted the names for themselves. Stendhal used more than a hundred pseudonyms, which were astonishingly diverse. Some he used no more than once, while others he returned to throughout his life. "Dominique" and "Salviati" served as intimate pet names. He coins comic names "that make him even more bourgeois than he really is: Cotonnet, Bombet, Chamier." He uses many ridiculous names: "Don phlegm", "Giorgio Vasari", "William Crocodile", "Poverino", "Baron de Cutendre". One of his correspondents, Prosper Mérimée, said: "He never wrote a letter without signing a false name."

Stendhal's "Journal" and autobiographical writings include many comments on masks and the pleasures of "feeling alive in many versions." "Look upon life as a masked ball," is the advice that Stendhal gives himself in his diary for 1814. In "Memoirs of an Egotist" he writes: "Will I be believed if I say I'd wear a mask with pleasure and be delighted to change my name?...for me the supreme happiness would be to change into a lanky, blonde German and to walk about like that in Paris."

Contemporary readers did not fully appreciate Stendhal's realistic style during the Romantic period in which he lived. He was not fully appreciated until the beginning of the 20th century. He dedicated his writing to "the Happy Few" (in English in the original). This can be interpreted as a reference to Canto 11 of Lord Byron's "Don Juan", which refers to "the thousand happy few" who enjoy high society, or to the "we few, we happy few, we band of brothers" line of William Shakespeare's "Henry V", but Stendhal's use more likely refers to "The Vicar of Wakefield" by Oliver Goldsmith, parts of which he had memorized in the course of teaching himself English.

In "The Vicar of Wakefield", "the happy few" refers ironically to the small number of people who read the title character's obscure and pedantic treatise on monogamy. As a literary critic, such as in "Racine and Shakespeare", Stendhal championed the Romantic aesthetic by unfavorably comparing the rules and strictures of Jean Racine's classicism to the freer verse and settings of Shakespeare, and supporting the writing of plays in prose.

Today, Stendhal's works attract attention for their irony and psychological and historical dimensions. Stendhal was an avid fan of music, particularly the works of the composers Domenico Cimarosa, Wolfgang Amadeus Mozart and Gioacchino Rossini. He wrote a biography of Rossini, "Vie de Rossini" (1824), now more valued for its wide-ranging musical criticism than for its historical content.

In his works, Stendhal reprised excerpts appropriated from Giuseppe Carpani, Théophile Frédéric Winckler, Sismondi and others.




Stendhal's brief memoir, "Souvenirs d'Égotisme" ("Memoirs of an Egotist") was published posthumously in 1892. Also published was a more extended autobiographical work, thinly disguised as the "Life of Henry Brulard".


His other works include short stories, journalism, travel books ("A Roman Journal"), a famous collection of essays on Italian painting, and biographies of several prominent figures of his time, including Napoleon, Haydn, Mozart, Rossini and Metastasio.

In Stendhal's 1822 classic "On Love" he describes or compares the "birth of love", in which the love object is 'crystallized' in the mind, as being a process similar or analogous to a trip to Rome. In the analogy, the city of Bologna represents "indifference" and Rome represents "perfect love":

When we are in Bologna, we are entirely indifferent; we are not concerned to admire in any particular way the person with whom we shall perhaps one day be madly in love; even less is our imagination inclined to overrate their worth. In a word, in Bologna "crystallization" has not yet begun. When the journey begins, love departs. One leaves Bologna, climbs the Apennines, and takes the road to Rome. The departure, according to Stendhal, has nothing to do with one's will; it is an instinctive moment. This transformative process actuates in terms of four steps along a journey:


This journey or crystallization process (shown above) was detailed by Stendhal on the back of a playing card while speaking to Madame Gherardi, during his trip to the Salzburg salt mine.

Hippolyte Taine considered the psychological portraits of Stendhal's characters to be "real, because they are complex, many-sided, particular and original, like living human beings." Émile Zola concurred with Taine's assessment of Stendhal's skills as a "psychologist", and although emphatic in his praise of Stendhal's psychological accuracy and rejection of convention, he deplored the various implausibilities of the novels and Stendhal's clear authorial intervention.

The German philosopher Friedrich Nietzsche refers to Stendhal as "France's last great psychologist" in "Beyond Good and Evil" (1886). He also mentions Stendhal in the "Twilight of the Idols" (1889) during a discussion of Dostoevsky as a psychologist, saying that encountering Dostoevsky was "the most beautiful accident of my life, more so than even my discovery of Stendhal".

Ford Madox Ford, in "The English Novel", asserts that to Diderot and Stendhal "the Novel owes its next great step forward...At that point it became suddenly evident that the Novel as such was capable of being regarded as a means of profoundly serious and many-sided discussion and therefore as a medium of profoundly serious investigation into the human case."

Erich Auerbach considers modern "serious realism" to have begun with Stendhal and Balzac. In "", he remarks of a scene in "The Red and the Black" that "it would be almost incomprehensible without a most accurate and detailed knowledge of the political situation, the social stratification, and the economic circumstances of a perfectly definite historical moment, namely, that in which France found itself just before the July Revolution."

In Auerbach's view, in Stendhal's novels "characters, attitudes, and relationships of the "dramatis personæ", then, are very closely connected with contemporary historical circumstances; contemporary political and social conditions are woven into the action in a manner more detailed and more real than had been exhibited in any earlier novel, and indeed in any works of literary art except those expressly purporting to be politico-satirical tracts."

Simone de Beauvoir uses Stendhal as an example of a feminist author. In The Second Sex de Beauvoir writes “Stendhal never describes his heroines as a function of his heroes: he provides them with their own destinies.” She furthermore points out that it “is remarkable that Stendhal is both so profoundly romantic and so decidedly feminist; feminists are usually rational minds that adopt a universal point of view in all things; but it is not only in the name of freedom in general but also in the name of individual happiness that Stendhal calls for women’s emancipation.” Yet, Beauvoir criticises Stendhal for, although wanting a woman to be his equal, her only destiny he envisions for her remains a man.

Even Stendhal's autobiographical works, such as "The Life of Henry Brulard" or "Memoirs of an Egotist", are "far more closely, essentially, and concretely connected with the politics, sociology, and economics of the period than are, for example, the corresponding works of Rousseau or Goethe; one feels that the great events of contemporary history affected Stendhal much more directly than they did the other two; Rousseau did not live to see them, and Goethe had managed to keep aloof from them." Auerbach goes on to say: 

Vladimir Nabokov was dismissive of Stendhal, in "Strong Opinions" calling him "that pet of all those who like their French plain". In the notes to his translation of "Eugene Onegin", he asserts that "Le Rouge et le Noir" is "much overrated", and that Stendhal has a "paltry style". In "Pnin" Nabokov wrote satirically, "Literary departments still labored under the impression that Stendhal, Galsworthy, Dreiser, and Mann were great writers."

In 2019, rumours began amongst the undergraduate body at the University of Oxford that Stendhal was actually a woman posing as a man. Despite having been reported as such in several essays received by the French Department, this is most likely false.

Michael Dirda considers Stendhal "the greatest all round French writer – author of two of the top 20 French novels, author of a highly original autobiography ("Vie de Henry Brulard"), a superb travel writer, and as inimitable a presence on the page as any writer you'll ever meet."

In 1817 Stendhal was reportedly overcome by the cultural richness of Florence he encountered when he first visited the Tuscan city. As he described in his book "Naples and Florence: A Journey from Milan to Reggio":

As I emerged from the porch of Santa Croce, I was seized with a fierce palpitation of the heart (that same symptom which, in Berlin, is referred to as an attack of the nerves); the well-spring of life was dried up within me, and I walked in constant fear of falling to the ground.

The condition was diagnosed and named in 1979 by Italian psychiatrist Dr. Graziella Magherini, who had noticed similar psychosomatic conditions (racing heart beat, nausea and dizziness) amongst first-time visitors to the city.

In homage to Stendhal, Trenitalia named their overnight train service from Paris to Venice the Stendhal Express.






</doc>
<doc id="26775" url="https://en.wikipedia.org/wiki?curid=26775" title="Syndicalism">
Syndicalism

Syndicalism is a radical current in the labor movement and was most active in the early 20th century. Its main idea is worker-based local organization and advancement through strikes. According to the Marxist historian Eric Hobsbawm, it predominated in the revolutionary left in the decade preceding World War I as Marxism was mostly reformist at that time.

Major syndicalist organizations included the General Confederation of Labor in France, the National Confederation of Labor in Spain, the Italian Syndicalist Union, the Free Workers' Union of Germany, and the Argentine Regional Workers' Federation. Although they did not regard themselves as syndicalists, the Industrial Workers of the World, the Irish Transport and General Workers' Union and the Canadian One Big Union are considered by most historians to belong to this current.

A number of syndicalist organizations were and still are to this day linked in the International Workers' Association, but some of its member organizations left for the International Confederation of Labor, formed in 2018.

The term "syndicalism" has French origins. In French, a "syndicat" is a trade union, usually a local union. The corresponding words in Spanish and Portuguese, "sindicato", and Italian, "sindacato", are similar. By extension, the French "syndicalisme" refers to trade unionism in general. The concept "syndicalisme révolutionnaire" or "revolutionary syndicalism" emerged in French socialist journals in 1903 and the French General Confederation of Labor ("Confédération générale du travail", CGT) came to use the term to describe its brand of unionism. "Revolutionary syndicalism", or more commonly "syndicalism" with the "revolutionary" implied, was then adapted to a number of languages by unionists following the French model.

Many scholars, including Ralph Darlington, Marcel van der Linden, and Wayne Thorpe, apply the term "syndicalism" to a number of organizations or currents within the labor movement that did not identify as "syndicalist". They apply the label to one big unionists or industrial unionists in North America and Australia, Larkinists in Ireland, and groups that identify as revolutionary industrialists, revolutionary unionists, anarcho-syndicalists, or councilists. This includes the Industrial Workers of the World (IWW) in the United States, for example, which claimed its industrial unionism was "a higher type of revolutionary labor organization than that proposed by the syndicalists". Van der Linden and Thorpe use "syndicalism" to refer to "all revolutionary, direct-actionist organizations". Darlington proposes that syndicalism be defined as "revolutionary trade unionism". He and van der Linden argue that it is justified to group together such a wide range of organizations because their similar modes of action or practice outweigh their ideological differences.

Others, like Larry Peterson and Erik Olssen, disagree with this broad definition. According to Olssen, this understanding has a "tendency to blur the distinctions between industrial unionism, syndicalism, and revolutionary socialism". Peterson gives a more restrictive definition of "syndicalism" based on five criteria: 

This definition excludes the IWW and the Canadian One Big Union (OBU). Peterson proposes the broader category "revolutionary industrial unionism" to encompass syndicalism, groups like the IWW and the OBU, and others. The defining commonality between these groups is that they sought to unite all workers in a general organization.

Syndicalism originated in France and spread from there. The French CGT was the model and inspiration for syndicalist groups throughout Europe and the world. Revolutionary industrial unionism, part of syndicalism in the broader sense, originated with the IWW in the United States and then caught on in other countries. In a number of countries, however, certain syndicalist practices and ideas predate the coining of the term in France or the founding of the IWW. In Bert Altena's view, a number of movements in Europe can be called syndicalist, even before 1900. According to the English social historian E.P. Thompson and the anarcho-syndicalist theorist Rudolf Rocker, there were syndicalist tendencies in Britain's labor movement as early as the 1830s. Syndicalists saw themselves as the heirs of the First International, the international socialist organization formed in 1864, particularly its anti-authoritarian wing led by Mikhail Bakunin. Bakunin and his followers advocated the general strike, rejected electoral politics, and anticipated workers' organizations replacing rule by the state. According to Lucien van der Walt, the Spanish section of the First International, formed in 1870, was in fact syndicalist. Kenyon Zimmer sees a "proto-syndicalism" in the influence the anarchist-led International Working People's Association (IWPA) and Central Labor Union, which originated in the American section of the First International, had in the Chicago labor movement of the 1880s. They were involved in the nationwide struggle for an eight-hour day. On May 3, 1886, the police killed three striking workers at a demonstration in Chicago. Seven policemen and four workers were killed the following day when someone, possibly a police member, threw a bomb into the crowd. Four anarchists were eventually executed for allegedly conspiring to the events. The Haymarket Affair, as these events become known, led anarchists and labor organizers, including syndicalists, in both the United States and Europe to re-evaluate the revolutionary meaning of the general strike.
According to Émile Pouget, a French anarchist and CGT leader, from "the United States, the idea of the general strike fertilized by the blood of anarchists hanged in Chicago [...] was imported to France". In the 1890s, French anarchists, conceding that individual action such as assassinations had failed, turned their focus to the labor movement. They were able to gain influence, particularly in the "bourses du travail", which served as labor exchanges, meeting places for unions, and trades councils and organized in a national federation in 1893. In 1895, the CGT was formed as a rival to the "bourses", but was at first much weaker. From the start, it advocated the general strike and aimed to unite all workers. Pouget, who was active in the CGT, supported the use of sabotage and direct action. In 1902, the "bourses" merged into the CGT. In 1906, the federation adopted the Charter of Amiens, which reaffirmed the CGT's independence from party politics and fixed the goal of uniting all French workers.

In 1905, the Industrial Workers of the World were formed in the United States by the Western Federation of Miners, the American Labor Union, and a broad coalition of socialists, anarchists, and labor unionists. Its base was mostly in the Western US where labor conflicts were most violent and workers therefore radicalized. Although Wobblies insisted their union was a distinctly American form of labor organization and not an import of European syndicalism, the IWW was syndicalist in the broader sense of the word. According to Melvyn Dubofsky and most other IWW historians, the IWW's industrial unionism was the specifically American form of syndicalism. Nevertheless, the IWW also had a presence in Canada and Mexico nearly from its inception, as the US economy and labor force was intertwined with those countries.
French syndicalism and American industrial unionism influenced the rise of syndicalism elsewhere. Syndicalist movements and organizations in a number of countries were established by activists who had spent time in France. Ervin Szabó visited Paris in 1904 and then established a Syndicalist Propaganda Group in his native Hungary in 1910. Several of the founders of the Spanish CNT had visited France. Alceste de Ambris and Armando Borghi, both leaders in Italy's USI, were in Paris for a few months from 1910 to 1911. French influence also spread through publications. Emile Pouget's pamphlets could be read in Italian, Spanish, Portuguese, English, German, and Swedish translations. Journals and newspapers in a number of countries advocated syndicalism. For example, " L'Action directe", a journal mainly for miners in Charleroi, Belgium, urged its readers to follow "the example of our confederated friends of France". The IWW's newspapers published articles on French syndicalism, particularly the tactic of sabotage and the CGT's "La Vie Ouvrière" carried articles about Britain's labor movement by the British syndicalist Tom Mann. Migration played a key role in spreading syndicalist ideas. The Argentine Regional Workers' Federation ("Federación Obrera Regional Argentina", FORA), openly anarchist by 1905, was formed by Italian and Spanish immigrants in 1901. Many IWW leaders were European immigrants, including Edmondo Rossoni who moved between the United States and Italy and was active in both the IWW and USI. International work processes also contributed to the diffusion of syndicalism. For example, sailors helped establish IWW presences in port cities around the world.

Syndicalists formed different kinds of organizations. Some, like the French radicals, worked within existing unions to infuse them with their revolutionary spirit. Some found existing unions entirely unsuitable and built federations of their own, a strategy known as "dual unionism". American syndicalists formed the IWW, though William Z. Foster later abandoned the IWW after a trip to France and set up the Syndicalist League of North America (SLNA), which sought to radicalize the established American Federation of Labor (AFL). In Ireland, the ITGWU broke away from a more moderate, and British-based, union. In Italy and Spain, syndicalists initially worked within the established union confederations before breaking away and forming USI and the CNT respectively. In Norway, there were both the Norwegian Trade Union Opposition ("Norske Fagopposition", NFO), syndicalists working within the mainstream Norwegian Confederation of Trade Unions ("Landsorganisasjonen i Norge" in Norwegian, LO), and the Norwegian Syndicalist Federation ("Norsk Syndikalistik Federation" in Norwegian, NSF), an independent syndicalist organization set up by the Swedish SAC. In Britain, there was a similar conflict between ISEL and the local IWW organization.

By 1914, there were syndicalist national labor confederations in Peru, Brazil, Argentina, Mexico, the Netherlands, Germany, Sweden, Spain, Italy, and France, while Belgian syndicalists were in the process of forming one. There were also groups advocating syndicalism in Russia, Japan, the United States, Portugal, Norway, Denmark, Hungary, and Great Britain. Outside of North America, the IWW also had organizations in Australia, New Zealand, where it was part of the Federation of Labour (FOL), Great Britain, though its membership had imploded by 1913, and South Africa. In Ireland, syndicalism took the form of the ITGWU, which espoused a mix of industrial unionism and socialist republicanism, and was labeled Larkinism.

Scholars have given several explanations for the emergence of syndicalism. Werner Sombart, a German economist and sociologist, commenting in 1905, ascribes the rise of syndicalism to the Italian and particularly the French mentality. He writes: "The only people who could possibly act up to such a system of teaching are Frenchmen and Italians. They are generally men who do things impulsively [...], who are seized upon by a sudden passionate enthusiasm [...], but they have little application, perseverance, calm or steadiness."

There was a significant uptick in workers' radicalism in most developed capitalist countries from 1911 to 1922, though it relented during World War I. Strikes increased in frequency, numbers of workers involved, and duration. According to van der Linden and Thorpe, syndicalism was only one way this radicalization expressed itself. In the United Kingdom, for example, the period from 1910 to 1914 became known as the Great Labour Unrest. Many historians see syndicalism as a consequence of this unrest, but Elie Halévy and the politician Lord Robert Cecil claim it was its cause. Employers in France likewise blamed an upsurge in workers' militancy in the same period on syndicalist leaders. Syndicalism was further encouraged by employers' hostility to workers' actions. The economist Ernesto Screpanti hypothesized that strike waves such as the one from 1911 to 1922 generally occur during the upper turning-points of the periodic global long cycles of boom and bust known as Kondratieff waves. Such waves of proletarian insurgency, claims Screpanti, were global in reach, saw workers breaking free of the dynamics of the capitalist system, and aimed to overthrow that system.

According to van der Linden and Thorpe, workers' radicalization manifested itself in their rejection of the dominant strategies in the, mostly socialist, labor movement, which was led by reformist trade unions and socialist parties. Lenin posited that "revolutionary syndicalism in many countries was a direct and inevitable result of opportunism, reformism and parliamentary cretinism." A feeling that ideological disputes were draining workers' power led Dutch, French, and American syndicalist organizations to declare themselves independent of any political groups. In countries like Italy, Spain, and Ireland, which was still under British rule, parliamentary politics were not seen as a serious means for workers to express their grievances. Most workers were disenfranchised. Yet even in France or Britain, where most male workers had the right to vote, many workers did not trust party politics. The enormous numerical growth of well-organized socialist parties, such as in Germany and Italy, did not, in the minds of many workers, correlate with any real advance in the class struggle as these parties were thought to be overly concerned with building the parties themselves and with electoral politics than with the class struggle and had therefore lost their original revolutionary edge. The socialists preached the inevitability of socialism, but were in practice bureaucratic and reformist. Similarly, the trade unions frequently allied with those parties, equally growing in numbers, were denounced for their expanding bureaucracies, their centralization, and for failing to represent workers' interests. For example, between 1902 and 1913 the German free trade unions' membership grew by 350% but its bureaucracy by more than 1900%.

Another common explanation for the rise of syndicalism is that it was a result of the economic backwardness of the countries in which it emerged, particularly France. Newer studies have questioned this account. According to van der Linden and Thorpe, changes in labor processes contributed to the radicalization of workers and thereby to the rise of syndicalism. This rise took place during the Second Industrial Revolution. Two groups of workers were most attracted to syndicalism: casual or seasonal laborers who frequently changed jobs, and workers whose occupations were becoming obsolete as a result of technological advances. The first group includes landless agricultural workers, construction workers, and dockers, all of whom were disproportionately represented in several countries' syndicalist movements. Because they frequently changed jobs, such workers did not have close relationships with their employers and the risk of losing one's job as a result of a strike was reduced. Moreover, because of the time constraints of their jobs they were forced to act immediately in order to achieve anything and could not plan for the long term by building up strike funds or powerful labor organizations or by engaging in mediation. Their working conditions gave them an inclination to engage in direct confrontation with employers and apply direct action. The second group includes miners, railway employees, and certain factory workers. Their occupations were deskilled by technological and organizational changes. These changes made workers from the second group similar in some respects to the first group. They did not entirely result from the introduction of new technology, but were also caused by changes in management methods. This included increased supervision of workers, piecework, internal promotions, all designed make workers docile and loyal and to transfer knowledge and control over the process of production from workers to employers. Frustration with this loss of power led to formal and informal resistance by workers. Altena disagrees with this explanation. According to him, it was workers with significant autonomy in their jobs and pride in their skills who were most attracted to syndicalism. Moreover, he argues, explanations based on workers' occupations cannot explain why only a minority of workers in those jobs became syndicalists or why in some professions workers in different locations had vastly different patterns of organization. The small size of many syndicalist unions also makes observations about which workers joined statistically irrelevant.
Syndicalism came to be seen as a viable strategy because the general strike became a practical possibility. Although it had been advocated before, there were not sufficient numbers of wage workers to bring society to a standstill and they had not achieved a sufficient degree of organization and solidarity until the 1890s, according van der Linden and Thorpe. Several general or political strikes then took place before World War: in 1893 and in 1902 in Belgium, in 1902 and in 1909 in Sweden, in 1903 in the Netherlands, in 1904 in Italy in addition to significant work stoppages during the Russian Revolution of 1905.

Darlington cites the significance of the conscious intervention by syndicalist militants. The industrial unrest of the period created conditions which made workers receptive to syndicalist leaders' agitation. They spread their ideas through pamphlets and newspapers and had considerable influence in a number of labor disputes. Finally, van der Linden and Thorpe point to spatial and geographical factors that shaped the rise of syndicalism. Workers who would otherwise not have had an inclination to syndicalism joined because syndicalism was dominant in their locales. Workers in the Canadian and American West for example, were generally more radical and drawn to the IWW and One Big Union than their counterparts in the East. Similarly, southern workers were more drawn to syndicalism in Italy. According to Altena, the emergence of syndicalism must be analyzed at the level of local communities. Only differences in local social and economic structures explain why some towns had a strong syndicalist presence, but others did not.
Syndicalism was not informed by theory or a systematically elaborated ideology the same way socialism was by Marxism. Émile Pouget, a CGT leader, maintained that: "What sets syndicalism apart from the various schools of socialism and makes it superior is its doctrinal sobriety. Inside the unions, there is little philosophising. They do better than that: they act!" Similarly, Andreu Nin of the Spanish CNT proclaimed in 1919: "I am a fanatic of action, of revolution. I believe in actions more than in remote ideologies and abstract questions." Though workers' education was important at least to committed activists, syndicalists distrusted bourgeois intellectuals, wanting to maintain workers' control over the movement. Syndicalist thinking was elaborated in pamphlets, leaflets, speeches, and articles and in the movement's own newspapers. These writings consisted mainly in calls to action and discussions of tactics in class struggle. The philosopher Georges Sorel's "Reflections on Violence" introduced syndicalist ideas to a broader audience. Sorel fancied himself the premier theorist of syndicalism and was frequently thought of as such, but he was not a part of the movement and his influence on syndicalism was insignificant, except in Italy and Poland.

The extent to which syndicalist positions reflected merely the views of leaders and to what extent those positions were shared by syndicalist organizations' rank-and-file is a matter of dispute. The historian Peter Stearns, commenting on French syndicalism, concludes that most workers did not identify with syndicalism's long-range goals and that syndicalist hegemony accounts for the relatively slow growth of the French labor movement as a whole. Workers who joined the syndicalist movement, he claims, were on the whole indifferent to doctrinal questions, their membership in syndicalist organizations was partly accidental and leaders were unable to convert workers to syndicalist ideas. Frederick Ridley, a political scientist, is more equivocal. According to him, leaders were very influential in the drafting of syndicalist ideas, but syndicalism was more than a mere tool of a few leaders, but a genuine product of the French labor movement. Darlington adds that in the Irish ITGWU most members were won over by the union's philosophy of direct action. Bert Altena argues that, though evidence of ordinary workers' convictions is scant, it indicates that they were aware of doctrinal differences between various currents in the labor movement and able to defend their own views. He points out that they likely understood syndicalist newspapers and debated political issues.

"Syndicalism" is used by some interchangeably with "anarcho-syndicalism". This term was first used in 1907, by socialists criticizing the political neutrality of the CGT, although it was rarely used until the early 1920s when communists used it disparagingly. Only from 1922 was it used by self-avowed anarcho-syndicalists. Syndicalism has traditionally been seen as a current within anarchism, but in some countries it was dominated by Marxists rather than anarchists. This was the case in Italy and much of the Anglophone world, including Ireland where anarchists had no significant influence on syndicalism. The extent to which syndicalist doctrine was a product of anarchism is debated. The anarchist Iain McKay argues that "syndicalism" is but a new name for ideas and tactics developed by Bakunin and the anarchist wing of the First International, while it is wholly inconsistent with positions Marx and Engels took. According to him, the fact that many Marxists embraced syndicalism merely indicates that they abandoned Marx's views and converted to Bakunin's. Altena too views syndicalism as part of the broader anarchist movement, but concedes there was a tension between this and the fact that it was also a labor movement. He also sees Marxist ideas reflected in the movement, as leading syndicalists such as F. Domela Nieuwenhuis and Christiaan Cornelissen as well as much of the Australian syndicalist movement were influenced by them, as well as older socialist notions. According to Darlington, anarchism, Marxism, and revolutionary trade unionism equally contributed to syndicalism, in addition to various influences in specific countries, including Blanquism, anti-clericalism, republicanism, and agrarian radicalism.

Bill Haywood, a leading figure in the IWW, defined the union's purpose at its founding congress as "the emancipation of the working class from the slave bondage of capitalism". Syndicalists held that society was divided into two great classes, the working class and the bourgeoisie. Their interests being irreconcilable, they must be in a constant state of class struggle. Tom Mann, a British syndicalist, declared that "the object of the unions is to wage the Class War". This war, according to syndicalist doctrine, was aimed not just at gaining concessions such as higher wages or a shorter working day, but at the revolutionary overthrow of capitalism.

Syndicalists agreed with Karl Marx's characterization of the state as the "executive committee of the ruling class". They held that a society's economic order determined its political order and concluded that the former could not be overthrown by changes to the latter. Nevertheless, a number of leading syndicalist figures worked in political parties and some ran for elected office. Jim Larkin, the leader of the Irish ITGWU, was active in the Labour Party, Haywood in the Socialist Party of America. Yet, they saw the economic sphere as the primary arena for revolutionary struggle, while involvement in politics could at best be an "echo" of industrial struggle. They were skeptical of parliamentary politics. According to Father Thomas Hagerty, a Catholic priest and IWW leader, "dropping pieces of paper into a hole in a box never did achieve emancipation for the working class, and to my thinking it will never achieve it". Syndicalist trade unions declared their political neutrality and autonomy from political parties. Political parties, syndicalists reasoned, grouped people according to their political views, uniting members of different classes. Unions, on the other hand, were to be purely working-class organizations, uniting the entire class, and could therefore not be divided on political grounds. The French syndicalist Pouget explained: "The CGT embraces outside of all the schools of politics all workers cognisant of the struggle to be waged for the elimination of wage-slavery and the employer class." In practice, however, this neutrality was more ambiguous. The CGT, for example, worked with the Socialist Party in the struggle against the Three-Year Law, which extended conscription. During the Spanish Civil War the CNT, whose policy barred anyone who had been a candidate for political office or had participated in political endeavors from representing it, was intimately connected with the Iberian Anarchist Federation ("Federación Anarquista Ibérica", FAI).

In the syndicalist conception, unions played a dual role. They were organs of struggle within capitalism for better working conditions, but they were also to play a key role in the revolution to overthrow capitalism. Victor Griffuelhes expressed this at the CGT's 1906 congress in the following manner: "In its day-to-day demands, syndicalism seeks the co-ordination of workers' efforts, the increase of workers' well-being by the achievement of immediate improvements, such as the reduction of working hours, the increase of wages, etc. But this task is only one aspect of the work of syndicalism; it prepares for complete emancipation, which can be realised only by expropriating the capitalist class". For unions to fulfill this role, it was necessary to prevent bureaucrats "whose sole purpose in life seems to be apologising for and defending the capitalist system of exploitation", according to Larkin from inhibiting workers' militant zeal. Battling bureaucracy and reformism within the labor movement was a major theme for syndicalists. One expression of this was many syndicalists' rejection of collective bargaining agreements, which were thought to force labor peace upon workers and break their solidarity. The Wobblie Vincent St. John declared: "There is but one bargain that the Industrial Workers of the World will make with the employing class complete surrender of the means of production." The Argentine Regional Workers' Federation ("Federación Obrera Regional Argentina", FORA) and the OBU did, however, accept such deals and others began accepting them eventually. Similarly, syndicalist unions did not work to build large strike funds, for fear that they would create bureaucracy separate from the rank-and-file and instill in workers the expectation that the union rather than they would wage the class struggle.
Syndicalists advocated direct action, including working to rule, passive resistance, sabotage, and strikes, particularly the general strike, as tactics in the class struggle, as opposed to indirect action such as electoral politics. The IWW engaged in around 30 mostly successful civil disobedience campaigns they deemed free speech fights. Wobblies would defy laws restricting public speeches, in order to clog up prisons and court systems as a result of hundreds of arrests, ultimately forcing public officials to rescind such laws. Sabotage ranged from slow or inefficient work to destruction of machinery and physical violence. French railway and postal workers cut telegraph and signal lines during strikes in 1909 and 1910.

The final step towards revolution, according to syndicalists, would be a general strike. It would be "the curtain drop on a tired old scene of several centuries, and the curtain raising on another", according to Griffuelhes.

Syndicalists remained vague about the society they envisioned to replace capitalism, claiming that it was impossible to foresee in detail. Labor unions were seen as being the embryo of a new society in addition to being the means of struggle within the old. Syndicalists generally agreed that in a free society production would be managed by workers. The state apparatus would be replaced by the rule of workers' organizations. In such a society individuals would be liberated, both in the economic sphere but also in their private and social lives.

Syndicalist policies on gender issues were mixed. The CNT did not admit women as members until 1918. The CGT dismissed feminism as a bourgeois movement. Syndicalists were mostly indifferent to the question of women's suffrage. Elizabeth Gurley Flynn, an IWW organizer, insisted that women "find their power at the point of production where they work", rather than at the ballot box. Of the 230 delegates present at the founding of Canada's One Big Union, a mere 3 were women. When a female radical criticized the masculinist atmosphere at the meeting, she was rebuffed by men who insisted that labor only concern itself with class rather than gender issues. The historian Todd McCallum concludes that syndicalists in the OBU advocated values of "radical manhood". Francis Shor argues that the "IWW promotion of sabotage represents a kind of masculine posturing which directly challenged the individualizing techniques of power mobilized by industrial capitalism". Thus, "the IWW's masculine identity incorporated features of working-class solidarity and protest [...] through 'virile' syndicalism." For example, while defending a black fellow worker against a racist insult, an IWW organizer in Louisiana insisted that "he is a man, a union man, an IWW—a MAN! ... and he has proven it by his action". During WWI, one of the IWW's anti-war slogans was "Don’t Be a Soldier! Be a Man!" In some case syndicalist attitudes towards women changed. In 1901, the CGT's agricultural union in southern France was hostile to women, but by 1909 this had changed. The CNT, initially hostile to independent women's organizations, worked closely with the libertarian feminist organization "Mujeres Libres" during the Civil War.
According to the historian Sharif Gemie, the male orientation of parts of the syndicalist labor movement reflected the ideas of the anarchist Pierre-Joseph Proudhon, who defended patriarchy because women, of their own accord, are "chained to nature".

Syndicalists were involved in a number strikes, labor disputes, and other struggles. In the United States, the IWW was involved in at least 150 strikes including miners' strikes in Goldfield, Nevada in 1906–1907, a steel workers' strike in McKees Rocks, Pennsylvania in 1909, a textile workers' strike in Lawrence, Massachusetts, timber workers' strikes in Louisiana and Arkansas in 1912–1913, and a silk workers' strike in Paterson, New Jersey. The most prominent was the struggle in Lawrence. Wobblie leaders brought together 23,000 mostly immigrant workers, many of whom did not speak English. They arranged for workers' children to be sent to live with sympathetic families outside of Lawrence for the duration of the strike so their parents could focus on the struggle. Unlike most IWW-led strikes, the struggle was successful. In Mexico, syndicalism first emerged in 1906 during a violent miners' strike in Cananea and an even more violent textile workers' strike in Río Blanco, Veracruz. In 1912, during the 1910–1920 Mexican Revolution, anarchists formed the syndicalist union House of the World Worker ("Casa del Obrero Mundial"). It led a series of successful strikes in 1913 in Mexico City and central Mexico. After the Constitutionalist Army occupied the capital in 1914, syndicalists allied with the government it established to defeat rural forces such as the Zapatistas and therefore received government support. Once those forces had been suppressed, this alliance broke apart and the "Casa" campaigned for workers' control of factories and the nationalization of foreign capital. It contributed to a rise in labor unrest that began in mid-1915. It led general strikes in May and in July–August 1916 in greater Mexico City. The latter was quelled by the army, marking the defeat of the "Casa", which was also suppressed.

In Portugal, the deposition of the King in 1910 was followed by a strike wave throughout the country. After the police occupied the offices of an agricultural union, syndicalists called for a general strike. During the strike, Lisbon was controlled by workers and there were armed uprisings in several other cities. In 1912, the strike wave ebbed off. Italian syndicalists successfully organized agricultural workers in the Po Valley by uniting different parts of agricultural working class. They were most successful in areas where the reformist union "Federterra" had been thwarted by employers. Syndicalists led large strikes by farm workers in Parma and Ferrara in 1907–1908, but these strikes failed as a result of employers' strikebreaking tactics and infighting among workers. In 1911–1913, syndicalists played an important role in a large strike wave in Italy's industrial centers. The syndicalist union confederation USI was formed in 1912 by veterans of both strike movements.

British Wobblies were involved in two major strikes in Scotland, one at Argyll Motor Works and the second at a Singer's sewing machine factory in Clydebank. In 1906, several industrial unionists began to spread their ideas and organize workers at Singer's. In 1911, they organized a strike after a woman was fired for not working hard enough. The strike was cleverly defeated by management and most activists lost their jobs. The ISEL leader Tom Mann was also at the center of several labor disputes during the Great Labour Unrest, including the 1911 Liverpool general transport strike where he chaired the strike committee. In Ireland, Jim Larkin and the ITGWU led 20,000 during the 1913 Dublin lockout. After the ITGWU attempted to unionize Dublin's trams and tram workers went on strike, the city's employers threatened to fire any workers who did not sign a pledge to not support the ITGWU, thereby turning the dispute into a city-wide conflict in late September. Workers' resistance crumbled in January 1914.
There was no international syndicalist organization prior to World War I. In 1907, CGT activists presented the Charter of Amiens and syndicalism to an international audience a higher form of anarchism at the International Anarchist Congress of Amsterdam in 1907. Discussions at the Congress led to the formation of the international syndicalist journal "Bulletin international du mouvement syndicaliste". The CGT was affiliated with the International Secretariat of National Trade Union Centers (ISNTUC), which brought together reformist socialist unions. Both the Dutch NAS and the British ISEL attempted to remedy the lack of a syndicalist counterpart to ISNTUC in 1913, simultaneously publishing calls for an international syndicalist congress in 1913. The CGT rejected the invitation. Its leaders feared that leaving ISNTUC, which it intended to revolutionize from within, would split the CGT and harm working-class unity. The IWW also did not participate, as it considered itself an international in its own right. The First International Syndicalist Congress was held in London from September 27 to October 2. It was attended by 38 delegates from 65 organizations in Argentina, Austria, Belgium, Brazil, Cuba, France, Germany, Italy, the Netherlands, Poland, Spain, Sweden, and the United Kingdom. Discussions were contentious and did not lead to the founding of a syndicalist international. Delegates did agree on a declaration of principles describing syndicalism's core tenets. They also decided to launch an International Syndicalist Information Bureau and to hold another congress in Amsterdam. This congress did not take place due to the outbreak of World War I.
Syndicalists had long opposed nationalism and militarism. Haywood held that "it is better to be a traitor to your country than to your class". French syndicalists viewed the Army as the primary defender of the capitalist order. In 1901, the CGT published a manual for soldiers encouraging desertion. Similarly, in 1911 British syndicalists distributed an "Open Letter to British Soldiers" imploring them not to shoot on striking workers, but to join the working class's struggle against capital. Patriotism, syndicalists argued, was a means of integrating workers into capitalist society by distracting them from their true class interest. In 1908, the CGT's congress invoked the slogan of the First International, proclaiming that the "workers have no fatherland".
When World War I broke out in July 1914, socialist parties and trade unions both in neutral and belligerent countries supported their respective nations' war efforts or national defense, despite previous pledges to do the opposite. Socialists agreed to put aside class conflict and vote for war credits. German socialists argued that war was necessary to defend against Russia's barbaric Tsarism, while their French counterparts pointed to the need to defend against Prussian militarism and the German "instinct of domination and of discipline". This collaboration between the socialist movement and the state was known as the "union sacrée" in France, the "Burgfrieden" in Germany, and "godsvrede" in the Netherlands. Moreover, a number of anarchists led by Peter Kropotkin, including the influential syndicalist Christiaan Cornelissen, issued the Manifesto of the Sixteen, supporting the Allied cause in the war. Most syndicalists, however, remained true to their internationalist and anti-militarist principles by opposing the war and their respective nation's participation in it.

The majority of the French CGT and a sizable minority in the Italian USI did not. The CGT had long had a moderate, reformist wing, which gained the upper hand. As a result, according to historians like Darlington or van der Linden and Thorpe, the CGT was no longer a revolutionary syndicalist organization after the start of World War I. It followed the French president's call for national unity by agreeing to a no-strike pledge and to resolve labor disputes through arbitration and by actively participating in the French war effort. Most of its members of military age were conscripted without resistance and its ranks shrank from 350,000 in 1913 to 49,000 dues-paying members in 1915. CGT leaders defended this course by arguing that France's war against Germany was a war between democracy and republicanism on the one side and barbaric militarism on the other. Italy did not initially participate in World War I, which was deeply unpopular in the country, when it broke out. The Socialist Party and the reformist General Confederation of Labor opposed Italian intervention in the Great War. Once Italy became a participant, the socialists refused to support the war effort, but also refrained from working against it. From the start of the war, even before Italy did so, a minority within USI, led by the most famous Italian syndicalist, Alceste De Ambris, called on the Italian state to take the Allies' side. The pro-war syndicalists saw Italian participation in the war as the completion of nationhood. They also felt compelled to oppose the socialists' neutrality and therefore support the war. Finally, they gave similar arguments as the French, warning of the dangers posed by the "suffocating imperialism of Germany", and felt obliged to follow the CGT's lead.
USI's pro-war wing had the support of less than a third of the organization's members and it was forced out in September 1914. Its anarchist wing, led by Armando Borghi, was firmly opposed to the war, deeming it incompatible with workers' internationalism and predicting that it would only serve elites and governments. Its opposition was met with government repression and Borghi and others were interned by the end of the war. The anti-war faction in the CGT, on the other hand, was a small minority. It was led by the likes of Pierre Monatte and Alphonse Merrheim. They would link up with anti-war socialists from around Europe at the 1915 Zimmerwald conference. They faced considerable difficulties putting up meaningful resistance against the war. The government called up militants to the Army, including Monatte. He considered refusing the order and being summarily executed, but decided this would be futile. Syndicalist organizations in other countries nearly unanimously opposed the war. "Let Germany win, let France win, it is all the same to the workers," José Negre of the CNT in neutral Spain declared. The CNT insisted that syndicalists could support neither side in an imperialist conflict. A wave of pro-British sentiment swept Ireland during the war, although the ITGWU and the rest of the Irish labor movement opposed it, and half of the ITGWU's membership enlisted in the British military. The ITGWU had also been significantly weakened in 1913 in the Dublin Lockout. After Jim Larkin left Ireland in 1914, James Connolly took over leadership of the union. Because of the organization's weakness, Connolly allied it along with its paramilitary force, the Irish Citizen Army, with the Irish Republican Brotherhood. Together, they instigated the Easter Rising, seeking to weaken the British Empire and hoping that the insurrection would spread throughout Europe. The uprising was quickly quelled by the British army and Connolly was executed. In Germany, the small FVdG opposed the socialists' "Burgfrieden" and Germany's involvement in the war, challenging the claim that the country was waging a defensive war. Its journals were suppressed and a number of its members were arrested. The United States did not enter the war until the spring of 1917. The start of the war had induced an economic boom in the US, tightening the labor market and thereby strengthening workers' bargaining position. The IWW profited from this, more than doubling its membership between 1916 and 1917. At the same time, the Wobblies fervently denounced the war and mulled calling an anti-war general strike. Once America became a combatant, the IWW maintained its anti-war stance, while its bitter rival, the AFL, supported the war. It did not, however, launch an anti-war campaign, as it feared the government would crush it if it did and wanted to focus on its economic struggles. The IWW's practical opposition to the war was limited, 95% of eligible IWW members registered for the draft, and most of those drafted served. Syndicalists in the Netherlands and Sweden, both neutral countries, criticized the truce socialists entered with their governments in order to shore up national defense. The Dutch NAS disowned Cornelissen, one of its founders, for his support for the war.

Syndicalists from Spain, Portugal, Great Britain, France, Brazil, Argentina, Italy, and Cuba met at an anti-war congress in El Ferrol, Spain, in April 1915. The congress was poorly planned and prohibited by the Spanish authorities, but delegates managed to discuss resistance to the war and extending international cooperation between syndicalist groups. Argentine, Brazilian, Spanish, and Portuguese delegates later met in October in Rio de Janeiro to continue discussions and resolved to deepen cooperation between South American syndicalists. While syndicalists were only able to put up a rather limited practical struggle against World War I, they also looked to challenge the war on an ideological or cultural level. They pointed to the horrors of war and spurned efforts to legitimate it as something noble. German syndicalists drew attention to the death, injury, destruction, and misery that the war wrought. German, Swedish, Dutch, and Spanish syndicalists denounced nationalism with "Tierra y Libertad", a syndicalist journal in Barcelona, calling it a "grotesque mentality". The Dutch newspaper "De Arbeid" criticized nationalism, because "it finds its embodiment in the state and is the denial of class antagonism between the haves and the have-nots". German and Spanish syndicalists went further still by putting into question the concept of nationhood itself and dismissing it as a mere social construct. The Germans pointed out that most inhabitants of the German Empire identified not as Germans, but in regional terms as Prussians or Bavarians and the like. Multilingual countries like Germany and Spain also could not claim a common language as a defining characteristic of the nation nor did members of the same nation share the same values or experiences, syndicalists in Spain and Germany argued. Syndicalists also argued against the notion that the war was a clash of different cultures or that it could be justified as a defense of civilization. Various cultures were not mutually hostile, they claimed, and the state should not be seen as the embodiment of culture, since culture was the product of the entire population, while the state acted in the interests of just a few. Moreover, they argued that if culture was to be understood as "high culture", the very workers dying in the war were denied access to that culture by capitalist conditions. Finally, syndicalists railed against religious justifications for war. Before the war, they had rejected religion as divisive at best, but support for the war by both Catholic and Protestant clergy revealed their hypocrisy and disgraced the principles Christianity claimed to uphold, they claimed.

As the war progressed, disaffection with worsening living conditions at home and a growing numbers of casualties at the front eroded the enthusiasm and patriotism the outbreak of war had aroused. Prices were on the rise, food was scarce, and it became increasingly clear that the war would not be short. In Germany, for example, food shortages led to demonstrations and riots in a number of cities in the summer of 1916. At the same time, anti-war demonstrations started. Strikes picked up from around 1916 or 1917 on across Europe and soldiers began to mutiny. Workers distrusted their socialist leaders who had joined the war effort. Thanks in part to their fidelity to internationalism, syndicalist organizations profited from this development and expanded as the war drew to an end.

Disaffection with the war condensed in the post-World War I revolutions that began with the 1917 Russian Revolution. In February 1917, strikes, riots, and troop mutinies broke out in Petrograd, forcing the Russian Tsar Nicholas II to abdicate on March 2 in favor of a provisional government. Immediately, anarchist groups emerged. Russian syndicalists organized around the journal "Golos Truda" ("The Voice of Labor"), which had a circulation of around 25,000, and the Union of Anarcho-Syndicalist Propaganda. Anarchists found themselves agreeing with the Bolsheviks led by Vladimir Lenin, who returned to Russia in April, as both sought to bring down the provisional government. Lenin abandoned the idea that capitalism is a necessary stage on Russia's path to communism; dismissed the establishment of a parliament, favoring that power be taken by soviets; and called for the abolition of the police, the army, the bureaucracy, and finally the state all sentiments syndicalists shared. Although the syndicalists also welcomed the soviets, they were most enthusiastic about the factory committees and workers' councils that had emerged in all industrial centers in the course of strikes and demonstrations in the February Revolution. The committees fought for higher wages and shorter hours, but above all for workers' control over production, which both the syndicalists and Bolsheviks supported. The syndicalists viewed the factory committees as the true form of syndicalist organization, not unions. Because they were better organized, the Bolsheviks were able to gain more traction in the committees with six times as many delegates in a typical factory. Despite the goals they had in common, syndicalists became anxious about the Bolsheviks' growing influence, especially after they won majorities in the Petrograd and Moscow soviets in September.

The Petrograd Soviet established the 66-member Military Revolutionary Committee, which included four anarchists, among them the syndicalist Shatov. On October 25, this committee led the October Revolution; after taking control of the Winter Palace and key points in the capital with little resistance, it proclaimed a Soviet government. Anarchists were jubilant at the toppling of the provisional government. They were concerned about the proclamation of a new government, fearing a dictatorship of the proletariat, even more so after the Bolsheviks created the central Soviet of People's Commissars composed only of members of their party. They called for decentralization of power, but agreed with Lenin's labor program, which endorsed workers' control in all enterprises of a certain minimum size. The introduction of workers' control led to economic chaos. Lenin turned to restoring discipline in the factories and order to the economy in December by putting the economy under state control. At a trade union congress in January, the syndicalists, who had paid little attention to the unions, only had 6 delegates, while the Bolsheviks had 273. No longer depending on their help in toppling the provisional government, the Bolsheviks were now in a position to ignore the syndicalists' opposition and outvoted them at this congress. They opted to disempower local committees by subordinating them to the trade unions, which in turn became organs of the state. The Bolsheviks argued that workers' control did not mean that workers controlled factories at the local level and that this control had to be centralized and put under a broader economic plan. The syndicalists criticized the Bolshevik regime bitterly, characterizing it as state capitalist. They denounced state control over the factories and agitated for decentralization of power in politics and the economy and "syndicalization" of industry. The Civil War against the White Army split anarchists. The syndicalists were criticized harshly, because most supported the Bolshevik regime in the war even as they excoriated Bolshevik policy. They reasoned that a White victory would be worse and that the Whites had to be defeated before a third revolution could topple the Bolsheviks. Yet, syndicalists were harassed and repeatedly arrested by the police, particularly the Cheka, from 1919 on. Their demands had some sway with workers and dissidents within the Bolshevik party and the Bolshevik leadership viewed them as the most dangerous part of the libertarian movement. After the Civil War ended, workers and sailors, including both anarchists and Bolsheviks, rose up in 1921 in Kronstadt, a bastion of radicalism since 1905, against what they saw as the rule of a small number of bureaucrats. Anarchists hailed the rebellion as the start of the third revolution. The government reacted by having anarchists throughout the country arrested, including a number of syndicalist leaders. The Russian syndicalist movement was thereby defeated.

Syndicalists in the West who had opposed World War I reacted gushingly to the Russian Revolution. Though they were still coming to grips with the evolving Bolshevik ideology and despite traditional anarchist suspicions of Marxism, they saw in Russia a revolution that had taken place against parliamentary politics and under the influence of workers' councils. They also, at this point, had only limited knowledge of the reality in Russia. Augustin Souchy, a German anarcho-syndicalist, hailed it "the great passion that swept us all along. In the East, so we believed, the sun of freedom rose." The Spanish CNT declared: "Bolshevism is the name, but the idea is that of all revolutions: economic freedom. [...] Bolshevism is the new life for which we struggle, it is freedom, harmony, justice, it is the life that we want and will enforce in the world." Borghi recalled: "We exulted in its victories. We trembled at its risks. [...] We made a symbol and an altar of its name, its dead, its living and its heroes." He called on Italians to "do as they did in Russia". Indeed, a revolutionary wave, inspired in part by Russia, swept Europe in the following years.

In Germany, strikes and protests against food shortage, mainly by women, escalated and by 1917 had eroded public confidence in the government. The German Emperor was forced to abdicate in November 1918 after sailors' mutinies sparked an insurrectionary movement throughout the country. The syndicalist FVdG, which had just 6,000 members before the war and was almost completely suppressed by the state during the war, regrouped at a conference in Berlin in December 1918. It was active in the revolutionary events of the following years, particularly in the Ruhr area. It supported spontaneous strikes and championed direct action and sabotage. The FVdG started to be held in high regard for its radicalism by workers, particularly miners, who appreciated the syndicalists' ability to theorize their struggles and their experience with direct action methods. Starting in the second half of 1919, workers disappointed by the socialist party's and unions' support for the war and previously non-unionized unskilled workers who were radicalized during the war flocked to the FVdG. The revolution also saw the introduction to Germany of industrial unionism along the lines of the IWW with some support from the American organization, but also with links to the left wing of the Communist Party. In December 1919, the Free Workers' Union of Germany (Syndicalists) ("Freie Arbeiter-Union Deutschlands (Syndikalisten)", FAUD) was formed, claiming to represent over 110,000 workers, more than eighteen times the FVdG's pre-war membership. Most of the new organization came from the FVdG, but industrial unionists, whose influence was dwindling, were also involved. Rudolf Rocker, an anarchist recently returned to Germany after spending several years in London, wrote the FAUD's program.
Class struggle peaked in Italy in the years 1919–1920, which became known as the "biennio rosso" or red biennium. Throughout this wave of labor radicalism, syndicalists, along with anarchists, formed the most consistently revolutionary faction on the left as socialists sought to rein in workers and prevent unrest. The Italian syndicalist movement had split during the war, as the syndicalist supporters of Italian intervention left USI. The interventionists, led by Alceste de Ambris and Edmondo Rossoni, formed the Italian Union of Labor ("Unione Italiana del Lavoro", UIL) in 1918. The UIL's national syndicalism emphasized workers' love of labor, self-sacrifice, and the nation rather than anti-capitalist class struggle. Both USI and the UIL grew significantly during the "biennio rosso". The first factory occupation of the "biennio" was carried out by the UIL at a steel plant in Dalmine in February 1919, before the military put an end to it. In July, a strike movement spread through Italy, culminating in a general strike on July 20. While USI supported it and was convinced by the workers' enthusiasm that revolution could be possible, the UIL and the socialists were opposed. The socialists succeeded in curtailing the general strike and it imploded with a day. The government, unsettled by the radicalism on display, reacted with repression against the far left and concessions to workers and peasants.

In Portugal, working class unrest picked up from the start of the war. In 1917, radicals began to dominate the labor movement as a result of the war, the dictatorship established that year, and the influence of the Russian Revolution. In November 1918, a general strike was called but failed and in 1919 the syndicalist General Confederation of Labour ("Confederação Geral do Trabalho", CGT) was formed as the country's first national union confederation.
In Brazil, in both Rio de Janeiro and São Paulo, syndicalists, along with anarchists and socialists, were leaders in a cycle of labor struggles from 1917 to 1919. It included a general strike in 1917, a failed uprising in 1918 inspired by the Russian Revolution, and a number of smaller strikes. The movement was put down by increased organization by employers to resist workers' demands and by government repression, including the closure of unions, arrests, deportations of foreign militants, and violence, with some 200 workers killed in São Paulo alone. In Argentina, FORA had split into the anarcho-communist FORA V and the syndicalist FORA IX. While FORA V called for a futile general strike in 1915, FORA IX was more careful. It called off general strikes it had planned in 1917 and 1918. In January 1919, five workers were by the authorities during a strike led by a union with tenuous links to FORA V. At the funeral, police killed another 39 workers. Both FORA organizations called for a general strike, which continued after FORA IX reached a settlement. Vigilantes, supported by business and the military, attacked unions and militants. In all, between 100 and 700 people died in what became known as the Tragic Week. Nevertheless, strikes continued to increase and both FORA V and IX grew.

The United States underwent an increase in labor militancy during the post-war period. 1919 saw a general strike in Seattle, large miners' strikes, a police strike in Boston, and a nationwide steel strike. The IWW, however, had been nearly destroyed in the previous two years by local criminal syndicalism laws, the federal government, and vigilante violence. It attempted to take credit for some of the strikes, but in reality was too weak to play a significant role. The post-war Red Scare intensified the attacks on the IWW and by the end of 1919 the IWW was practically powerless. In 1919 Canada was hit by a labor revolt, leading to the formation of One Big Union, which was only partly industrial unionist.
Though the Bolsheviks suppressed syndicalism in Russia, they courted syndicalists abroad as part of their international strategy. In March 1919, the Comintern or Third International was founded at a conference in Moscow. The Bolsheviks acknowledged syndicalism's opposition to socialist reformism and saw them as part of the revolutionary wing of the labor movement. No syndicalists attended the founding convention, mainly because the blockade of Russia by the Allies powers made travel to Moscow near impossible. After long discussions, the CNT opted to join the Comintern, though it classified its adherence as provisional as a concession to detractors of Bolshevism. USI also decided to join, though some like Borghi had reservations about the course of events in Russia. In France, the CGT's radical minority that had opposed the war enthusiastically supported Bolshevism. They formed the Revolutionary Syndicalist Committees and attempted to push the CGT as a whole to support the Comintern. The General Executive Board of the IWW decided join the Comintern, but this decision was never confirmed by a convention. German and Swedish syndicalists were more critical of Bolshevism from the start. Rocker declared already in August 1918 that the Bolshevik regime was "but a new system of tyranny".

Syndicalists became more estranged from the Comintern in 1920. The second congress of the Comintern in the summer of 1920 was attended by numerous syndicalists. The Italian USI, the Spanish CNT, the British shop stewards, and the revolutionary minority of the CGT had official representatives, but others like John Reed of the American IWW, Augustin Souchy of the German FAUD, and the Japanese Wobbly Taro Yoshiharo also attended in an unofficial capacity. This was the first major international gathering of syndicalists since the end of the war. Western syndicalists' knowledge of the facts on the ground in Russia was at this point rather limited. They thought of the soviets as organs of workers' control over production and the Bolsheviks depicted them as such. Syndicalists were not aware of the extent to which they were in reality subordinated to the communist party. The congress, however, revealed the irreconcilable differences between the syndicalist and the Bolshevik approach. Before the congress, the Comintern's Executive Committee arranged discussions with syndicalists to challenge the reformist International Federation of Trade Unions (IFTU). A document proposed by Alexander Lozovsky derided the apolitical unions as "lackeys of imperialist capitalism" for their betrayal during the war, to which syndicalists replied that of the syndicalist unions this only applied to the CGT. Throughout the preliminary meetings, syndicalists clashed with other delegates on the questions of the dictatorship of the proletariat and the conquest of state power as well as on relations with communists and the Comintern. Eventually all syndicalists agreed to the formation of a council tasked with spreading revolutionizing the trade union movement. Disagreements continued at the congress itself.

The International Workers' Association, formed in 1922, is an international syndicalist federation of various labour unions from different countries. At its peak, it represented millions of workers and competed directly for the hearts and minds of the working class with social democratic unions and parties.

From the early 1920s, syndicalist movements in most countries began to wane. State repression played a significant role, but movements that were not suppressed also declined. Faced with this decline, syndicalist organizations had three choices: They could stay true to their revolutionary principles and be marginalized. They could give up those principles in order to adapt to new conditions. Finally, they could disband or merge into non-syndicalist organizations. The IWW is an example of the first case. The French CGT, which according to van der Linden and Thorpe was no longer syndicalist after 1914, went the second route. By the end of the 1930s, meaningful legal syndicalist organizations existed only in Bolivia, Chile, Sweden and Uruguay.

Georges Sorel played a role in shaping the views of Benito Mussolini and by extension the wider Italian fascist movement. In March 1921, Sorel wrote that Mussolini was "a man no less extraordinary than Lenin". After Sorel's death in 1922, Agostino Lanzillo, a one-time syndicalist leader who had become a fascist, wrote in the Italian fascist review "Gerarchia", which was edited by Mussolini: "Perhaps fascism may have the good fortune to fulfill a mission that is the implicit aspiration of the whole oeuvre of the master of syndicalism: to tear away the proletariat from the domination of the Socialist party, to reconstitute it on the basis of spiritual liberty, and to animate it with the breath of creative violence. This would be the true revolution that would mold the forms of the Italy of tomorrow". This movement has been often called Fascist syndicalism.

Syndicalists were involved in the resistance against fascism in several countries. In Germany, the FAUD had already been reduced to a small organization, with a membership of just over 4000 in 1932. Augustin Souchy had urged his comrades to prepare for illegality and the FAUD congress in 1932 had made plans for this. When the Nazis took power in January 1933, most local groups preemptively dissolved and hid their money and other resources to use them in their illegal work. On March 9, shortly after the Reichstag fire, the FAUD's headquarters in Berlin were searched by the police and ten people were arrested. As the SS and SA rounded up opponents of Nazism, so too many anarcho-syndicalists were put in prisons, concentration camps, and torture chambers. Syndicalists distributed a number of newspapers, pamphlets, and leaflets, some smuggled from the Netherlands and Czechoslovakia, some printed in Germany. They passed information on the situation in Germany to their fellow syndicalists abroad. They organized clandestine meetings to coordinate their activities and build an underground resistance network. Illegal syndicalist activity peaked in 1934, but by late 1934 the Gestapo started to infiltrate the underground organization and another round of arrests began. Although the start of the Spanish Civil War in 1936 briefly revitalized syndicalist activity, the syndicalist network was ultimately crushed by the Gestapo by 1937 or 1938. Most syndicalists who had not been arrested gave up at this point. Several dozen German syndicalists went into exile and some ended up in Barcelona, working for the CNT and fighting in the Spanish Civil War. In France, too, many anarchists and syndicalists were involved in the Resistance. For instance, the anarchist Georges Gourdin, an activist in the CGT's Technicians' Federation, organized links between anarchists and aided them and other refugees in escaping the Gestapo. He was arrested by the Gestapo in 1944, tortured without giving up any information, and died at a camp near Nordhausen. The best known French anarchist resister was the pacifist Jean-René Saulière. He organized an anarchist resistance group which included the exiled Russian syndicalist Volin. The same day Toulouse was liberated in August 1944, a leaflet titled "Manifesto of the Anarcho-Syndicalist Libertarian Groups" was distributed by Saulière's network throughout the city.

In Poland, syndicalists were among the first to organize resistance against Nazism. In October 1939, they formed Union of Polish Syndicalists (ZSP) with 2,000–4,000 members. It published newspapers, but also had fighting units in the resistance. In 1942, it joined the Home Army (AK) led by the Polish government-in-exile. Syndicalists also formed the "Freedom" Syndicalist Organisation (SOW), which comprised several hundred activists and also had combatant units. The ZSP and the SOW were involved in the Warsaw Uprising of 1944. They formed a company consisting of several hundred soldiers who wore red and black bands and hung red and black flags on the building they captured.
The anarcho-syndicalist revolution during the Spanish Civil War resulted in the widespread implementation of anarchist and more broadly socialist organisational principles throughout various portions of the country for two to three years, primarily Catalonia, Aragon, Andalusia and parts of the Levante, with the main organisation being the Confederación Nacional del Trabajo. Much of Spain's economy was put under worker control—in anarchist strongholds like Catalonia, the figure was as high as 75%.
On the other side, there was a national syndicalist thread represented originally by the Juntas de Ofensiva Nacional-Sindicalista of Onésimo Redondo and Ramiro Ledesma, inspired by Georges Sorel and Action Française, and primarily based amongst students in Madrid and workers and peasants in and around Valladolid. Ledesma failed to win approval for his ideas from the CNT in 1931, and instead merged into the Falange, creating the "Central Obrera Nacional-Sindicalista" in 1934. After the nationalist victory in the civil war, a corporatist and vertical Spanish Labour Organization became the sole legal trade union in Francoist Spain.

Syndicalism's decline was the result of a number of factors. In Russia, Italy, Portugal, Germany, Spain, and the Netherlands, syndicalist movements were suppressed by authoritarian governments. The IWW in the United States and the Mexican House of the World Worker were weakened considerably by state repression. Syndicalist movements that were not suppressed also declined. According to van der Linden and Thorpe, this was primarily the result of the integration of the working class into capitalist relations. Proletarian families became units of individualized consumption as standards of living increased. This was partly the result of state intervention, particularly the emergence of the welfare state. Avenues for social reform opened up and the franchise was widened, giving parliamentary reformism legitimacy. Altena agrees that the state's growing influence in society was decisive for syndicalism's diminished influence. In addition to the welfare state, he refers to the increased significance of national policies, which eroded local autonomy. This made centralized unions able to negotiate national agreements more important and national and parliamentary politics more enticing for workers. They therefore turned to social democracy in larger numbers. Additionally, according to Altena syndicalism lost out to sports and entertainment in the cultural sphere.

Vadim Dam'e adds to this that the development of capitalist production and changes in the division of labor diminished syndicalism's recruitment base. According to authors like Stearns, Edward Shorter, Charles Tilly, and Bob Holton, who deem syndicalism a transitional form of workers' resistance between older craft-based and modern factory-based industry, syndicalism's decline was a product of that transition having been completed and workers being assimilated to capitalist factory discipline. Darlington counters that syndicalism attracted a variety of workers, not just artisans and skilled workers, but concedes that such changes did play a role in Spain, France, and some other countries.

Several authors claim that syndicalism's demise was the result of workers' inherent pragmatism or conservatism, causing them to only be interested in immediate material gains, rather than long-term goals like overthrowing capitalism. Robert Hoxie, Selig Perlman, and Patrick Renshaw invoke this argument to explain the IWW's decline and Stearns, Dermot Keogh, and G. D. H. Cole do so with respect to French, Irish, and British syndicalism, respectively. Darlington disputes the assumption that workers are incapable of developing a revolutionary consciousness. Seeking material gains is not incompatible, he claims, with developing class consciousness, which entails the awareness that workers' material interests conflict with capitalism, particularly in times of crisis.

According to many Marxists, syndicalism was a reaction to reformism in the labor movement and could not survive without it. The collapse of reformism after the war therefore automatically weakened syndicalism. According Eric Hobsbawm, the biggest reason for syndicalism's decline, however, was the rise of communism. Several communist parties drew their cadres from the syndicalists' ranks. To radical workers, the programmatic distinctions between syndicalism and communism were not all that relevant. The key is that after the war communism represented militancy or revolutionary attitude as such. Darlington, too, sees the effects of the Russian Revolution as an important reason for the decline of syndicalism. The emergence of communism highlighted syndicalism's inherent weaknesses: the contradiction of building organizations that sought to be both revolutionary cadre organizations and mass labor unions, the emphasis on economic struggle to the detriment of political action and the commitment to localism limiting its ability to provide an effective centralized organization and leadership. Bolshevism's overcoming of these limitations and its success in Russia drew syndicalist leaders and members. It also exacerbated splits within the syndicalist camp.

Fascist victory in the Spanish Civil War put an end to syndicalism as a mass movement. Immediately after World War II, there were attempts to rekindle anarcho-syndicalism in Germany, but they were thwarted by Cold War anti-communism, Stalinism, and a failure to attract newer younger activists. Syndicalists maintained some influence in Latin American labor movements into the 1970s. The protest movements of the late 1960s saw renewed interest in syndicalism by activists in Germany, the US, and Britain. During its Hot Autumn of 1969, Italy experienced labor actions reminiscent of syndicalism, but syndicalists did not actually exert any influence, according to Carl Levy. In the 1980s, in communist Poland, the trade union Solidarity ("Solidarność"), though not strictly syndicalist, attracted masses of dissident workers by reviving many syndicalist ideas and practices.

The IWA exists to this day, but with very little influence. At most, it is a "flicker of history, the custodian of doctrine" according to Wayne Thorpe. Among its member organizations is the British Solidarity Federation, which was formed in 1994, but has roots going back to 1950. The German Free Workers' Union ("Freie Arbeiterinnen- und Arbeiter-Union", FAU) was formed to carry on the FAUD's tradition in 1977, but has a membership of just 350 as of 2011. It left the IWA in 2018 to form the International Confederation of Labor (ICL). Spain has several syndicalist federations, including the CNT, which has around 50,000 members as of 2018. It, too, was a member of the IWA until 2018, when it joined the FAU in forming the ICT. After being defeated in the Civil War, tens of thousands of CNT militants went into exile, mostly in France. In exile, the organization atrophied, with just 5,000 mostly older members by 1960. During Spain's transition to democracy, the CNT was revived with a peak membership of over 300,000 in 1978. However, it was soon weakened, first by accusations of having been involved in the bombing of a nightclub, then by a schism. Members who favored participation in state-sponsored union elections left and formed an organization they would eventually name the General Confederation of Labor ("Confederación General del Trabajo", CGT). Despite these concessions, the CGT still views itself as an anarcho-syndicalist organization and has around 100,000 members as of 2018.

According to Darlington, syndicalism left a legacy that was widely admired by labor and political activists in a number of countries. For example, the IWW song "Solidarity Forever" became part of the American labor movement's canon. The strike wave, including the recruitment of unskilled and foreign-born workers by the Congress of Industrial Organizations, that swept the United States in the 1930s followed in the IWW's footsteps. The tactic of the sit-down strike, made famous by the United Auto Workers in the Flint sit-down strike, was pioneered by Wobblies in 1906.

In his study of French syndicalism, Stearns concludes that it was a dismal failure. The radicalism of syndicalist labor leaders, he claims, shocked French workers and the government and thereby weakened the labor movement as a whole. Syndicalism was most popular among workers not yet fully integrated into modern capitalist industry, but most French workers had adapted to this system and accepted it. Therefore, syndicalism was not able to seriously challenge prevailing conditions or even scare politicians and employers.



</doc>
<doc id="26779" url="https://en.wikipedia.org/wiki?curid=26779" title="Soviet Union">
Soviet Union

The Soviet Union, officially known as the Union of Soviet Socialist Republics (USSR or СССР) , was a federal sovereign state in northern Eurasia that existed from 1922 to 1991. Nominally a union of multiple national Soviet republics, in practice its government and economy were highly centralized. The country was a one-party state, governed by the Communist Party with Moscow as its capital in its largest republic, the Russian Soviet Federative Socialist Republic (Russian SFSR). Other major urban centers were Leningrad, Kiev, Minsk, Tashkent, Alma-Ata, and Novosibirsk. It spanned over 10,000 kilometers (6,200 mi) east to west across 11 time zones, and over 7,200 kilometers (4,500 mi) north to south. Its territory included much of Eastern Europe, as well as part of Northern Europe and all of Northern and Central Asia. It had five climate zones: tundra, taiga, steppes, desert and mountains.

The Soviet Union had its roots in the 1917 October Revolution, when the Bolsheviks, led by Vladimir Lenin, overthrew the Russian Provisional Government which had replaced the autocratic regime of Tsar Nicholas II during World War I. In 1922, after a civil war ending in the Bolsheviks' victory, the USSR was formed by a treaty which united the Russian, Transcaucasian, Ukrainian and Byelorussian republics. Following Lenin's death in 1924 and a brief power struggle, Joseph Stalin came to power in the mid-1920s. Stalin formalized the Communist Party's ideology of Marxism–Leninism and replaced the market economy with a command economy which led to a period of rapid industrialization and collectivization. During this period, rapid economic development resulted in dramatic improvements in the average standard of living, particularly in urban areas. Despite these improvements, significant tragedies also occurred. In addition to drought, which was a primary factor in a long history of regularly occurring famines in the region, agricultural collectivization contributed to a major famine in 1932-33, causing millions of deaths. Political paranoia fermented, especially after the rise of the Nazis in Germany in 1933, culminating in the Great Purge, during which hundreds of thousands of persons accused of spying or sabotage were arrested and executed without trial.

On 23 August 1939, after unsuccessful efforts to form an anti-fascist alliance with Western powers, the Soviets signed the non-aggression agreement with Nazi Germany. After the start of World War II, the formally neutral Soviets invaded and annexed territories of several Eastern European states, including Poland and the Baltic states. In June 1941, Germany invaded the Soviet Union, opening the most extensive and bloodiest theater of war in history. Soviet casualties accounted for the highest proportion of the war in the effort of acquiring the upper hand over the Axis forces at intense battles such as Stalingrad and Kursk. In most of the territories occupied by the Red Army after its westward advance, local communists assumed power and formed governments allied with the Soviets. The post-war division of Europe into capitalist and communist halves led to increased tensions with the United States-led Western Bloc, known as the Cold War. Stalin died in 1953 and was eventually succeeded by Nikita Khrushchev, who in 1956 denounced Stalin and began a period of liberal reforms known as de-Stalinization. The Cuban Missile Crisis occurred during Khrushchev's rule, which was among the many factors that led to his removal in 1964. In the early 1970s, there was a brief détente of relations with the United States, but tensions resumed with the Soviet–Afghan War in 1979. In 1985, the last Soviet premier, Mikhail Gorbachev, sought to reform and liberalize the economy through his policies of "glasnost" (openness) and "perestroika" (restructuring). These policies caused political instability arising from nationalist and separatist movements. In 1989, Soviet-allied states in Eastern Europe were overthrown in a wave of revolutions which ended communist rule.

As part of an attempt to prevent the country's collapse, a referendum was held in March 1991, boycotted by three republics, that resulted in a majority favoring the preservation of the union as a renewed federation. Gorbachev's power was greatly diminished after Russian President Boris Yeltsin's high-profile role in facing down a coup d'état by party hardliners. In late 1991, Gorbachev resigned, and the Supreme Soviet of the Soviet Union met and formally dissolved the union. The remaining 12 constituent republics emerged as independent post-Soviet states. Russian Federation—formerly the Russian SFSR—assumed the Soviet Union's rights and obligations and became recognized as the de facto successor state. At the same time, Ukraine by law declared that it is a state-successor of both Ukrainian SSR and the Soviet Union. Today, Russia and Ukraine have an ongoing dispute over formerly-Soviet property.

The Soviet Union produced many significant technological achievements and innovations of the 20th century, including the world's first human-made satellite, the first humans in space and the first probe to land on another planet, Venus. The country had the world's second-largest economy and the largest standing military in the world. The USSR was recognized as one of the five nuclear weapons states and possessed the largest stockpile of weapons of mass destruction. It was a founding permanent member of the United Nations Security Council as well as a member of the Organization for Security and Co-operation in Europe (OSCE), the World Federation of Trade Unions (WFTU) and the leading member of the Council for Mutual Economic Assistance (CMEA) and the Warsaw Pact.

The word "Soviet" is derived from a Russian word, сове́т (sovét), meaning council, assembly, advice, harmony, concord, and all ultimately deriving from the proto-Slavic verbal stem of "vět-iti" ("to inform"), related to Slavic "věst" ("news"), English "wise", the root in "ad-vis-or" (which came to English through French), or the Dutch "weten" ("to know"; cf. "wetenschap" meaning "science"). The word "sovietnik" means "councillor".

Some organizations in Russian history were called "council" (). For example, in the Russian Empire the State Council, which functioned from 1810 to 1917, was referred to as a Council of Ministers after the revolt of 1905.

During the Georgian Affair, Vladimir Lenin envisioned an expression of Great Russian ethnic chauvinism by Joseph Stalin and his supporters, calling for these nation-states to join Russia as semi-independent parts of a greater union, which he initially named as the Union of Soviet Republics of Europe and Asia (). Stalin initially resisted the proposal but ultimately accepted it, although with Lenin's agreement changed the name to the Union of Soviet Socialist Republics (USSR), albeit all the republics began as "Socialist Soviet" and did not change to the other order until 1936. In addition, in the national languages of several republics, the word "Council/Conciliar" in the respective language was only quite late changed to an adaptation of the Russian "Soviet" and never in others, e.g. Ukraine.

The word "СССР" (in Latin alphabet: SSSR) is the abbreviation of USSR in Russian ("С"ою́з "С"ове́тских "С"оциалисти́ческих "Р"еспу́блик). It is written in Cyrillic alphabets, but Latin alphabets users sometimes borrow the word orthographically as "CCCP". In some cases, due to the length of its name, the state was referred to as the Soviet Union or the USSR, primarily when used in the Western media. It was also informally called Russia (and its citizens Russians), though that was technically incorrect since Russia was only one of the republics.

With an area of , the Soviet Union was the world's largest country, a status that is retained by the Russian Federation. Covering a sixth of Earth's land surface, its size was comparable to that of North America. Two other successor states, Kazakhstan and Ukraine, rank among the top 10 countries by land area, and the largest country entirely in Europe, respectively. The European portion accounted for a quarter of the country's area and was the cultural and economic center. The eastern part in Asia extended to the Pacific Ocean to the east and Afghanistan to the south, and, except some areas in Central Asia, was much less populous. It spanned over east to west across 11 time zones, and over north to south. It had five climate zones: tundra, taiga, steppes, desert and mountains.

The USSR had the world's longest border, like Russia, measuring over , or circumferences of Earth. Two-thirds of it was a coastline. Across the Bering Strait was the United States. The country bordered Afghanistan, China, Czechoslovakia, Finland, Hungary, Iran, Mongolia, North Korea, Norway, Poland, Romania, and Turkey from 1945 to 1991.

The country's highest mountain was Communism Peak (now Ismoil Somoni Peak) in Tajikistan, at . The USSR also included most of the world's largest lakes; the Caspian Sea (shared with Iran), and Lake Baikal, the world's largest and deepest freshwater lake that is also an internal body of water in Russia.

Modern revolutionary activity in the Russian Empire began with the 1825 Decembrist revolt. Although serfdom was abolished in 1861, it was done on terms unfavorable to the peasants and served to encourage revolutionaries. A parliament—the State Duma—was established in 1906 after the Russian Revolution of 1905, but Tsar Nicholas II resisted attempts to move from absolute to a constitutional monarchy. Social unrest continued and was aggravated during World War I by military defeat and food shortages in major cities.

A spontaneous popular uprising in Petrograd, in response to the wartime decay of Russia's economy and morale, culminated in the February Revolution and the toppling of Nicholas II and the imperial government in March 1917. The tsarist autocracy was replaced by the Russian Provisional Government, which intended to conduct elections to the Russian Constituent Assembly and to continue fighting on the side of the Entente in World War I.

At the same time, workers' councils, known in Russian as "Soviets", sprang up across the country. The Bolsheviks, led by Vladimir Lenin, pushed for socialist revolution in the Soviets and on the streets. On 7 November 1917, the Red Guards stormed the Winter Palace in Petrograd, ending the rule of the Provisional Government and leaving all political power to the Soviets. This event would later be officially known in Soviet bibliographies as the Great October Socialist Revolution. In December, the Bolsheviks signed an armistice with the Central Powers, though by February 1918, fighting had resumed. In March, the Soviets ended involvement in the war and signed the Treaty of Brest-Litovsk.

A long and bloody Civil War ensued between the Reds and the Whites, starting in 1917 and ending in 1923 with the Reds' victory. It included foreign intervention, the execution of the former tsar and his family, and the famine of 1921, which killed about five million people. In March 1921, during a related conflict with Poland, the Peace of Riga was signed, splitting disputed territories in Belarus and Ukraine between the Republic of Poland and Soviet Russia. Soviet Russia had to resolve similar conflicts with the newly established republics of Finland, Estonia, Latvia, and Lithuania.

On 28 December 1922, a conference of plenipotentiary delegations from the Russian SFSR, the Transcaucasian SFSR, the Ukrainian SSR and the Byelorussian SSR approved the Treaty on the Creation of the USSR and the Declaration of the Creation of the USSR, forming the Union of Soviet Socialist Republics. These two documents were confirmed by the first Congress of Soviets of the USSR and signed by the heads of the delegations, Mikhail Kalinin, Mikhail Tskhakaya, Mikhail Frunze, Grigory Petrovsky, and Alexander Chervyakov, on 30 December 1922. The formal proclamation was made from the stage of the Bolshoi Theatre.

An intensive restructuring of the economy, industry and politics of the country began in the early days of Soviet power in 1917. A large part of this was done according to the Bolshevik Initial Decrees, government documents signed by Vladimir Lenin. One of the most prominent breakthroughs was the GOELRO plan, which envisioned a major restructuring of the Soviet economy based on total electrification of the country. The plan became the prototype for subsequent Five-Year Plans and was fulfilled by 1931. After the economic policy of "War communism" during the Russian Civil War, as a prelude to fully developing socialism in the country, the Soviet government permitted some private enterprise to coexist alongside nationalized industry in the 1920s, and total food requisition in the countryside was replaced by a food tax.

From its creation, the government in the Soviet Union was based on the one-party rule of the Communist Party (Bolsheviks). The stated purpose was to prevent the return of capitalist exploitation, and that the principles of democratic centralism would be the most effective in representing the people's will in a practical manner. The debate over the future of the economy provided the background for a power struggle in the years after Lenin's death in 1924. Initially, Lenin was to be replaced by a "troika" consisting of Grigory Zinoviev of the Ukrainian SSR, Lev Kamenev of the Russian SFSR, and Joseph Stalin of the Transcaucasian SFSR.

On 1 February 1924, the USSR was recognized by the United Kingdom. The same year, a Soviet Constitution was approved, legitimizing the December 1922 union. Despite the foundation of the Soviet state as a federative entity of many constituent republics, each with its own political and administrative entities, the term "Soviet Russia"strictly applicable only to the Russian Federative Socialist Republicwas often applied to the entire country by non-Soviet writers and politicians.

On 3 April 1922, Stalin was named the General Secretary of the Communist Party of the Soviet Union. Lenin had appointed Stalin the head of the Workers' and Peasants' Inspectorate, which gave Stalin considerable power. By gradually consolidating his influence and isolating and outmanoeuvring his rivals within the party, Stalin became the undisputed leader of the country and, by the end of the 1920s, established a totalitarian rule. In October 1927, Zinoviev and Leon Trotsky were expelled from the Central Committee and forced into exile.

In 1928, Stalin introduced the first five-year plan for building a socialist economy. In place of the internationalism expressed by Lenin throughout the Revolution, it aimed to build Socialism in One Country. In industry, the state assumed control over all existing enterprises and undertook an intensive program of industrialization. In agriculture, rather than adhering to the "lead by example" policy advocated by Lenin, forced collectivization of farms was implemented all over the country.

Famines ensued as a result, causing deaths estimated at three to seven million; surviving kulaks were persecuted, and many were sent to Gulags to do forced labor. Social upheaval continued in the mid-1930s. Despite the turmoil of the mid-to-late 1930s, the country developed a robust industrial economy in the years preceding World War II.

Closer cooperation between the USSR and the West developed in the early 1930s. From 1932 to 1934, the country participated in the World Disarmament Conference. In 1933, diplomatic relations between the United States and the USSR were established when in November, the newly elected President of the United States, Franklin D. Roosevelt, chose to recognize Stalin's Communist government formally and negotiated a new trade agreement between the two countries. In September 1934, the country joined the League of Nations. After the Spanish Civil War broke out in 1936, the USSR actively supported the Republican forces against the Nationalists, who were supported by Fascist Italy and Nazi Germany.

In December 1936, Stalin unveiled a new constitution that was praised by supporters around the world as the most democratic constitution imaginable, though there was some skepticism.

Stalin's Great Purge resulted in the detainment or execution of many "Old Bolsheviks" who had participated in the October Revolution with Lenin. According to declassified Soviet archives, the NKVD arrested more than one and a half million people in 1937 and 1938, of whom 681,692 were shot. Over those two years, there were an average of over one thousand executions a day.

In 1939, the Soviet Union made a dramatic shift toward Nazi Germany. Almost a year after Britain and France had concluded the Munich Agreement with Germany, the Soviet Union made agreements with Germany as well, both militarily and economically during extensive talks. The two countries concluded the Molotov–Ribbentrop Pact and the German–Soviet Commercial Agreement in August 1939. The former made possible the Soviet occupation of Lithuania, Latvia, Estonia, Bessarabia, northern Bukovina, and eastern Poland. In late November, unable to coerce the Republic of Finland by diplomatic means into moving its border back from Leningrad, Stalin ordered the invasion of Finland. In the east, the Soviet military won several decisive victories during border clashes with the Empire of Japan in 1938 and 1939. However, in April 1941, the USSR signed the Soviet–Japanese Neutrality Pact with Japan, recognizing the territorial integrity of Manchukuo, a Japanese puppet state.

Germany broke the Molotov–Ribbentrop Pact and invaded the Soviet Union on 22 June 1941, starting what was known in the USSR as the "Great Patriotic War". The Red Army stopped the seemingly invincible German Army at the Battle of Moscow, aided by an unusually harsh winter. The Battle of Stalingrad, which lasted from late 1942 to early 1943, dealt a severe blow to Germany from which they never fully recovered and became a turning point in the war. After Stalingrad, Soviet forces drove through Eastern Europe to Berlin before Germany surrendered in 1945. The German Army suffered 80% of its military deaths in the Eastern Front. Harry Hopkins, a close foreign policy advisor to Franklin D. Roosevelt, spoke on 10 August 1943 of the USSR's decisive role in the war.

In the same year, the USSR, in fulfilment of its agreement with the Allies at the Yalta Conference, denounced the Soviet–Japanese Neutrality Pact in April 1945 and invaded Manchukuo and other Japan-controlled territories on 9 August 1945. This conflict ended with a decisive Soviet victory, contributing to the unconditional surrender of Japan and the end of World War II.

The USSR suffered greatly in the war, losing around 27 million people. Approximately 2.8 million Soviet POWs died of starvation, mistreatment, or executions in just eight months of 1941–42. During the war, the country together with the United States, the United Kingdom and China were considered the Big Four Allied powers, and later became the Four Policemen that formed the basis of the United Nations Security Council. It emerged as a superpower in the post-war period. Once denied diplomatic recognition by the Western world, the USSR had official relations with practically every country by the late 1940s. A member of the United Nations at its foundation in 1945, the country became one of the five permanent members of the United Nations Security Council, which gave it the right to veto any of its resolutions.

During the immediate post-war period, the Soviet Union rebuilt and expanded its economy, while maintaining its strictly centralized control. It took effective control over most of the countries of Eastern Europe (except Yugoslavia and later Albania), turning them into satellite states. The USSR bound its satellite states in a military alliance, the Warsaw Pact, in 1955, and an economic organization, The Council for Mutual Economic Assistance or Comecon, a counterpart to the European Economic Community (EEC), from 1949 to 1991. The USSR concentrated on its own recovery, seizing and transferring most of Germany's industrial plants, and it exacted war reparations from East Germany, Hungary, Romania, and Bulgaria using Soviet-dominated joint enterprises. It also instituted trading arrangements deliberately designed to favor the country. Moscow controlled the Communist parties that ruled the satellite states, and they followed orders from the Kremlin. Later, the Comecon supplied aid to the eventually victorious Communist Party of China, and its influence grew elsewhere in the world. Fearing its ambitions, the Soviet Union's wartime allies, the United Kingdom and the United States, became its enemies. In the ensuing Cold War, the two sides clashed indirectly in proxy wars.

Stalin died on 5 March 1953. Without a mutually agreeable successor, the highest Communist Party officials initially opted to rule the Soviet Union jointly through a troika headed by Georgy Malenkov. This did not last, however, and Nikita Khrushchev eventually won the ensuing power struggle by the mid-1950s. In 1956, he denounced Stalin's use of repression and proceeded to ease controls over the party and society. This was known as de-Stalinization.

Moscow considered Eastern Europe to be a critically vital buffer zone for the forward defence of its western borders, in case of another major invasion such as the German invasion of 1941. For this reason, the USSR sought to cement its control of the region by transforming the Eastern European countries into satellite states, dependent upon and subservient to its leadership. Soviet military force was used to suppress anti-Stalinist uprisings in Hungary and Poland in 1956.

In the late 1950s, a confrontation with China regarding the Soviet rapprochement with the West, and what Mao Zedong perceived as Khrushchev's revisionism, led to the Sino–Soviet split. This resulted in a break throughout the global Marxist–Leninist movement, with the governments in Albania, Cambodia and Somalia choosing to ally with China.

During this period of the late 1950s and early 1960s, the USSR continued to realize scientific and technological exploits in the Space Race, rivaling the United States: launching the first artificial satellite, Sputnik 1 in 1957; a living dog named Laika in 1957; the first human being, Yuri Gagarin in 1961; the first woman in space, Valentina Tereshkova in 1963; Alexei Leonov, the first person to walk in space in 1965; the first soft landing on the Moon by spacecraft Luna 9 in 1966; and the first Moon rovers, Lunokhod 1 and Lunokhod 2.

Khrushchev initiated "The Thaw", a complex shift in political, cultural and economic life in the country. This included some openness and contact with other nations and new social and economic policies with more emphasis on commodity goods, allowing a dramatic rise in living standards while maintaining high levels of economic growth. Censorship was relaxed as well. Khrushchev's reforms in agriculture and administration, however, were generally unproductive. In 1962, he precipitated a crisis with the United States over the Soviet deployment of nuclear missiles in Cuba. An agreement was made with the United States to remove nuclear missiles from both Cuba and Turkey, concluding the crisis. This event caused Khrushchev much embarrassment and loss of prestige, resulting in his removal from power in 1964.

Following the ousting of Khrushchev, another period of collective leadership ensued, consisting of Leonid Brezhnev as General Secretary, Alexei Kosygin as Premier and Nikolai Podgorny as Chairman of the Presidium, lasting until Brezhnev established himself in the early 1970s as the preeminent Soviet leader.

In 1968, the Soviet Union and Warsaw Pact allies invaded Czechoslovakia to halt the Prague Spring reforms. In the aftermath, Brezhnev justified the invasion along with the earlier invasions of Eastern European states by introducing the Brezhnev Doctrine, which claimed the right of the Soviet Union to violate the sovereignty of any country that attempted to replace Marxism–Leninism with capitalism.

Brezhnev presided throughout "détente" with the West that resulted in treaties on armament control (SALT I, SALT II, Anti-Ballistic Missile Treaty) while at the same time building up Soviet military might.

In October 1977, the third Soviet Constitution was unanimously adopted. The prevailing mood of the Soviet leadership at the time of Brezhnev's death in 1982 was one of aversion to change. The long period of Brezhnev's rule had come to be dubbed one of "standstill", with an ageing and ossified top political leadership. This period is also known as the Era of Stagnation, a period of adverse economic, political, and social effects in the country, which began during the rule of Brezhnev and continued under his successors Yuri Andropov and Konstantin Chernenko.

In late 1979, the Soviet Union's military intervened in the ongoing civil war in neighboring Afghanistan, effectively ending a détente with the West.

Two developments dominated the decade that followed: the increasingly apparent crumbling of the Soviet Union's economic and political structures, and the patchwork attempts at reforms to reverse that process. Kenneth S. Deffeyes argued in "Beyond Oil" that the Reagan administration encouraged Saudi Arabia to lower the price of oil to the point where the Soviets could not make a profit selling their oil, and resulted in the depletion of the country's hard currency reserves.

Brezhnev's next two successors, transitional figures with deep roots in his tradition, did not last long. Yuri Andropov was 68 years old and Konstantin Chernenko 72 when they assumed power; both died in less than two years. In an attempt to avoid a third short-lived leader, in 1985, the Soviets turned to the next generation and selected Mikhail Gorbachev. He made significant changes in the economy and party leadership, called "perestroika". His policy of "glasnost" freed public access to information after decades of heavy government censorship. Gorbachev also moved to end the Cold War. In 1988, the USSR abandoned its war in Afghanistan and began to withdraw its forces. In the following year, Gorbachev refused to interfere in the internal affairs of the Soviet satellite states, which paved the way for the Revolutions of 1989. With the tearing down of the Berlin Wall and with East and West Germany pursuing unification, the Iron Curtain between the West and Soviet-controlled regions came down.

At the same time, the Soviet republics started legal moves towards potentially declaring sovereignty over their territories, citing the freedom to secede in Article 72 of the USSR constitution. On 7 April 1990, a law was passed allowing a republic to secede if more than two-thirds of its residents voted for it in a referendum. Many held their first free elections in the Soviet era for their own national legislatures in 1990. Many of these legislatures proceeded to produce legislation contradicting the Union laws in what was known as the "War of Laws". In 1989, the Russian SFSR convened a newly elected Congress of People's Deputies. Boris Yeltsin was elected its chairman. On 12 June 1990, the Congress declared Russia's sovereignty over its territory and proceeded to pass laws that attempted to supersede some of the Soviet laws. After a landslide victory of Sąjūdis in Lithuania, that country declared its independence restored on 11 March 1990.

A referendum for the preservation of the USSR was held on 17 March 1991 in nine republics (the remainder having boycotted the vote), with the majority of the population in those republics voting for preservation of the Union. The referendum gave Gorbachev a minor boost. In the summer of 1991, the New Union Treaty, which would have turned the country into a much looser Union, was agreed upon by eight republics. The signing of the treaty, however, was interrupted by the August Coup—an attempted coup d'état by hardline members of the government and the KGB who sought to reverse Gorbachev's reforms and reassert the central government's control over the republics. After the coup collapsed, Yeltsin was seen as a hero for his decisive actions, while Gorbachev's power was effectively ended. The balance of power tipped significantly towards the republics. In August 1991, Latvia and Estonia immediately declared the restoration of their full independence (following Lithuania's 1990 example). Gorbachev resigned as general secretary in late August, and soon afterwards, the party's activities were indefinitely suspended—effectively ending its rule. By the fall, Gorbachev could no longer influence events outside Moscow, and he was being challenged even there by Yeltsin, who had been elected President of Russia in July 1991.

The remaining 12 republics continued discussing new, increasingly looser, models of the Union. However, by December all except Russia and Kazakhstan had formally declared independence. During this time, Yeltsin took over what remained of the Soviet government, including the Moscow Kremlin. The final blow was struck on 1 December when Ukraine, the second-most powerful republic, voted overwhelmingly for independence. Ukraine's secession ended any realistic chance of the country staying together even on a limited scale.

On 8 December 1991, the presidents of Russia, Ukraine and Belarus (formerly Byelorussia), signed the Belavezha Accords, which declared the Soviet Union dissolved and established the Commonwealth of Independent States (CIS) in its place. While doubts remained over the authority of the accords to do this, on 21 December 1991, the representatives of all Soviet republics except Georgia signed the Alma-Ata Protocol, which confirmed the accords. On 25 December 1991, Gorbachev resigned as the President of the USSR, declaring the office extinct. He turned the powers that had been vested in the presidency over to Yeltsin. That night, the Soviet flag was lowered for the last time, and the Russian tricolor was raised in its place.

The following day, the Supreme Soviet, the highest governmental body, voted both itself and the country out of existence. This is generally recognized as marking the official, final dissolution of the Soviet Union as a functioning state, and the end of the Cold War. The Soviet Army initially remained under overall CIS command but was soon absorbed into the different military forces of the newly independent states. The few remaining Soviet institutions that had not been taken over by Russia ceased to function by the end of 1991.

Following the dissolution, Russia was internationally recognized as its legal successor on the international stage. To that end, Russia voluntarily accepted all Soviet foreign debt and claimed Soviet overseas properties as its own. Under the 1992 Lisbon Protocol, Russia also agreed to receive all nuclear weapons remaining in the territory of other former Soviet republics. Since then, the Russian Federation has assumed the Soviet Union's rights and obligations. Ukraine has refused to recognize exclusive Russian claims to succession of the USSR and claimed such status for Ukraine as well, which was codified in Articles 7 and 8 of its 1991 law On Legal Succession of Ukraine. Since its independence in 1991, Ukraine has continued to pursue claims against Russia in foreign courts, seeking to recover its share of the foreign property that was owned by the USSR.

The dissolution was followed by a severe drop in economic and social conditions in post-Soviet states, including a rapid increase in poverty, crime, corruption, unemployment, homelessness, rates of disease, demographic losses, income inequality and the rise of an oligarchical class, along with decreases in calorie intake, life expectancy, adult literacy, and income. Between 1988/1989 and 1993/1995, the Gini ratio increased by an average of 9 points for all former socialist countries. The economic shocks that accompanied wholesale privatization were associated with sharp increases in mortality. Data shows Russia, Kazakhstan, Latvia, Lithuania and Estonia saw a tripling of unemployment and a 42% increase in male death rates between 1991 and 1994. In the following decades, only five or six of the post-communist states are on a path to joining the wealthy capitalist West while most are falling behind, some to such an extent that it will take over fifty years to catch up to where they were before the fall of the Soviet Bloc.

In summing up the international ramifications of these events, Vladislav Zubok stated: "The collapse of the Soviet empire was an event of epochal geopolitical, military, ideological, and economic significance." Before the dissolution, the country had maintained its status as one of the world's two superpowers for four decades after World War II through its hegemony in Eastern Europe, military strength, economic strength, aid to developing countries, and scientific research, especially in space technology and weaponry. 

The analysis of the succession of states for the 15 post-Soviet states is complex. The Russian Federation is seen as the legal "continuator" state and is for most purposes the heir to the Soviet Union. It retained ownership of all former Soviet embassy properties, as well as the old Soviet UN membership and permanent membership on the Security Council.

There are additionally four states that claim independence from the other internationally recognized post-Soviet states but possess limited international recognition: Abkhazia, Nagorno-Karabakh, South Ossetia, and Transnistria. The Chechen separatist movement of the Chechen Republic of Ichkeria lacks any international recognition.

During his rule, Stalin always made the final policy decisions. Otherwise, Soviet foreign policy was set by the Commission on the Foreign Policy of the Central Committee of the Communist Party of the Soviet Union, or by the party's highest body the Politburo. Operations were handled by the separate Ministry of Foreign Affairs. It was known as the People's Commissariat for Foreign Affairs (or Narkomindel), until 1946. The most influential spokesmen were Georgy Chicherin (1872–1936), Maxim Litvinov (1876–1951), Vyacheslav Molotov (1890–1986), Andrey Vyshinsky (1883–1954) and Andrei Gromyko (1909–1989). Intellectuals were based in the Moscow State Institute of International Relations.

The Communist leadership of the Soviet Union intensely debated foreign policy issues and change directions several times. Even after Stalin assumed dictatorial control in the late 1920s, there were debates, and he frequently changed positions.

During the country's early period, it was assumed that Communist revolutions would break out soon in every major industrial country, and it was the Soviet responsibility to assist them. The Comintern was the weapon of choice. A few revolutions did break out, but they were quickly suppressed (the longest lasting one was in Hungary)—the Hungarian Soviet Republic—lasted only from 21 March 1919 to 1 August 1919. The Russian Bolsheviks were in no position to give any help.

By 1921, Lenin, Trotsky, and Stalin realized that capitalism had stabilized itself in Europe and there would not be any widespread revolutions anytime soon. It became the duty of the Russian Bolsheviks to protect what they had in Russia, and avoid military confrontations that might destroy their bridgehead. Russia was now a pariah state, along with Germany. The two came to terms in 1922 with the Treaty of Rapallo that settled long-standing grievances. At the same time, the two countries secretly set up training programs for the illegal German army and air force operations at hidden camps in the USSR.

Moscow eventually stopped threatening other states, and instead worked to open peaceful relationships in terms of trade, and diplomatic recognition. The United Kingdom dismissed the warnings of Winston Churchill and a few others about a continuing communist threat, and opened trade relations and "de facto" diplomatic recognition in 1922. There was hope for a settlement of the pre-war tsarist debts, but it was repeatedly postponed. Formal recognition came when the new Labour Party came to power in 1924. All the other countries followed suit in opening trade relations. Henry Ford opened large-scale business relations with the Soviets in the late 1920s, hoping that it would lead to long-term peace. Finally, in 1933, the United States officially recognized the USSR, a decision backed by the public opinion and especially by US business interests that expected an opening of a new profitable market.

In the late 1920s and early 1930s, Stalin ordered Communist parties across the world to strongly oppose non-communist political parties, labor unions or other organizations on the left. Stalin reversed himself in 1934 with the Popular Front program that called on all Communist parties to join together with all anti-Fascist political, labor, and organizational forces that were opposed to fascism, especially of the Nazi variety.

In 1939, half a year after the Munich Agreement, the USSR attempted to form an anti-Nazi alliance with France and Britain. Adolf Hitler proposed a better deal, which would give the USSR control over much of Eastern Europe through the Molotov–Ribbentrop Pact. In September, Germany invaded Poland, and the USSR also invaded later that month, resulting in the partition of Poland. In response, Britain and France declared war on Germany, marking the beginning of World War II.

There were three power hierarchies in the Soviet Union: the legislature represented by the Supreme Soviet of the Soviet Union, the government represented by the Council of Ministers, and the Communist Party of the Soviet Union (CPSU), the only legal party and the final policymaker in the country.

At the top of the Communist Party was the Central Committee, elected at Party Congresses and Conferences. In turn, the Central Committee voted for a Politburo (called the Presidium between 1952–1966), Secretariat and the General Secretary (First Secretary from 1953 to 1966), the "de facto" highest office in the Soviet Union. Depending on the degree of power consolidation, it was either the Politburo as a collective body or the General Secretary, who always was one of the Politburo members, that effectively led the party and the country (except for the period of the highly personalized authority of Stalin, exercised directly through his position in the Council of Ministers rather than the Politburo after 1941). They were not controlled by the general party membership, as the key principle of the party organization was democratic centralism, demanding strict subordination to higher bodies, and elections went uncontested, endorsing the candidates proposed from above.

The Communist Party maintained its dominance over the state mainly through its control over the system of appointments. All senior government officials and most deputies of the Supreme Soviet were members of the CPSU. Of the party heads themselves, Stalin (1941–1953) and Khrushchev (1958–1964) were Premiers. Upon the forced retirement of Khrushchev, the party leader was prohibited from this kind of double membership, but the later General Secretaries for at least some part of their tenure occupied the mostly ceremonial position of Chairman of the Presidium of the Supreme Soviet, the nominal head of state. The institutions at lower levels were overseen and at times supplanted by primary party organizations.

However, in practice, the degree of control the party was able to exercise over the state bureaucracy, particularly after the death of Stalin, was far from total, with the bureaucracy pursuing different interests that were at times in conflict with the party. Nor was the party itself monolithic from top to bottom, although factions were officially banned.

The Supreme Soviet (successor of the Congress of Soviets and Central Executive Committee) was nominally the highest state body for most of the Soviet history, at first acting as a rubber stamp institution, approving and implementing all decisions made by the party. However, its powers and functions were extended in the late 1950s, 1960s and 1970s, including the creation of new state commissions and committees. It gained additional powers relating to the approval of the Five-Year Plans and the government budget. The Supreme Soviet elected a Presidium to wield its power between plenary sessions, ordinarily held twice a year, and appointed the Supreme Court, the Procurator General and the Council of Ministers (known before 1946 as the Council of People's Commissars), headed by the Chairman (Premier) and managing an enormous bureaucracy responsible for the administration of the economy and society. State and party structures of the constituent republics largely emulated the structure of the central institutions, although the Russian SFSR, unlike the other constituent republics, for most of its history had no republican branch of the CPSU, being ruled directly by the union-wide party until 1990. Local authorities were organized likewise into party committees, local Soviets and executive committees. While the state system was nominally federal, the party was unitary.

The state security police (the KGB and its predecessor agencies) played an important role in Soviet politics. It was instrumental in the Great Purge, but was brought under strict party control after Stalin's death. Under Yuri Andropov, the KGB engaged in the suppression of political dissent and maintained an extensive network of informers, reasserting itself as a political actor to some extent independent of the party-state structure, culminating in the anti-corruption campaign targeting high-ranking party officials in the late 1970s and early 1980s.

The constitution, which was promulgated in 1918, 1924, 1936 and 1977, did not limit state power. No formal separation of powers existed between the Party, Supreme Soviet and Council of Ministers that represented executive and legislative branches of the government. The system was governed less by statute than by informal conventions, and no settled mechanism of leadership succession existed. Bitter and at times deadly power struggles took place in the Politburo after the deaths of Lenin and Stalin, as well as after Khrushchev's dismissal, itself due to a decision by both the Politburo and the Central Committee. All leaders of the Communist Party before Gorbachev died in office, except Georgy Malenkov and Khrushchev, both dismissed from the party leadership amid internal struggle within the party.

Between 1988 and 1990, facing considerable opposition, Mikhail Gorbachev enacted reforms shifting power away from the highest bodies of the party and making the Supreme Soviet less dependent on them. The Congress of People's Deputies was established, the majority of whose members were directly elected in competitive elections held in March 1989. The Congress now elected the Supreme Soviet, which became a full-time parliament, and much stronger than before. For the first time since the 1920s, it refused to rubber stamp proposals from the party and Council of Ministers. In 1990, Gorbachev introduced and assumed the position of the President of the Soviet Union, concentrated power in his executive office, independent of the party, and subordinated the government, now renamed the Cabinet of Ministers of the USSR, to himself.

Tensions grew between the Union-wide authorities under Gorbachev, reformists led in Russia by Boris Yeltsin and controlling the newly elected Supreme Soviet of the Russian SFSR, and communist hardliners. On 19–21 August 1991, a group of hardliners staged a coup attempt. The coup failed, and the State Council of the Soviet Union became the highest organ of state power "in the period of transition". Gorbachev resigned as General Secretary, only remaining President for the final months of the existence of the USSR.

The judiciary was not independent of the other branches of government. The Supreme Court supervised the lower courts (People's Court) and applied the law as established by the constitution or as interpreted by the Supreme Soviet. The Constitutional Oversight Committee reviewed the constitutionality of laws and acts. The Soviet Union used the inquisitorial system of Roman law, where the judge, procurator, and defence attorney collaborate to establish the truth.

Constitutionally, the USSR was a federation of constituent Union Republics, which were either unitary states, such as Ukraine or Byelorussia (SSRs), or federations, such as Russia or Transcaucasia (SFSRs), all four being the founding republics who signed the Treaty on the Creation of the USSR in December 1922. In 1924, during the national delimitation in Central Asia, Uzbekistan and Turkmenistan were formed from parts of Russia's Turkestan ASSR and two Soviet dependencies, the Khorezm and Bukharan SSRs. In 1929, Tajikistan was split off from the Uzbekistan SSR. With the constitution of 1936, the Transcaucasian SFSR was dissolved, resulting in its constituent republics of Armenia, Georgia and Azerbaijan being elevated to Union Republics, while Kazakhstan and Kirghizia were split off from Russian SFSR, resulting in the same status. In August 1940, Moldavia was formed from parts of Ukraine and Bessarabia and Northern Bukovina. Estonia, Latvia and Lithuania (SSRs) were also admitted into the union which was not recognized by most of the international community and was considered an illegal occupation. Karelia was split off from Russia as a Union Republic in March 1940 and was reabsorbed in 1956. Between July 1956 and September 1991, there were 15 union republics (see map below).

While nominally a union of equals, in practice the Soviet Union was dominated by Russians. The domination was so absolute that for most of its existence, the country was commonly (but incorrectly) referred to as "Russia". While the RSFSR was technically only one republic within the larger union, it was by far the largest (both in terms of population and area), most powerful, most developed, and the industrial center of the Soviet Union. Historian Matthew White wrote that it was an open secret that the country's federal structure was "window dressing" for Russian dominance. For that reason, the people of the USSR were usually called "Russians", not "Soviets", since "everyone knew who really ran the show".

The Soviet Union became the first country to adopt a command economy, whereby production and distribution of goods were centralized and directed by the government. The first Bolshevik experience with a command economy was the policy of War communism, which involved the nationalization of industry, centralized distribution of output, coercive requisition of agricultural production, and attempts to eliminate money circulation, private enterprises and free trade. After the severe economic collapse, Lenin replaced war communism by the New Economic Policy (NEP) in 1921, legalizing free trade and private ownership of small businesses. The economy quickly recovered as a result.

After a long debate among the members of Politburo about the course of economic development, by 1928–1929, upon gaining control of the country, Stalin abandoned the NEP and pushed for full central planning, starting forced collectivization of agriculture and enacting draconian labor legislation. Resources were mobilized for rapid industrialization, which significantly expanded Soviet capacity in heavy industry and capital goods during the 1930s. The primary motivation for industrialization was preparation for war, mostly due to distrust of the outside capitalist world. As a result, the USSR was transformed from a largely agrarian economy into a great industrial power, leading the way for its emergence as a superpower after World War II. The war caused extensive devastation of the Soviet economy and infrastructure, which required massive reconstruction.

By the early 1940s, the Soviet economy had become relatively self-sufficient; for most of the period until the creation of Comecon, only a tiny share of domestic products was traded internationally. After the creation of the Eastern Bloc, external trade rose rapidly. However, the influence of the world economy on the USSR was limited by fixed domestic prices and a state monopoly on foreign trade. Grain and sophisticated consumer manufactures became major import articles from around the 1960s. During the arms race of the Cold War, the Soviet economy was burdened by military expenditures, heavily lobbied for by a powerful bureaucracy dependent on the arms industry. At the same time, the USSR became the largest arms exporter to the Third World. Significant amounts of Soviet resources during the Cold War were allocated in aid to the other socialist states.

From the 1930s until its dissolution in late 1991, the way the Soviet economy operated remained essentially unchanged. The economy was formally directed by central planning, carried out by Gosplan and organized in five-year plans. However, in practice, the plans were highly aggregated and provisional, subject to "ad hoc" intervention by superiors. All critical economic decisions were taken by the political leadership. Allocated resources and plan targets were usually denominated in rubles rather than in physical goods. Credit was discouraged, but widespread. The final allocation of output was achieved through relatively decentralized, unplanned contracting. Although in theory prices were legally set from above, in practice they were often negotiated, and informal horizontal links (e.g. between producer factories) were widespread.

A number of basic services were state-funded, such as education and health care. In the manufacturing sector, heavy industry and defence were prioritized over consumer goods. Consumer goods, particularly outside large cities, were often scarce, of poor quality and limited variety. Under the command economy, consumers had almost no influence on production, and the changing demands of a population with growing incomes could not be satisfied by supplies at rigidly fixed prices. A massive unplanned second economy grew up at low levels alongside the planned one, providing some of the goods and services that the planners could not. The legalization of some elements of the decentralized economy was attempted with the reform of 1965.

Although statistics of the Soviet economy are notoriously unreliable and its economic growth difficult to estimate precisely, by most accounts, the economy continued to expand until the mid-1980s. During the 1950s and 1960s, it had comparatively high growth and was catching up to the West. However, after 1970, the growth, while still positive, steadily declined much more quickly and consistently than in other countries, despite a rapid increase in the capital stock (the rate of capital increase was only surpassed by Japan).

Overall, between 1960 and 1989, the growth rate of per capita income in the Soviet Union was slightly above the world average (based on 102 countries). According to Stanley Fischer and William Easterly, growth could have been faster. By their calculation, per capita income in 1989 should have been twice higher than it was, considering the amount of investment, education and population. The authors attribute this poor performance to the low productivity of capital. Steven Rosenfielde states that the standard of living declined due to Stalin's despotism, and while there was a brief improvement after his death, it lapsed into stagnation.

In 1987, Mikhail Gorbachev attempted to reform and revitalize the economy with his program of "perestroika". His policies relaxed state control over enterprises but did not replace it by market incentives, resulting in a sharp decline in output. The economy, already suffering from reduced petroleum export revenues, started to collapse. Prices were still fixed, and the property was still largely state-owned until after the country's dissolution. For most of the period after World War II until its collapse, Soviet GDP (PPP) was the second-largest in the world, and third during the second half of the 1980s, although on a per-capita basis, it was behind that of First World countries. Compared to countries with similar per-capita GDP in 1928, the Soviet Union experienced significant growth.

In 1990, the country had a Human Development Index of 0.920, placing it in the "high" category of human development. It was the third-highest in the Eastern Bloc, behind Czechoslovakia and East Germany, and the 25th in the world of 130 countries.

The need for fuel declined in the Soviet Union from the 1970s to the 1980s, both per ruble of gross social product and per ruble of industrial product. At the start, this decline grew very rapidly but gradually slowed down between 1970 and 1975. From 1975 and 1980, it grew even slower, only 2.6%. David Wilson, a historian, believed that the gas industry would account for 40% of Soviet fuel production by the end of the century. His theory did not come to fruition because of the USSR's collapse. The USSR, in theory, would have continued to have an economic growth rate of 2–2.5% during the 1990s because of Soviet energy fields. However, the energy sector faced many difficulties, among them the country's high military expenditure and hostile relations with the First World.

In 1991, the Soviet Union had a pipeline network of for crude oil and another for natural gas. Petroleum and petroleum-based products, natural gas, metals, wood, agricultural products, and a variety of manufactured goods, primarily machinery, arms and military equipment, were exported. In the 1970s and 1980s, the USSR heavily relied on fossil fuel exports to earn hard currency. At its peak in 1988, it was the largest producer and second-largest exporter of crude oil, surpassed only by Saudi Arabia.

The Soviet Union placed great emphasis on science and technology within its economy, however, the most remarkable Soviet successes in technology, such as producing the world's first space satellite, typically were the responsibility of the military. Lenin believed that the USSR would never overtake the developed world if it remained as technologically backward as it was upon its founding. Soviet authorities proved their commitment to Lenin's belief by developing massive networks, research and development organizations. In the early 1960s, the Soviets awarded 40% of chemistry PhDs to women, compared to only 5% in the United States. By 1989, Soviet scientists were among the world's best-trained specialists in several areas, such as energy physics, selected areas of medicine, mathematics, welding and military technologies. Due to rigid state planning and bureaucracy, the Soviets remained far behind technologically in chemistry, biology, and computers when compared to the First World.

Under the Reagan administration, Project Socrates determined that the Soviet Union addressed the acquisition of science and technology in a manner that was radically different from what the US was using. In the case of the US, economic prioritization was being used for indigenous research and development as the means to acquire science and technology in both the private and public sectors. In contrast, the USSR was offensively and defensively maneuvering in the acquisition and utilization of the worldwide technology, to increase the competitive advantage that they acquired from the technology while preventing the US from acquiring a competitive advantage. However, technology-based planning was executed in a centralized, government-centric manner that greatly hindered its flexibility. This was exploited by the US to undermine the strength of the Soviet Union and thus foster its reform.

Transport was a vital component of the country's economy. The economic centralization of the late 1920s and 1930s led to the development of infrastructure on a massive scale, most notably the establishment of Aeroflot, an aviation enterprise. The country had a wide variety of modes of transport by land, water and air. However, due to inadequate maintenance, much of the road, water and Soviet civil aviation transport were outdated and technologically backward compared to the First World.

Soviet rail transport was the largest and most intensively used in the world; it was also better developed than most of its Western counterparts. By the late 1970s and early 1980s, Soviet economists were calling for the construction of more roads to alleviate some of the burdens from the railways and to improve the Soviet government budget. The street network and automotive industry remained underdeveloped, and dirt roads were common outside major cities. Soviet maintenance projects proved unable to take care of even the few roads the country had. By the early-to-mid-1980s, the Soviet authorities tried to solve the road problem by ordering the construction of new ones. Meanwhile, the automobile industry was growing at a faster rate than road construction. The underdeveloped road network led to a growing demand for public transport.

Despite improvements, several aspects of the transport sector were still riddled with problems due to outdated infrastructure, lack of investment, corruption and bad decision-making. Soviet authorities were unable to meet the growing demand for transport infrastructure and services.

The Soviet merchant navy was one of the largest in the world.

Excess deaths throughout World War I and the Russian Civil War (including the postwar famine) amounted to a combined total of 18 million, some 10 million in the 1930s, and more than 26 million in 1941–5. The postwar Soviet population was 45 to 50 million smaller than it would have been if pre-war demographic growth had continued. According to Catherine Merridale, "... reasonable estimate would place the total number of excess deaths for the whole period somewhere around 60 million."

The birth rate of the USSR decreased from 44.0 per thousand in 1926 to 18.0 in 1974, mainly due to increasing urbanization and the rising average age of marriages. The mortality rate demonstrated a gradual decrease as well – from 23.7 per thousand in 1926 to 8.7 in 1974. In general, the birth rates of the southern republics in Transcaucasia and Central Asia were considerably higher than those in the northern parts of the Soviet Union, and in some cases even increased in the post–World War II period, a phenomenon partly attributed to slower rates of urbanistion and traditionally earlier marriages in the southern republics. Soviet Europe moved towards sub-replacement fertility, while Soviet Central Asia continued to exhibit population growth well above replacement-level fertility.

The late 1960s and the 1970s witnessed a reversal of the declining trajectory of the rate of mortality in the USSR, and was especially notable among men of working age, but was also prevalent in Russia and other predominantly Slavic areas of the country. An analysis of the official data from the late 1980s showed that after worsening in the late-1970s and the early 1980s, adult mortality began to improve again. The infant mortality rate increased from 24.7 in 1970 to 27.9 in 1974. Some researchers regarded the rise as mostly real, a consequence of worsening health conditions and services. The rises in both adult and infant mortality were not explained or defended by Soviet officials, and the Soviet government stopped publishing all mortality statistics for ten years. Soviet demographers and health specialists remained silent about the mortality increases until the late-1980s, when the publication of mortality data resumed, and researchers could delve into the real causes.

Under Lenin, the state made explicit commitments to promote the equality of men and women. Many early Russian feminists and ordinary Russian working women actively participated in the Revolution, and many more were affected by the events of that period and the new policies. Beginning in October 1918, the Lenin's government liberalized divorce and abortion laws, decriminalized homosexuality (re-criminalized in the 1930s), permitted cohabitation, and ushered in a host of reforms. However, without birth control, the new system produced many broken marriages, as well as countless out-of-wedlock children. The epidemic of divorces and extramarital affairs created social hardships when Soviet leaders wanted people to concentrate their efforts on growing the economy. Giving women control over their fertility also led to a precipitous decline in the birth rate, perceived as a threat to their country's military power. By 1936, Stalin reversed most of the liberal laws, ushering in a pronatalist era that lasted for decades.

By 1917, Russia became the first great power to grant women the right to vote. After heavy casualties in World War I and II, women outnumbered men in Russia by a 4:3 ratio. This contributed to the larger role women played in Russian society compared to other great powers at the time.

Anatoly Lunacharsky became the first People's Commissar for Education of Soviet Russia. In the beginning, the Soviet authorities placed great emphasis on the elimination of illiteracy. All left-handed kids were forced to write with their right hand in the Soviet school system. Literate people were automatically hired as teachers. For a short period, quality was sacrificed for quantity. By 1940, Stalin could announce that illiteracy had been eliminated. Throughout the 1930s, social mobility rose sharply, which has been attributed to reforms in education. In the aftermath of World War II, the country's educational system expanded dramatically, which had a tremendous effect. In the 1960s, nearly all children had access to education, the only exception being those living in remote areas. Nikita Khrushchev tried to make education more accessible, making it clear to children that education was closely linked to the needs of society. Education also became important in giving rise to the New Man. Citizens directly entering the workforce had the constitutional right to a job and to free vocational training.

The education system was highly centralized and universally accessible to all citizens, with affirmative action for applicants from nations associated with cultural backwardness. However, as part of the general antisemitic policy, an unofficial Jewish quota was applied in the leading institutions of higher education by subjecting Jewish applicants to harsher entrance examinations. The Brezhnev era also introduced a rule that required all university applicants to present a reference from the local Komsomol party secretary. According to statistics from 1986, the number of higher education students per the population of 10,000 was 181 for the USSR, compared to 517 for the US.

The Soviet Union was an ethnically diverse country, with more than 100 distinct ethnic groups. The total population was estimated at 293 million in 1991. According to a 1990 estimate, the majority were Russians (50.78%), followed by Ukrainians (15.45%) and Uzbeks (5.84%).

All citizens of the USSR had their own ethnic affiliation. The ethnicity of a person was chosen at the age of sixteen by the child's parents. If the parents did not agree, the child was automatically assigned the ethnicity of the father. Partly due to Soviet policies, some of the smaller minority ethnic groups were considered part of larger ones, such as the Mingrelians of Georgia, who were classified with the linguistically related Georgians. Some ethnic groups voluntarily assimilated, while others were brought in by force. Russians, Belarusians, and Ukrainians shared close cultural ties, while other groups did not. With multiple nationalities living in the same territory, ethnic antagonisms developed over the years.

Members of various ethnicities participated in legislative bodies. Organs of power like the Politburo, the Secretariat of the Central Committee etc., were formally ethnically neutral, but in reality, ethnic Russians were overrepresented, although there were also non-Russian leaders in the Soviet leadership, such as Joseph Stalin, Grigory Zinoviev, Nikolai Podgorny or Andrei Gromyko. During the Soviet era, a significant number of ethnic Russians and Ukrainians migrated to other Soviet republics, and many of them settled there. According to the last census in 1989, the Russian "diaspora" in the Soviet republics had reached 25 million.

In 1917, before the revolution, health conditions were significantly behind those of developed countries. As Lenin later noted, "Either the lice will defeat socialism, or socialism will defeat the lice". The Soviet principle of health care was conceived by the People's Commissariat for Health in 1918. Health care was to be controlled by the state and would be provided to its citizens free of charge, a revolutionary concept at the time. Article 42 of the 1977 Soviet Constitution gave all citizens the right to health protection and free access to any health institutions in the USSR. Before Leonid Brezhnev became General Secretary, the Soviet healthcare system was held in high esteem by many foreign specialists. This changed, however, from Brezhnev's accession and Mikhail Gorbachev's tenure as leader, during which the health care system was heavily criticized for many basic faults, such as the quality of service and the unevenness in its provision. Minister of Health Yevgeniy Chazov, during the 19th Congress of the Communist Party of the Soviet Union, while highlighting such successes as having the most doctors and hospitals in the world, recognized the system's areas for improvement and felt that billions of Soviet rubles were squandered. 

After the revolution, life expectancy for all age groups went up. This statistic in itself was seen by some that the socialist system was superior to the capitalist system. These improvements continued into the 1960s when statistics indicated that the life expectancy briefly surpassed that of the United States. Life expectancy started to decline in the 1970s, possibly because of alcohol abuse. At the same time, infant mortality began to rise. After 1974, the government stopped publishing statistics on the matter. This trend can be partly explained by the number of pregnancies rising drastically in the Asian part of the country where infant mortality was the highest while declining markedly in the more developed European part of the Soviet Union.

Under Lenin, the government gave small language groups their own writing systems. The development of these writing systems was highly successful, even though some flaws were detected. During the later days of the USSR, countries with the same multilingual situation implemented similar policies. A serious problem when creating these writing systems was that the languages differed dialectally greatly from each other. When a language had been given a writing system and appeared in a notable publication, it would attain "official language" status. There were many minority languages which never received their own writing system; therefore, their speakers were forced to have a second language. There are examples where the government retreated from this policy, most notably under Stalin where education was discontinued in languages that were not widespread. These languages were then assimilated into another language, mostly Russian. During World War II, some minority languages were banned, and their speakers accused of collaborating with the enemy.

As the most widely spoken of the Soviet Union's many languages, Russian "de facto" functioned as an official language, as the "language of interethnic communication" (), but only assumed the "de jure" status as the official national language in 1990.

Being Communist, the Soviet Union was officially atheist. Nevertheless, many citizens engaged in religious practices, some secretly. Christianity and Islam had the highest number of adherents among the religious citizens. Eastern Christianity predominated among Christians, with Russia's traditional Russian Orthodox Church being the largest Christian denomination. About 90% of the Soviet Union's Muslims were Sunnis, with Shias being concentrated in the Azerbaijan SSR. Smaller groups included Roman Catholics, Jews, Buddhists, and a variety of Protestant denominations (especially Baptists and Lutherans).

Religious influence had been strong in the Russian Empire. The Russian Orthodox Church enjoyed a privileged status as the church of the monarchy and took part in carrying out official state functions. The immediate period following the establishment of the Soviet state included a struggle against the Orthodox Church, which the revolutionaries considered an ally of the former ruling classes.

In Soviet law, the "freedom to hold religious services" was constitutionally guaranteed, although the ruling Communist Party regarded religion as incompatible with the Marxist spirit of scientific materialism. In practice, the Soviet system subscribed to a narrow interpretation of this right, and in fact utilized a range of official measures to discourage religion and curb the activities of religious groups.

The 1918 Council of People's Commissars decree establishing the Russian SFSR as a secular state also decreed that "the teaching of religion in all [places] where subjects of general instruction are taught, is forbidden. Citizens may teach and may be taught religion privately." Among further restrictions, those adopted in 1929 included express prohibitions on a range of church activities, including meetings for organized Bible study. Both Christian and non-Christian establishments were shut down by the thousands in the 1920s and 1930s. By 1940, as many as 90% of the churches, synagogues, and mosques that had been operating in 1917 were closed.

Under the doctrine of state atheism, there was a "government-sponsored program of forced conversion to atheism" conducted by the Communists. The regime targeted religions based on state interests, and while most organized religions were never outlawed, religious property was confiscated, believers were harassed, and religion was ridiculed while atheism was propagated in schools. In 1925, the government founded the League of Militant Atheists to intensify the propaganda campaign. Accordingly, although personal expressions of religious faith were not explicitly banned, a strong sense of social stigma was imposed on them by the formal structures and mass media, and it was generally considered unacceptable for members of certain professions (teachers, state bureaucrats, soldiers) to be openly religious. As for the Russian Orthodox Church, Soviet authorities sought to control it and, in times of national crisis, to exploit it for the regime's own purposes; but their ultimate goal was to eliminate it. During the first five years of Soviet power, the Bolsheviks executed 28 Russian Orthodox bishops and over 1,200 Russian Orthodox priests. Many others were imprisoned or exiled. Believers were harassed and persecuted. Most seminaries were closed, and the publication of most religious material was prohibited. By 1941, only 500 churches remained open out of about 54,000 in existence before World War I.

Convinced that religious anti-Sovietism had become a thing of the past with most Soviet Christians, and with the looming threat of war, the Stalin regime began shifting to a more moderate religion policy in the late 1930s. Soviet religious establishments overwhelmingly rallied to support the war effort during World War II. Amid other accommodations to religious faith after the German invasion, churches were reopened. Radio Moscow began broadcasting a religious hour, and a historic meeting between Stalin and Orthodox Church leader Patriarch Sergius of Moscow was held in 1943. Stalin had the support of the majority of the religious people in the USSR even through the late 1980s. The general tendency of this period was an increase in religious activity among believers of all faiths.

Under Nikita Khrushchev, the state leadership clashed with the churches in 1958–1964, a period when atheism was emphasized in the educational curriculum, and numerous state publications promoted atheistic views. During this period, the number of churches fell from 20,000 to 10,000 from 1959 to 1965, and the number of synagogues dropped from 500 to 97. The number of working mosques also declined, falling from 1,500 to 500 within a decade.

Religious institutions remained monitored by the Soviet government, but churches, synagogues, temples, and mosques were all given more leeway in the Brezhnev era. Official relations between the Orthodox Church and the government again warmed to the point that the Brezhnev government twice honored Orthodox Patriarch Alexy I with the Order of the Red Banner of Labour. A poll conducted by Soviet authorities in 1982 recorded 20% of the Soviet population as "active religious believers."

The culture of the Soviet Union passed through several stages during the USSR's existence. During the first decade following the revolution, there was relative freedom and artists experimented with several different styles to find a distinctive Soviet style of art. Lenin wanted art to be accessible to the Russian people. On the other hand, hundreds of intellectuals, writers, and artists were exiled or executed, and their work banned, such as Nikolay Gumilyov who was shot for alleged conspiring against the Bolshevik regime, and Yevgeny Zamyatin.

The government encouraged a variety of trends. In art and literature, numerous schools, some traditional and others radically experimental, proliferated. Communist writers Maxim Gorky and Vladimir Mayakovsky were active during this time. As a means of influencing a largely illiterate society, films received encouragement from the state, and much of director Sergei Eisenstein's best work dates from this period.

During Stalin's rule, the Soviet culture was characterized by the rise and domination of the government-imposed style of socialist realism, with all other trends being severely repressed, with rare exceptions, such as Mikhail Bulgakov's works. Many writers were imprisoned and killed.

Following the Khrushchev Thaw, censorship was diminished. During this time, a distinctive period of Soviet culture developed, characterized by conformist public life and an intense focus on personal life. Greater experimentation in art forms was again permissible, resulting in the production of more sophisticated and subtly critical work. The regime loosened its emphasis on socialist realism; thus, for instance, many protagonists of the novels of author Yury Trifonov concerned themselves with problems of daily life rather than with building socialism. Underground dissident literature, known as "samizdat", developed during this late period. In architecture, the Khrushchev era mostly focused on functional design as opposed to the highly decorated style of Stalin's epoch.

In the second half of the 1980s, Gorbachev's policies of "perestroika" and "glasnost" significantly expanded freedom of expression throughout the country in the media and the press.

Founded on 20 July 1924 in Moscow, "Sovetsky Sport" was the first sports newspaper of the Soviet Union.

The Soviet Olympic Committee formed on 21 April 1951, and the IOC recognized the new body in its 45th session. In the same year, when the Soviet representative Konstantin Andrianov became an IOC member, the USSR officially joined the Olympic Movement. The 1952 Summer Olympics in Helsinki thus became first Olympic Games for Soviet athletes.

The Soviet Union national ice hockey team won nearly every world championship and Olympic tournament between 1954 and 1991 and never failed to medal in any International Ice Hockey Federation (IIHF) tournament in which they competed.

The advent of the state-sponsored "full-time amateur athlete" of the Eastern Bloc countries further eroded the ideology of the pure amateur, as it put the self-financed amateurs of the Western countries at a disadvantage. The Soviet Union entered teams of athletes who were all nominally students, soldiers, or working in a profession – in reality, the state paid many of these competitors to train on a full-time basis. Nevertheless, the IOC held to the traditional rules regarding amateurism.

A 1989 report by a committee of the Australian Senate claimed that "there is hardly a medal winner at the Moscow Games, certainly not a gold medal winner...who is not on one sort of drug or another: usually several kinds. The Moscow Games might well have been called the Chemists' Games".

A member of the IOC Medical Commission, Manfred Donike, privately ran additional tests with a new technique for identifying abnormal levels of testosterone by measuring its ratio to epitestosterone in urine. Twenty percent of the specimens he tested, including those from sixteen gold medalists, would have resulted in disciplinary proceedings had the tests been official. The results of Donike's unofficial tests later convinced the IOC to add his new technique to their testing protocols. The first documented case of "blood doping" occurred at the 1980 Summer Olympics when a runner was transfused with two pints of blood before winning medals in the 5000 m and 10,000 m.

Documentation obtained in 2016 revealed the Soviet Union's plans for a statewide doping system in track and field in preparation for the 1984 Summer Olympics in Los Angeles. Dated before the decision to boycott the 1984 Games, the document detailed the existing steroids operations of the program, along with suggestions for further enhancements. Dr. Sergei Portugalov of the Institute for Physical Culture prepared the communication, directed to the Soviet Union's head of track and field. Portugalov later became one of the leading figures involved in the implementation of Russian doping before the 2016 Summer Olympics.













</doc>
<doc id="26781" url="https://en.wikipedia.org/wiki?curid=26781" title="Social science">
Social science

Social science is the branch of science devoted to the study of human societies and the relationships among individuals within those societies. The term was formerly used to refer to the field of sociology, the original "science of society", established in the 19th century. In addition to sociology, it now encompasses a wide array of academic disciplines, including anthropology, archaeology, economics, human geography, linguistics, management science, media studies, musicology, political science, psychology, and social history. (For a more detailed list of sub-disciplines within the social sciences see: Outline of social science.)

Positivist social scientists use methods resembling those of the natural sciences as tools for understanding society, and so define science in its stricter modern sense. Interpretivist social scientists, by contrast, may use social critique or symbolic interpretation rather than constructing empirically falsifiable theories, and thus treat science in its broader sense. In modern academic practice, researchers are often eclectic, using multiple methodologies (for instance, by combining both quantitative and qualitative research). The term "social research" has also acquired a degree of autonomy as practitioners from various disciplines share in its aims and methods.

The history of the social sciences begins in the Age of Enlightenment after 1650, which saw a revolution within natural philosophy, changing the basic framework by which individuals understood what was "scientific". Social sciences came forth from the moral philosophy of the time and were influenced by the Age of Revolutions, such as the Industrial Revolution and the French Revolution. The "social sciences" developed from the sciences (experimental and applied), or the systematic knowledge-bases or prescriptive practices, relating to the social improvement of a group of interacting entities.

The beginnings of the social sciences in the 18th century are reflected in the grand encyclopedia of Diderot, with articles from Jean-Jacques Rousseau and other pioneers. The growth of the social sciences is also reflected in other specialized encyclopedias. The modern period saw ""social science"" first used as a distinct conceptual field. Social science was influenced by positivism, focusing on knowledge based on actual positive sense experience and avoiding the negative; metaphysical speculation was avoided. Auguste Comte used the term ""science sociale"" to describe the field, taken from the ideas of Charles Fourier; Comte also referred to the field as "social physics".

Following this period, five paths of development sprang forth in the social sciences, influenced by Comte in other fields. One route that was taken was the rise of social research. Large statistical surveys were undertaken in various parts of the United States and Europe. Another route undertaken was initiated by Émile Durkheim, studying "social facts", and Vilfredo Pareto, opening metatheoretical ideas and individual theories. A third means developed, arising from the methodological dichotomy present, in which social phenomena were identified with and understood; this was championed by figures such as Max Weber. The fourth route taken, based in economics, was developed and furthered economic knowledge as a hard science. The last path was the correlation of knowledge and social values; the antipositivism and verstehen sociology of Max Weber firmly demanded this distinction. In this route, theory (description) and prescription were non-overlapping formal discussions of a subject.

Around the start of the 20th century, Enlightenment philosophy was challenged in various quarters. After the use of classical theories since the end of the scientific revolution, various fields substituted mathematics studies for experimental studies and examining equations to build a theoretical structure. The development of social science subfields became very quantitative in methodology. The interdisciplinary and cross-disciplinary nature of scientific inquiry into human behaviour, social and environmental factors affecting it, made many of the natural sciences interested in some aspects of social science methodology. Examples of boundary blurring include emerging disciplines like social research of medicine, sociobiology, neuropsychology, bioeconomics and the history and sociology of science. Increasingly, quantitative research and qualitative methods are being integrated in the study of human action and its implications and consequences. In the first half of the 20th century, statistics became a free-standing discipline of applied mathematics. Statistical methods were used confidently.

In the contemporary period, Karl Popper and Talcott Parsons influenced the furtherance of the social sciences. Researchers continue to search for a unified consensus on what methodology might have the power and refinement to connect a proposed "grand theory" with the various midrange theories that, with considerable success, continue to provide usable frameworks for massive, growing data banks; for more, see consilience. The social sciences will for the foreseeable future be composed of different zones in the research of, and sometime distinct in approach toward, the field.

The term "social science" may refer either to the specific "sciences of society" established by thinkers such as Comte, Durkheim, Marx, and Weber, or more generally to all disciplines outside of "noble science" and arts. By the late 19th century, the academic social sciences were constituted of five fields: jurisprudence and amendment of the law, education, health, economy and trade, and art.

Around the start of the 21st century, the expanding domain of economics in the social sciences has been described as economic imperialism.

The social science disciplines are branches of knowledge taught and researched at the college or university level. Social science disciplines are defined and recognized by the academic journals in which research is published, and the learned social science societies and academic departments or faculties to which their practitioners belong. Social science fields of study usually have several sub-disciplines or branches, and the distinguishing lines between these are often both arbitrary and ambiguous.

Anthropology is the holistic "science of man", a science of the totality of human existence. The discipline deals with the integration of different aspects of the social sciences, humanities, and human biology. In the twentieth century, academic disciplines have often been institutionally divided into three broad domains. The "natural sciences" seek to derive general laws through reproducible and verifiable experiments. The "humanities" generally study local traditions, through their history, literature, music, and arts, with an emphasis on understanding particular individuals, events, or eras. The "social sciences" have generally attempted to develop scientific methods to understand social phenomena in a generalizable way, though usually with methods distinct from those of the natural sciences.

The anthropological social sciences often develop nuanced descriptions rather than the general laws derived in physics or chemistry, or they may explain individual cases through more general principles, as in many fields of psychology. Anthropology (like some fields of history) does not easily fit into one of these categories, and different branches of anthropology draw on one or more of these domains. Within the United States, anthropology is divided into four sub-fields: archaeology, physical or biological anthropology, anthropological linguistics, and cultural anthropology. It is an area that is offered at most undergraduate institutions. The word "anthropos" (ἄνθρωπος) in Ancient Greek means "human being" or "person". Eric Wolf described sociocultural anthropology as "the most scientific of the humanities, and the most humanistic of the sciences".

The goal of anthropology is to provide a holistic account of humans and human nature. This means that, though anthropologists generally specialize in only one sub-field, they always keep in mind the biological, linguistic, historic and cultural aspects of any problem. Since anthropology arose as a science in Western societies that were complex and industrial, a major trend within anthropology has been a methodological drive to study peoples in societies with more simple social organization, sometimes called "primitive" in anthropological literature, but without any connotation of "inferior". Today, anthropologists use terms such as "less complex" societies or refer to specific modes of subsistence or production, such as "pastoralist" or "forager" or "horticulturalist" to refer to humans living in non-industrial, non-Western cultures, such people or folk ("ethnos") remaining of great interest within anthropology.

The quest for holism leads most anthropologists to study a people in detail, using biogenetic, archaeological, and linguistic data alongside direct observation of contemporary customs. In the 1990s and 2000s, calls for clarification of what constitutes a culture, of how an observer knows where his or her own culture ends and another begins, and other crucial topics in writing anthropology were heard. It is possible to view all human cultures as part of one large, evolving global culture. These dynamic relationships, between what can be observed on the ground, as opposed to what can be observed by compiling many local observations remain fundamental in any kind of anthropology, whether cultural, biological, linguistic or archaeological.

Communication studies deals with processes of human communication, commonly defined as the sharing of symbols to create meaning. The discipline encompasses a range of topics, from face-to-face conversation to mass media outlets such as television broadcasting. Communication studies also examines how messages are interpreted through the political, cultural, economic, and social dimensions of their contexts. Communication is institutionalized under many different names at different universities, including "communication", "communication studies", "speech communication", "rhetorical studies", "communication science", "media studies", "communication arts", "mass communication", "media ecology", and "communication and media science".

Communication studies integrates aspects of both social sciences and the humanities. As a social science, the discipline often overlaps with sociology, psychology, anthropology, biology, political science, economics, and public policy, among others. From a humanities perspective, communication is concerned with rhetoric and persuasion (traditional graduate programs in communication studies trace their history to the rhetoricians of Ancient Greece). The field applies to outside disciplines as well, including engineering, architecture, mathematics, and information science.

Economics is a social science that seeks to analyze and describe the production, distribution, and consumption of wealth. The word "economics" is from the Ancient Greek "oikos", "family, household, estate", and νόμος "nomos", "custom, law", and hence means "household management" or "management of the state". An economist is a person using economic concepts and data in the course of employment, or someone who has earned a degree in the subject. The classic brief definition of economics, set out by Lionel Robbins in 1932, is "the science which studies human behavior as a relation between scarce means having alternative uses". Without scarcity and alternative uses, there is no economic problem. Briefer yet is "the study of how people seek to satisfy needs and wants" and "the study of the financial aspects of human behavior".

Economics has two broad branches: microeconomics, where the unit of analysis is the individual agent, such as a household or firm, and macroeconomics, where the unit of analysis is an economy as a whole. Another division of the subject distinguishes positive economics, which seeks to predict and explain economic phenomena, from normative economics, which orders choices and actions by some criterion; such orderings necessarily involve subjective value judgments. Since the early part of the 20th century, economics has focused largely on measurable quantities, employing both theoretical models and empirical analysis. Quantitative models, however, can be traced as far back as the physiocratic school. Economic reasoning has been increasingly applied in recent decades to other social situations such as politics, law, psychology, history, religion, marriage and family life, and other social interactions.
This paradigm crucially assumes (1) that resources are scarce because they are not sufficient to satisfy all wants, and (2) that "economic value" is willingness to pay as revealed for instance by market (arms' length) transactions. Rival heterodox schools of thought, such as institutional economics, green economics, Marxist economics, and economic sociology, make other grounding assumptions. For example, Marxist economics assumes that economics primarily deals with the investigation of exchange value, of which human labour is the source.

The expanding domain of economics in the social sciences has been described as economic imperialism.

Education encompasses teaching and learning specific skills, and also something less tangible but more profound: the imparting of knowledge, positive judgement and well-developed wisdom. Education has as one of its fundamental aspects the imparting of culture from generation to generation (see socialization). To educate means 'to draw out', from the Latin "educare", or to facilitate the realization of an individual's potential and talents. It is an application of pedagogy, a body of theoretical and applied research relating to teaching and learning and draws on many disciplines such as psychology, philosophy, computer science, linguistics, neuroscience, sociology and anthropology.

The education of an individual human begins at birth and continues throughout life. (Some believe that education begins even before birth, as evidenced by some parents' playing music or reading to the baby in the womb in the hope it will influence the child's development.) For some, the struggles and triumphs of daily life provide far more instruction than does formal schooling (thus Mark Twain's admonition to "never let school interfere with your education").

Geography as a discipline can be split broadly into two main sub fields: human geography and physical geography. The former focuses largely on the built environment and how space is created, viewed and managed by humans as well as the influence humans have on the space they occupy. This may involve cultural geography, transportation, health, military operations, and cities. The latter examines the natural environment and how the climate, vegetation and life, soil, oceans, water and landforms are produced and interact. Physical geography examines phenomena related to the measurement of earth. As a result of the two subfields using different approaches a third field has emerged, which is environmental geography. Environmental geography combines physical and human geography and looks at the interactions between the environment and humans. Other branches of geography include social geography, regional geography, and geomatics.

Geographers attempt to understand the Earth in terms of physical and spatial relationships. The first geographers focused on the science of mapmaking and finding ways to precisely project the surface of the earth. In this sense, geography bridges some gaps between the natural sciences and social sciences. Historical geography is often taught in a college in a unified Department of Geography.

Modern geography is an all-encompassing discipline, closely related to GISc, that seeks to understand humanity and its natural environment. The fields of urban planning, regional science, and planetology are closely related to geography. Practitioners of geography use many technologies and methods to collect data such as GIS, remote sensing, aerial photography, statistics, and global positioning systems (GPS).

History is the continuous, systematic narrative and research into past human events as interpreted through historiographical paradigms or theories.

History has a base in both the social sciences and the humanities. In the United States the National Endowment for the Humanities includes history in its definition of humanities (as it does for applied linguistics). However, the National Research Council classifies history as a social science. The historical method comprises the techniques and guidelines by which historians use primary sources and other evidence to research and then to write history. The Social Science History Association, formed in 1976, brings together scholars from numerous disciplines interested in social history.

The social science of law, jurisprudence, in common parlance, means a rule that (unlike a rule of ethics) is capable of enforcement through institutions. However, many laws are based on norms accepted by a community and thus have an ethical foundation. The study of law crosses the boundaries between the social sciences and humanities, depending on one's view of research into its objectives and effects. Law is not always enforceable, especially in the international relations context. It has been defined as a "system of rules", as an "interpretive concept" to achieve justice, as an "authority" to mediate people's interests, and even as "the command of a sovereign, backed by the threat of a sanction". However one likes to think of law, it is a completely central social institution. Legal policy incorporates the practical manifestation of thinking from almost every social science and the humanities. Laws are politics, because politicians create them. Law is philosophy, because moral and ethical persuasions shape their ideas. Law tells many of history's stories, because statutes, case law and codifications build up over time. And law is economics, because any rule about contract, tort, property law, labour law, company law and many more can have long-lasting effects on the distribution of wealth. The noun "law" derives from the late Old English "lagu", meaning something laid down or fixed and the adjective "legal" comes from the Latin word "lex".

Linguistics investigates the cognitive and social aspects of human language. The field is divided into areas that focus on aspects of the linguistic signal, such as syntax (the study of the rules that govern the structure of sentences), semantics (the study of meaning), morphology (the study of the structure of words), phonetics (the study of speech sounds) and phonology (the study of the abstract sound system of a particular language); however, work in areas like evolutionary linguistics (the study of the origins and evolution of language) and psycholinguistics (the study of psychological factors in human language) cut across these divisions.

The overwhelming majority of modern research in linguistics takes a predominantly synchronic perspective (focusing on language at a particular point in time), and a great deal of it—partly owing to the influence of Noam Chomsky—aims at formulating theories of the cognitive processing of language. However, language does not exist in a vacuum, or only in the brain, and approaches like contact linguistics, creole studies, discourse analysis, social interactional linguistics, and sociolinguistics explore language in its social context. Sociolinguistics often makes use of traditional quantitative analysis and statistics in investigating the frequency of features, while some disciplines, like contact linguistics, focus on qualitative analysis. While certain areas of linguistics can thus be understood as clearly falling within the social sciences, other areas, like acoustic phonetics and neurolinguistics, draw on the natural sciences. Linguistics draws only secondarily on the humanities, which played a rather greater role in linguistic inquiry in the 19th and early 20th centuries. Ferdinand Saussure is considered the father of modern linguistics.

Political science is an academic and research discipline that deals with the theory and practice of politics and the description and analysis of political systems and political behaviour. Fields and subfields of political science include political economy, political theory and philosophy, civics and comparative politics, theory of direct democracy, apolitical governance, participatory direct democracy, national systems, cross-national political analysis, political development, international relations, foreign policy, international law, politics, public administration, administrative behaviour, public law, judicial behaviour, and public policy. Political science also studies power in international relations and the theory of great powers and superpowers.

Political science is methodologically diverse, although recent years have witnessed an upsurge in the use of the scientific method, that is, the proliferation of formal-deductive model building and quantitative hypothesis testing. Approaches to the discipline include rational choice, classical political philosophy, interpretivism, structuralism, and behaviouralism, realism, pluralism, and institutionalism. Political science, as one of the social sciences, uses methods and techniques that relate to the kinds of inquiries sought: primary sources such as historical documents, interviews, and official records, as well as secondary sources such as scholarly articles are used in building and testing theories. Empirical methods include survey research, statistical analysis or econometrics, case studies, experiments, and model building. Herbert Baxter Adams is credited with coining the phrase "political science" while teaching history at Johns Hopkins University.

Psychology is an academic and applied field involving the study of behaviour and mental processes. Psychology also refers to the application of such knowledge to various spheres of human activity, including problems of individuals' daily lives and the treatment of mental illness. The word "psychology" comes from the Ancient Greek ψυχή "psyche" ("soul", "mind") and "logy" ("study").

Psychology differs from anthropology, economics, political science, and sociology in seeking to capture explanatory generalizations about the mental function and overt behaviour of individuals, while the other disciplines focus on creating descriptive generalizations about the functioning of social groups or situation-specific human behaviour. In practice, however, there is quite a lot of cross-fertilization that takes place among the various fields. Psychology differs from biology and neuroscience in that it is primarily concerned with the interaction of mental processes and behaviour, and of the overall processes of a system, and not simply the biological or neural processes themselves, though the subfield of neuropsychology combines the study of the actual neural processes with the study of the mental effects they have subjectively produced.
Many people associate psychology with clinical psychology, which focuses on assessment and treatment of problems in living and psychopathology. In reality, psychology has myriad specialties including social psychology, developmental psychology, cognitive psychology, educational psychology, industrial-organizational psychology, mathematical psychology, neuropsychology, and quantitative analysis of behaviour.

Psychology is a very broad science that is rarely tackled as a whole, major block. Although some subfields encompass a natural science base and a social science application, others can be clearly distinguished as having little to do with the social sciences or having a lot to do with the social sciences. For example, biological psychology is considered a natural science with a social scientific application (as is clinical medicine), social and occupational psychology are, generally speaking, purely social sciences, whereas neuropsychology is a natural science that lacks application out of the scientific tradition entirely. In British universities, emphasis on what tenet of psychology a student has studied and/or concentrated is communicated through the degree conferred: B.Psy. indicates a balance between natural and social sciences, B.Sc. indicates a strong (or entire) scientific concentration, whereas a B.A. underlines a majority of social science credits. This is not always necessarily the case however, and in many UK institutions students studying the B.Psy, B.Sc, and B.A. follow the same curriculum as outlined by The British Psychological Society and have the same options of specialism open to them regardless of whether they choose a balance, a heavy science basis, or heavy social science basis to their degree. If they applied to read the B.A. for example, but specialized in heavily science-based modules, then they will still generally be awarded the B.A.

Sociology is the systematic study of society, individuals' relationship to their societies, the consequences of difference, and other aspects of human social action. The meaning of the word comes from the suffix "-logy", which means "study of", derived from Ancient Greek, and the stem "soci-", which is from the Latin word "socius", meaning "companion", or society in general.

Auguste Comte (1798–1857) coined the term, Sociology, as a way to apply natural science principles and techniques to the social world in 1838. Comte endeavoured to unify history, psychology and economics through the descriptive understanding of the social realm. He proposed that social ills could be remedied through sociological positivism, an epistemological approach outlined in "The Course in Positive Philosophy" [1830–1842] and "A General View of Positivism" (1844). Though Comte is generally regarded as the "Father of Sociology", the discipline was formally established by another French thinker, Émile Durkheim (1858–1917), who developed positivism as a foundation to practical social research. Durkheim set up the first European department of sociology at the University of Bordeaux in 1895, publishing his "Rules of the Sociological Method". In 1896, he established the journal "L'Année Sociologique". Durkheim's seminal monograph, "Suicide" (1897), a case study of suicide rates among Catholic and Protestant populations, distinguished sociological analysis from psychology or philosophy.

Karl Marx rejected Comte's positivism but nevertheless aimed to establish a "science of society" based on historical materialism, becoming recognized as a founding figure of sociology posthumously as the term gained broader meaning. Around the start of the 20th century, the first wave of German sociologists, including Max Weber and Georg Simmel, developed sociological antipositivism. The field may be broadly recognized as an amalgam of three modes of social thought in particular: Durkheimian positivism and structural functionalism; Marxist historical materialism and conflict theory; and Weberian antipositivism and verstehen analysis. American sociology broadly arose on a separate trajectory, with little Marxist influence, an emphasis on rigorous experimental methodology, and a closer association with pragmatism and social psychology. In the 1920s, the Chicago school developed symbolic interactionism. Meanwhile, in the 1930s, the Frankfurt School pioneered the idea of critical theory, an interdisciplinary form of Marxist sociology drawing upon thinkers as diverse as Sigmund Freud and Friedrich Nietzsche. Critical theory would take on something of a life of its own after World War II, influencing literary criticism and the Birmingham School establishment of cultural studies.

Sociology evolved as an academic response to the challenges of modernity, such as industrialization, urbanization, secularization, and a perceived process of enveloping rationalization. The field generally concerns the social rules and processes that bind and separate people not only as individuals, but as members of associations, groups, communities and institutions, and includes the examination of the organization and development of human social life. The sociological field of interest ranges from the analysis of short contacts between anonymous individuals on the street to the study of global social processes. In the terms of sociologists Peter L. Berger and Thomas Luckmann, social scientists seek an understanding of the "Social Construction of Reality". Most sociologists work in one or more subfields. One useful way to describe the discipline is as a cluster of sub-fields that examine different dimensions of society. For example, social stratification studies inequality and class structure; demography studies changes in a population size or type; criminology examines criminal behaviour and deviance; and political sociology studies the interaction between society and state.

Since its inception, sociological epistemologies, methods, and frames of enquiry, have significantly expanded and diverged. Sociologists use a diversity of research methods, collect both quantitative and qualitative data, draw upon empirical techniques, and engage critical theory. Common modern methods include case studies, historical research, interviewing, participant observation, social network analysis, survey research, statistical analysis, and model building, among other approaches. Since the late 1970s, many sociologists have tried to make the discipline useful for purposes beyond the academy. The results of sociological research aid educators, lawmakers, administrators, developers, and others interested in resolving social problems and formulating public policy, through subdisciplinary areas such as evaluation research, methodological assessment, and public sociology.

In the early 1970s, women sociologists began to question sociological paradigms and the invisibility of women in sociological studies, analysis, and courses. In 1969, feminist sociologists challenged the discipline's androcentrism at the American Sociological Association's annual conference. This led to the founding of the organization Sociologists for Women in Society, and, eventually, a new sociology journal, Gender & Society. Today, the sociology of gender is considered to be one of the most prominent sub-fields in the discipline.

New sociological sub-fields continue to appear — such as community studies, computational sociology, environmental sociology, network analysis, actor-network theory, gender studies, and a growing list, many of which are cross-disciplinary in nature.

Additional applied or interdisciplinary fields related to the social sciences include:

The origin of the survey can be traced back at least early as the Domesday Book in 1086, while some scholars pinpoint the origin of demography to 1663 with the publication of John Graunt's "Natural and Political Observations upon the Bills of Mortality". Social research began most intentionally, however, with the positivist philosophy of science in the 19th century.

In contemporary usage, "social research" is a relatively autonomous term, encompassing the work of practitioners from various disciplines that share in its aims and methods. Social scientists employ a range of methods in order to analyse a vast breadth of social phenomena; from census survey data derived from millions of individuals, to the in-depth analysis of a single agent's social experiences; from monitoring what is happening on contemporary streets, to the investigation of ancient historical documents. The methods originally rooted in classical sociology and statistical mathematics have formed the basis for research in other disciplines, such as political science, media studies, and marketing and market research.

Social research methods may be divided into two broad schools:

Social scientists will commonly combine quantitative and qualitative approaches as part of a multi-strategy design. Questionnaires, field-based data collection, archival database information and laboratory-based data collections are some of the measurement techniques used. It is noted the importance of measurement and analysis, focusing on the (difficult to achieve) goal of objective research or statistical hypothesis testing. A mathematical model uses mathematical language to describe a system. The process of developing a mathematical model is termed 'mathematical modelling' (also modeling). Eykhoff (1974) defined a "mathematical model" as 'a representation of the essential aspects of an existing system (or a system to be constructed) that presents knowledge of that system in usable form'. Mathematical models can take many forms, including but not limited to dynamical systems, statistical models, differential equations, or game theoretic models.

These and other types of models can overlap, with a given model involving a variety of abstract structures. The system is a set of interacting or interdependent entities, real or abstract, forming an integrated whole. The concept of an "integrated whole" can also be stated in terms of a system embodying a set of relationships that are differentiated from relationships of the set to other elements, and from relationships between an element of the set and elements not a part of the relational regime. A dynamical system modeled as a mathematical formalization has a fixed "rule" that describes the time dependence of a point's position in its ambient space. Small changes in the state of the system correspond to small changes in the numbers. The "evolution rule" of the dynamical system is a fixed rule that describes what future states follow from the current state. The rule is deterministic: for a given time interval only one future state follows from the current state.

Social scientists often conduct Program Evaluation, which is a systematic method for collecting, analyzing, and using information to answer questions about projects, policies and programs, particularly about their effectiveness and efficiency. In both the public and private sectors, stakeholders often want to know whether the programs they are funding, implementing, voting for, receiving or objecting to are producing the intended effect. While program evaluation first focuses around this definition, important considerations often include how much the program costs per participant, how the program could be improved, whether the program is worthwhile, whether there are better alternatives, if there are unintended outcomes, and whether the program goals are appropriate and useful.

Other social scientists emphasize the subjective nature of research. These writers share social theory perspectives that include various types of the following:

Other fringe social scientists delve in alternative nature of research. These writers share social theory perspectives that include various types of the following:

Most universities offer degrees in social science fields. The Bachelor of Social Science is a degree targeted at the social sciences in particular. It is often more flexible and in-depth than other degrees that include social science subjects.

In the United States, a university may offer a student who studies a social sciences field a Bachelor of Arts degree, particularly if the field is within one of the traditional liberal arts such as history, or a BSc: Bachelor of Science degree such as those given by the London School of Economics, as the social sciences constitute one of the two main branches of science (the other being the natural sciences). In addition, some institutions have degrees for a particular social science, such as the Bachelor of Economics degree, though such specialized degrees are relatively rare in the United States.

Graduate students may get a Master's degree (Master of Arts, Master of Science or a field-specific degree such as Master of Public Administration) or Ph.D.















</doc>
