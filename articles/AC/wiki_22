<doc id="26410" url="https://en.wikipedia.org/wiki?curid=26410" title="Conversion therapy">
Conversion therapy

Conversion therapy is the pseudoscientific practice of trying to change an individual's sexual orientation from homosexual or bisexual to heterosexual using psychological or spiritual interventions. There is no reliable evidence that sexual orientation can be changed and medical institutions warn that conversion therapy practices are ineffective and potentially harmful. Medical, scientific, and government organizations in the United States and United Kingdom have expressed concern over the validity, efficacy and ethics of conversion therapy. Various jurisdictions around the world have passed laws against conversion therapy.

The American Psychiatric Association (APA) opposes psychiatric treatment "based upon the assumption that homosexuality "per se" is a mental disorder or based upon the "a priori" assumption that a patient should change his/her sexual homosexual orientation" and describes attempts to change a person's sexual orientation by practitioners as unethical. It also states that the advancement of conversion therapy may cause social harm by disseminating unscientific views about sexual orientation. In 2001, United States Surgeon General David Satcher issued a report stating that "there is no valid scientific evidence that sexual orientation can be changed".

Techniques used in conversion therapy in the United States and Western Europe have included ice-pick lobotomies; chemical castration with hormonal treatment; aversive treatments, such as "the application of electric shock to the hands and/or genitals"; "nausea-inducing drugs ... administered simultaneously with the presentation of homoerotic stimuli"; and masturbatory reconditioning. More recent clinical techniques used in the United States have been limited to counseling, visualization, social skills training, psychoanalytic therapy, and spiritual interventions such as "prayer and group support and pressure", though there are some reports of aversive treatments through unlicensed practice as late as the early 2000s. The term reparative therapy has been used as a synonym for conversion therapy in general, but it has been argued that strictly speaking it refers to a specific kind of therapy associated with the psychologists Elizabeth Moberly and Joseph Nicolosi.

The National Association for Research & Therapy of Homosexuality (NARTH) was the main organization advocating for conversion therapy. Fundamentalist Christian groups, and some other organizations, have used religious justification for the therapy.

The history of conversion therapy can be divided broadly into three periods: an early Freudian period; a period of mainstream approval of conversion therapy, when the mental health establishment became the "primary superintendent" of sexuality; and a post-Stonewall period where the mainstream medical profession disavowed conversion therapy.

During the earliest parts of psychoanalytic history, analysts granted that homosexuality was non-pathological in certain cases, and the ethical question of whether it ought to be changed was discussed. By the 1920s analysts assumed that homosexuality was pathological and that attempts to treat it were appropriate, although psychoanalytic opinion about changing homosexuality was largely pessimistic. Those forms of homosexuality that were considered perversions were usually held to be incurable. Analysts' tolerant statements about homosexuality arose from recognition of the difficulty of achieving change. Beginning in the 1930s and continuing for roughly twenty years, major changes occurred in how analysts viewed homosexuality, which involved a shift in the rhetoric of analysts, some of whom felt free to ridicule and abuse their gay patients.

Sigmund Freud was a physician and the founder of psychoanalysis. Freud stated that homosexuality could sometimes be removed through hypnotic suggestion, and was influenced by Eugen Steinach, a Viennese endocrinologist who transplanted testicles from straight men into gay men in attempts to change their sexual orientation, stating that his research had "thrown a strong light on the organic determinants of homo-eroticism". Freud cautioned that Steinach's operations would not necessarily make possible a therapy that could be generally applied, arguing that such transplant procedures would be effective in changing homosexuality in men only in cases in which it was strongly associated with physical characteristics typical of women, and that probably no similar therapy could be applied to lesbianism. Steinach's method was doomed to failure because the immune system rejects transplanted glands, and was eventually exposed as ineffective and often harmful.

Freud's main discussion of female homosexuality was the 1920 paper "The Psychogenesis of a Case of Homosexuality in a Woman", which described his analysis of a young woman who had entered therapy because her parents were concerned that she was a lesbian. Her father wanted this condition changed. In Freud's view, the prognosis was unfavourable because of the circumstances under which she entered therapy, and because homosexuality was not an illness or neurotic conflict. Freud wrote that changing homosexuality was difficult and possible only under unusually favourable conditions, observing that "in general to undertake to convert a fully developed homosexual into a heterosexual does not offer much more prospect of success than the reverse". Success meant making heterosexual feeling possible, not eliminating homosexual feelings.

Gay people could seldom be convinced that heterosexual sex would provide them with the same pleasure they derived from homosexual sex. Patients often wanted to become heterosexual for reasons Freud considered superficial, including fear of social disapproval, an insufficient motive for change. Some might have no real desire to become heterosexual, seeking treatment only to convince themselves that they had done everything possible to change, leaving them free to return to homosexuality after the failure they expected.

In 1935, a mother asked Freud to treat her son. Freud replied in a letter that later became famous:

SÃ¡ndor Ferenczi was an influential psychoanalyst. Ferenczi hoped to cure some kinds of homosexuality completely, but was content in practice with reducing what he considered gay men's hostility to women, along with the urgency of their homosexual desires, and with helping them to become attracted to and potent with women. In his view, a gay man who was confused about his sexual identity and felt himself to be "a woman with the wish to be loved by a man" was not a promising candidate for cure. Ferenczi believed that complete cures of homosexuality might become possible in the future when psychoanalytic technique had been improved.

Daughter of Sigmund Freud, Anna Freud became an influential psychoanalytic theorist in the UK.

Anna Freud reported the successful treatment of homosexuals as neurotics in a series of unpublished lectures. In 1949 she published "Some Clinical Remarks Concerning the Treatment of Cases of Male Homosexuality" in the "International Journal of Psychoanalysis". In her view, it was important to pay attention to the interaction of passive and active homosexual fantasies and strivings, the original interplay of which prevented adequate identification with the father. The patient should be told that his choice of a passive partner allows him to enjoy a passive or receptive mode, while his choice of an active partner allows him to recapture his lost masculinity. She claimed that these interpretations would reactivate repressed castration anxieties, and childhood narcissistic grandiosity and its complementary fear of dissolving into nothing during heterosexual intercourse would come with the renewal of heterosexual potency.

Anna Freud in 1951 published "Clinical Observations on the Treatment of Male Homosexuality" in "The Psychoanalytic Quarterly" and "Homosexuality" in the "American Psychoanalytic Association (APsaA) Bulletin". In these articles, she insisted on the attainment of full object-love of the opposite sex as a requirement for cure of homosexuality. In 1951 she gave a lecture about treatment of homosexuality which was criticised by Edmund Bergler, who emphasised the oral fears of patients and minimized the importance of the phallic castration fears she had discussed.

Anna Freud recommended in 1956 to a journalist who was preparing an article about psychoanalysis for "The Observer" of London that she not quote Freud's letter to the American mother, on the grounds that "nowadays we can cure many more homosexuals than was thought possible in the beginning. The other reason is that readers may take this as a confirmation that all analysis can do is to convince patients that their defects or 'immoralities' do not matter and that they should be happy with them. That would be unfortunate."

Melanie Klein was a pupil of Ferenczi. Her seminal book "The Psycho-Analysis of Children", based on lectures given to the British Psychoanalytical Society in the 1920s, was published in 1932. Klein claimed that entry into the Oedipus Complex is based on mastery of primitive anxiety from the oral and anal stages. If these tasks are not performed properly, developments in the Oedipal stage will be unstable. Complete analysis of patients with such unstable developments would require uncovering these early concerns. The analysis of homosexuality required dealing with paranoid trends based on the oral stage. "The Psycho-Analysis of Children" ends with the analysis of Mr. B., a gay man. Klein claimed that he illustrated pathologies that enter into all forms of homosexuality: a gay man idealizes "the good penis" of his partner to allay the fear of attack he feels due to having projected his paranoid hatred onto the imagined "bad penis" of his mother as an infant. She stated that Mr. B.'s homosexual behaviour diminished after he overcame his need to adore the "good penis" of an idealized man. This was made possible by his recovering his belief in the good mother and his ability to sexually gratify her with his good penis and plentiful semen.

In March 2018, a majority of 435 against 109 representatives in the European parliament passed a resolution condemning conversion therapy and urging European Union member states to ban the practice.

Psychoanalysis started to receive recognition in the United States in 1909, when Sigmund Freud delivered a series of lectures at Clark University in Massachusetts at the invitation of G. Stanley Hall. In 1913, Abraham Brill wrote "The Conception of Homosexuality", which he published in the "Journal of the American Medical Association" and read before the American Medical Association's annual meeting. Brill criticised physical treatments for homosexuality such as bladder washing, rectal massage, and castration, along with hypnosis, but referred approvingly to Freud and Sadger's use of psychoanalysis, calling its results "very gratifying". Since Brill understood curing homosexuality as restoring heterosexual potency, he claimed that he had cured his patients in several cases, even though many remained homosexual.

Wilhelm Stekel, an Austrian, published his views on treatment of homosexuality, which he considered a disease, in the American "Psychoanalytic Review" in 1930. Stekel believed that "success was fairly certain" in changing homosexuality through psychoanalysis provided that it was performed correctly and the patient wanted to be treated. In 1932, "The Psychoanalytic Quarterly" published a translation of Helene Deutsch's paper "On Female Homosexuality". Deutsch reported her analysis of a lesbian, who did not become heterosexual as a result of treatment, but who managed to achieve a "positive libidinal relationship" with another woman. Deutsch indicated that she would have considered heterosexuality a better outcome.

Edmund Bergler was the most important psychoanalytic theorist of homosexuality in the 1950s. He was vociferous in his opposition to Alfred Kinsey. Kinsey's work, and its reception, led Bergler to develop his own theories for treatment, which were essentially to "blame the victim", in the evaluation of Jennifer Terry, associate professor of Woman's Studies. Bergler claimed that if gay people wanted to change, and the right therapeutic approach was taken, then they could be cured in 90% of cases. Bergler used confrontational therapy in which gay people were punished in order to make them aware of their masochism. Bergler openly violated professional ethics to achieve this, breaking patient confidentiality in discussing the cases of patients with other patients, bullying them, calling them liars and telling them they were worthless. He insisted that gay people could be cured. Bergler confronted Kinsey because Kinsey thwarted the possibility of cure by presenting homosexuality as an acceptable way of life, which was the basis of the gay rights activism of the time. Bergler popularised his views in the United States in the 1950s using magazine articles and books aimed at non-specialists.

In 1951, the mother who wrote to Freud asking him to treat her son sent Freud's response to the "American Journal of Psychiatry", in which it was published. The 1952 first edition of the American Psychiatric Association's Diagnostic and Statistical Manual of Mental Disorders (DSM-I) classified homosexuality as a mental disorder.

During the three decades between Freud's death in 1939 and the Stonewall riots in 1969, conversion therapy received approval from most of the psychiatric establishment in the United States. In 1962, Irving Bieber "et al." published "", in which they concluded that "although this change may be more easily accomplished by some than by others, in our judgment a heterosexual shift is a possibility for all homosexuals who are strongly motivated to change".

There was a riot in 1969 at the Stonewall Bar in New York after a police raid. The Stonewall riot acquired symbolic significance for the gay rights movement and came to be seen as the opening of a new phase in the struggle for gay liberation. Following these events, conversion therapy came under increasing attack. Activism against conversion therapy increasingly focused on the DSM's designation of homosexuality as a psychopathology. In 1973, after years of criticism from gay activists and bitter dispute among psychiatrists, the American Psychiatric Association removed homosexuality as a mental disorder from the "Diagnostic and Statistical Manual of Mental Disorders". Supporters of the change used evidence from researchers such as Kinsey and Evelyn Hooker. Psychiatrist Robert Spitzer, a member of the APA's Committee on Nomenclature, played an important role in the events that led to this decision. Critics argued that it was a result of pressure from gay activists, and demanded a referendum among voting members of the Association. The referendum was held in 1974 and the APA's decision was upheld by a 58% majority.

The APA removed ego-dystonic homosexuality from the DSM-III-R in 1987 and opposes the diagnosis of either homosexuality or ego-dystonic homosexuality as any type of disorder.

Joseph Nicolosi had a significant role in the development of conversion therapy as early as the 1990s, publishing his first book, "Reparative Therapy of Male Homosexuality", in 1991. In 1992, Nicolosi, with Charles Socarides and Benjamin Kaufman, founded the National Association for Research & Therapy of Homosexuality (NARTH), an organization that opposed the mainstream medical view of homosexuality and aimed to "make effective psychological therapy available to all homosexual men and women who seek change". NARTH has operated under the name "Alliance for Therapeutic Choice and Scientific Integrity" (ATCSI) since 2014. 

In 1998, Christian right groups including the Family Research Council and the American Family Association spent $600,000 on advertising promoting conversion therapy. John Paulk and his then wife Anne featured in full-page newspaper spreads.

United States Surgeon General David Satcher in 2001 issued a report stating that "there is no valid scientific evidence that sexual orientation can be changed". The same year, a study by Robert Spitzer concluded that some highly motivated individuals whose orientation is predominantly homosexual can become predominantly heterosexual with some form of reparative therapy. Spitzer based his findings on structured interviews with 200 self-selected individuals (143 males, 57 females). He told "The Washington Post" that the study "shows some people can change from gay to straight, and we ought to acknowledge that". Spitzer's study caused controversy and attracted media attention. Spitzer recanted his study in 2012, and apologized to the gay community for making unproven claims of the efficacy of reparative therapy, calling it his only professional regret.

The American Psychoanalytic Association spoke against NARTH in 2004, stating "that organization does not adhere to our policy of nondiscrimination and ... their activities are demeaning to our members who are gay and lesbian". The same year, a survey of members of the American Psychological Association rated reparative therapy as "certainly discredited", though the authors warn that the results should be interpreted carefully as an initial step, not as a final deliberation.

The American Psychological Association in 2007 convened a task force to evaluate its policies regarding reparative therapy.

In 2008, the organizers of an APA panel on the relationship between religion and homosexuality canceled the event after gay activists objected that "conversion therapists and their supporters on the religious right use these appearances as a public relations event to try and legitimize what they do".

In 2009, American Psychological Association stated that it "encourages mental health professionals to avoid misrepresenting the efficacy of sexual orientation change efforts by promoting or promising change in sexual orientation when providing assistance to individuals distressed by their own or others' sexual orientation and concludes that the benefits reported by participants in sexual orientation change efforts can be gained through approaches that do not attempt to change sexual orientation".

The ethics guidelines of major mental health organizations in the United States vary from cautionary statements to recommendations that ethical practitioners refrain from practicing conversion therapy (American Psychiatric Association) or from referring patients to those who do (American Counseling Association). In a letter dated February 23, 2011 to the Speaker of the U.S. House of Representatives, the Attorney General of the United States stated "while sexual orientation carries no visible badge, a growing scientific consensus accepts that sexual orientation is a characteristic that is immutable".

Gay rights groups and other groups concerned with mental health fear that reparative therapy can increase the chances of depression and suicide. President Barack Obama expressed opposition to the practice in 2015.

Before the American Psychological Association's 1973 decision to remove homosexuality from the DSM, practitioners of conversion therapy employed aversive conditioning techniques, involving electric shock and nausea-inducing drugs during presentation of same-sex erotic images. Cessation of the aversive stimuli was typically accompanied by the presentation of opposite-sex erotic images, with the objective of strengthening heterosexual feelings. In "Aversion therapy for sexual deviation: a critical review", published in 1966, M. P. Feldman claimed a 58% cure rate, but Douglas Haldeman is skeptical that such stressful methods permit feelings of sexual responsiveness, and notes that Feldman defined success as suppression of homosexuality and increased capacity for heterosexual behavior.

Another method used was the covert sensitization method, which involves instructing patients to imagine vomiting or receiving electric shocks, writing that only single case studies have been conducted, and that their results cannot be generalized. Haldeman writes that behavioral conditioning studies tend to decrease homosexual feelings, but do not increase heterosexual feelings, citing Rangaswami's "Difficulties in arousing and increasing heterosexual responsiveness in a homosexual: A case report", published in 1982, as typical in this respect.

Haldeman concludes that such methods can be called torture, besides being ineffective. He writes that "Individuals undergoing such treatments do not emerge heterosexually inclined; rather they become shamed, conflicted, and fearful about their homosexual feelings."

Some sources describe ex-gay ministries as a form of conversion therapy, while others state that ex-gay organizations and conversion therapy are distinct methods of attempting to convert gay people to heterosexuality. Ex-gay ministries have also been called transformational ministries. Some state that they do not conduct clinical treatment of any kind. Exodus International once believed reparative therapy could be a beneficial tool. The umbrella organization in the United States ceased activities in June 2013, and the three member board issued a statement which repudiated its aims and apologized for the harm their pursuit has caused to LGBT people.

Haldeman writes that psychoanalytic treatment of homosexuality is exemplified by the work of Irving Bieber "et al." in "Homosexuality: A Psychoanalytic Study of Male Homosexuals". They advocated long-term therapy aimed at resolving the unconscious childhood conflicts that they considered responsible for homosexuality. Haldeman notes that Bieber's methodology has been criticized because it relied upon a clinical sample, the description of the outcomes was based upon subjective therapist impression, and follow-up data were poorly presented. Bieber reported a 27% success rate from long-term therapy, but only 18% of the patients in whom Bieber considered the treatment successful had been exclusively homosexual to begin with, while 50% had been bisexual. In Haldeman's view, this makes even Bieber's unimpressive claims of success misleading.

Haldeman discusses other psychoanalytic studies of attempts to change homosexuality. Curran and Parr's "Homosexuality: An analysis of 100 male cases", published in 1957, reported no significant increase in heterosexual behavior. Mayerson and Lief's "Psychotherapy of homosexuals: A follow-up study of nineteen cases", published in 1965, reported that half of its 19 subjects were exclusively heterosexual in behavior four and a half years after treatment, but its outcomes were based on patient self-report and had no external validation. In Haldeman's view, those participants in the study who reported change were bisexual at the outset, and its authors wrongly interpreted capacity for heterosexual sex as change of sexual orientation.

The term "reparative therapy" has been used as a synonym for conversion therapy generally, but according to Jack Drescher it properly refers to a specific kind of therapy associated with the psychologists Elizabeth Moberly and Joseph Nicolosi. The term "reparative" refers to Nicolosi's postulate that same-sex attraction is a person's unconscious attempt to "self-repair" feelings of inferiority.

Most mental health professionals and the American Psychological Association consider reparative therapy discredited, but it is still practiced by some. In 2014 the Republican Party of Texas endorsed "counseling, which offers reparative therapy and treatment" in their party platform. Exodus International regarded reparative therapy as a useful tool to eliminate "unwanted same-sex attraction" but ceased activities in June 2013 and issued a statement repudiating its aims and apologizing for the harm the organization had caused to LGBT people. Psychoanalysts critical of Nicolosi's theories have offered gay-affirmative approaches as an alternative to reparative therapy.

Haldeman has described William Masters' and Virginia Johnson's work on sexual orientation change as a form of conversion therapy.

In "Homosexuality in Perspective", published in 1979, Masters and Johnson viewed homosexuality as the result of blocks that prevented the learning that facilitated heterosexual responsiveness, and described a study of 54 gay men who were dissatisfied with their sexual orientation. The original study did not describe the treatment methodology used, but this was published five years later. John C. Gonsiorek criticized their study on several grounds in 1981, pointing out that while Masters and Johnson stated that their patients were screened for major psychopathology or severe neurosis, they did not explain how this screening was performed, or how the motivation of the patients to change was assessed. Nineteen of their subjects were described as uncooperative during therapy and refused to participate in a follow-up assessment, but all of them were assumed without justification to have successfully changed.

Haldeman writes that Masters and Johnson's study was founded on heterosexist bias, and that it would be tremendously difficult to replicate. In his view, the distinction Masters and Johnson made between "conversion" (helping gay men with no previous heterosexual experience to learn heterosexual sex) and "reversion" (directing men with some previous heterosexual experience back to heterosexuality) was not well founded. Many of the subjects Masters and Johnson labelled homosexual may not have been homosexual, since, of their participants, only 17% identified themselves as exclusively homosexual, while 83% were in the predominantly heterosexual to bisexual range. Haldeman observed that since 30% of the sample was lost to the follow-up, it is possible that the outcome sample did not include any people attracted mainly or exclusively to the same sex. Haldeman concludes that it is likely that, rather than converting or reverting gay people to heterosexuality, Masters and Johnson only strengthened heterosexual responsiveness in people who were already bisexual.

In the 1940s and 1950s, U.S. neurologist Walter Freeman popularized the ice-pick lobotomy as a treatment for homosexuality. He personally performed as many as 3,439 lobotomy surgeries in 23 states, of which 2,500 used his ice-pick procedure, despite the fact that he had no formal surgical training. Up to 40% of Freeman's patients were gay individuals subjected to a lobotomy in order to change their homosexual orientation, leaving most of these individuals severely disabled for the rest of their lives. While promoted at the time as a treatment for various psychoses, the effectiveness of lobotomy in changing sexual orientation was already the subject of critical research in 1948 when a single case was investigated by Joseph Friedlander and Ralph Banay. A video depicting the "ice-pick lobotomy" of a homosexual man was featured in the documentary film, "".

In May 2001, Robert Spitzer presented "Can Some Gay Men and Lesbians Change Their Sexual Orientation? 200 Participants Reporting a Change from Homosexual to Heterosexual Orientation", a study of attempts to change homosexual orientation through ex-gay ministries and conversion therapy, at the American Psychiatric Association's convention in New Orleans. The study was partly a response to the APA's 2000 statement cautioning against clinical attempts at changing homosexuality, and was aimed at determining whether such attempts were ever successful rather than how likely it was that change would occur for any given individual. Spitzer wrote that some earlier studies provided evidence for the effectiveness of therapy in changing sexual orientation, but that all of them suffered from methodological problems.

In 2012, Spitzer renounced and retracted this study, stating "I was quite wrong in the conclusions that I made from this study. The study does not provide evidence, really, that gays can change. And that's quite an admission on my part." He also apologized to the gay community for making unproven claims of the efficacy of reparative therapy, calling it his only professional regret. Spitzer has requested that all "ex-gay" therapy organizations such as NARTH, PFOX, American College of Pediatricians, and Focus on the Family stop citing his study as evidence for conversion therapy.

Spitzer reported that after intervention, 66% of the men and 44% of the women had achieved "Good Heterosexual Functioning", which he defined as requiring five criteria (being in a loving heterosexual relationship during the last year, overall satisfaction in emotional relationship with a partner, having heterosexual sex with the partner at least a few times a month, achieving physical satisfaction through heterosexual sex, and not thinking about having homosexual sex more than 15% of the time while having heterosexual sex). He found that the most common reasons for seeking change were lack of emotional satisfaction from gay life, conflict between same-sex feelings and behavior and religious beliefs, and desire to marry or remain married. This paper was widely reported in the international media and taken up by politicians in the United States, Germany, and Finland, and by conversion therapists.

In 2003, Spitzer published the paper in the "Archives of Sexual Behavior". Spitzer's study has been criticized on numerous ethical and methodological grounds, and "press releases from both NGLTF and HRC sought to undermine Spitzer's credibility by connecting him politically to right-wing groups that had backed the ex-gay movement". Gay activists argued that the study would be used by conservatives to undermine gay rights. Spitzer acknowledged that the study sample consisted of people who sought treatment primarily because of their religious beliefs (93% of the sample), served in various church-related functions, and who publicly spoke in favor of changing homosexual orientation (78%), and thus were strongly motivated to overreport success. Critics felt he dismissed this source of bias, without even attempting to measure deception or self-deception (a standard practice in self-reporting psychological tests like MMPI-2). That participants had to rely upon their memories of what their feelings were before treatment may have distorted the findings. It was impossible to determine whether any change that occurred was due to the treatment because it was not clear what it involved and there was no control group. Spitzer's own data showed that claims of change were reflected mostly in changes in self-labelling and behavior, less in attractions, and least in the homoerotic content during the masturbatory fantasies; this particular finding was consistent with other studies in this area. Participants may have been bisexual before treatment. Follow-up studies were not conducted. Spitzer stressed the limitations of his study. Spitzer said that the number of gay people who could successfully become heterosexual was likely to be "pretty low", and conceded that his subjects were "unusually religious".

Ariel Shidlo and Michael Schroeder found in "Changing Sexual Orientation: A Consumer's Report", a peer-reviewed study of 202 respondents published in 2002, that 88% of participants failed to achieve a sustained change in their sexual behavior and 3% reported changing their orientation to heterosexual. The remainder reported either losing all sexual drive or attempting to remain celibate, with no change in attraction. Some of the participants who failed felt a sense of shame and had gone through conversion therapy programs for many years. Others who failed believed that therapy was worthwhile and valuable. Many respondents felt harmed by the attempt to change, and reported depression, suicidal ideation and attempts, hypervigilance of gender-deviant mannerisms, social isolation, fear of being a child abuser and poor self-esteem. Of the 8 respondents (out of a sample of 202) who reported a change in sexual orientation, 7 worked as ex-gay counselors or group leaders.

Although no national ban exists, several US states and individual counties ban therapy attempting to change sexual orientation as shown in the map below.

Many health organizations around the world have denounced and criticized sexual orientation change efforts. National health organizations in the United States have announced that there has been no scientific demonstration of conversion therapy's efficacy in the last forty years.
They find that conversion therapy is ineffective, risky and can be harmful. Anecdotal claims of cures are counterbalanced by assertions of harm, and the American Psychiatric Association, for example, cautions ethical practitioners under the Hippocratic oath to do no harm and to refrain from attempts at conversion therapy.

Mainstream medical bodies state that conversion therapy can be harmful because it may exploit guilt and anxiety, thereby damaging self-esteem and leading to depression and even suicide. There is also concern in the mental health community that the advancement of conversion therapy can cause social harm by disseminating inaccurate views about sexual orientation and the ability of gay and bisexual people to lead happy, healthy lives.

Major health organizations critical of conversion therapy include:






The American Psychological Association undertook a study of the peer-reviewed literature in the area of sexual orientation change efforts (SOCE) and found a myriad of issues with the procedures used in conducting the research. The taskforce did find that some participants experienced a lessening of same sex attraction and arousal, but that these instances were "rare" and "uncommon". The taskforce concluded that, "given the limited amount of methodically sound research, claims that recent SOCE is effective are not supported". Two issues with SOCE claims are that conversion therapists falsely assume that homosexuality is a mental disorder and that their research focuses almost exclusively on gay men and rarely includes lesbians.

The American Psychological Association's code of conduct states that "Psychologists respect the dignity and worth of all people, and the rights of individuals to privacy, confidentiality, and self-determination", but also that "Psychologists are aware that special safeguards may be necessary to protect the rights and welfare of persons or communities whose vulnerabilities impair autonomous decision making." The American Counseling Association says that "it is of primary importance to respect a client's autonomy to request a referral for a service not offered by a counselor". They said that no one should be forced to attempt to change their sexual orientation against their will, including children being forced by their parents.

Supporters of SOCE focus on patient self-determination when discussing whether therapy should be available. Mark Yarhouse, of Pat Robertson's Regent University, wrote that "psychologists have an ethical responsibility to allow individuals to pursue treatment aimed at curbing experiences of same-sex attraction or modifying same-sex behaviors, not only because it affirms the client's rights to dignity, autonomy, and agency, as persons presumed capable of freely choosing among treatment modalities and behavior, but also because it demonstrates regard for diversity". Yarhouse and Throckmorton, of the private Christian school Grove City College, argue that the procedure should be available out of respect for a patient's values system and because they find evidence that it can be effective. Haldeman similarly argues for a client's right to access to therapy if requested from a fully informed position: "For some, religious identity is so important that it is more realistic to consider changing sexual orientation than abandoning one's religion of origin ... and if there are those who seek to resolve the conflict between sexual orientation and spirituality with conversion therapy, they must not be discouraged."

In response to Yarhouse's paper, Jack Drescher argued that "any putative ethical obligation to refer a patient for reparative therapy is outweighed by a stronger ethical obligation to keep patients away from mental health practitioners who engage in questionable clinical practices". Chuck Bright wrote that refusing to endorse a procedure that "has been deemed unethical and potentially harmful by most medical and nearly every professional psychotherapy regulating body cannot be justifiably identified as prohibiting client self-determination". Some commentators, recommending a hard stand against the practice, have found therapy inconsistent with a psychologist's ethical duties because "it is more ethical to let a client continue to struggle honestly with her or his identity than to collude, even peripherally, with a practice that is discriminatory, oppressive, and ultimately ineffective in its own stated ends". They argue that clients who request it do so out of social pressure and internalized homophobia, pointing to evidence that rates of depression, anxiety, alcohol and drug abuse and suicidal feelings are roughly doubled in those who undergo therapy.

Haldeman argues that, due to concern for people whose "spiritual or religious concerns" may assume priority over their sexual orientation, mental health organizations do not ban conversion therapy outright.

In 1998, the American Psychiatric Association issued a statement opposing any treatment which is based upon the assumption that homosexuality is a mental disorder or that a person should change their orientation, but did not have a formal position on other treatments that attempt to change a person's sexual orientation. In 2000, they augmented that statement by saying that as a general principle, a therapist should not determine the goal of treatment, but recommends that ethical practitioners refrain from attempts to change clients' sexual orientation until more research is available.

The American Counseling Association has stated that they do not condone any training to educate and prepare a counselor to practice conversion therapy. Counselors who do offer training in conversion therapy must inform students that the techniques are unproven. They suggest counselors do not refer clients to a conversion therapist or to proceed cautiously once they know the counselor fully informs clients of the unproven nature of the treatment and the potential risks. However, "it is of primary importance to respect a client's autonomy to request a referral for a service not offered by a counselor". A counselor performing conversion therapy must provide complete information about the treatment, offer referrals to gay-affirmative counselors, discuss the right of clients, understand the client's request within a cultural context, and only practice within their level of expertise.

NARTH stated in 2012 that refusing to offer therapy aimed at change to a client who requests it, and telling them that their only option is to claim a gay identity, could also be considered ethically unacceptable. In 2012 the British Psychological Society issued a position statement opposing any treatments that are based on an assumption that non-heterosexual orientations are pathological.

A 2013 article by the Committee on Adolescence of the American Academy of Pediatrics stated "Referral for 'conversion' or 'reparative therapy' is never indicated; therapy is not effective and may be harmful to LGBTQ individuals by increasing internalized stigma, distress, and depression."

In 2014, the American Association of Christian Counselors amended its code of ethics to eliminate the promotion of conversion therapy for homosexuals and encouraged them to be celibate instead. An article in the American Medical Association's Journal of Ethics argues that if a pediatrician learns that parents of a 12-year-old patient seek conversion therapy, the pediatrician can advise against "the ineffective and potentially harmful intervention" while being culturally sensitive of their religious objections to homosexuality. The authors argue that the doctor's medical ethics means they should place the interests of the patient above the cultural sensitivities of the parents, and confidentially counsel the patient about resources for LGBT youth facing bullying, and advise the parents about resources for parents of LGBT children. In 2014, major therapy professional bodies in the United Kingdom issued a joint consensus statement opposing conversion therapy. Professional bodies supporting the statement included the UK Council for Psychotherapy, the British Psychoanalytic Council, the Royal College of Psychiatrists, the British Association for Counselling and Psychotherapy, the British Psychological Society and the National Counselling Society.

In 2015, with support of the UK Government's Department of Health, a wide range of UK organisations signed a memorandum of understanding (MoU) setting out an agreed framework for activities by parties concerned to help address the issues raised by the practice of conversion therapy in the UK. In addition to many of the professional bodies that previously issued the consensus statement, signatories included the UK Association of Christian Counsellors, the Royal College of General Practitioners, NHS England and NHS Scotland. The signatory organisations recognised a shared commitment to protecting the public from the risks of conversion therapy. They committed to raise awareness among healthcare professionals and psychological therapists of ethical issues involved in conversion therapy and to provide training to enable therapists to support clients in distress in an appropriate way.

The World Health Organization's ICD-10, which along with the DSM-IV is widely used internationally, states that "sexual orientation by itself is not to be regarded as a disorder". It lists ego-dystonic sexual orientation as a disorder instead, which it defines as occurring where "the gender identity or sexual preference (heterosexual, homosexual, bisexual, or prepubertal) is not in doubt, but the individual wishes it were different because of associated psychological and behavioural disorders, and may seek treatment in order to change it".

In 2012, the Pan American Health Organization (the North and South American branch of the World Health Organization) released a statement cautioning against services that purport to "cure" people with non-heterosexual sexual orientations as they lack medical justification and represent a serious threat to the health and well-being of affected people, and noted that the global scientific and professional consensus is that homosexuality is a normal and natural variation of human sexuality and cannot be regarded as a pathological condition. The Pan American Health Organization further called on governments, academic institutions, professional associations and the media to expose these practices and to promote respect for diversity. The World Health Organization affiliate further noted that gay minors have sometimes been forced to attend these "therapies" involuntarily, being deprived of their liberty and sometimes kept in isolation for several months, and that these findings were reported by several United Nations bodies. Additionally, the Pan American Health Organization recommended that such practices be denounced and subject to sanctions and penalties under national legislation, as they constitute a violation of the ethical principles of health care and violate human rights that are protected by international and regional agreements.

The development of theoretical models of sexual orientation in countries outside the United States that have established mental health professions often follows the history within the U.S. (although often at a slower pace), shifting from pathological to non-pathological conceptions of homosexuality.

Major medical and psychological bodies in Australia uniformly prohibit conversion therapy practices, with published statements having come from peak bodies representing psychologists, psychiatrists, and medical practitioners. In a statement issued jointly with the College of Psychiatrists, Royal Australasian College of Physicians President Catherine Yelland summarised the view of the Australian medical community: "[g]ay conversion therapy is unethical, harmful and not supported by medical evidence." The approaches taken by peak medical bodies is exemplified by the 2015 Australian Psychological Society Position Statement, which declares (emphasis in original) that the:
The Position Statement supports this position by reference to the Society's Code of Ethics, which were adopted in 2007 and mandated as the Code of Ethics for Australian psychologists in 2010 by the Psychology Board of Australia. Under the Code, psychologists are required to "avoid discriminating unfairly against people on the basis of age, religion, sexuality, ethnicity, gender, disability, or any other basis proscribed by law" and mandates that they
The Position Statement explicitly states that this ethical "requirement not to discriminate and to respect clients' moral rights does not equate to a justification to treat homosexuality or bisexuality as a disorder requiring treatment," relying on the Code of Ethics' section on propriety: "psychologists only provide psychological services within the boundaries of their professional competence [which] includes but is not restricted to ... basing their service on established knowledge of the discipline and profession of psychology". Regarding the knowledge base relating to conversion therapy, the statement is unequivocal (emphasis in original):
The Society's position concludes by noting that it "is, of course, appropriate for psychologists to provide clinical services to clients who experience distress in regards to their sexual orientation ... [but this practice] should seek to understand "the reasons for" distress and how it may be alleviated. Evidence-based strategies to alleviate distress do not include attempts at changing sexual orientation, but could include challenging negative stereotypes, seeking social support, and self-acceptance, among others."

The Government of Victoria announced in 2016 that it would be legislating to ban all LGBTQI conversion therapy. The new law began operating in February 2017 and allows the Health Complaints Commissioner to act against any health professional engaged in practices that are "found to be making false claims and to be acting in a manner that puts people's physical, mental or psychological health, safety or welfare at risk"âand in a world first, this law applies to conversion therapy for adults as well as for minors. Western Australia and the Australian Capital Territory announced in September 2017 that they are investigating similar laws. Advocates for a ban on conversion therapy argued that reviews need to go beyond the practices of health professionals and into activities of religious groups and the unregulated (non-medical) counselling sector.

A Fairfax Media investigation in 2018 reported that "across Australia, organisations who believe that LGBTI people can or should change are hard at work. Conversion practices are hidden in evangelical churches and ministries, taking the form of exorcisms, prayer groups or counselling disguised as pastoral care. They're also present in some religious schools or practised in the private offices of health professionals. They're pushed out through a thriving network of courses and mentors in the borderless world of cyberspace, cloaked in the terminology of 'self improvement' or 'spiritual healing.'" A study of Pentecostal-Charismatic Churches found that LGBTI parishioners were faced with four options: remain closeted, come out but commit to remaining celibate, undergo conversion therapy, or leave the church... the majority took the last option, though typically only after "agonising attempts to reconcile their faith and their sexuality." The study provides corroboration that conversion therapy remains practiced within religious communities.

Following the Fairfax investigation, Victorian Premier Daniel Andrews called on Prime Minister Malcolm Turnbull to support outlawing conversion therapy as part of the national mental health strategy. Federal Health Minister Greg Hunt declared that the issue is one for the states as no Commonwealth funding goes to sexual orientation change effortsâthough "gay conversion ideology has been quietly pushed in schools as part of the federal government's chaplaincy program." The report noted that the Victorian law applies only to people offering health services and so does not catch religious groups and charities "who say they are helping same-sex attracted people to live in accordance with their faith."

Chris, a survivor of conversion therapy, joined Andrews in calling for the Federal Government to outlaw conversion therapy, declaring that "praying the gay away nearly killed me." He established a petition calling on Turnbull and Hunt to act to outlaw conversion therapy, declaring: "I prayed to God asking him to either heal me, or kill me. I was so depressed, I wanted to die." In April 2018, Shadow Health Minister Catherine King wrote a response to the petition: "I'm writing to let you know that Labor stands with you, Chris Csabs and the medical experts in opposing gay conversion therapy...two Turnbull Government ministersâthe Acting Prime Minister and the Health Ministerâhave now failed to condemn the practice when given the chance." Shortly after Catherine King's response, Health Minister for Queensland Dr Steven Miles voiced his concerns over the practise and stated that the Federal Health Minister should be working with the states to enact change. In May 2018, the Victorian Health Minister Jill Hennessy called for an inquiry into gay conversion therapies. In an unprecedented move, the state government indicated it would not only investigate health professionals but will focus on religious and faith-based ministries propagating Gay Conversion ideologies. The following day, Health Minister for the Australian Capital Territory Meegan Fitzharris followed Catherine King's lead by also responding to the petition, stating that, "The ACT government will ban gay conversion therapy. It is abhorrent and completely inconsistent with the inclusive values of Canberrans."

On June 25, 2015, a New Jersey jury found the Jewish conversion therapy organization JONAH guilty of consumer fraud in the case Ferguson v JONAH for promising to be able to change its clients' sexual urges and determined its commercial practices to be unconscionable.

In a 1997 U.S. case, the Ninth Circuit addressed conversion therapy in the context of an asylum application. A Russian citizen "had been apprehended by the Russian militia, registered at a clinic as a 'suspected lesbian', and forced to undergo treatment for lesbianism, such as 'sedative drugs' and hypnosis. ... The Ninth Circuit held that the conversion treatments to which Pitcherskaia had been subjected constituted mental and physical torture." The court rejected the argument that the treatments to which Pitcherskaia had been subjected did not constitute persecution because they had been intended to help her, not harm her, and stated "human rights laws cannot be sidestepped by simply couching actions that torture mentally or physically in benevolent terms such as 'curing' or 'treating' the victims".

In 1993, the Superior Court of San Francisco's Family Court placed 15-year-old lesbian Lyn Duff under the guardianship of a foster couple after her mother committed her to Rivendell Psychiatric Center in West Jordan, Utah, where she allegedly endured physical abuse under the guise of conversion therapy. Lyn Duff's petition to leave her mother was granted without court opinion.





</doc>
<doc id="26411" url="https://en.wikipedia.org/wiki?curid=26411" title="Ring homomorphism">
Ring homomorphism

In abstract algebra, more specifically ring theory, a ring homomorphism is a structure-preserving function between two rings. More explicitly, if "R" and "S" are rings, then a ring homomorphism is a function such that "f" is

Additive inverses and the additive identity are part of the structure too, but it is not necessary to require explicitly that they too are respected, because these conditions are consequences of the three conditions above. On the other hand, neglecting to include the condition "f"(1) = 1 would cause several of the properties below to fail.

If in addition "f" is a bijection, then its inverse "f" is also a ring homomorphism. In this case, "f" is called a ring isomorphism, and the rings "R" and "S" are called isomorphic. From the standpoint of ring theory, isomorphic rings cannot be distinguished.

If "R" and "S" are rngs (also known as "pseudo-rings", or "non-unital rings"), then the natural notion is that of a rng homomorphism, defined as above except without the third condition "f"(1) = 1. It is possible to have a rng homomorphism between (unital) rings that is not a ring homomorphism.

The composition of two ring homomorphisms is a ring homomorphism. It follows that the class of all rings forms a category with ring homomorphisms as the morphisms (cf. the category of rings).
In particular, one obtains the notions of ring endomorphism, ring isomorphism, and ring automorphism.

Let be a ring homomorphism. Then, directly from these definitions, one can deduce:

Moreover,




Injective ring homomorphisms are identical to monomorphisms in the category of rings: If is a monomorphism that is not injective, then it sends some "r" and "r" to the same element of "S". Consider the two maps "g" and "g" from Z["x"] to "R" that map "x" to "r" and "r", respectively; and are identical, but since "f" is a monomorphism this is impossible.

However, surjective ring homomorphisms are vastly different from epimorphisms in the category of rings. For example, the inclusion is a ring epimorphism, but not a surjection. However, they are exactly the same as the strong epimorphisms.




</doc>
<doc id="26413" url="https://en.wikipedia.org/wiki?curid=26413" title="Real Madrid CF">
Real Madrid CF

Real Madrid Club de FÃºtbol (, meaning "Royal Madrid Football Club"), commonly referred to as Real Madrid, is a Spanish professional football club based in Madrid.

Founded on 6 March 1902 as Madrid Football Club, the club has traditionally worn a white home kit since inception. The word "real" is Spanish for "royal" and was bestowed to the club by King Alfonso XIII in 1920 together with the royal crown in the emblem. The team has played its home matches in the 81,044-capacity Santiago BernabÃ©u Stadium in downtown Madrid since 1947. Unlike most European sporting entities, Real Madrid's members ("socios") have owned and operated the club throughout its history.

The club was estimated to be worth â¬3.8Â billion ($4.2Â billion) in 2019, and it was the second highest-earning football club in the world, with an annual revenue of â¬757,3Â million in 2019. The club is one of the most widely supported teams in the world. Real Madrid is one of three founding members of La Liga that have never been relegated from the top division since its inception in 1929, along with Athletic Bilbao and Barcelona. The club holds many long-standing rivalries, most notably "El ClÃ¡sico" with Barcelona and "El Derbi" with AtlÃ©tico Madrid.

Real Madrid established itself as a major force in both Spanish and European football during the 1950s, winning five consecutive European Cups and reaching the final seven times. This success was replicated in the league, which the club won five times in the space of seven years. This team, which consisted of players such as Alfredo Di StÃ©fano, Ferenc PuskÃ¡s, Francisco Gento, and Raymond Kopa, is considered by some in the sport to be the greatest team of all time. In domestic football, the club has won 65 trophies; a record 33 La Liga titles, 19 Copa del Rey, 11 Supercopa de EspaÃ±a, a Copa Eva Duarte, and a Copa de la Liga. In European and worldwide competitions, the club has won a record 26 trophies; a record 13 European Cup/UEFA Champions League titles, two UEFA Cups and four UEFA Super Cups. In international football, they have achieved a record seven club world championships.

Real Madrid was recognised as the FIFA Club of the 20th Century on 11 December 2000, and received the FIFA Centennial Order of Merit on 20 May 2004. The club was also awarded Best European Club of the 20th Century by the IFFHS on 11 May 2010. In June 2017, the team succeeded in becoming the first club to win back to back Champions Leagues, then made it three in a row in May 2018, extending their lead atop the UEFA club rankings.

Real Madrid's origins go back to when football was introduced to Madrid by the academics and students of the "InstituciÃ³n Libre de EnseÃ±anza", which included several Cambridge and Oxford University graduates. They founded "(Sociedad) Sky Football" in 1897, commonly known as "La Sociedad" (The Society) as it was the only one based in Madrid, playing on Sunday mornings at Moncloa. In 1900, conflict between members caused some of them to leave and create a new club, "Nueva Sociedad de Football" (New Society of Football), to distinguish themselves from "Sky Football". Among the dissenters were JuliÃ¡n Palacios, recognized as the first Real Madrid president, Juan PadrÃ³s and Carlos PadrÃ³s, the latter two being brothers and future presidents of Real Madrid. In 1901 this new club was renamed as Madrid Football Club. Later, following a restructuring in 1902, "Sky" was renamed as "New Foot-Ball Club". On 6 March 1902, after a new Board presided by Juan PadrÃ³s had been elected, Madrid Football Club was officially founded.

Three years after its foundation, in 1905, "Madrid FC" won its first title after defeating Athletic Bilbao in the Spanish Cup final. The club became one of the founding sides of the Royal Spanish Football Federation on 4 January 1909, when club president Adolfo MelÃ©ndez signed the foundation agreement of the Spanish FA. After moving between grounds the team moved to the "Campo de O'Donnell" in 1912. In 1920, the club's name was changed to Real Madrid after King Alfonso XIII granted the title of Real (Royal) to the club.

In 1929, the first Spanish football league was founded. Real Madrid led the first league season until the last match, a loss to Athletic Bilbao, meant they finished runners-up to Barcelona. Real Madrid won its first League title in the 1931â32 season and retained the title the following year, becoming the first team to win the championship twice.

On 14 April 1931, the arrival of the Second Spanish Republic caused the club to lose the title Real and went back to being named Madrid Football Club. Football continued during the Second World War, and on 13 June 1943 Madrid beat Barcelona 11â1 in the second leg of a semi-final of the Copa del GeneralÃ­simo, the Copa del Rey having been renamed in honour of General Franco. It has been suggested Barcelona players were intimidated by police, including by the director of state security who "allegedly told the team that some of them were only playing because of the regime's generosity in permitting them to remain in the country." The Barcelona chairman, Enrique PiÃ±eyro, was assaulted by Madrid fans. However, none of these allegations have been proven and FIFA and UEFA still consider the result as legitimate. According to Spanish journalist and writer, Juan Carlos Pasamontes, Barcelona player Josep Valle denied that the Spanish security forces came before the match. Instead, at the end of the first half, Barcelona coach Juan JosÃ© NoguÃ©s and all of his players were angry with the hard-style of play Real Madrid was using and with the aggressiveness of the home crowd. When they refused to take the field, the Superior Chief of Police of Madrid appeared, identified himself, and ordered the team to take the field.

Santiago BernabÃ©u became president of Real Madrid in 1945. Under his presidency, the club, its stadium Santiago BernabÃ©u and its training facilities Ciudad Deportiva were rebuilt after the Spanish Civil War damages. Additionally, during the 1950s former "Real Madrid Amateurs" player Miguel Malbo founded Real Madrid's youth academy, or ""cantera"," known today as La FÃ¡brica. Beginning in 1953, he embarked upon a strategy of signing world-class players from abroad, the most prominent being Alfredo Di StÃ©fano.

In 1955, acting upon the idea proposed by Gabriel Hanot, a French sports journalist and editor of "L'Ãquipe", BernabÃ©u, Bedrignan and GusztÃ¡v Sebes created a tournament for the champions teams around Europe, under invitation, that would eventually become what today is known as the UEFA Champions League. It was under BernabÃ©u's guidance that Real Madrid established itself as a major force in both Spanish and European football. The club won the European Cup five times in a row between 1956 and 1960, which included the 7â3 Hampden Park final against Eintracht Frankfurt in 1960. After these five consecutive successes, Real was permanently awarded the original cup and earning the right to wear the UEFA badge of honour.

The club won the European Cup for a sixth time in 1966 defeating Partizan Belgrade 2â1 in the final with a team composed entirely of same nationality players, a first in the competition. This team became known as the YÃ©-yÃ©. The name "YÃ©-yÃ©" came from the "Yeah, yeah, yeah" chorus in The Beatles' song "She Loves You" after four members of the team posed for "Marca" and impersonated the Beatles. The YÃ©-yÃ© generation was also European Cup runner-up in 1962 and 1964. In the 1970s, Real Madrid won five league championships and three Spanish Cups. The club played its first UEFA Cup Winners' Cup final in 1971 and lost to English side Chelsea 2â1. On 2 July 1978, club president Santiago BernabÃ©u died while the World Cup was being played in Argentina. FIFA decreed three days of mourning to honour him during the tournament. The following year, the club organized the first edition of the Trofeo Santiago BernabÃ©u in memory of its former president.

By the early 1980s, Real Madrid had lost its grasp on the Liga title until a new cohort of home-grown stars brought domestic success back to the club. Spanish sport journalist Julio CÃ©sar Iglesias gave to this generation the name "La Quinta del Buitre" ("Vulture's Cohort"), which was derived from the nickname given to one of its members, Emilio ButragueÃ±o. The other four members were Manuel SanchÃ­s, MartÃ­n VÃ¡zquez, MÃ­chel and Miguel Pardeza; all five footballers were graduates of Real Madrid's youth academy. With "La Quinta del Buitre" (reduced to four members when Pardeza left for Zaragoza in 1986) and notable players like goalkeeper Francisco Buyo, right-back Miguel PorlÃ¡n "Chendo" and Mexican striker Hugo SÃ¡nchez, Real Madrid had one of the best teams in Spain and Europe during the second half of the 1980s, winning two UEFA Cups, five Spanish championships in a row, one Spanish cup and three Spanish Super Cups. In the early 1990s, "La Quinta del Buitre" split up after MartÃ­n VÃ¡zquez, Emilio ButragueÃ±o and MÃ­chel left the club.

In 1996, President Lorenzo Sanz appointed Fabio Capello as coach. Although his tenure lasted only one season, Real Madrid were proclaimed league champions, and players like Predrag MijatoviÄ, Davor Å uker, Clarence Seedorf, Roberto Carlos and keeper Bodo Illgner, arrived at the club to strengthen a squad that already boasted the likes of RaÃºl, Fernando Hierro and Fernando Redondo. As a result, Real Madrid (with the addition of Fernando Morientes in 1997) finally ended its 32-year wait for its seventh European Cup: in 1998, under manager Jupp Heynckes, they defeated Juventus 1â0 in the final with a goal from MijatoviÄ.

In November 1999 Vicente del Bosque took over as coach. For the last season of the century, 1999â2000, the squad was still led by the older veterans such as Fernando Hierro, Fernando Redondo, Roberto Carlos and RaÃºl GonzÃ¡lez. Real added the budding young talents of Fernando Morientes, Guti and Iker Casillas, supported by the arrival of Steve McManaman and Nicolas Anelka from the English Premier League, alongside local talents MÃ­chel Salgado and IvÃ¡n Helguera. In Del Bosque's first season in charge Real won the Champions League for the eighth time, following a 3â0 victory over Valencia in the final, with goals from Morientes, McManaman and RaÃºl. This victory marked the beginning of a successful period in Real Madrid's history.

In July 2000, Florentino PÃ©rez was elected club president. He vowed in his campaign to erase the club's â¬270Â million debt and modernize the club's facilities. However, the primary electoral promise that propelled PÃ©rez to victory was the signing of LuÃ­s Figo from arch-rivals Barcelona. The following year, the club had its training ground rezoned and used the money to begin assembling the "GalÃ¡cticos" team by signing a global star every summer, which included Zinedine Zidane, Ronaldo, LuÃ­s Figo, David Beckham and Fabio Cannavaro. It is debatable whether the gamble paid off, as despite winning the UEFA Champions League and an Intercontinental Cup in 2002, followed by La Liga in 2003, the club failed to win a major trophy for the next three seasons.

The few days after the capturing of the 2003 Liga title were surrounded with controversy. The first controversial decision came when PÃ©rez sacked winning coach Vicente del Bosque. Over a dozen players left the club, including Madrid captain Fernando Hierro, while defensive midfielder Claude Makelele refused to take part in training in protest at being one of the lowest-paid players at the club and subsequently moved to Chelsea. "That's a lot [of players leaving] when the normal rule is: never change a winning team," stated Zidane. Real Madrid, with newly appointed coach Carlos Queiroz, started their domestic league slowly after a hard win over Real Betis.

The 2005â06 season began with the promise of several new signings: JÃºlio Baptista (â¬24Â million), Robinho (â¬30Â million) and Sergio Ramos (â¬27Â million). However, Real Madrid suffered from some poor results, including a 0â3 loss at the hands of Barcelona at the Santiago BernabÃ©u in November 2005. Madrid's coach Wanderley Luxemburgo was sacked the following month and his replacement was Juan RamÃ³n LÃ³pez Caro. A brief return to form came to an abrupt halt after losing the first leg of the Copa del Rey quarterfinal, 6â1 to Real Zaragoza. Shortly after, Real Madrid were eliminated from the Champions League for a fourth successive year, this time at the hands of Arsenal. On 27 February 2006, Florentino PÃ©rez resigned.

RamÃ³n CalderÃ³n was elected as club president on 2 July 2006 and subsequently appointed Fabio Capello as the new coach and Predrag MijatoviÄ as the new sporting director. Real Madrid won the Liga title in 2007 for the first time in four years, but Capello was nonetheless sacked at the end of the campaign. The title was won on 17 June, where Real faced Mallorca at the BernabÃ©u while Barcelona and Sevilla, the other title challengers, faced GimnÃ stic de Tarragona and Villarreal, respectively. At half-time, Real were 0â1 down, while Barcelona had surged ahead into a 0â3 lead in Tarragona. However, three goals in the last half-hour secured Madrid a 3â1 win and their first league title since 2003.

On 1 June 2009, Florentino PÃ©rez regained Real Madrid's presidency. PÃ©rez continued with the "GalÃ¡cticos" policy pursued in his first term, buying KakÃ¡ from Milan for a record-breaking (in pounds sterling) sum of Â£56Â million, and then breaking the record again by purchasing Cristiano Ronaldo from Manchester United for Â£80Â million.

JosÃ© Mourinho took over as manager in May 2010. In April 2011, a rare occurrence happened when, for the first time ever, four "ClÃ¡sicos" were to be played in a span of just 18 days. The first fixture was for the Liga campaign on 17 April (which ended 1â1 with penalty goals for both sides), the Copa del Rey final (which ended 1â0 to Madrid) and the controversial two-legged Champions League semifinal on 27 April and 2 May (3â1 loss on aggregate) to Barcelona.

In the 2011â12 La Liga season, Real Madrid won La Liga for a record 32nd time in the league's history, also finishing the season with numerous club-level records set, including 100 points reached in a single season, a total of 121 goals scored, a goal difference of +89 and 16 away wins, with 32 wins overall. In the same season, Cristiano Ronaldo become the fastest player to reach 100 goals scored in Spanish league history. In reaching 101 goals in 92 games, Ronaldo surpassed Real Madrid legend Ferenc PuskÃ¡s, who scored 100 goals in 105 matches. Ronaldo set a new club mark for individual goals scored in one year (60), and became the first player ever to score against all 19 opposition teams in a single season.

Real Madrid began the 2012â13 season winning the Supercopa de EspaÃ±a, defeating Barcelona on away goals, but finished second in the league competition. A major transfer of the season was signing of Luka ModriÄ from Tottenham Hotspur for a fee in the region of Â£33Â million. After a disappointing extra time loss to AtlÃ©tico Madrid in the 2013 Copa del Rey Final, PÃ©rez announced the departure of JosÃ© Mourinho at the end of the season by "mutual agreement".

On 25 June 2013, Carlo Ancelotti succeeded Mourinho to become the manager of Real Madrid on a three-year deal, with Zinedine Zidane named as one of his assistants. On 1 September 2013, the long-awaited transfer of Gareth Bale from Tottenham Hotspur was announced. The transfer of the Welshman was reportedly a new world record signing, with the transfer price approximated at â¬100Â million. In Ancelotti's first season at the club, Real Madrid won the Copa del Rey, with Bale scoring the winner in the final against Barcelona. On 24 May, Real Madrid defeated city rivals AtlÃ©tico Madrid in the 2014 Champions League Final, winning their first European title since 2002, and becoming the first team to win ten European Cups/Champions League titles, an achievement known as ""La DÃ©cima"".

After winning the 2014 Champions League, Real Madrid signed goalkeeper Keylor Navas, midfielder Toni Kroos and attacking midfielder James RodrÃ­guez. The club won the 2014 UEFA Super Cup against Sevilla, the club's 79th official trophy. During the last week of the 2014 summer transfer window, Real Madrid sold two players key to the previous season's successes: Xabi Alonso to Bayern Munich and Ãngel Di MarÃ­a to Manchester United. This decision by the club was surrounded by controversy, with Cristiano Ronaldo stating, "If I was in charge, maybe I would have done things differently," while Carlo Ancelotti admitted, "We must start again from zero."

After a slow start to the 2014â15 La Liga season, Real Madrid went on a record-breaking 22-match winning streak, which included wins against Barcelona and Liverpool, surpassing the previous Spanish record of 18 successive wins set by Frank Rijkaard's "BarÃ§a" in the 2005â06 season. The streak came to an end in their opening match of 2015 with a loss to Valencia, leaving the club two short of equalling the world record of 24 consecutive wins. The club failed to retain the Champions League (losing to Juventus in the semi-finals) and the Copa del Rey, and also failed to land the league title (finishing two points and a place behind champions Barcelona), shortcomings that all preceded Ancelotti's dismissal on 25 May 2015.

On 3 June 2015, Rafael BenÃ­tez was confirmed as the new Real Madrid manager, signing a three-year contract. Real Madrid remained unbeaten in the league until a 3â2 loss at Sevilla on the matchday 11. This was followed by a 0â4 home loss in the first "ClÃ¡sico" of the season against Barcelona. In the Copa del Rey Round of 32, Real fielded an ineligible player in Denis Cheryshev in a 1â3 first leg win away against CÃ¡diz, resulting in the second leg being cancelled and Real being disqualified. BenÃ­tez was relieved of his duties on 4 January 2016 following allegations of unpopularity with supporters, displeasure with players and a failure to get good results against top teams.

On 4 January 2016, BenÃ­tez's departure was announced along with the promotion of Zinedine Zidane to his first head coaching role. Under Zidane, Real ended up finishing in second place, just one point behind champions Barcelona, in the 2015â16 La Liga. On 28 May, Real Madrid won their 11th Champions League title, extending their record for most successes in the competition, with the achievement being termed ""La UndÃ©cima"".

Real Madrid began their 2016â17 campaign, which was to be Zidane's first full season in charge of the club, with victory in the 2016 UEFA Super Cup. On 10 December 2016, Madrid won their 35th-straight match without a loss, which set a new club record. On 18 December 2016, Madrid defeated Japanese club Kashima Antlers 4â2 in the final of the 2016 FIFA Club World Cup. With a 3â3 draw at Sevilla on 12 January 2017, Madrid's unbeaten run extended to 40 matches, breaking Barcelona's Spanish record of 39 matches unbeaten in all competitions from the previous season. Their unbeaten streak ended after a 1â2 away loss against Sevilla in La Liga three days later. In May that year, Madrid won the 2016â17 La Liga for a record 33rd time, their first title in five years. On 3 June, the club's Champions League Final win against Juventus resulted in Real Madrid being the first team to successfully defend their title in the UEFA Champions League era, and the first to win consecutive titles in the competition since Milan in 1989 and 1990, when the tournament was known as the European Cup. Real Madrid's title was its 12th, extending its record, and its third in four years. The achievement is also known as ""La DuodÃ©cima"". The 2016â17 season was the greatest campaign in terms of trophies won in the history of Real Madrid.

Real Madrid won the 2017 UEFA Super Cup 2â1 against Manchester United. Five days later, Real Madrid beat Barcelona at the Camp Nou in the first leg of the 2017 Supercopa de EspaÃ±a, before winning the second leg 2â0, ending a 24 consecutive match scoring record of Barcelona in "El ClÃ¡sico" matches, and with a 5â1 aggregate score. On 16 December 2017, Real beat Brazilian club GrÃªmio 1â0 in the Final of 2017 FIFA Club World Cup and became the first club to retain the trophy. Real Madrid also won their third successive UEFA Champions League in 2018, becoming the first club to win three straight UEFA Champions League titles since the tournament's inception, as well as the first team to win three straight titles in European competition since Bayern Munich in the 1970s. On 31 May, only five days after winning the final, Zidane announced his resignation as Real Madrid manager, citing the club's "need for change" as his rationale for departing.

On 12 June, Real Madrid named Julen Lopetegui, the head coach of the Spanish national team, as their new manager. It was announced that he would officially become manager after the 2018 FIFA World Cup. However, the Spanish national team sacked Lopetegui a day prior to the tournament, stating that he negotiated terms with the club without informing them. The club then began aggressively re-shaping the squad in the summer of 2018, which included the sale of Cristiano Ronaldo to Juventus for a reported â¬100 million. After a string of poor performances and losses from the team, Lopetegui was dismissed and replaced by then Castilla coach, Santiago Solari. On 22 December 2018, Real Madrid beat Al Ain by a 4â1 margin in the final of 2018 FIFA Club World Cup. With their win, Real Madrid became the outright record winners of the Club World Cup with four titles. They are considered to have been the world champions for grand total of seven time because FIFA recognises the Intercontinental Cup as the predecessor of FIFA Club World Cup. They also extended the record for most consecutive titles with their third in a row. On 11 March 2019, Real Madrid reinstated Zidane as the head coach of the club.

The first crest had a simple design consisting of a decorative interlacing of the three initials of the club, "MCF" for Madrid Club de FÃºtbol, in dark blue on a white shirt.
The first change in the crest occurred in 1908 when the letters adopted a more streamlined form and appeared inside a circle. The next change in the configuration of the crest did not occur until the presidency of Pedro Parages in 1920. At that time, King Alfonso XIII granted the club his royal patronage which came in the form of the title "Real Madrid," meaning "Royal." Thus, Alfonso's crown was added to the crest and the club styled itself "Real Madrid Club de FÃºtbol".

With the dissolution of the monarchy in 1931, all the royal symbols (the crown on the crest and the title of Real) were eliminated. The crown was replaced by the dark mulberry band of the Region of Castile. In 1941, two years after the end of the Civil War, the crest's "Real Corona", or "Royal Crown", was restored while the mulberry stripe of Castile was retained as well. In addition, the whole crest was made full color, with gold being the most prominent, and the club was again called Real Madrid Club de FÃºtbol. The most recent modification to the crest occurred in 2001 when the club wanted to better situate itself for the 21st century and further standardize its crest. One of the modifications made was changing the mulberry stripe to a more bluish shade.

Real Madrid has maintained the white shirt for its home kit throughout the history of the club. There was, however, one season that the shirt and shorts were not both white. It was an initiative undertaken by Escobal and Quesada in 1925; the two were traveling through England when they noticed the kit worn by London-based team Corinthian F.C., one of the most famous teams at the time known for its elegance and sportsmanship. It was decided that Real Madrid would wear black shorts in an attempt to replicate the English team, but the initiative lasted just one year. After being eliminated from the cup by Barcelona with a 1â5 defeat in Madrid and a 2â0 defeat in Catalonia, President Parages decided to return to an all-white kit, claiming that the other kit brought bad luck. By the early 1940s, the manager changed the kit again by adding buttons to the shirt and the club's crest on the left breast, which has remained ever since. On 23 November 1947, in a game against AtlÃ©tico Madrid at the Metropolitano Stadium, Real Madrid became the first Spanish team to wear numbered shirts. English club Leeds United permanently switched their blue shirt for a white one in the 1960s, to emulate the dominant Real Madrid of the era.

Real's traditional away colours are all blue or all purple. Since the advent of the replica kit market, the club has also released various other one colour designs, including red, green, orange and black. The club's kit is manufactured by Adidas, whose contract extends from 1998. Real Madrid's first shirt sponsor, Zanussi, agreed for the 1982â83, 1983â84 and 1984â85 seasons. Following that, the club was sponsored by Parmalat and Otaysa before a long-term deal was signed with Teka in 1992. In 2001, Real Madrid ended their contract with Teka and for one season and used the Realmadrid.com logo to promote the club's website. Then, in 2002, a deal was signed with Siemens Mobile and in 2006, the BenQ Siemens logo appeared on the club's shirt. Real Madrid's shirt sponsor from 2007 until 2013 was bwin.com following the economic problems of BenQ Siemens. Fly Emirates became their shirt sponsor in 2013, and in 2017 the club renewed their sponsorship with the airliner, signing a deal until 2022 worth â¬70 million per year. In 2015, Madrid signed a new 10-year contract with Adidas believed to be worth a total of Â£850 million (â¬1 billion), earning Â£59 million (â¬64 million) per season including contract penalty or contract termination clause anytime if Real Madrid failed to qualify for any European competitions even relegation.

"Note: Early contract termination clauses are activated at any time depending on team's on-pitch performance."

After moving between grounds, the team moved to the Campo de O'Donnell in 1912, which remained its home ground for 11 years. After this period, the club moved for one year to the Campo de Ciudad Lineal, a small ground with a capacity of 8,000Â spectators. After that, Real Madrid moved its home matches to Estadio ChamartÃ­n, which was inaugurated on 17 May 1923 with a match against Newcastle United. In this stadium, which hosted 22,500Â spectators, Real Madrid celebrated its first Spanish league title. After some successes, the 1943 elected president Santiago BernabÃ©u decided that the Estadio ChamartÃ­n was not big enough for the ambitions of the club, and thus a new stadium was built and was inaugurated on 14 December 1947. This was the Santiago BernabÃ©u Stadium as it is known today, although it did not acquire the present name until 1955. The first match at the BernabÃ©u was played between Real Madrid and the Portuguese club Belenenses and won by "Los Blancos", 3â1, the first goal being scored by Sabino Barinaga.

The capacity has changed frequently, peaking at 120,000 after a 1953 expansion. Since then, there have been a number of reductions due to modernizations (the last standing areas were removed in 1998â99 in response to UEFA regulations which forbids standing at matches in UEFA competitions), countered to some extent by expansions. The latest capacity is 81,044 spectators. A plan to add a retractable roof has been announced. Real Madrid has the fourth-highest of the average attendances of European football clubs, behind only Borussia Dortmund, Barcelona and Manchester United.

The BernabÃ©u has hosted the 1964 UEFA European Championship final, the 1982 FIFA World Cup final, the 1957, 1969 and 1980 European Cup finals and the 2010 UEFA Champions League Final. The stadium has its own Madrid Metro station along the 10 line called "Santiago BernabÃ©u". On 14 November 2007, the BernabÃ©u was upgraded to Elite Football Stadium status by UEFA.

On 9 May 2006, the Alfredo Di StÃ©fano Stadium was inaugurated in the Real Madrid City, where Real Madrid usually trains. The inaugural match was played between Real Madrid and Stade de Reims, a rematch of the 1956 European Cup final. Real Madrid won the match 6â1, with goals from Sergio Ramos, Antonio Cassano (2), Roberto Soldado (2) and JosÃ© Manuel Jurado. The venue is now part of the Ciudad Real Madrid, the club's training facility located outside Madrid, in Valdebebas. The stadium holds 5,000 people and is Real Madrid Castilla's home ground. It is named after former Real legend Alfredo Di StÃ©fano.

RaÃºl holds the record for most Real Madrid appearances, having played 741Â first-team matches from 1994 to 2010. Iker Casillas comes second with 725Â appearances, followed by Manuel Sanchis, Jr., having played 710Â times. The record for a goalkeeper is held by Iker Casillas, with 725Â appearances. WithÂ 167 caps (162 while at the club), he is also Real's most capped international player.
Cristiano Ronaldo is Real Madrid's all-time top goalscorer, with 450 goals. Six other players have also scored over 200 goals for Real: Alfredo Di StÃ©fano (1953â64), Santillana (1971â88), Ferenc PuskÃ¡s (1958â66), Hugo SÃ¡nchez (1985â92), Karim Benzema (2009âcurrent) and the previous goalscoring record-holder RaÃºl (1994â2010). Cristiano Ronaldo also holds the record for the most league goals scored in one season (48 in 2014â15), alongside being Real's top goalscorer of all time in La Liga history with 311 goals. Di StÃ©fano's 49Â goals in 58Â matches was for decades the all-time highest tally in the European Cup, until it was surpassed by RaÃºl in 2005, and is now held by Cristiano Ronaldo with 105 goals. The fastest goal in the history of the club (12 seconds) was scored by the Brazilian Ronaldo on 3 December 2003 during a league match against AtlÃ©tico Madrid.

Officially, the highest home attendance figure for a Real Madrid match is 83,329, which was for a Copa del Rey match in 2006. The current official capacity of the Santiago BernabÃ©u is 81,044. The club's average attendance in the 2007â08 season was 76,234, the highest in European Leagues. Real has also set records in Spanish football, most notably the most domestic titles (33 as of 2016â17) and the most seasons won in a row (five, during 1960â65 and 1985â90). With 121 matches (from 17 February 1957 to 7 March 1965), the club holds the record for longest unbeaten run at home in La Liga.

The club also holds the record for winning the European Cup/UEFA Champions League thirteen times and for the most semi-final appearances (28). As of April 2019, Cristiano Ronaldo is the all-time top scorer in the UEFA Champions League, with 126 (127 including qualifiers)Â goals in total, 105 while playing for Real Madrid. The team has the record number of consecutive participations in the European Cup (before it became the Champions League) with 15, from 1955â56 to 1969â70. Among the club's on-field records is a 22-game winning streak in all competitions during the 2014â15 season, a Spanish record and fourth worldwide. The same season the team tied the win-streak for games in the Champions League, with ten. In September 2017, the club equalled the record of the Brazilian club Santos, starring PelÃ©, by scoring in their 73rd consecutive game.

In June 2009, the club broke its own record for the highest transfer fee ever paid in the history of football by agreeing to pay Manchester United â¬94 million (Â£80 million) for the services of Cristiano Ronaldo. The fee of â¬77.5 million (100 billion lire) for Zinedine Zidane's transfer from Juventus to Real Madrid in 2001 was the previous highest transfer fee ever paid. This record (in pounds sterling) had been broken previously in June 2009, for a few days, when Real Madrid agreed to buy KakÃ¡ from Milan for â¬67 million (Â£65 million). The transfer of Tottenham Hotspur's Gareth Bale in 2013 was reportedly a new world record signing, with the transfer fee reported at around â¬100 million. In January 2016, documents pertaining to Bale's transfer were leaked which confirmed a world record transfer fee of â¬100,759,418. Real Madrid equalled their record signing in 2019, when the club reportedly signed Eden Hazard from Chelsea for â¬100 million. The club's sale record came on 10 July 2018, when Juventus signed Cristiano Ronaldo for â¬100 million.

During most home matches the majority of the seats in the stadium are occupied by season-ticket holders, of which the figure is capped at 65,000. To become a season ticket holder one must first be a "socio", or club member. In addition to members, the club has more than 1,800Â "peÃ±as" (official, club-affiliated supporters' groups) in Spain and around the world. Real Madrid has the second highest average all-time attendance in Spanish football and regularly attracts over 74,000 fans to the BernabÃ©u. One of the best supported teams globally, Real Madrid was the first sports team (and first brand) to reach 100 million fans on Facebook in April 2017.

Real Madrid's hardcore supporters are the so-called "Ultras Sur" supporters, or simply Ultras. They are known for their extreme right-wing politics, akin to Barcelona's hardcore supporters group Boixos Nois. The Ultras Surs have developed an alliance with other right wing groups, most notably Lazio "Irriducibili" fans, and have also developed an alliance with left-wing groups. On several occasions, they have racially abused opposing players and have been investigated by UEFA for doing so. Florentino PÃ©rez took it upon himself to ban the Ultras from the BernabÃ©u and assign their seats to the general public. This decision was controversial with some of the BernabÃ©u faithful, however, as the lively atmosphere of games would suffer as a result. The Ultras have since held protests outside the BernabÃ©u and have demanded to be reinstated and allowed to enter the grounds.

Questioned over Pope Francis' adherence to 2014 FIFA Club World Cup Final opponents San Lorenzo, Madrid captain Sergio Ramos stated, âIn the semi-finals we noticed the love from supporters in Marrakesh and it seemed like we were playing at home. That sums up the greatness of this team. Madrid is God's team and the team of the world.â

There is often a fierce rivalry between the two strongest teams in a national league, and this is particularly the case in La Liga, where the game between Real Madrid and Barcelona is known as "The Classic" ("El ClÃ¡sico"). From the start of national competitions, the clubs were seen as representatives of two rival regions in Spain, Catalonia and Castile, as well as of the two cities. The rivalry reflects what many regard as the political and cultural tensions felt between Catalans and the Castilians, seen by one author as a re-enactment of the Spanish Civil War.
Over the years, the record from Real Madrid and Barcelona is 95 victories for Madrid, 96 victories for Barcelona, and 51 draws.

During the dictatorships of Primo de Rivera and especially of Francisco Franco (1939â1975), all regional cultures were suppressed. All of the languages spoken in Spanish territory, except Spanish (Castilian) itself, were officially banned. Symbolising the Catalan people's desire for freedom, Barcelona became "More than a club" (""MÃ©s que un club"") for the Catalans. According to Manuel VÃ¡zquez MontalbÃ¡n, the best way for the Catalans to demonstrate their identity was by joining Barcelona. It was less risky than joining a clandestine anti-Franco movement, and allowed them to express their dissidence.

On the other hand, Real Madrid was widely seen as the embodiment of the sovereign oppressive centralism and the fascist regime at management level and beyondâ Santiago BernabÃ©u, the former club president for whom Real Madrid's stadium is named, fought on the Nationalist side during the Spanish Civil War. During the war, however, members of both clubs, such as Josep Sunyol and Rafael SÃ¡nchez Guerra, suffered at the hands of Francoists.

During the 1950s, the rivalry was exacerbated further when there was a controversy surrounding the transfer of Alfredo Di StÃ©fano, who eventually played for Real Madrid and was key to their subsequent success. The 1960s saw the rivalry reach the European stage when they met twice in a controversial knock-out round of the European Cup, with Madrid receiving unfavourable treatment from the referee. In 2002, the European encounter between the clubs was dubbed the "Match of The Century" by Spanish media, and Madrid's win was watched by more than 500 million people. The fixture has featured memorable goal celebrations from both teams, often involving mocking the opposition. A notable example of this occurred in October 1999 when RaÃºl silenced the hostile crowd of 100,000 Barcelona fans when he scored and then celebrated his goal by putting a finger to his lips as though to tell them to be quiet.

The club's nearest neighbour is AtlÃ©tico Madrid, a rivalry being shared between fans of both football teams. Although AtlÃ©tico was founded by three Basque students in 1903, it was joined in 1904 by dissident members of "Madrid FC". Tensions escalated further after AtlÃ©tico were merged with the football team of the Spanish airforce (and thus renamed AtlÃ©tico AviaciÃ³n), and in the 1940s, AtlÃ©tico was perceived as the preferred team of Franco's regime before he revelled in Real's European success in the 1950s. Furthermore, Real supporters initially came from the middle and upper classes while the AtlÃ©tico supporters were drawn from the working class. Today, however, these distinctions are largely blurred. They met for the first time on 21 February 1929 in matchday three of the first League Championship at the former ChamartÃ­n. It was the first official derby of the new tournament, and Real won 2â1.

The rivalry first gained international attention in 1959 during the European Cup when the two clubs met in the semi-final. Real won the first leg 2â1 at the BernabÃ©u while AtlÃ©tico won 1â0 at the "Metropolitano". The tie went to a replay, which Real won 2â1. AtlÃ©tico, however, gained some revenge when, led by former Real Madrid coach JosÃ© Villalonga, it defeated its city rivals in two successive "Copa del GeneralÃ­simo" finals in 1960 and 1961.

Between 1961 and 1989, when Real dominated La Liga, only AtlÃ©tico offered it any serious challenge, winning Liga titles in 1966, 1970, 1973 and 1977. In 1965, AtlÃ©tico became the first team to beat Real at the BernabÃ©u in eight years. Real Madrid's record against AtlÃ©tico in more recent times is very favorable. A high point coming in the 2002â03 season, when Real clinched the La Liga title after a 0â4 victory at AtlÃ©tico at the Vicente CalderÃ³n Stadium. AtlÃ©tico's first win over its city rivals since 1999 came with the Copa del Rey win in May 2013. In 2013â14, Real and AtlÃ©tico were finalists of UEFA Champions League, the first final which hosted two clubs from same city. Real Madrid triumphed with 4â1 in extra time. On 7 February 2015, Real suffered their first defeat in 14 years at the Vicente CalderÃ³n, a 4â0 loss. On 28 May 2016, Real and AtlÃ©tico met again for the Champions League title in Milan, which resulted in a win for Real after a penalty shootout.

A further minor rivalry exists between Real Madrid and Athletic Bilbao. This is known as El Viejo ClÃ¡sico ("the old classic"), so named as the two clubs were dominant in the first half of the 20th century, meeting in nine Copa del Rey finals including the first in 1903. Until 10 December 2011, this fixture was the most played in the history of Spanish football, when it was surpassed by "El ClÃ¡sico".

Athletic Bilbao, who operate a policy of only using local players, have long since ceased to be a competitive rival to clubs such as Real Madrid who scour the globe for the best talent; the "Lions" have collected no major trophies since 1984 and won only two of the 26 matches between the teams from 2005â06 to 2016â17. However, the matches remain keenly fought due to their historical and cultural significance, with some parallels to the political aspect of the Barcelona/Catalonia rivalry as Athletic are the largest club in the Basque region.

Real Madrid and Germany's Bayern Munich are two of the most successful clubs in the UEFA Champions League/European Cup competition, Real winning thirteen times and Bayern winning five times. Although they have never met in a final, Real Madrid versus Bayern is the match that has historically been played most often in the Champions League/European Cup with 26 matches (12 wins for Madrid, 11 wins for Bayern, with 3 draws), Real Madrid supporters often refer to Bayern as the ""Bestia negra"" ("Black Beast").

During the 2010s, the two teams met in the 2011â12 Champions League semi-finals, which ended 3â3 on aggregate (Bayern won 3â1 on penalties after extra time, but lost the final at their own stadium), and then at the same stage in the 2013â14 edition with Real Madrid winning 5â0 on aggregate on their way to winning the competition. They were also drawn together in the 2016â17 quarter-finals; Real Madrid won 6â3 on aggregate and subsequently lifted the trophy. The following year, they met in the semi-finals, with Real Madrid again progressing 4â3.

Until the 2018â19 season when they were twice defeated in Madrid by three-goal margins, Real's biggest loss at home in the Champions League had been at the hands of Bayern on 29 February 2000, 2â4.

Arjen Robben, Xabi Alonso, Toni Kroos and James RodrÃ­guez are among the players to appear for both clubs in the early 21st century.

Another match that is often played in the European Cup/Champions League is Real Madrid vs Juventus, the most decorated Italian club. They have played each other in 21 matches and have an almost perfectly balanced record (9 wins for Juventus, 10 wins for Real Madrid, 2 draws), as well as nearly the same goal difference (Madrid ahead 26 to 25).
Their first meeting was in the 1961â62 European Cup, which Real Madrid won 3â1 in a replay held in Paris. At the quarter-final stage in 1995â96, Juventus prevailed 2â1 and went on to lift the trophy. In the 1998 UEFA Champions League Final between the teams in Amsterdam, Real Madrid won 1â0. They met again in the 2002â03 UEFA Champions League semi-finals, when both clubs were in their respective 'golden eras'; Juventus won 4â3 on aggregate. By that time, star midfielder Zinedine Zidane, who played for the "Bianconeri" in the 1998 final, had moved from Turin to Madrid in a world record â¬77 million deal.

In the 2014â15 UEFA Champions League semi-finals, former Real Madrid player Ãlvaro Morata scored one goal in each leg to take Juventus to the final, winning 3â2 on aggregate. They faced off again in the 2017 UEFA Champions League Final in Cardiff, which Real Madrid won 4â1. Portuguese player Cristiano Ronaldo scored two goals in the match, and was named man of the match.

The latest Champions League meeting was in the 2017â18 quarter-finals, which Real Madrid won 4â3 on aggregate; the tie ended in dramatic and controversial fashion, with a debatable penalty awarded to Real Madrid in the last minute of the second leg after Juventus built a 3â0 lead at the Bernabeu to pull level in the tie following a defeat at their Juventus Stadium by the same scoreline. Cristiano Ronaldo scored three goals over the two matches including the decisive penalty and a spectacular overhead kick, and having won the Champions League with Madrid for a fourth time, he transferred to Juventus a few months later for a â¬100 million transfer fee. In previous decades, Luis Del Sol, Michael Laudrup, Robert Jarni, Fabio Cannavaro and Emerson also featured for both clubs.

It was under Florentino PÃ©rez's first presidency (2000â2006) that Real Madrid started its ambition of becoming the world's richest professional football club. The club ceded part of its training grounds to the city of Madrid in 2001, and sold the rest to four corporations: Repsol YPF, Mutua AutomovilÃ­stica de Madrid, Sacyr Vallehermoso and OHL. The sale eradicated the club's debts, paving the way for it to buy the world's most expensive players, such as Zinedine Zidane, LuÃ­s Figo, Ronaldo and David Beckham. The city had previously rezoned the training grounds for development, a move which in turn increased their value, and then bought the site. The European Commission started an investigation into whether the city overpaid for the property, to be considered a form of state subsidy.

The sale of the training ground for office buildings cleared Real Madrid's debts of â¬270 million and enabled the club to embark upon an unprecedented spending spree which brought big-name players to the club. In addition, profit from the sale was spent on a state-of-the-art training complex on the city's outskirts. Although PÃ©rez's policy resulted in increased financial success from the exploitation of the club's high marketing potential around the world, especially in Asia, it came under increasing criticism for being too focused on marketing the Real Madrid brand and not enough on the performances of the team.

By September 2007, Real Madrid was considered the most valuable football brand in Europe by BBDO. In 2008, it was ranked the second-most valuable club in football, with a value of â¬951 million (Â£640Â million / $1.285Â billion), only beaten by Manchester United, which was valued at â¬1.333 billion (Â£900 million). In 2010, Real Madrid had the highest turnover in football worldwide. In September 2009, Real Madrid's management announced plans to open its own dedicated theme park by 2013.

A study at Harvard University concluded that Real Madrid "is one of the 20 most important brand names and the only one in which its executives, the players, are well-known. We have some spectacular figures in regard to worldwide support of the club. There are an estimated 287 million people worldwide who follow Real Madrid." In 2010, "Forbes" evaluated Real Madrid's worth to be around â¬992 million (US$1.323 billion), ranking them second after Manchester United, based on figures from the 2008â09 season. According to Deloitte, Real Madrid had a recorded revenue of â¬401 million in the same period, ranking first.

Along with Barcelona, Athletic Bilbao and Osasuna, Real Madrid is organised as a registered association. This means that Real Madrid is owned by its supporters who elect the club president. The club president cannot invest his own money into the club and the club can only spend what it earns, which is mainly derived through merchandise sales, television rights and ticket sales. Unlike a limited company, it is not possible to purchase shares in the club, but only membership. The members of Real Madrid, called "socios", form an assembly of delegates which is the highest governing body of the club. As of 2010, the club has 60,000 "socios". At the end of the 2009â10 season, the club's board of directors stated that Real Madrid had a net debt of â¬244.6 million, â¬82.1 million lower than the previous fiscal year. Real Madrid announced that it had a net debt of â¬170 million after the 2010â11 season. From 2007 to 2011, the club made a net profit of â¬190 million.

During the 2009â10 season, Real Madrid made â¬150 million through ticket sales, which was the highest in top-flight football. The club has the highest number of shirt sales a season, around 1.5 million. For the 2010â11 season its wage bill totalled â¬169 million, which was second-highest in Europe behind Barcelona. However, its wage bill to turnover ratio was the best in Europe at 43 percent, ahead of Manchester United and Arsenal at 46 percent and 50 percent, respectively. In 2013, "Forbes" listed the club as the world's most valuable sports team, worth $3.3Â billion. It was valued at â¬3.47Â billion ($4.1Â billion) in 2018, and in the 2016â17 season it was the second highest-earning football club in the world, with an annual revenue of â¬674.6Â million. The second highest paid sports team in the world â after Barcelona â in November 2018 the average first-team pay at Real Madrid was Â£8.1m ($10.6m) per year.

Real Madrid was the featured club in the second installment of the "Goal!" football movie trilogy, "" (2007). The film follows former Newcastle United star Santiago MuÃ±ez as he is first scouted, and then signed by Real Madrid for the 2005â06 season. The film's creators wanted to put emphasis on the changes in MuÃ±ez's life after his move to Madrid. Production was done with the full support of UEFA, allowing the film crew to use many real life players in cameo roles. Real Madrid squad members featured in the film included Iker Casillas, Zinedine Zidane, David Beckham, Ronaldo, Roberto Carlos, RaÃºl, Sergio Ramos, Robinho, Michael Owen, MÃ­chel Salgado, JÃºlio Baptista, Steve McManaman and IvÃ¡n Helguera. Non-Real Madrid players to make cameo appearances included Ronaldinho, Thierry Henry, Lionel Messi, Samuel Eto'o, AndrÃ©s Iniesta, Pablo Aimar, Freddie Ljungberg, Cesc FÃ bregas and Santiago CaÃ±izares. In the film, both Florentino PÃ©rez and Alfredo Di StÃ©fano presented the fictional player MuÃ±ez to the club after his signing.

"Real, The Movie" is a 2005 part feature, part documentary film that showcases the worldwide passion for Real Madrid. Produced by the club and directed by Borja Manso, it follows five sub-stories of fans from around the world and their love for the club. Along with the fictional portion of the film, it also contains real footage of the squad, during training at Ciudad Real Madrid, matches, and interviews. Although the film mentions all of the squad, it mainly focuses on "galÃ¡cticos" such as David Beckham, Zinedine Zidane, RaÃºl, LuÃ­s Figo, Ronaldo, Iker Casillas, and Roberto Carlos, among others. The film was originally produced in Spanish, but has been dubbed for their worldwide fanbase.

The book "White Storm: 100 years of Real Madrid" by Phil Ball was the first English-language history of Real Madrid. Published in 2002, it talks about the most successful moments of the club during its first centenary, having been translated into various languages. In late 2011, Real Madrid released a digital music album, entitled "Legends", and a remix of the club's anthem, "Himno del Real Madrid," was released as the first single from the album.

Real Madrid TV is an encrypted digital television channel, operated by Real Madrid and specialising in the club. The channel is available in Spanish and English. It is located at Ciudad Real Madrid in Valdebebas (Madrid), Real Madrid's training centre.

"Hala Madrid" is a magazine published quarterly for the Real Madrid club members and the "Madridistas Fan Club" card holders. The phrase Hala Madrid, meaning "Forward Madrid" or "Go Madrid", is also the title of the club's official anthem, which is often sung by the Madridistas (the club's fans). The magazine includes reports on the club's matches in the previous month, as well as information about the reserve and youth teams. Features often include interviews with players, both past and present, and the club's historic matches.

Real Madrid has appeared in many football-based video games, namely in the FIFA and Pro Evolution Soccer series. A Real Madrid player has appeared on the cover of both titles a combined seven times.

In 2007, Spanish game publisher Virgin Play signed a deal with the club to make officially licensed Real Madrid video games. The only one released under the deal (due to Virgin Play's liquidation in September 2009) would end up being "Real Madrid: The Game", which was developed by Atomic Planet Entertainment and was published under Virgin Play's publishing division V.2 Play in May 2009 for the PlayStation 2, PlayStation Portable, Windows, Wii and Nintendo DS exclusively in European territories Virgin Play released their products in. The game featured a career mode with a mixture of role-playing and simulation as well as arcade-styled Football gameplay.


Spanish teams are limited to three players without EU citizenship. The squad list includes only the principal nationality of each player; several non-European players on the squad have dual citizenship with an EU country. Also, players from the ACP countries in Africa, the Caribbean, and the Pacific that are signatories to the Cotonou Agreement are not counted against non-EU quotas due to the Kolpak ruling.




</doc>
<doc id="26414" url="https://en.wikipedia.org/wiki?curid=26414" title="Resurrection of Jesus">
Resurrection of Jesus

The resurrection of Jesus, or anastasis is the Christian belief that God raised Jesus after his crucifixion as first of the dead, starting his exalted life as Christ and Lord. In Christian theology, the death and resurrection of Jesus are the most important events, a foundation of the Christian faith, and commemorated by Easter. His resurrection is the guarantee that all the Christian dead will be resurrected at Christ's second coming. For the Christian tradition, the bodily resurrection was the restoration to life of a transformed body powered by spirit, as described by Paul and the Gospels, that led to the establishment of Christianity.

In secular and liberal Christian scholarship, the appearances of Jesus are explained as visionary experiences that gave the impetus to the belief in the exaltation of Jesus and a resumption of the missionary activity of Jesus' followers.

The idea of any resurrection at all first emerges clearly in the 2nd-century BC Book of Daniel, but as a belief in the resurrection of the soul alone. Josephus tells of the three main Jewish sects of the 1st century AD, that the Sadducees held that both soul and body perished at death; the Essenes that the soul was immortal but the flesh was not; and the Pharisees that the soul was immortal and that the body would be resurrected to house it. Of these three positions, Jesus and the early Christians appear to have been closest to that of the Pharisees. Steve Mason notes that for the Pharisees, "the new body is a special, holy body," which is different from the old body, "a view shared to some extent by the ex-Pharisee Paul (1. Cor. 15:35ff)."

EndsjÃ¸ notes that the evidence from Jewish texts and from tomb inscriptions points to a more complex reality. For example, when the 2nd century BC author the Book of Daniel wrote that "many of those sleeping in the dust shall awaken" (), he probably had in mind rebirth as stars in God's Heaven, stars having been identified with angels from early times â such a rebirth would rule out a bodily resurrection, as angels were believed to be fleshless. Other texts range from the traditional Old Testament view that the soul would spend eternity in the underworld, to a metaphorical belief in the raising of the spirit. Most avoided defining what resurrection might imply, but a resurrection of the flesh was a marginal belief.

The Greeks held that a meritorious man could be resurrected as a god (the process of "apotheosis"), and the successors of Alexander the Great made this idea very well known throughout the Middle East through coins bearing his image, a privilege previously reserved for gods. The idea was adopted by the Roman emperors, and in Imperial Roman apotheosis the earthly body of the recently deceased emperor was replaced by a new and divine one as he ascended into heaven. The apotheosised dead remained recognisable to those who met them, as when Romulus appeared to witnesses after his death, but as the biographer Plutarch (c. AD 46â120) explained of this incident, while something within humans comes from the gods and returns to them after death, this happens "only when it is most completely separated and set free from the body, and becomes altogether pure, fleshless, and undefiled".

According to the New Testament, "God raised him from the dead", he ascended to heaven, to the "right hand of God", and will return again to fulfill the rest of Messianic prophecy such as the resurrection of the dead, the Last Judgment and establishment of the Kingdom of God.

The writings in the New Testament do not contain any descriptions of the moment of resurrection itself, but rather two types of eyewitness descriptions: appearances of Jesus to various people, and accounts of seeing the tomb empty.

The earliest surviving Christian writings are the letters of Paul, written between 50â57 (or possibly 48â57). The First Epistle to the Corinthians contains one of the earliest Christian creeds reporting post-mortem appearances of Jesus, and expressing the belief that he was raised from the dead, namely : 
In the Jerusalem "ekklÄsia", from which Paul received this creed, the phrase "died for our sins" probably was an apologetic rationale for the death of Jesus as being part of God's plan and purpose, as evidenced in the scriptures. For Paul, it gained a deeper significance, providing "a basis for the salvation of sinful Gentiles apart from the Torah." The phrase "died for our sins" was derived from Isaiah, especially , and Maccabees 4, especially . "Raised on the third day" is derived from : 
Paul says that Jesus subsequently appeared to him in the same way he did to the others. In 2 Corinthians 12 Paul describes "a man in Christ [presumably Paul himself] who ... was caught up to the third heaven", and while the language is obscure it is plausible that he saw Jesus enthroned at the right hand of God.

It is widely accepted that this creed predates the Apostle Paul. Scholars have contended that in his presentation of the resurrection, Paul refers to an earlier authoritative tradition, transmitted in a rabbinic style, that he received and has passed on to the church at Corinth. Geza Vermes writes that the creed is "a tradition he [Paul] has inherited from his seniors in the faith concerning the death, burial and resurrection of Jesus". The creed's ultimate origins are probably within the Jerusalem apostolic community, having been formalised and passed on within a few years of the resurrection. Hans Grass argues for an origin in Damascus, and according to Paul Barnett, this creedal formula, and others, were variants of the "one basic early tradition that Paul "received" in Damascus from Ananias in about 34 [AD]" after his conversion.

All four gospels climax with appearances of Jesus after his crucifixion, preparing the reader for his resurrection by having Jesus predict it , or through allusions that only the reader will understand , and elsewhere). The moment of resurrection is not described.

Jesus is the "firstborn of the dead," "prÅtotokos", the first to be raised from the dead, and thereby acquiring the "special status of the firstborn as the preeminent son and heir." His resurrection is also the guarantee that all the Christian dead will be resurrected at Christ's "parousia".

After his resurrection, Jesus starts proclaiming "eternal salvation" through the disciples, and subsequently calls the apostles to the Great Commission, as described in , , , , and , in which the disciples receive the call "to let the world know the good news of a victorious Saviour and the very presence of God in the world by the spirit." Jesus says that they "will receive power when the Holy Spirit has come upon you", that "repentance and forgiveness of sins is to be proclaimed in [the Messiah's] name to all nations, beginning from Jerusalem", and that "[i]f you forgive the sins of any, they are forgiven them; if you retain the sins of any, they are retained".

The Gospel of Mark, written c. 65â75, ends with the discovery of the empty tomb by Mary Magdalene, Mary the mother of James, and Salome. An angel announces to them that Jesus has risen, and instructs them to "tell Peter and the disciples that he will meet them in Galilee, 'just as he told you'"; yet, Mary does not do so. There are no appearances, but the author does seem to know of the appearances claimed for Peter and the Twelve. The longer ending, Mark 16:9â20, written c. 2nd century and similar to Luke and John, says that Jesus first appeared to Mary Magdalene, then to two followers walking outside Jerusalem, and then to the eleven remaining Apostles, commissioning them to spread "the good news": "The one who believes and is baptized will be saved; but the one who does not believe will be condemned."

In Matthew, Luke and John, the resurrection announcement is followed by appearances of Jesus to Mary Magdalene and other followers. The Book of Matthew describes a single appearance in Galilee, Luke describes several appearances in Jerusalem, John mentions appearances in both Jerusalem and Galilee. At some point, these appearances ceased in the early Christian community, as reflected in the Gospel-narratives: the "Acts of the Apostles" says that "for forty days he had continued to appear to them". The Book of Luke describes Jesus ascending to heaven at a location near Bethany .

In the Gospel of Matthew, an angel appears to Mary Magdalene at the empty tomb, telling her that Jesus is not there because he's been raised from the dead, and instructing her to tell the other followers to go to Galilee, to meet Jesus. Jesus then appears to Mary Magdalene and "the other Mary" at the tomb; and next, based on Mark 16:7, Jesus appears to all the disciples on a mountain in Galilee, where Jesus claims authority over heaven and earth, and commissions the disciples to preach the gospel to the whole world. Matthew presents Jesus's second appearance as an apotheosis (deification), commissioning his followers to "make disciples of all nations, baptizing them in the name of the Father and of the Son and of the Holy Spirit, [20] and teaching them to obey everything that I have commanded you." In this message, the end-times are delayed, "to bring the world to discipleship."

In the Gospel of Luke, "the woman who had come with him from Galilee" come to his tomb, which they find empty. Two angelic beings appear to announce that Jesus is not there, but has been raised. Jesus then appears to two followers on their way to Emmaus, who notify the eleven remaining Apostles, who respond that Jesus has appeared to Peter. While telling this, Jesus appears again, explaining that he is the messiah who raised from the dead according to the scriptures, "and that repentance and forgiveness of sins is to be proclaimed in his name to all nations, beginning from Jerusalem." In Luke-Acts (two works from the same author) he then ascends into heaven, his rightful home.

In the Gospel of John, Mary Magdalene finds the tomb empty, and informs Peter. She then sees two angels, after which Jesus himself appears to her. In the evening, Jesus appears to the other followers, followed by another appearance a week later. He later appears in Galilee to Peter, Tomas, and two other followers, commanding Peter to take care of his sheep.

In Acts of the Apostles, Jesus appears to apostles for forty days, and commands them to stay in Jerusalem whereafter Jesus ascends to heaven, followed by the coming of the Holy Spirit at Pentecost, and the missionary task of the early church.

The historicity and origin of the resurrection of Jesus has been the subject of historical research and debate, as well as a topic of discussion among theologians. The accounts of the Gospels, including the empty tomb and the appearances of the risen Jesus to his followers, have been interpreted and analyzed in diverse ways, and have been seen variously as historical accounts of a literal event, as accurate accounts of visionary experiences, as non-literal eschatological parables, and as fabrications of early Christian writers, among various other interpretations. One hypothesis, for example, that Jesus did not die on the cross, that the empty tomb was the result of Jesus' body having been stolen, or, as was common with Roman crucifixions, that Jesus was never entombed.

Post-Enlightenment historians work with methodological naturalism, and therefore reject miracles as objective historical facts.

Both Ware and Cook argue, primarily from Paul's terminology and the contemporary Jewish, pagan and cultural understanding of the nature of resurrection, that Paul held to a physically resurrected body ("sÅma"), restored to life, but animated by spirit ("pneumatikos") instead of soul ("psuchikos"), just like the later Gospel accounts. The nature of this resurrected body is a matter of debate. In , Paul uses the phrase "spiritual body" ("sÅma pneumatikos"), which has been explained as a "Spirit-empowered body," but also as a "celestial body," made of a finer material than the flesh. In the Epistle to the Philippians Paul describes how the body of the resurrected Christ is utterly different to the one he wore when he had "the appearance of a man," and holds out a similar glorified state, when Christ "will transform our lowly body," as the goal of the Christian life â "flesh and blood cannot inherit the kingdom of God" (I Corinthians 15:50), and Christians entering the kingdom will be "putting off the body of the flesh" (Colossians 2:11). Paul opposed the notion of a purely spiritual resurrection, as propagated by some Christians in Corinth, which he addresses in 1 Corinthians. The developing Gospel-tradition emphasized the material aspects to counter this spiritual interpretation.

Paul's views of a bodily resurrection went against the thoughts of the Greek philosophers to whom a bodily resurrection meant a new imprisonment in a corporeal body, which was what they wanted to avoid â given that for them the corporeal and the material fettered the spirit.

Dunn notes that there is a great difference between Paul's resurrection appearance, and the appearances described in the Gospels. Where "Paul's seeing was visionary [...], 'from heaven'," in contrast, the Gospel-accounts have a "massive realism" to them. Dunn contends that the "massive realism' [...] of the [Gospel] appearances themselves can only be described as visionary with great difficulty â and Luke would certainly reject the description as inappropriate." According to Dunn, most scholars explain this as a "legendary materialization" of the visionary experiences, "borrowing the traits of the earthly Jesus." Yet, according to Dunn, there was both "a tendency away from the physical [...] and a reverse tendency towards the physical." The tendency towards the material is most clear, but there are also signs for the tendency away from the physical, and "there are some indications that a more physical understanding was current in the earliest Jerusalem community."

GÃ©za Vermes notes that "[t]he empty tomb and the apparitions are never directly associated to form a combined argument." While the coherence of the empty tomb-narrative is questionable, it is "clearly an early tradition." Vermes rejects the literal interpretation of the story, as being proof of the resurrection, and also notes that the story of the empty tomb conflicts with notions of a spiritual resurrection. According to Vermes, "[t]he strictly Jewish bond of spirit and body is better served by the idea of the empty tomb and is no doubt responsible for the introduction of the notions of palpability (Thomas in John) and eating (Luke and John)."

According to Brown, the body of Jesus was buried in a new tomb by Joseph of Arimathea in accordance with Mosaic Law, which stated that a person hanged on a tree must not be allowed to remain there at night, but should be buried before sundown. New Testament historian Bart D. Ehrman dismisses the story of the empty tomb; according to Ehrman, "an empty tomb had nothing to do with it [...] an empty tomb would not produce faith." According to Ehrman, the empty tomb was needed to underscore the physical resurrection of Jesus, but is it doubtful that Jesus was buried by Joseph of Arimathea. It is unlikely that a member of the Sanhedrin would have buried Jesus; crucifixion was meant "to torture and humiliate a person as fully as possible," and the body was left on the stake to be eaten by animals; criminals were usually buried in common graves; and Pilate had no concern for Jewish sensitivities, which makes it unlikely that he would have allowed for Jesus to be buried. The English theologian and historian N. T. Wright, however, emphatically and extensively argues for the reality of the empty tomb and the subsequent appearances of Jesus, reasoning that as a matter of history both a bodily resurrection and later bodily appearances of Jesus are far better explanations for the rise of Christianity than are any other theories, including those of Ehrman.

In Christian theology, the death, resurrection, and exaltation of Jesus are the most important events, and a foundation of the Christian faith. The Nicene Creed states: "On the third day he rose again in accordance with the Scriptures". According to Terry Miethe, a Christian philosopher at Oxford University, the question "â'Did Jesus rise from the dead?' is the most important question regarding the claims of the Christian faith." According to John R. Rice, a Baptist evangelist, the resurrection of Jesus was part of the plan of salvation and redemption by atonement for man's sin. Summarizing its traditional analysis, the Catholic Church states in its Catechism: 
For Christians, including some scholars, the resurrection is taken to have been a concrete, material resurrection. According to N. T. Wright in his book "The Resurrection of the Son of God", "There can be no question: Paul is a firm believer in bodily resurrection. He stands with his fellow Jews against the massed ranks of pagans; with his fellow Pharisees against other Jews." According to New Testament scholar Gary Habermas, "Many other scholars have spoken in support of a bodily notion of Jesusâ resurrection." According to Craig L. Blomberg, there are sufficient arguments for the historicity of the resurrection.

The belief of Jesus' first followers in the resurrection formed the proclamation of the first "ekklÄsia". The appearances reinforced the impact Jesus and his ministry had on his early followers, and interpreted in a scriptural framework they gave the impetus to Christ-devotion and the belief in the exaltation of Jesus. Jesus' death was interpreted in light of the scriptures as a redemptive death, being part of God's plan, and the appearances also led to the resumation of the missionary activity of Jesus' followers, with Peter assuming the first leader-role in the first "ekklÄsia", forming the basis for the Apostolic succession.

The New Testament writings contend that the resurrection was "the beginning of His exalted life" as Christ and Lord.Jesus is the "firstborn of the dead," "prÅtotokos", the first to be raised from the dead, and thereby acquiring the "special status of the firstborn as the preeminent son and heir." According to Beale,
Hurtado notes that soon after his death, Jesus was called Lord ("Kyrios"), which "associates him in astonishing ways with God." The term Lord reflected the belief that God had exalted to a divine status "at God's 'right hand'." The worship of God as expressed in the phrase "call upon the name of the Lord ["Yahweh"]" was also applied to Jesus, invocating his name "in corporate worship and in the wider devotional pattern of Christian believers (e.g., baptism, exorcism, healing)."

According to Hurtado, powerful religious experiences were an indispensable factor in the emergence of Christ-devotion. Those experiences "seem to have included visions of (and/or ascents to) God's heaven, in which the glorified Christ was seen in an exalted position." Those experiences were interpreted in the framework of God's redemptive purposes, as reflected in the scriptures, in a "dynamic interaction between devout, prayerful searching for, and pondering over, scriptural texts and continuing powerful religious experiences." This initiated a "new devotional pattern unprecedented in Jewish monotheism," that is, the worship of Jesus next to God, giving Jesus a central place because his ministry, and its consequences, had a strong impact on his early followers. Revelations, including those visions, but also inspired and spontaneous utterances, and "charismatic exegesis" of the Jewish scriptures, convinced them that this devotion was commanded by God.

Ehrman notes that both Jesus and his early followers were apocalyptic Jews, who believed in the bodily resurrection, which would start when the coming of God's Kingdom was near. According to Ehrman, "the disciples' belief in the resurrection was based on visionary experiences," arguing that visions usually have a strong persuasive power, but also noting that the Gospel-accounts record a tradition of doubt about the appearances of Jesus. Ehrman's "tentative suggestion" is that only a few followers had visions, including Peter, Paul and Mary. They told others about those visions, convincing most of their close associates that Jesus was raised from the dead, but not all of them. Eventually, these stories were retold and embellished, leading to the story that all disciples had seen the risen Jesus. The belief in Jesus' resurrection radically changed their perceptions, concluding from his absence that he must have been exalted to heaven, by God himself, exalting him to an unprecedented status and authority.

It has long been argued that the New Testament writings contain two different Christologies, namely a "low" or adoptionist Christology, and a "high" or "incarnation Christology." The "low Christology" or "adoptionist Christology" is the belief "that God exalted Jesus to be his Son by raising him from the dead," thereby raising him to "divine status." The other early Christology is "high Christology," which is "the view that Jesus was a pre-existent divine being who became a human, did the Fatherâs will on earth, and then was taken back up into heaven whence he had originally come," and from where he appeared on earth. The chronology of the development of these early Christologies is a matter of debate within contemporary scholarship.

According to the "evolutionary model" c.q. "evolutionary theories," as proposed by Bousset, followed by Brown, the Christological understanding of Christ developed over time, from a low Christology to a high Christology, as witnessed in the Gospels. According to the evolutionary model, the earliest Christians believed that Jesus was a human who was exalted, c.q. adopted as God's Son, when he was resurrected, signaling the nearness of the Kingdom of God, when all dead would be resurrected and the righteous exalted. Later beliefs shifted the exaltation to his baptism, birth, and subsequently to the idea of his eternal existence, as witnessed in the Gospel of John. Mark shifted the moment of when Jesus became the son to the baptism of Jesus, and later still Matthew and Luke shifted it to the moment of the divine conception, and finally John declared that Jesus had been with God from the beginning: "In the beginning was the Word".

Since the 1970s, the late datings for the development of a "high Christology" have been contested, and a majority of scholars argue that this "High Christology" existed already before the writings of Paul. This "incarnation Christology" or "high Christology" did not evolve over a longer time, but was a "big bang" of ideas which were already present at the start of Christianity, and took further shape in the first few decades of the church, as witnessed in the writings of Paul.

According to Ehrman, these two Christologies existed alongside each other, calling the "low Christology" an "adoptionist Christology, and "the "high Christology" an "incarnation Christology." While adoptionism was declared heresy at the end of the 2nd century, it was adhered to by the Ebionites, who regarded Jesus as the Messiah while rejecting his divinity and his virgin birth, and insisted on the necessity of following Jewish law and rites. They revered James the brother of Jesus (James the Just); and rejected Paul the Apostle as an apostate from the Law. They show strong similarities with the earliest form of Jewish Christianity, and their specific theology may have been a "reaction to the law-free Gentile mission."

Jesus' death was interpreted as a redemptive death "for our sins," in accordance with God's plan as contained in the Jewish scriptures. The significance lay in "the theme of divine necessity and fulfillment of the scriptures," not in the later Pauline emphasis on "Jesus' death as a sacrifice or an expiation for our sins." For the early Jewish Christians, "the idea that Messiah's death was a necessary redemptive event functioned more as an apologetic explanation for Jesus' crucifixion" "proving that Jesus' death was no surprise to God."

According to Dunn, the appearances to the disciples have "a sense of obligation to make the vision known." Helmut Koester states that the stories of the resurrection were originally epiphanies in which the disciples were called to a ministry by the risen Jesus, and at a secondary stage were interpreted as physical proof of the event. He contends that the more detailed accounts of the resurrection are also secondary and do not come from historically trustworthy sources, but instead belong to the genre of the narrative types. Biblical scholar GÃ©za Vermes argues that the resurrection is to be understood as a reviving of the self-confidence of the followers of Jesus, under the influence of the Spirit, "prompting them to resume their apostolic mission." They felt the presence of Jesus in their own actions, "rising again, today and tomorrow, in the hearts of the men who love him and feel he is near." According to Gerd LÃ¼demann, Peter convinced the other disciples that the resurrection of Jesus signaled that the end-times were near and God's Kingdom was coming, when the dead who would rise again, as evidenced by Jesus. This revitalized the disciples, starting-off their new mission.

Peter claimed forcefully that Jesus appeared to him, and legitimised by Jesus' appearance he assumed leadership of the group of early followers, forming the Jerusalem "ekklÄsia" mentioned by Paul. He was soon eclipsed in this leadership by James the Just, "the Brother of the Lord," which may explain why the early texts contain scarce information about Peter. According to Gerd LÃ¼demann, Peter was the first who had a vision of Jesus, noting that Peter and Mary both had appearance-experiences, but arguing that the tradition of Mary's appearance is a later development, and her appearance probably was not the first.

According to Christian proto-orthodoxy, Peter was the first to who Jesus appeared, and therefore the rightful leader of the Church. The resurrection forms the basis of the Apostolic succession and the institutional power of orthodoxy, as the heirs of Peter, to who Jesus appeared, and is described as "the rock" on which the church will be built. Though the Gospels, and Paul's letters, describe appearances to a greater number of people, only the appearances to the Twelve Apostles count as lending authority and Apostolic succession.

The appearance of Jesus to Paul convinced him that Jesus was the risen Lord and Christ, who commissioned him to be an apostle to the Gentiles. According to Newbigin, "Paul presents himself not as the teacher of a new theology but as the messenger commissioned by the authority of the Lord himself to announce a new fact - namely that in the ministry, death and resurrection of Jesus God has acted decisively to reveal and effect his purpose of redemption for the whole world." The teachings of the apostle Paul form a key element of the Christian tradition and theology. Fundamental to Pauline theology is the connection between Christ's resurrection, and redemption. In , , Paul writes: 
The "kerygma" of 1 Corinthians 15:3 states that "Christ died for our sins." The meaning of that "kerygma" is a matter of debate, and open to multiple interpretations. Traditionally, this "kerygma" is interpreted as meaning that Jesus' death was an atonement or ransom for, or propitiation or expiation of, God's wrath against humanity because of their sins. With Jesus death, humanity was freed from this wrath. In the classical Protestant understanding, which has dominated the understanding o Paul's writings, humans partake in this salvation by faith in Jesus Christ; this faith is a grace given by God, and people are justified by God through Jesus Christ and faith in Him.

More recent scholarship has raised several concerns regarding these interpretations. According to E.P. Sanders, who initiated the so-called New Perspective on Paul, Paul saw the faithful redeemed by participation in Jesus' death and rising. Though "Jesusâ death substituted for that of others and thereby freed believers from sin and guilt," a metaphor derived from "ancient sacrificial theology," the essence of Paul's writing is not in the "legal terms" regarding the expiation of sin, but the act of "participation in Christ through dying and rising with him." According to Sanders, "those who are baptized into Christ are baptized into his death, and thus they escape the power of sin [...] he died so that the believers may die with him and consequently live with him." Just as Christians share in Jesus' death in baptism, so they will share in his resurrection. James F. McGrath notes that Paul "prefers to use the language of participation. One died for all, so that all died (). This is not only different from substitution, it is the opposite of it."

Paul insists that salvation is received by the grace of God; according to Sanders, this insistence is in line with Judaism of ca. 200 BC until 200 AD, which saw God's covenant with Israel as an act of grace of God. Observance of the Law is needed to maintain the covenant, but the covenant is not earned by observing the Law, but by the grace of God.

The Apostolic Fathers, discussed the death and resurrection of Jesus, including Ignatius (50â115), Polycarp (69â155), and Justin Martyr (100â165). The understanding of the Greek Fathers of the death and resurrection of Jesus as an atonement is the "classic paradigm" of the Church Fathers, who developed the themes found in the New Testament.

During the first millennium AD, the ransom theory of atonement was the dominant metaphor, both in eastern and western Christianity, until it was replaced in the west by Anselmus' satisfaction theory of atonement. The ransom theory of atonement says that Christ liberated humanity from slavery to sin and Satan, and thus death, by giving his own life as a ransom sacrifice to Satan, swapping the life of the perfect (Jesus), for the lives of the imperfect (humans). It entails the idea that God deceived the devil, and that Satan, or death, had "legitimate rights" over sinful souls in the afterlife, due to the fall of man and inherited sin.

The ransom theory was first clearly enunciated by Irenaeus (c. 130âc. 202), who was an outspoken critic of Gnosticism, but borrowed ideas from their dualistic worldview. In this worldview, humankind is under the power of the Demiurg, a lesser God who has created the world. Yet, humans have a spark of the true divine nature within them, which can be liberated by gnosis (knowledge) of this divine spark. This knowledge is revealed by the Logos, "the very mind of the supreme God," who entered the world in the person of Jesus. Nevertheless, the Logos could not simply undo the power of the Demiurg, and had to hide his real identity, appearing as a physical form, thereby misleading the Demiurg, and liberating humankind. In Irenaeus' writings, the Demiurge is replaced by the devil, while Justin Martyr had already equated Jesus and the Logos.

Origen (184â253) introduced the idea that the devil held legitimate rights over humans, who were bought free by the blood of Christ. He also introduced the notion that the devil was deceived in thinking that he could master the human soul.

Following the conversion of Constantine and the Edict of Milan in 313, the ecumenical councils of the 4th, 5th and 6th centuries, that focused on Christology, helped shape the Christian understanding of the redemptive nature of resurrection, and influenced both the development of its iconography, and its use within Liturgy.

Belief in bodily resurrection was a constant note of the Christian church in antiquity. Augustine of Hippo accepted it at the time of his conversion in 386. Augustine defended resurrection, and argued that given that Christ has risen, there is resurrection of the dead. Moreover, he argued that the death and resurrection of Jesus was for the salvation of man, stating: "to achieve each resurrection of ours, the savior paid with his single life, and he pre-enacted and presented his one and only one by way of sacrament and by way of model."

The 5th-century theology of Theodore of Mopsuestia provides an insight into the development of the Christian understanding of the redemptive nature of resurrection. The crucial role of the sacraments in the mediation of salvation was well accepted at the time. In Theodore's representation of the Eucharist, the sacrificial and salvific elements are combined in the "One who saved us and delivered us by the sacrifice of Himself". Theodore's interpretation of the Eucharistic rite is directed towards the triumph over the power of death brought about by the resurrection.

The emphasis on the salvific nature of the resurrection continued in Christian theology in the next centuries, e.g., in the 8th century Saint John of Damascus wrote that: "...Â When he had freed those who were bound from the beginning of time, Christ returned again from among the dead, having opened for us the way to resurrection" and Christian iconography of the ensuing years represented that concept.

Lorenzen finds "a strange silence about the resurrection in many pulpits". He writes that among some Christians, ministers and professors, it seems to have become "a cause for embarrassment or the topic of apologetics". According to Warnock, many Christians neglect the resurrection because of their understandable preoccupation with the Cross.

Easter is the preeminent Christian feast that celebrates the resurrection of Jesus, and, according to Susan J. White, "is clearly the earliest Christian festival." According to Dunn, "In Easter we celebrate man become God [...] that in the death and resurrection of Christ God has broken the stranglehold of human selfishness, has proved the enduring and conquering strength of divine love." According to Thorwald Lorenzen, the first Easter led to a shift in emphasis from faith "in God" to faith "in Christ". According to Raymond Harfgus Taylor, "focuses upon the consumation of the redemptive act of God in the death/resurrection of Jesus Christ."

Easter is linked to the Passover and Exodus from Egypt recorded in the Old Testament through the Last Supper and crucifixion that preceded the resurrection. According to the New Testament, Jesus gave the Passover meal a new meaning, as he prepared himself and his disciples for his death in the upper room during the Last Supper. He identified the loaf of bread and cup of wine as his body soon to be sacrificed and his blood soon to be shed. states, "Get rid of the old yeast that you may be a new batch without yeastas you really are. For Christ, our Passover lamb, has been sacrificed"; this refers to the Passover requirement to have no yeast in the house and to the allegory of Jesus as the Paschal lamb.

In the Catacombs of Rome, artists indirectly hinted at the resurrection by using images from the Old Testament such as the fiery furnace and Daniel in the Lion's den. Depictions prior to the 7th century generally showed secondary events such as the Myrrhbearers at the tomb of Jesus to convey the concept of the resurrection. An early symbol of the resurrection was the wreathed Chi Rho (Greek letters representing the word "Khristos" or "Christ"), whose origin traces to the victory of emperor Constantine I at the Battle of the Milvian Bridge in 312, which he attributed to the use of a cross on the shields of his soldiers. Constantine used the Chi Rho on his standard and his coins showed a labarum with the Chi Rho killing a serpent.

The use of a wreath around the Chi Rho symbolizes the victory of the resurrection over death, and is an early visual representation of the connection between the Crucifixion of Jesus and his triumphal resurrection, as seen in the 4th-century sarcophagus of Domitilla in Rome. Here, in the wreathed Chi Rho the death and Resurrection of Christ are shown as inseparable, and the Resurrection is not merely a happy ending tucked at the end of the life of Christ on earth. Given the use of similar symbols on the Roman military banner, this depiction also conveyed another victory, namely that of the Christian faith: the Roman soldiers who had once arrested Jesus and marched him to Calvary now walked under the banner of a resurrected Christ.

The cosmic significance of the resurrection in Western theology goes back to Saint Ambrose, who in the 4th century said that "The universe rose again in Him, the heaven rose again in Him, the earth rose again in Him, for there shall be a new heaven and a new earth". This theme developed gradually in the West, later than in the East where the resurrection had been linked from an earlier date to redemption and the renewal and rebirth of the whole world. In art this was symbolized by combining the depictions of the resurrection with the Harrowing of Hell in icons and paintings. A good example is from the Chora Church in Istanbul, where John the Baptist, Solomon and other figures are also present, depicting that Christ was not alone in the resurrection. The depiction sequence at the 10th-century Hosios Loukas shows Christ as he pulls Adam from his tomb, followed by Eve, signifying the salvation of humanity after the resurrection.

The resurrection of Jesus has long been central to Christian faith and appears within diverse elements of the Christian tradition, from feasts to artistic depictions to religious relics. In Christian teachings, the sacraments derive their saving power from the passion and resurrection of Christ, upon which the salvation of the world entirely depends.

An example of the interweaving of the teachings on the resurrection with Christian relics is the application of the concept of "miraculous image formation" at the moment of resurrection to the Shroud of Turin. Christian authors have stated the belief that the body around whom the shroud was wrapped was not merely human, but divine, and that the image on the shroud was miraculously produced at the moment of resurrection. Quoting Pope Paul VI's statement that the shroud is "the wonderful document of His Passion, Death and Resurrection, written for us in letters of blood" author Antonio Cassanelli argues that the shroud is a deliberate divine record of the five stages of the Passion of Christ, created at the moment of resurrection.

Groups such as Jews, Muslims, BahÃ¡'Ã­s, and other non-Christians, as well as some liberal Christians, dispute whether Jesus actually rose from the dead. Arguments over death and resurrection claims occur at many religious debates and interfaith dialogues.

Christianity split from Judaism in the 1st century AD, and the two faiths have differed in their theology since. According to the "Toledot Yeshu", the body of Jesus was removed in the same night by a gardener named Juda, after hearing the disciples planned to steal the body of Jesus. However, "Toledot Yeshu" is not considered either canonical or normative within rabbinic literature. Van Voorst states that "Toledot Yeshu" is a medieval document set without a fixed form which is "most unlikely" to have reliable information about Jesus. The Blackwell Companion to Jesus states that the "Toledot Yeshu" has no historical facts as such, and was perhaps created as a tool for warding off conversions to Christianity.

Some Gnostics did not believe in a literal physical resurrection. "For the gnostic any resurrection of the dead was excluded from the outset; the flesh or substance is destined to perish. 'There is no resurrection of the flesh, but only of the soul', say the so-called Archontics, a late gnostic group in Palestine".

Muslims believe that Ê¿ÄªsÄ (Jesus) son of Mariam (Mary) was a holy prophet with a divine message. The Islamic perspective is that Jesus was not crucified and will return to the world at the end of times. "But AllÄh raised him up to Himself. And AllÄh is Ever All-Powerful, All-Wise". The Quran says in Surah An-Nisa [Ch 004: Verse 157] "And because of their saying, "We killed Messiah Ê¿ÄªsÄ, son of Maryam, the Messenger of AllÄh",but they killed him not, nor crucified him, but it appeared so to them, and those who differ therein are full of doubts".










</doc>
<doc id="26415" url="https://en.wikipedia.org/wiki?curid=26415" title="Rube Foster">
Rube Foster

Andrew "Rube" Foster (September 17, 1879 â December 9, 1930) was an American baseball player, manager, and executive in the Negro leagues. He was elected to the Baseball Hall of Fame in 1981.

Foster, considered by historians to have been perhaps the best African-American pitcher of the first decade of the 1900s, also founded and managed the Chicago American Giants, one of the most successful black baseball teams of the pre-integration era. Most notably, he organized the Negro National League, the first long-lasting professional league for African-American ballplayers, which operated from 1920 to 1931. He is known as the "father of Black Baseball."

Foster adopted his longtime nickname, "Rube", as his official middle name later in life.

Foster was born in Calvert, Texas on September 17, 1879. His father, also named Andrew, was a reverend and elder of the local American Methodist Episcopal Church. Foster started his professional career with the Waco Yellow Jackets, an independent black team, in 1897. Over the next few years he gradually built up a reputation among white and black fans alike, until he was signed by Frank Leland's Chicago Union Giants, a team in the top ranks of black baseball, in 1902. After a slump, he was released, and signed with a white semipro team based in Otsego, Michigan â Bardeen's Otsego Independents. According to Phil Dixon's American Baseball Chronicles: Great Teams, The 1905 Philadelphia Giants, Volume III "In completing the summer of 1902 with Otsego's multi-ethnic teamââthe only multi-race team with which he would ever regularly performââFoster is reported to have pitched twelve games. He finished with a documented record of eight wins and four losses along with eighty-two documented strikeouts. Ironically, strikeout totals for five games which he appeared were not recorded. If found the totals would likely show that Foster struck out more than one-hundred batters for Otsego. In the seven games where details exist, Foster average eleven strikeouts per outing." Toward the end of the season he joined the Cuban X-Giants of Philadelphia, perhaps the best team in black baseball. The 1903 season saw Foster establish himself as the X-Giants' pitching star. In a post-season series for the eastern black championship, the X-Giants defeated Sol White's Philadelphia Giants five games to two, with Foster himself winning four games.

According to various accounts, including his own, Foster acquired the nickname "Rube" after defeating star Philadelphia Athletics left-hander Rube Waddell in a postseason exhibition game played sometime between 1902 and 1905. A newspaper story in the Trenton (NJ) "Times" from July 26, 1904, contains the earliest known example of Foster being referred to as "Rube", indicating that the supposed meeting with Waddell must have taken place earlier than that. Recent research has uncovered a game played on August 2, 1903, in which Foster met and defeated Waddell while the latter was playing under an assumed name for a semi-pro team in New York City.

Now a star, Foster jumped to the Philadelphia Giants for the 1904 season. Legend has it that John McGraw, manager of the New York Giants, hired Foster to teach the young Christy Mathewson the "fadeaway", or screwball, though historians have cast this story in doubt. During the 1904 season, Foster won 20 games against all competition (including two no-hitters) and lost six. In a rematch with Foster's old team, the Cuban X-Giants, he won two games and batted .400 in leading the Philadelphia Giants to the black championship.

In 1905, Foster (by his own account several years later) compiled a fantastic record of 51â4, though recent research has confirmed only a 25â3 record. He led the Giants to another championship series victory, this time over the Brooklyn Royal Giants. The "Philadelphia Telegraph" wrote that "Foster has never been equalled in a pitcher's box." The following season, the Philadelphia Giants helped form the International League of Independent Professional Ball Players, composed of both all-black and all-white teams in the Philadelphia and Wilmington, Delaware, areas.

In 1907, Foster's manager Sol White published his "Official Baseball Guide: History of Colored Baseball", with Foster contributing an article on "How to Pitch." However, before the season began, he and several other stars (including, most importantly, the outfielder Pete Hill) left the Philadelphia Giants for the Chicago Leland Giants, with Foster named playing manager. Under his leadership, the Lelands won 110 games (including 48 straight) and lost only ten, and took the Chicago City League pennant. The following season the Lelands tied a national championship series with the Philadelphia Giants, each team winning three games.

Foster suffered a broken leg in July 1909, but rushed himself back into the lineup in time for an October exhibition series against the Chicago Cubs. Foster, pitching the second game, squandered a 5â2 lead in the ninth inning, then lost the game on a controversial play when a Cubs runner stole home while Foster was arguing with the umpire. The Lelands lost the series, three games to nothing. The Lelands also lost the unofficial western black championship to the St. Paul Colored Gophers.

In 1910, Foster wrested legal control of the team from its founder, Frank Leland. He proceeded to put together the team he later considered his finest. He signed John Henry Lloyd away from the Philadelphia Giants; along with Hill, second baseman Grant Johnson, catcher Bruce Petway, and pitchers Frank Wickware and Pat Dougherty, Lloyd sparked the Lelands to a 123â6 record (with Foster himself contributing a 13â2 record on the mound).

The following season, Foster established a partnership with John Schorling, the son-in-law of Chicago White Sox owner Charlie Comiskey. The White Sox had just moved into Comiskey Park, and Schorling arranged for Foster's team to use the vacated South Side Park, at 39th and Wentworth. Settling into their new home (now called Schorling's Park), the Lelands became the Chicago American Giants. For the next four seasons, the American Giants claimed the western black baseball championship, though they lost a 1913 series to the Lincoln Giants for the national championship.

By 1915, Foster's first serious rival in the midwest had emerged: C. I. Taylor's Indianapolis ABCs, who claimed the western championship after defeating the American Giants four games to none in July. One of the victories was a forfeit called after a brawl between the two teams broke out. After the series, Foster and Taylor engaged in a public dispute about that game and the championship. In 1916, both teams again claimed the western title. The continued wrangling led to calls for a black baseball league to be formed, but Foster, Taylor, and the other major clubs in the midwest were unable to come to any agreement.

By this time, Foster was pitching very little, compiling only a 2â2 record in 1915. His last recorded outing on the mound was in 1917; from this time he became purely a bench manager. As a manager and team owner, Foster was a disciplinarian. He asserted control over every aspect of the game, and set a high standard for personal conduct, appearance, and professionalism among his players. Given Schorling Park's huge dimensions, Foster developed a style of play that emphasized speed, bunting, place hitting, power pitching, and defense. He was also considered a great teacher, and many of his players themselves eventually became managers, including Pete Hill, Bruce Petway, Bingo DeMoss, Dave Malarcher, Sam Crawford, Poindexter Williams, and many others.

In 1919, Foster helped Tenny Blount finance a new club in Detroit, the Stars. He also transferred several of his veteran players there, including Hill, who was to manage the new team, and Petway. He may have been preparing the way for the formation, the following year, of the Negro National League (NNL).

In 1920, Foster, Taylor, and the owners of six other midwestern clubs met in the spring to form a professional baseball circuit for African-American teams. Foster, as president, controlled league operations, while remaining owner and manager of the American Giants. He was periodically accused of favoring his own team, especially in matters of scheduling (the Giants in the early years tended to have a disproportionate number of home games) and personnel: Foster seemed able to acquire whatever talent he needed from other clubs, such as Jimmie Lyons, the Detroit Stars' best player in 1920, who was transferred to the American Giants for 1921, or Foster's own younger brother, Bill, who joined the American Giants unwillingly when Rube forced the Memphis Red Sox to give him up in 1926. His critics believed he had organized the league primarily for purposes of booking games for the American Giants. With a stable schedule and reasonably solvent opponents, Foster was able to improve receipts at the gate. It is also true that when opposing clubs lost money, he was known to help them meet payroll, sometimes out of his own pocket. 

His American Giants won the new league's first three pennants, before being overtaken by the Kansas City Monarchs in 1923. In the same year the Hilldale Club and Bacharach Giants, the most important eastern clubs, pulled out of an agreement with the NNL and founded their own league, the Eastern Colored League (ECL). The ECL raided the older circuit for players, Foster's own ace pitcher Dave Brown among them. Eventually the two leagues reached an agreement to respect one another's contracts, and to play a world series.

After two years of finishing behind the Monarchs, Foster "cleaned house" in spring, 1925, releasing several veterans (including Lyons and pitchers Dick Whitworth and Tom Williams). On May 26, Foster was nearly asphyxiated by a gas leak in Indianapolis. Though he recovered and returned to his team, his behavior grew erratic from then on. Foster had instituted a split-season format, and his American Giants finished third in both halves.

1926 saw him complete his team's reshaping, leaving only a handful of veterans from the championship squads of 1920â22. The club finished third in the season's first half; but Foster would never finish the second. Over the years, "Foster grew increasingly paranoid. Took to carrying a revolver with him everywhere he went." Suffering from serious delusions, he was institutionalized midway through the 1926 season at an asylum in Kankakee, Illinois.

The American Giants and the NNL lived onâin fact, led by Dave Malarcher, the Giants won the pennant and World Series in both 1926 and 1927âbut the league clearly suffered in the absence of Foster's leadership. Foster died in 1930, never having recovered his sanity, and a year later, the league he had founded fell apart.

Foster is interred in Lincoln Cemetery in Blue Island, Illinois.

In 1981, Foster was elected to the National Baseball Hall of Fame. He was the first representative of the Negro leagues elected as a pioneer or executive.

On December 30, 2009, the U.S. Postal Service announced that it planned to issue a pair of postage stamps in June honoring Negro leagues Baseball. On July 17, 2010, the Postal Service issued a se-tenant pair of 44-cent, first-class, U.S. commemorative postage stamps, to honor the all-black professional baseball leagues that operated from 1920 to about 1960. One of the stamps depicts Foster, along with his name and the words "NEGRO LEAGUES BASEBALL". The stamps were formally issued at the Negro Leagues Baseball Museum, during the celebration of the museum's twentieth anniversary.

The Negro Leagues Baseball Museum hosts the annual Andrew "Rube" Foster Lecture, in September.





</doc>
<doc id="26416" url="https://en.wikipedia.org/wiki?curid=26416" title="Ring Lardner">
Ring Lardner

Ringgold Wilmer "Ring" Lardner (March 5, 1885 â September 25, 1933) was an American sports columnist and short-story writer best known for his satirical writings on sports, marriage, and the theatre. His contemporaries Ernest Hemingway, Virginia Woolf, and F. Scott Fitzgerald all professed strong admiration for his writing.

Born in Niles, Michigan, Ring Lardner was the son of wealthy parents, Henry and Lena Phillips Lardner. He was the youngest of nine children. Lardner's name came from a cousin of the same name. The cousin had been named by Lardner's uncle, Rear Admiral James L. Lardner, who had decided to name his son after a friend, Rear Admiral Cadwalader Ringgold, who was from a distinguished military family. Lardner never liked his given name and abbreviated it to Ring, naming one of his sons Ring Jr.

In childhood he wore a brace for his deformed foot until he was eleven. He also had a passion for baseball, stage, and music. He later attended the Armour Institute in Chicago.

Lardner married Ellis Abbott of Goshen, Indiana, in 1911. They had four sons, John, James, Ring Jr., and David.

Lardner died on September 25, 1933, at the age of 48 in East Hampton, New York, of a heart attack due to complications from tuberculosis.

Lardner started his writing career as a sports columnist, finding work with the newspaper "South Bend Times" in 1905. In 1907, he relocated to Chicago, where he got a job with the " Inter-Ocean". Within a year, he quit to work for the "Chicago Examiner", and then for the "Tribune". Two years later, Lardner was in St. Louis, writing the humorous baseball column "Pullman Pastimes" for Taylor Spink and the "Sporting News". Some of this work was the basis for his book "You Know Me, Al". Within three months, he was an employee of the "Boston American".

In 1913, Lardner returned to the "Chicago Tribune", which became the home newspaper for his syndicated column "In the Wake of the News" (started by Hugh Keough, who had died in 1912). The column appeared in more than 100 newspapers, and is still published in the "Tribune". Lardner's Tribune and syndicated writing was not exclusively sports related: his dispatches from/near the World War One front were collected in the book "My Four Weeks in France", and his immersive coverage of the 1920 Democratic Convention resulted in Lardner receiving 0.5 votes on the 23rd ballot.

In 1916, Lardner published his first successful book, "You Know Me Al", an epistolary novel written in the form of letters by "Jack Keefe", a bush-league baseball player, to a friend back home. The letters made much use of the fictional author's idiosyncratic vernacular. It had initially been published as six separate but interrelated short stories in "The Saturday Evening Post", causing some to classify the book as a collection of stories, others as a novel. Like most of Lardner's stories, "You Know Me Al" employs satire, in this case, to show the stupidity and avarice of a certain type of athlete. The journalist Andrew Ferguson wrote that "Ring Lardner thought of himself as primarily a sports columnist whose stuff wasn't destined to last, and he held to that absurd belief even after his first masterpiece, "You Know Me Al", was published in 1916 and earned the awed appreciation of Virginia Woolf, among other very serious, unfunny people." Ferguson termed the book one of the top five pieces of American humor writing.

Sarah Bembrey has written about a singular event in Lardner's sportswriting experience: "In 1919 something happened that changed his way of reporting about sports and changed his love for baseball. This was the Black Sox scandal when the Chicago White Sox sold out the World Series to the Cincinnati Reds. Ring was exceptionally close to the White Sox and felt he was betrayed by the team. After the scandal, Ring always wrote about sports as if there were some kink to the outcome." Lardner's last fictional baseball writing was collected in the book "Lose with a Smile" (1933).

Lardner later published such stories as "Haircut", "Some Like Them Cold", "The Golden Honeymoon", "Alibi Ike", and "A Day with Conrad Green". He also continued to write follow-up stories to "You Know Me Al", with the protagonist of that book, the headstrong but gullible Jack Keefe, experiencing various ups and downs in his major league career and in his personal life. Private Keefe's World War I training camp letters home to his friend Al were collected in the book "Treat 'Em Rough: Letters From Jack the Kaiser Killer". The sequel, "The Real Dope", followed Keefe overseas to the trenches in France.

Lardner also had a lifelong fascination with the theatre, although his only Broadway three-act successes were the thrice-filmed "Elmer The Great", co-written with George M. Cohan, and "June Moon", a comedy authored with Broadway veteran George S. Kaufman. Lardner also wrote skits for the Ziegfeld Follies. and a series of brief nonsense plays that ridiculed the conventions of the theatre using zany humor and outrageous, impossible stage directions, such as "The curtain is lowered for seven days to denote the lapse of a week."

He was a dedicated composer and lyricist: both his first â "Zanzibar" (1903) â and last â "June Moon" (1920) â published stage works included several Lardner tunes. He wrote at least one recorded song for Bert Williams, and provided the lyrics for the song "That Old Quartet" (1913) by Nathaniel D. Mann. Other collaborators of note included Aubrey Stauffer, Jerome Kern, and Vincent Youmans â with whom he toiled on the Ziegfeld Astaires musical, "Smiles" (1930).

Lardner was a good friend of F. Scott Fitzgerald and other authors of the Jazz Age. His books were published by Maxwell Perkins, who also served as Fitzgerald's editor. To create his first book of short stories Lardner had to get copies from the magazines who bought the stories â he held his own short stories in low regard and did not save copies.

Lardner was in some respects the model for the tragic character Abe North of Fitzgerald's last completed novel, "Tender Is the Night".

Lardner influenced Ernest Hemingway, who sometimes wrote articles for his high school newspaper using the pseudonym Ring Lardner, Jr. The two met during December 1928, thanks to Max Perkins, but did not become friends. Lardner's gift for dialogue heavily influenced the writer John O'Hara, who said he learned from reading Lardner "that if you wrote down speech as it is spoken truly, you produce true characters, and the opposite is also true: if your characters don't talk like people they aren't good characters" and added, "it's the attribute most lacking in American writers and almost totally lacking in the British."

J.D. Salinger referred to Lardner in two of his works," The Catcher in the Rye" and "Franny and Zooey". The protagonist says "My favorite author is my brother D.B. and my next favorite is Ring Lardner". Wayne C. Booth mentioned Lardner's famous short story "The Haircut" in his essay "Telling and Showing."

In his movie "Eight Men Out" (1988) about the Black Sox scandal, writer-director John Sayles portrayed Lardner as one of the clear-eyed observers who was not taken in by the conspiracy. In one scene, Lardner strolls through the White Sox train, singing a parody of the song "I'm Forever Blowing Bubbles", changed to "I'm Forever Throwing Ballgames".

In 2016, Lardner was inducted into the Chicago Literary Hall of Fame.

Harry Turtledove describes his short story "Batboy" as a Ring Lardner pastiche.

Neil Simon references Ring Lardner in his play Brighton Beach Memoirs.

John Lardner was a newspaperman, sports columnist, and magazine writer.

James Lardner, also a newspaperman, was killed in the Spanish Civil War fighting with the International Brigades.

Ring Lardner, Jr. was a screenwriter who was blacklisted after the Second World War as one of the Hollywood Ten, screenwriters who were incarcerated for contempt of Congress after refusing to answer questions posed by the House Un-American Activities Committee (HUAC). He won two Academy Awards for his screenplaysâone before his imprisonment and blacklisting (for Woman of the Year in 1942), and one after (for M*A*S*H in 1970). His book, "The Lardners, My Family Remembered" (), is a source of information on his father.

David Lardner worked for "The New Yorker" as a general reporter and war correspondent before he was killed by a landmine near Aachen, Germany on October 19, 1944, less than one month after his arrival in Europe.

Ring Lardner was a great-uncle to 1993 Pulitzer Prize winner George Lardner, Jr., a journalist at "The Washington Post" since 1963.






</doc>
<doc id="26417" url="https://en.wikipedia.org/wiki?curid=26417" title="River Clyde">
River Clyde

The River Clyde (, , , or "Watter o Clyde") is a river that flows into the Firth of Clyde in Scotland. It is the eighth-longest river in the United Kingdom, and the second-longest in Scotland. Traveling through the major city of Glasgow, it was an important river for shipbuilding and trade in the British Empire. To the Romans, it was "Clota", and in the early medieval Cumbric language, it was known as "Clud" or "Clut", and was central to the Kingdom of Strathclyde ("Teyrnas Ystrad Clut").

The Clyde is formed by the confluence of two streams, the Daer Water (the headwaters of which are dammed to form the Daer Reservoir) and the Potrail Water. The Southern Upland Way crosses both streams before they meet at Watermeetings () to form the River Clyde proper. At this point, the Clyde is only from Tweed's Well, the source of the River Tweed, and is a similar distance from Annanhead Hill, the source of the River Annan.

From there, it meanders northeastward before turning to the west, its flood plain used for many major roads in the area, until it reaches the town of Lanark. On the banks of the Clyde, the industrialists David Dale and Robert Owen built their mills and the model settlement of New Lanark. The mills harness the power of the Falls of Clyde, the most spectacular of which is Cora Linn. A hydroelectric power station still generates electricity here, although the mills are now a museum and World Heritage Site.
The river then makes its way north-west, past (although not through) the towns of Wishaw on its eastern side and Larkhall on its western, with the surrounding environment becoming increasingly suburban. Between the towns of Motherwell and Hamilton, the course of the river has been altered to create an artificial loch within Strathclyde Park. Part of the original course can still be seen, and lies between the island and the east shore of the loch. The river then flows through Blantyre and Bothwell, where the ruined Bothwell Castle stands on a defensible promontory.
Past Uddingston and into the southeast of Glasgow, the river begins to widen, meandering a course through Cambuslang, Rutherglen, and Dalmarnock. Flowing past Glasgow Green, the river is artificially straightened and widened through the centre, and although the new Clyde Arc now hinders access to the traditional Broomielaw dockland area, seagoing ships can still come upriver as far as Finnieston, where the PS "Waverley" docks. From there, it flows past the shipbuilding heartlands, through Govan, Partick, Whiteinch, Scotstoun, and Clydebank, all of which housed major shipyards, of which only two remain.

The river flows out west of Glasgow, past Renfrew, and under the Erskine Bridge past Dumbarton on the north shore to the sandbank at Ardmore Point between Cardross and Helensburgh. Opposite, on the south shore, the river continues past the last Lower Clyde shipyard at Port Glasgow to Greenock, where it reaches the Tail of the Bank as the river merges into the Firth of Clyde. A significant issue of oxygen depletion in the water column has occurred at the mouth of the River Clyde.

The valley of the Clyde was the focus for the G-BASE project from the British Geological Survey in the summer of 2010.

The success of the Clyde at the beginning of the Industrial Revolution was driven by the location of Glasgow, being a port facing the Americas. Tobacco and cotton trade began the drive in the early 18th century. However, the shallow Clyde was not navigable for the largest ocean-going ships, so cargo had to be transferred at Greenock or Port Glasgow to smaller ships to sail upstream into Glasgow itself.

In 1768, John Golborne advised the narrowing of the river and the increasing of the scour by the construction of rubble jetties and the dredging of sandbanks and shoals. A particular problem was the division of the river into two shallow channels by the Dumbuck shoal near Dumbarton. After James Watt's report on this in 1769, a jetty was constructed at Longhaugh Point to block off the southern channel. This being insufficient, a training wall called the Lang Dyke was built in 1773 on the Dumbuck shoal to stop water flowing over into the southern channel. In the late 18th and early 19th centuries, hundreds of jetties were built out from the banks between Dumbuck and the Broomielaw quay in Glasgow itself. In some cases, this resulted in an immediate deepening as the constrained water flow washed away the river bottom; in others, dredging was required.

In the mid-19th century, engineers took on a much greater dredging of the Clyde, removing millions of cubic feet of silt to deepen and widen the channel. The major stumbling block in the project was a massive geological intrusion known as Elderslie Rock. As a result, the work was not completed until the 1880s. At this time, the Clyde also became an important source of inspiration for artists, such as John Atkinson Grimshaw and James Kay, willing to depict the new industrial era and the modern world.

The completion of the dredging was well-timed; as steelworking grew in the city, the channel finally became navigable all the way up to Glasgow. Shipbuilding replaced trade as the major activity on the river, and shipbuilding companies were rapidly establishing themselves on the river. Soon, the Clyde gained a reputation for being the best location for shipbuilding in the British Empire, and grew to become the world's pre-eminent shipbuilding centre. "Clydebuilt" became an industry benchmark of quality, and the river's shipyards were given contracts for prestigious ocean-going liners, as well as warships, including the "Queen Mary" and the "Queen Elizabeth 2" in later years, all built in the town of Clydebank.

From the founding of the Scott family's shipyard at Greenock in 1712 to the present day, over 25,000 ships have been built on the River Clyde and its Firth and on the tributary River Kelvin and River Cart together with boatyards at Maryhill and Kirkintilloch on the Forth & Clyde Canal and Blackhill on the Monkland Canal. In the same time, an estimated over 300 firms have engaged in shipbuilding on Clydeside, although probably a peak of 30 to 40 at any one time.

The shipbuilding firms became household names on Clydeside, but also known around the world â Denny of Dumbarton, Scott of Greenock, Lithgows of Port Glasgow, Simon and Lobnitz of Renfrew, Alexander Stephen and Sons of Linthouse, Fairfield of Govan, Inglis of Pointhouse, Barclay Curle of Whiteinch, Connell and Yarrow of Scotstoun, to name but a few. Almost as famous were the engineering firms that supplied the machinery to drive these vessels, the boilers and pumps, and steering gear â Rankin & Blackmore, Hastie's and Kincaid's of Greenock, Rowan's of Finnieston, Weir's of Cathcart, Howden's of Tradeston, and Babcock & Wilcox of Renfrew.

One 'Clyde' shipyard was not even located on one of these waterways: Alley & MacLellan's Sentinel Works in Jessie Street at Polmadie is around half a mile from the Clyde, yet it is reputed to have constructed over 500 vessels, many of which were assembled then 'knocked down' to kit form for despatch to some far distant and remote location â one such vessel being the SS "Chauncy Maples", still in service on Lake Malawi. Clyde shipbuilding reached its peak in the years just before World War I, and over 370 ships were thought to have been completed in 1913.

The first recorded Clyde racing yacht, a 46-ton cutter, was built by Scotts of Greenock in 1803, and the great Scottish yacht designer William Fyfe did not start designing yachts until 1807. The first yacht club on the Clyde was the Northern Yacht Club, which appeared in 1824 and received its Royal Charter in 1831. The club was founded to organise and encourage the sport, and by 1825, Scottish and Irish clubs were racing against each other on the Clyde. However, yachting and yacht building did not really take off until the middle of the 19th century.

The Clyde became famous worldwide for its significant contribution to yachting and yachtbuilding, and was the home of many notable designers: William Fife III, Alfred Mylne, G. L. Watson, E. McGruer, and David Boyd. It was also the home of many famous yacht yards.

Robertson's Yard started repairing boats in a small workshop at Sandbank in 1876, and went on to become one of the foremost wooden boat builders on the Clyde. The 'golden years' of Robertson's yard were in the early 20th century, when they started building classic 12- and 15-m racing yachts. More than 55 boats were built by Robertson's in preparation for the First World War, and the yard remained busy even during the Great Depression in the 1930s, as many wealthy businessmen developed a passion for yacht racing on the Clyde. During World War II, the yard was devoted to Admiralty work, producing large, high-speed Fairmile Marine motor boats (motor torpedo boats and motor gun boats). After the war, the yard built the successful one-class Loch Longs and two David Boyd-designed 12-m challengers for the America's Cup: "Sceptre" (1958) and "Sovereign" (1964). Due to difficult business conditions in 1965, the yard was turned over to GRP production work (mainly Pipers and Etchells) until it closed in 1980. During its 104-year history, Robertson's Yard built 500 boats, many of which are still sailing today.

Other notable boatyards on the Clyde, both situated on the Rosneath peninsula on the banks of the Gare Loch within half a mile of each other, were Silvers (1910-1970) and McGruers (1910-1973). Both yards built many well-known and classic yachts, some of which are still sailing today. McGruers built over 700 boats but both yards are no longer in business as boatbuilders.

The downfall of the Clyde as a major industrial centre came during and after World War II. Clydebank in particular was targeted by the Luftwaffe and sustained heavy damage. The immediate postwar period had a severe reduction in warship orders, which was balanced by a prolonged boom in merchant shipbuilding. By the end of the 1950s, however, the rise of other shipbuilding nations, recapitalised and highly productive, made many European yards uncompetitive. Several Clydeside yards booked a series of loss-making contracts in the hope of weathering the storm. However, by the mid-1960s, shipbuilding on the Clyde was becoming increasingly uneconomical and potentially faced collapse. This culminated in the closure of Harland and Wolff's Linthouse yard and a bankruptcy crisis facing Fairfields of Govan. The government responded by creating the Upper Clyde Shipbuilders consortium. After the consortium's controversial collapse in 1971, the Labour government of James Callaghan later passed the Aircraft and Shipbuilding Industries Act, which nationalised most of the Clyde's shipyards and grouped them with other major British shipyards such as British Shipbuilders.

Today, two major shipyards remain in operation on the Upper Clyde; they are owned by a naval defence contractor, BAE Systems Surface Ships, which specialises in the design and construction of technologically advanced warships for the Royal Navy and other navies around the world. The two yards are the former Yarrow yard at Scotstoun and Fairfields at Govan. Also, the King George V Dock is operated by the Clyde Port Authority. On the Lower Clyde, the Scottish Government owned Ferguson Shipbuilders at Port Glasgow is the last survivor of the many shipyards that once dominated Port Glasgow and Greenock â its mainstay being the construction of car ferries.

The Clyde Waterfront Regeneration project is expected to attract investment of up to Â£5.6bn in the area from Glasgow Green to Dumbarton. Market gardens and garden centres have grown up on the fertile plains of the Clyde Valley. Tourism has also brought many back to the riverside, especially in Glasgow, where former docklands have given way to housing and amenities on the banks in the city, for instance, the Glasgow Harbour project, the Glasgow Science Centre, and the creation of the Scottish Exhibition and Conference Centre. With the migration of the commercial Port of Glasgow downstream to the deeper waters of the Firth of Clyde, the river has been extensively cleaned up, once having a very poor reputation for pollution and sewage, to make it suitable for recreational use.

The Clyde Walkway is a foot and mountain bike path that follows the course of the Clyde between Glasgow and New Lanark. It was completed in 2005, and is now designated as one of Scotland's Great Trails by Scottish Natural Heritage.

Organic chemical pollution contained in sediments of the Clyde estuary have been evaluated by British Geological Survey.<ref name="Chemical Signatures of the Anthropocene in the Clyde Estuary, UK: Sediment hosted Pb, 207/206Pb, Polyaromatic Hydrocarbon (PAH) and Polychlorinated Biphenyl (PCB) Pollution Records."></ref> Surface sediments from the Glasgow reaches of the Clyde, Cunningar to Milton, contained polyaromatic hydrocarbons (PAH) from 630Â Âµg/kg to 23,711Â Âµg/kg and polychlorinated biphenyl (PCB) in the range of 5 to 130.5Â Âµg/kg, which classifies these sediments as non-toxic. However, a later study showed PCB concentrations as high as 5,797Â Âµg/kg, which is above published threshold levels for the chlorinated compounds. Comparison of individual PAH compounds with different thermal stabilities shows that the source of PAH pollution in the Clyde changes downriver. PAH in the inner Clyde (Cunningar to Milton) are from combustion sources (vehicle exhaust, coal burning), whereas PAH in the outer Clyde are from petroleum spills

The amount and type of sedimentary pollution contained in the Clyde is closely related to the area's industrial history. Assessment of the changing type of pollution through time (1750 to 2002) has been gained through the collection of seven sediment cores of one metre's depth which were dated using lead concentrations and changing lead isotope ratios. The sediments showed a long but declining history of coal usage and an increasing reliance on petroleum fuels from about the 1950s. Thereafter, declining hydrocarbon pollution was followed by the onset (1950s), peak (1965â1977), and decline (after 1980s) in total PCB concentrations.

Although pollution from heavy industry and power generation is falling, evidence indicates that man-made pollution from new synthetic compounds in electrical products and textiles is on the increase. The amounts of 16 polybrominated diphenyl ether (PBDE) compounds used as flame retardants in televisions, computers, and furniture upholstery were measured in sediment cores collected from six sites between Princes Dock and Greenock. Comparison of the amounts of PBDE compounds revealed a decline in certain compounds in line with the European ban on production of mixtures containing environmentally harmful PBDE with eight and nine bromine atoms and a rise in the less harmful mixture composed of ten bromine atoms.

The Clyde is central to the "Para Handy" novels of Neil Munro, and subsequent adaptations, and features in novels by Alasdair Gray, Matthew Fitt, and Robin Jenkins. It appears in the "Ossian" poetry of James Macpherson, as well as the works of John Wilson, William McGonagall, Edwin Morgan, Norman McCaig, Douglas Dunn and W.S. Graham. It features in artworks by, amongst others, William McTaggart, J.M.W. Turner, Robert Salmon, John Atkinson Grimshaw, Stanley Spencer, and George Wyllie.

The Clyde appears prominently in the films "Young Adam", "Sweet Sixteen", "Just a Boys' Game", and "Down Where The Buffalo Go", and was the subject of the Academy Award-winning film documentary "Seawards the Great Ships". It is referenced in the traditional folk songs "Clyde's Water" and "Black Is the Color (of My True Love's Hair)", as well as "Song of the Clyde", popularised by Kenneth McKellar.




</doc>
<doc id="26418" url="https://en.wikipedia.org/wiki?curid=26418" title="Reactor">
Reactor




</doc>
<doc id="26421" url="https://en.wikipedia.org/wiki?curid=26421" title="List of reproductive issues">
List of reproductive issues

List of reproductive issues may refer to:


</doc>
<doc id="26426" url="https://en.wikipedia.org/wiki?curid=26426" title="Rational root theorem">
Rational root theorem

In algebra, the rational root theorem (or rational root test, rational zero theorem, rational zero test or theorem) states a constraint on rational solutions of a polynomial equation

with integer coefficients formula_2 and formula_3. Solutions of the equation are also called roots or zeroes of the polynomial on the left side.

The theorem states that each rational solution , written in lowest terms so that and are relatively prime, satisfies:



The rational root theorem is a special case (for a single linear factor) of Gauss's lemma on the factorization of polynomials. The integral root theorem is the special case of the rational root theorem when the leading coefficient isÂ .

The theorem is used to find all rational roots of a polynomial, if any. It gives a finite number of possible fractions which can be checked to see if they are roots. If a rational root is found, a linear polynomial can be factored out of the polynomial using polynomial long division, resulting in a polynomial of lower degree whose roots are also roots of the original polynomial.

The general cubic equation

with integer coefficients has three solutions in the complex plane. If the rational root test finds no rational solutions, then the only way to express the solutions algebraically uses cube roots. But if the test finds a rational solution , then factoring out leaves a quadratic polynomial whose two roots, found with the quadratic formula, are the remaining two roots of the cubic, avoiding cube roots.

Let 

Suppose for some coprime :

Now multiply both sides by . 

By shifting the constant term (the term containing ) to the right side, and factoring out on the left side, produces

Thus, divides . But is coprime to and therefore to , so by Euclid's lemma must divide the remaining factor of the product.

On the other hand, shifting the leading term to the right side and factoring out on the left side, gives

Reasoning as before, it follows that divides .

Should there be a nontrivial factor dividing all the coefficients of the polynomial, then one can divide by the greatest common divisor of the coefficients so as to obtain a primitive polynomial in the sense of Gauss's lemma; this does not alter the set of rational roots and only strengthens the divisibility conditions. That lemma says that if the polynomial factors in , then it also factors in as a product of primitive polynomials. Now any rational root corresponds to a factor of degree 1 in of the polynomial, and its primitive representative is then , assuming that and are coprime. But any multiple in of has leading term divisible by and constant term divisible by , which proves the statement. This argument shows that more generally, any irreducible factor of can be supposed to have integer coefficients, and leading and constant coefficients dividing the corresponding coefficients ofÂ .

In the polynomial

any rational root fully reduced would have to have a numerator that divides evenly into 1 and a denominator that divides evenly into 2. Hence the only possible rational roots are Â±1/2 and Â±1; since neither of these equates the polynomial to zero, it has no rational roots.

In the polynomial

the only possible rational roots would have a numerator that divides 6 and a denominator that divides 1, limiting the possibilities to Â±1, Â±2, Â±3, and Â±6. Of these, 1, 2, and â3 equate the polynomial to zero, and hence are its rational roots. (In fact these are its only roots since a cubic has only three roots; in general, a polynomial could have some rational and some irrational roots.)

Every rational root of the polynomial
must be among the numbers symbolically indicated by:
These 8 root candidates can be tested by evaluating , for example using Horner's method. It turns out there is exactly one with . 

This process may be made more efficient: if , it can be used to shorten the list of remaining candidates. For example, does not work, as . Substituting yields a polynomial inÂ  with constant term , while the coefficient of remains the same as the coefficient of . Applying the rational root theorem thus yields the possible roots formula_13, so that 

True roots must occur on both lists, so list of rational root candidates has shrunk to just and 

If rational roots are found, Horner's method will also yield a polynomial of degree whose roots, together with the rational roots, are exactly the roots of the original polynomial. If none of the candidates is a solution, there can be no rational solution.





</doc>
<doc id="26427" url="https://en.wikipedia.org/wiki?curid=26427" title="Round Table">
Round Table

The Round Table is King Arthur's famed table in the Arthurian legend, around which he and his knights congregate. As its name suggests, it has no head, implying that everyone who sits there has equal status. The table was first described in 1155 by Wace, who relied on previous depictions of Arthur's fabulous retinue. The symbolism of the Round Table developed over time; by the close of the 12th century it had come to represent the chivalric order associated with Arthur's court, the Knights of the Round Table.

Though the Round Table is not mentioned in the earliest accounts, tales of Arthur having a marvelous court made up of many prominent warriors is ancient. Geoffrey of Monmouth, in his "Historia Regum Britanniae" (composed c. 1136) says that, after establishing peace throughout Britain, Arthur "increased his personal entourage by inviting very distinguished men from far-distant kingdoms to join it." The code of chivalry so important in later medieval romance figures in as well, as Geoffrey says Arthur established "such a code of courtliness in his household that he inspired peoples living far away to imitate him." 

Arthur's court was well known to Welsh storytellers; in the romance "Culhwch and Olwen", the protagonist Culhwch invokes the names of 225 individuals affiliated with Arthur. The fame of Arthur's entourage became so prominent in Welsh tradition that in the later additions to the Welsh Triads, the formula tying named individuals to "Arthur's Court" in the triad titles began to supersede the older "Island of Britain" formula. Though the code of chivalry crucial to later continental romances dealing with the Round Table is mostly absent from the Welsh material, some passages of "Culhwch and Olwen" seem to reference it. For instance, Arthur explains the ethos of his court, saying "[w]e are nobles as long as we are sought out: the greater the bounty we may give, the greater our nobility, fame and honour."

Though no Round Table appears in the early Welsh texts, Arthur is associated with various items of household furniture. The earliest of these is Saint Carannog's mystical floating altar in that saint's 12th century "Vita". In the story Arthur has found the altar and tries unsuccessfully to use it as a table; he returns it to Carannog in exchange for the saint ridding the land of a meddlesome dragon. Elements of Arthur's household figure into local topographical folklore throughout Britain as early as the early 12th century, with various landmarks being named "Arthur's Seat", "Arthur's Oven", and "Arthur's Bed-chamber". 

A henge at Eamont Bridge near Penrith, Cumbria is known as "King Arthur's Round Table". The still-visible Roman amphitheatre at Caerleon has been associated with the Round Table, and it has been suggested as a possible source for the legend. Following archaeological discoveries at the Roman ruins in Chester, some writers suggested that the Chester Roman Amphitheatre was the true prototype of the Round Table; however, the English Heritage Commission, acting as consultants to a History Channel documentary in which the claim was made, stated that there was no archaeological basis to the story.

The Round Table first appeared in Wace's "Roman de Brut", a Norman language adaptation of Geoffrey's "Historia" finished in 1155. Wace says Arthur created the Round Table to prevent quarrels among his barons, none of whom would accept a lower place than the others. Layamon added to the story when he adapted Wace's work into the Middle English "Brut" in the early 13th century, saying that the quarrel between Arthur's vassals led to violence at a Yuletide feast. In response, a Cornish carpenter built an enormous but easily transportable Round Table to prevent further dispute. Wace claims he was not the source of the Round Table; both he and Layamon credited it instead to the Bretons. Some scholars have doubted this claim, while others believe it may be true. There is some similarity between the chroniclers' description of the Round Table and a custom recorded in Celtic stories, in which warriors sit in a circle around the king or lead warrior, in some cases feuding over the order of precedence as in Layamon. There is a possibility that Wace, contrary to his own claims, derived Arthur's round table not from any Breton source, but rather from medieval biographies of Charlemagneânotably Einhard's "Vita Caroli" and Notker the Stammerer's "De Carolo Magno"âin which the king is said to have possessed a round table decorated with a map of Rome.

The Round Table takes on new dimensions in the romances of the late 12th and early 13th century, where it becomes a symbol of the famed order of chivalry which flourishes under Arthur. In Robert de Boron's "Merlin", written around 1200, the magician Merlin creates the Round Table in imitation of the table of the Last Supper and of Joseph of Arimathea's Holy Grail table. This table, here made for Arthur's father Uther Pendragon rather than Arthur himself, has twelve seats and one empty place to mark the betrayal of Judas. This seat must remain empty until the coming of the knight who will achieve the Grail. The Didot "Perceval", a prose continuation of Robert's work, takes up the story, and the knight Percival sits in the seat and initiates the Grail quest.

The prose cycles of the 13th century, the Lancelot-Grail (Vulgate) Cycle and the Post-Vulgate Cycle, further adapt the chivalric attributes of the Round Table. Here it is the perfect knight Galahad, rather than Percival, who assumes the empty seat, now called the Siege Perilous. Galahad's arrival marks the start of the Grail quest as well as the end of the Arthurian era. In these works the Round Table is kept by King Leodegrance of Cameliard after Uther's death; Arthur inherits it when he marries Leodegrance's daughter Guinevere. Other versions treat the Round Table differently, for instance Arthurian works from Italy like "La Tavola Ritonda" ("The Round Table") often distinguish between the knights of the "Old Table" of Uther's time and those of Arthur's "New Table". In the Post-Vulgate, the Table is eventually destroyed by King Mark during his invasion of Logres after the deaths of Arthur and almost all of the Knights, many of whom in fact had killed each other, especially in internal conflicts at the end of the cycle. 

During the Middle Ages, festivals called Round Tables were celebrated throughout Europe in imitation of Arthur's court. These events featured jousting, dancing, and feasting, and in some cases attending knights assumed the identities of Arthur's entourage.

The artifact is known as the "Winchester Round Table", a large tabletop hanging in Winchester Castle bearing the names of various knights of Arthur's court, was probably created for a Round Table tournament. The current paintwork is late; it was done by order of Henry VIII of England for Holy Roman Emperor Charles V's 1522 state visit, and depicts Henry himself sitting in Arthur's seat above a Tudor rose.

The table itself is considerably older; dendrochronology calculates the date of construction to 1250â1280âduring the reign of Edward Iâusing timber from store felled over a period of years. Edward was an Arthurian enthusiast who attended at least five Round Tables and hosted one himself in 1299, which may have been the occasion for the creation of the Winchester Round Table. Martin Biddle, from an examination of Edward's financial accounts, links it instead with a tournament Edward held near Winchester on 20 April 1290, to mark the betrothal of one of his daughters.




</doc>
<doc id="26428" url="https://en.wikipedia.org/wiki?curid=26428" title="Rosetta Stone">
Rosetta Stone

The Rosetta Stone is a granodiorite stele discovered in 1799 which is inscribed with three versions of a decree issued in Memphis, Egypt in 196 BC during the Ptolemaic dynasty on behalf of King Ptolemy V Epiphanes. The top and middle texts are in Ancient Egyptian using hieroglyphic and Demotic scripts respectively, while the bottom is in Ancient Greek. The decree has only minor differences among the three versions, so the Rosetta Stone became key to deciphering Egyptian hieroglyphs, thereby opening a window into ancient Egyptian history.

The stone was carved during the Hellenistic period and is believed to have originally been displayed within a temple, possibly at nearby Sais. It was probably moved in late antiquity or during the Mameluk period, and was eventually used as building material in the construction of Fort Julien near the town of Rashid (Rosetta) in the Nile Delta. It was discovered there in July 1799 by French soldier Pierre-FranÃ§ois Bouchard during the Napoleonic campaign in Egypt. It was the first Ancient Egyptian bilingual text recovered in modern times, and it aroused widespread public interest with its potential to decipher this previously untranslated hieroglyphic script. Lithographic copies and plaster casts began circulating among European museums and scholars. The British defeated the French and took the stone to London under the Capitulation of Alexandria in 1801. It has been on public display at the British Museum almost continuously since 1802 and is the most visited object there.

Study of the decree was already under way when the first full translation of the Greek text appeared in 1803. Jean-FranÃ§ois Champollion announced the transliteration of the Egyptian scripts in Paris in 1822; it took longer still before scholars were able to read Ancient Egyptian inscriptions and literature confidently. Major advances in the decoding were recognition that the stone offered three versions of the same text (1799); that the demotic text used phonetic characters to spell foreign names (1802); that the hieroglyphic text did so as well, and had pervasive similarities to the demotic (1814); and that phonetic characters were also used to spell native Egyptian words (1822â1824).

Three other fragmentary copies of the same decree were discovered later, and several similar Egyptian bilingual or trilingual inscriptions are now known, including three slightly earlier Ptolemaic decrees: the Decree of Alexandria in 243 BC, the Decree of Canopus in 238 BC, and the Memphis decree of PtolemyÂ  IV, c. 218 BC. The Rosetta Stone is no longer unique, but it was the essential key to the modern understanding of ancient Egyptian literature and civilisation. The term "Rosetta Stone" is now used to refer to the essential clue to a new field of knowledge.

The Rosetta Stone is listed as "a stone of black granodiorite, bearing three inscriptionsÂ ... found at Rosetta" in a contemporary catalogue of the artefacts discovered by the French expedition and surrendered to British troops in 1801. At some period after its arrival in London, the inscriptions were coloured in white chalk to make them more legible, and the remaining surface was covered with a layer of carnauba wax designed to protect it from visitors' fingers. This gave a dark colour to the stone that led to its mistaken identification as black basalt. These additions were removed when the stone was cleaned in 1999, revealing the original dark grey tint of the rock, the sparkle of its crystalline structure, and a pink vein running across the top left corner. Comparisons with the Klemm collection of Egyptian rock samples showed a close resemblance to rock from a small granodiorite quarry at Gebel Tingar on the west bank of the Nile, west of Elephantine in the region of Aswan; the pink vein is typical of granodiorite from this region.

The Rosetta Stone is high at its highest point, wide, and thick. It weighs approximately . It bears three inscriptions: the top register in Ancient Egyptian hieroglyphs, the second in the Egyptian demotic script, and the third in Ancient Greek. The front surface is polished and the inscriptions lightly incised on it; the sides of the stone are smoothed, but the back is only roughly worked, presumably because this would have not been visible when it was erected.

The Rosetta Stone is a fragment of a larger stele. No additional fragments were found in later searches of the Rosetta site. Owing to its damaged state, none of the three texts is absolutely complete. The top register, composed of Egyptian hieroglyphs, suffered the most damage. Only the last 14 lines of the hieroglyphic text can be seen; all of them are broken on the right side, and 12 of them on the left. Below it, the middle register of demotic text has survived best; it has 32 lines, of which the first 14 are slightly damaged on the right side. The bottom register of Greek text contains 54 lines, of which the first 27 survive in full; the rest are increasingly fragmentary due to a diagonal break at the bottom right of the stone.

The stele was erected after the coronation of King PtolemyÂ V and was inscribed with a decree that established the divine cult of the new ruler. The decree was issued by a congress of priests who gathered at Memphis. The date is given as "4 Xandikos" in the Macedonian calendar and "18 Mekhir" in the Egyptian calendar, which corresponds to . The year is stated as the ninth year of PtolemyÂ V's reign (equated with 197/196 BC), which is confirmed by naming four priests who officiated in that year: Aetos son of Aetos was priest of the divine cults of Alexander the Great and the five Ptolemies down to PtolemyÂ V himself; the other three priests named in turn in the inscription are those who led the worship of Berenice Euergetis (wife of PtolemyÂ III), Arsinoe Philadelphos (wife and sister of PtolemyÂ II), and Arsinoe Philopator, mother of PtolemyÂ V. However, a second date is also given in the Greek and hieroglyphic texts, corresponding to , the official anniversary of Ptolemy's coronation. The demotic text conflicts with this, listing consecutive days in March for the decree and the anniversary. It is uncertain why this discrepancy exists, but it is clear that the decree was issued in 196 BC and that it was designed to re-establish the rule of the Ptolemaic kings over Egypt.

The decree was issued during a turbulent period in Egyptian history. PtolemyÂ V Epiphanes reigned from 204 to 181 BC, the son of PtolemyÂ IV Philopator and his wife and sister Arsinoe. He had become ruler at the age of five after the sudden death of both of his parents, who were murdered in a conspiracy that involved PtolemyÂ IV's mistress Agathoclea, according to contemporary sources. The conspirators effectively ruled Egypt as PtolemyÂ V's guardians until a revolt broke out two years later under general Tlepolemus, when Agathoclea and her family were lynched by a mob in Alexandria. Tlepolemus, in turn, was replaced as guardian in 201 BC by Aristomenes of Alyzia, who was chief minister at the time of the Memphis decree.

Political forces beyond the borders of Egypt exacerbated the internal problems of the Ptolemaic kingdom. AntiochusÂ III the Great and PhilipÂ V of Macedon had made a pact to divide Egypt's overseas possessions. Philip had seized several islands and cities in Caria and Thrace, while the Battle of Panium (198 BC) had resulted in the transfer of Coele-Syria, including Judaea, from the Ptolemies to the Seleucids. Meanwhile, in the south of Egypt, there was a long-standing revolt that had begun during the reign of PtolemyÂ IV, led by Horwennefer and by his successor Ankhwennefer. Both the war and the internal revolt were still ongoing when the young PtolemyÂ V was officially crowned at Memphis at the age of 12 (seven years after the start of his reign) and when, just over a year later, the Memphis decree was issued.

Stelae of this kind, which were established on the initiative of the temples rather than that of the king, are unique to Ptolemaic Egypt. In the preceding Pharaonic period it would have been unheard of for anyone but the divine rulers themselves to make national decisions: by contrast, this way of honoring a king was a feature of Greek cities. Rather than making his eulogy himself, the king had himself glorified and deified by his subjects or representative groups of his subjects. The decree records that PtolemyÂ V gave a gift of silver and grain to the temples. It also records that there was particularly high flooding of the Nile in the eighth year of his reign, and he had the excess waters dammed for the benefit of the farmers. In return the priesthood pledged that the king's birthday and coronation days would be celebrated annually and that all the priests of Egypt would serve him alongside the other gods. The decree concludes with the instruction that a copy was to be placed in every temple, inscribed in the "language of the gods" (Egyptian hieroglyphs), the "language of documents" (Demotic), and the "language of the Greeks" as used by the Ptolemaic government.

Securing the favour of the priesthood was essential for the Ptolemaic kings to retain effective rule over the populace. The High Priests of Memphisâwhere the king was crownedâwere particularly important, as they were the highest religious authorities of the time and had influence throughout the kingdom. Given that the decree was issued at Memphis, the ancient capital of Egypt, rather than Alexandria, the centre of government of the ruling Ptolemies, it is evident that the young king was anxious to gain their active support. Thus, although the government of Egypt had been Greek-speaking ever since the conquests of Alexander the Great, the Memphis decree, like the three similar earlier decrees, included texts in Egyptian to show its connection to the general populace by way of the literate Egyptian priesthood.

There can be no one definitive English translation of the decree, not only because modern understanding of the ancient languages continues to develop, but also because of the minor differences between the three original texts. Older translations by E. A. Wallis Budge (1904, 1913) and Edwyn R. Bevan (1927) are easily available but are now outdated, as can be seen by comparing them with the recent translation by R. S. Simpson, which is based on the demotic text and can be found online, or, best of all, with the modern translations of all three texts, with introduction and facsimile drawing, that were published by Quirke and Andrews in 1989.

The stele was almost certainly not originally placed at Rashid (Rosetta) where it was found, but more likely came from a temple site farther inland, possibly the royal town of Sais. The temple from which it originally came was probably closed around AD 392 when Roman emperor Theodosius I ordered the closing of all non-Christian temples of worship. The original stele broke at some point, its largest piece becoming what we now know as the Rosetta Stone. Ancient Egyptian temples were later used as quarries for new construction, and the Rosetta Stone probably was re-used in this manner. Later it was incorporated in the foundations of a fortress constructed by the Mameluke Sultan Qaitbay (c. 1416/18â1496) to defend the Bolbitine branch of the Nile at Rashid. There it lay for at least another three centuries until its rediscovery.

Three other inscriptions relevant to the same Memphis decree have been found since the discovery of the Rosetta Stone: the Nubayrah Stele, a stele found in Elephantine and Noub Taha, and an inscription found at the Temple of Philae (on the Philae obelisk). Unlike the Rosetta Stone, the hieroglyphic texts of these inscriptions were relatively intact. The Rosetta Stone had been deciphered long before they were found, but later Egyptologists have used them to refine the reconstruction of the hieroglyphs that must have been used in the lost portions of the hieroglyphic text on the Rosetta Stone.

Napoleon's 1798 campaign in Egypt inspired a burst of Egyptomania in Europe, and especially France. A corps of 167 technical experts ("savants"), known as the "Commission des Sciences et des Arts", accompanied the French expeditionary army to Egypt. On 1799, French soldiers under the command of Colonel d'Hautpoul were strengthening the defences of Fort Julien, a couple of miles north-east of the Egyptian port city of Rosetta (modern-day Rashid). Lieutenant Pierre-FranÃ§ois Bouchard spotted a slab with inscriptions on one side that the soldiers had uncovered. He and d'Hautpoul saw at once that it might be important and informed General Jacques-FranÃ§ois Menou, who happened to be at Rosetta. The find was announced to Napoleon's newly founded scientific association in Cairo, the Institut d'Ãgypte, in a report by Commission member Michel Ange Lancret noting that it contained three inscriptions, the first in hieroglyphs and the third in Greek, and rightly suggesting that the three inscriptions were versions of the same text. Lancret's report, dated 1799, was read to a meeting of the Institute soon after . Bouchard, meanwhile, transported the stone to Cairo for examination by scholars. Napoleon himself inspected what had already begun to be called "la Pierre de Rosette", the Rosetta Stone, shortly before his return to France in August 1799.

The discovery was reported in September in "Courrier de l'Ãgypte", the official newspaper of the French expedition. The anonymous reporter expressed a hope that the stone might one day be the key to deciphering hieroglyphs. In 1800 three of the Commission's technical experts devised ways to make copies of the texts on the stone. One of these experts was Jean-Joseph Marcel, a printer and gifted linguist, who is credited as the first to recognise that the middle text was written in the Egyptian demotic script, rarely used for stone inscriptions and seldom seen by scholars at that time, rather than Syriac as had originally been thought. It was artist and inventor Nicolas-Jacques ContÃ© who found a way to use the stone itself as a printing block to reproduce the inscription. A slightly different method was adopted by Antoine Galland. The prints that resulted were taken to Paris by General Charles Dugua. Scholars in Europe were now able to see the inscriptions and attempt to read them.

After Napoleon's departure, French troops held off British and Ottoman attacks for another 18 months. In March 1801, the British landed at Aboukir Bay. Menou was now in command of the French expedition. His troops, including the Commission, marched north towards the Mediterranean coast to meet the enemy, transporting the stone along with many other antiquities. He was defeated in battle, and the remnant of his army retreated to Alexandria where they were surrounded and besieged, the stone now inside the city. Menou surrendered on August 30.

After the surrender, a dispute arose over the fate of the French archaeological and scientific discoveries in Egypt, including the artefacts, biological specimens, notes, plans, and drawings collected by the members of the commission. Menou refused to hand them over, claiming that they belonged to the Institute. British General John Hely-Hutchinson refused to end the siege until Menou gave in. Scholars Edward Daniel Clarke and William Richard Hamilton, newly arrived from England, agreed to examine the collections in Alexandria and claimed to have found many artefacts that the French had not revealed. In a letter home, Clarke said that "we found much more in their possession than was represented or imagined".

Hutchinson claimed that all materials were property of the British Crown, but French scholar Ãtienne Geoffroy Saint-Hilaire told Clarke and Hamilton that the French would rather burn all their discoveries than turn them over, referring ominously to the destruction of the Library of Alexandria. Clarke and Hamilton pleaded the French scholars' case to Hutchinson, who finally agreed that items such as natural history specimens would be considered the scholars' private property. Menou quickly claimed the stone, too, as his private property. Hutchinson was equally aware of the stone's unique value and rejected Menou's claim. Eventually an agreement was reached, and the transfer of the objects was incorporated into the Capitulation of Alexandria signed by representatives of the British, French, and Ottoman forces.

It is not clear exactly how the stone was transferred into British hands, as contemporary accounts differ. Colonel Tomkyns Hilgrove Turner, who was to escort it to England, claimed later that he had personally seized it from Menou and carried it away on a gun-carriage. In a much more detailed account, Edward Daniel Clarke stated that a French "officer and member of the Institute" had taken him, his student John Cripps, and Hamilton secretly into the back streets behind Menou's residence and revealed the stone hidden under protective carpets among Menou's baggage. According to Clarke, their informant feared that the stone might be stolen if French soldiers saw it. Hutchinson was informed at once and the stone was taken awayâpossibly by Turner and his gun-carriage.

Turner brought the stone to England aboard the captured French frigate HMS "Egyptienne", landing in Portsmouth in February 1802. His orders were to present it and the other antiquities to King GeorgeÂ III. The King, represented by War Secretary Lord Hobart, directed that it should be placed in the British Museum. According to Turner's narrative, he and Hobart agreed that the stone should be presented to scholars at the Society of Antiquaries of London, of which Turner was a member, before its final deposit in the museum. It was first seen and discussed there at a meeting on 1802.
In 1802, the Society created four plaster casts of the inscriptions, which were given to the universities of Oxford, Cambridge and Edinburgh and to Trinity College Dublin. Soon afterwards, prints of the inscriptions were made and circulated to European scholars. Before the end of 1802, the stone was transferred to the British Museum, where it is located today. New inscriptions painted in white on the left and right edges of the slab stated that it was "Captured in Egypt by the British Army in 1801" and "Presented by King GeorgeÂ III".

The stone has been exhibited almost continuously in the British Museum since June 1802. During the middle of the 19th century, it was given the inventory number "EA 24", "EA" standing for "Egyptian Antiquities". It was part of a collection of ancient Egyptian monuments captured from the French expedition, including a sarcophagus of NectaneboÂ II (EA 10), the statue of a high priest of Amun (EA 81), and a large granite fist (EA 9). The objects were soon discovered to be too heavy for the floors of Montagu House (the original building of The British Museum), and they were transferred to a new extension that was added to the mansion. The Rosetta Stone was transferred to the sculpture gallery in 1834 shortly after Montagu House was demolished and replaced by the building that now houses the British Museum. According to the museum's records, the Rosetta Stone is its most-visited single object, a simple image of it was the museum's best selling postcard for several decades, and a wide variety of merchandise bearing the text from the Rosetta Stone (or replicating its distinctive shape) is sold in the museum shops.

The Rosetta Stone was originally displayed at a slight angle from the horizontal, and rested within a metal cradle that was made for it, which involved shaving off very small portions of its sides to ensure that the cradle fitted securely. It originally had no protective covering, and it was found necessary by 1847 to place it in a protective frame, despite the presence of attendants to ensure that it was not touched by visitors. Since 2004 the conserved stone has been on display in a specially built case in the centre of the Egyptian Sculpture Gallery. A replica of the Rosetta Stone is now available in the King's Library of the British Museum, without a case and free to touch, as it would have appeared to early 19th-century visitors.

The museum was concerned about heavy bombing in London towards the end of the First World War in 1917, and the Rosetta Stone was moved to safety, along with other portable objects of value. The stone spent the next two years below ground level in a station of the Postal Tube Railway at Mount Pleasant near Holborn. Other than during wartime, the Rosetta Stone has left the British Museum only once: for one month in October 1972, to be displayed alongside Champollion's "Lettre" at the Louvre in Paris on the 150th anniversary of the letter's publication. Even when the Rosetta Stone was undergoing conservation measures in 1999, the work was done in the gallery so that it could remain visible to the public.

Prior to the discovery of the Rosetta Stone and its eventual decipherment, the ancient Egyptian language and script had not been understood since shortly before the fall of the Roman Empire. The usage of the hieroglyphic script had become increasingly specialised even in the later Pharaonic period; by the 4th century AD, few Egyptians were capable of reading them. Monumental use of hieroglyphs ceased as temple priesthoods died out and Egypt was converted to Christianity; the last known inscription is dated to , found at Philae and known as the Graffito of Esmet-Akhom. The last demotic text, also from Philae, was written in 452.

Hieroglyphs retained their pictorial appearance, and classical authors emphasised this aspect, in sharp contrast to the Greek and Roman alphabets. In the 5th century, the priest Horapollo wrote "Hieroglyphica", an explanation of almost 200 glyphs. His work was believed to be authoritative, yet it was misleading in many ways, and this and other works were a lasting impediment to the understanding of Egyptian writing. Later attempts at decipherment were made by Arab historians in medieval Egypt during the 9th and 10th centuries. Dhul-Nun al-Misri and Ibn Wahshiyya were the first historians to study hieroglyphs, by comparing them to the contemporary Coptic language used by Coptic priests in their time. The study of hieroglyphs continued with fruitless attempts at decipherment by European scholars, notably Johannes Goropius Becanus in the 16th century, Athanasius Kircher in the 17th, and Georg ZoÃ«ga in the 18th. The discovery of the Rosetta Stone in 1799 provided critical missing information, gradually revealed by a succession of scholars, that eventually allowed Jean-FranÃ§ois Champollion to solve the puzzle that Kircher had called the riddle of the Sphinx.

The Greek text on the Rosetta Stone provided the starting point. Ancient Greek was widely known to scholars, but they were not familiar with details of its use in the Hellenistic period as a government language in Ptolemaic Egypt; large-scale discoveries of Greek papyri were a long way in the future. Thus, the earliest translations of the Greek text of the stone show the translators still struggling with the historical context and with administrative and religious jargon. Stephen Weston verbally presented an English translation of the Greek text at a Society of Antiquaries meeting in April 1802.

Meanwhile, two of the lithographic copies made in Egypt had reached the Institut de France in Paris in 1801. There, librarian and antiquarian Gabriel de La Porte du Theil set to work on a translation of the Greek, but he was dispatched elsewhere on Napoleon's orders almost immediately, and he left his unfinished work in the hands of colleague Hubert-Pascal Ameilhon. Ameilhon produced the first published translations of the Greek text in 1803, in both Latin and French to ensure that they would circulate widely. At Cambridge, Richard Porson worked on the missing lower right corner of the Greek text. He produced a skilful suggested reconstruction, which was soon being circulated by the Society of Antiquaries alongside its prints of the inscription. At almost the same moment, Christian Gottlob Heyne in GÃ¶ttingen was making a new Latin translation of the Greek text that was more reliable than Ameilhon's and was first published in 1803. It was reprinted by the Society of Antiquaries in a special issue of its journal "Archaeologia" in 1811, alongside Weston's previously unpublished English translation, Colonel Turner's narrative, and other documents.

At the time of the stone's discovery, Swedish diplomat and scholar Johan David Ãkerblad was working on a little-known script of which some examples had recently been found in Egypt, which came to be known as demotic. He called it "cursive Coptic" because he was convinced that it was used to record some form of the Coptic language (the direct descendant of Ancient Egyptian), although it had few similarities with the later Coptic script. French Orientalist Antoine-Isaac Silvestre de Sacy had been discussing this work with Ãkerblad when he received one of the early lithographic prints of the Rosetta Stone in 1801 from Jean-Antoine Chaptal, French minister of the interior. He realised that the middle text was in this same script. He and Ãkerblad set to work, both focusing on the middle text and assuming that the script was alphabetical. They attempted to identify the points where Greek names ought to occur within this unknown text, by comparing it with the Greek. In 1802, Silvestre de Sacy reported to Chaptal that he had successfully identified five names (""Alexandros"", ""Alexandreia"", ""Ptolemaios"", ""Arsinoe"", and Ptolemy's title ""Epiphanes""), while Ãkerblad published an alphabet of 29 letters (more than half of which were correct) that he had identified from the Greek names in the demotic text. They could not, however, identify the remaining characters in the demotic text, which, as is now known, included ideographic and other symbols alongside the phonetic ones.

Silvestre de Sacy eventually gave up work on the stone, but he was to make another contribution. In 1811, prompted by discussions with a Chinese student about Chinese script, Silvestre de Sacy considered a suggestion made by Georg ZoÃ«ga in 1797 that the foreign names in Egyptian hieroglyphic inscriptions might be written phonetically; he also recalled that as early as 1761, Jean-Jacques BarthÃ©lemy had suggested that the characters enclosed in cartouches in hieroglyphic inscriptions were proper names. Thus, when Thomas Young, foreign secretary of the Royal Society of London, wrote to him about the stone in 1814, Silvestre de Sacy suggested in reply that in attempting to read the hieroglyphic text, Young might look for cartouches that ought to contain Greek names and try to identify phonetic characters in them.

Young did so, with two results that together paved the way for the final decipherment. In the hieroglyphic text, he discovered the phonetic characters ""p t o l m e s"" (in today's transliteration ""p t w l m y s"") that were used to write the Greek name ""Ptolemaios"". He also noticed that these characters resembled the equivalent ones in the demotic script, and went on to note as many as 80 similarities between the hieroglyphic and demotic texts on the stone, an important discovery because the two scripts were previously thought to be entirely different from one another. This led him to deduce correctly that the demotic script was only partly phonetic, also consisting of ideographic characters derived from hieroglyphs. Young's new insights were prominent in the long article "Egypt" that he contributed to the "EncyclopÃ¦dia Britannica" in 1819. He could make no further progress, however.

In 1814, Young first exchanged correspondence about the stone with Jean-FranÃ§ois Champollion, a teacher at Grenoble who had produced a scholarly work on ancient Egypt. Champollion saw copies of the brief hieroglyphic and Greek inscriptions of the Philae obelisk in 1822, on which William John Bankes had tentatively noted the names "Ptolemaios" and "Kleopatra" in both languages. From this, Champollion identified the phonetic characters "k l e o p a t r a" (in today's transliteration "q l iÒ w p 3 d r 3.t"). On the basis of this and the foreign names on the Rosetta Stone, he quickly constructed an alphabet of phonetic hieroglyphic characters, completing his work on 14 September and announcing it publicly on 27 September in a lecture to the "AcadÃ©mie royale des Inscriptions et Belles-Lettres". On the same day he wrote the famous ""Lettre Ã  M. Dacier"" to Bon-Joseph Dacier, secretary of the AcadÃ©mie, detailing his discovery. In the postscript Champollion notes that similar phonetic characters seemed to occur in both Greek and Egyptian names, a hypothesis confirmed in 1823, when he identified the names of pharaohs Ramesses and Thutmose written in cartouches at Abu Simbel. These far older hieroglyphic inscriptions had been copied by Bankes and sent to Champollion by Jean-Nicolas Huyot. From this point, the stories of the Rosetta Stone and the decipherment of Egyptian hieroglyphs diverge, as Champollion drew on many other texts to develop an Ancient Egyptian grammar and a hieroglyphic dictionary which were published after his death in 1832.

Work on the stone now focused on fuller understanding of the texts and their contexts by comparing the three versions with one another. In 1824 Classical scholar Antoine-Jean Letronne promised to prepare a new literal translation of the Greek text for Champollion's use. Champollion in return promised an analysis of all the points at which the three texts seemed to differ. Following Champollion's sudden death in 1832, his draft of this analysis could not be found, and Letronne's work stalled. FranÃ§ois Salvolini, Champollion's former student and assistant, died in 1838, and this analysis and other missing drafts were found among his papers. This discovery incidentally demonstrated that Salvolini's own publication on the stone, published in 1837, was plagiarism. Letronne was at last able to complete his commentary on the Greek text and his new French translation of it, which appeared in 1841. During the early 1850s, German Egyptologists Heinrich Brugsch and Max Uhlemann produced revised Latin translations based on the demotic and hieroglyphic texts. The first English translation followed in 1858, the work of three members of the Philomathean Society at the University of Pennsylvania.

Whether one of the three texts was the standard version, from which the other two were originally translated, is a question that has remained controversial. Letronne attempted to show in 1841 that the Greek version, the product of the Egyptian government under the Macedonian Ptolemies, was the original. Among recent authors, John Ray has stated that "the hieroglyphs were the most important of the scripts on the stone: they were there for the gods to read, and the more learned of their priesthood". Philippe Derchain and Heinz Josef Thissen have argued that all three versions were composed simultaneously, while Stephen Quirke sees in the decree "an intricate coalescence of three vital textual traditions". Richard Parkinson points out that the hieroglyphic version strays from archaic formalism and occasionally lapses into language closer to that of the demotic register that the priests more commonly used in everyday life. The fact that the three versions cannot be matched word for word helps to explain why the decipherment has been more difficult than originally expected, especially for those original scholars who were expecting an exact bilingual key to Egyptian hieroglyphs.

Even before the Salvolini affair, disputes over precedence and plagiarism punctuated the decipherment story. Thomas Young's work is acknowledged in Champollion's 1822 "Lettre Ã  M. Dacier", but incompletely, according to early British critics: for example, James Browne, a sub-editor on the "EncyclopÃ¦dia Britannica" (which had published Young's 1819 article), anonymously contributed a series of review articles to the "Edinburgh Review" in 1823, praising Young's work highly and alleging that the "unscrupulous" Champollion plagiarised it. These articles were translated into French by Julius Klaproth and published in book form in 1827. Young's own 1823 publication reasserted the contribution that he had made. The early deaths of Young (1829) and Champollion (1832) did not put an end to these disputes. In his work on the stone in 1904 E. A. Wallis Budge gave special emphasis to Young's contribution compared with Champollion's. In the early 1970s, French visitors complained that the portrait of Champollion was smaller than one of Young on an adjacent information panel; English visitors complained that the opposite was true. The portraits were in fact the same size.

Calls for the Rosetta Stone to be returned to Egypt were made in July 2003 by Zahi Hawass, then Secretary-General of Egypt's Supreme Council of Antiquities. These calls, expressed in the Egyptian and international media, asked that the stele be repatriated to Egypt, commenting that it was the "icon of our Egyptian identity". He repeated the proposal two years later in Paris, listing the stone as one of several key items belonging to Egypt's cultural heritage, a list which also included: the iconic bust of Nefertiti in the Egyptian Museum of Berlin; a statue of the Great Pyramid architect Hemiunu in the Roemer-und-Pelizaeus-Museum in Hildesheim, Germany; the Dendera Temple Zodiac in the Louvre in Paris; and the bust of Ankhhaf in the Museum of Fine Arts in Boston.

In 2005, the British Museum presented Egypt with a full-sized fibreglass colour-matched replica of the stele. This was initially displayed in the renovated Rashid National Museum, an Ottoman house in the town of Rashid (Rosetta), the closest city to the site where the stone was found. In November 2005, Hawass suggested a three-month loan of the Rosetta Stone, while reiterating the eventual goal of a permanent return. In December 2009, he proposed to drop his claim for the permanent return of the Rosetta Stone if the British Museum lent the stone to Egypt for three months for the opening of the Grand Egyptian Museum at Giza in 2013.

As John Ray has observed, "the day may come when the stone has spent longer in the British Museum than it ever did in Rosetta." There is strong opposition among national museums to the repatriation of objects of international cultural significance such as the Rosetta Stone. In response to repeated Greek requests for return of the Elgin Marbles from the Parthenon and similar requests to other museums around the world, in 2002 over 30 of the world's leading museumsâincluding the British Museum, the Louvre, the Pergamon Museum in Berlin and the Metropolitan Museum in New York Cityâissued a joint statement declaring that "objects acquired in earlier times must be viewed in the light of different sensitivities and values reflective of that earlier era" and that "museums serve not just the citizens of one nation but the people of every nation".

Various ancient bilingual or even trilingual epigraphical documents have sometimes been described as "Rosetta stones", as they permitted the decipherment of ancient written scripts. For example, the bilingual Greek-Brahmi coins of the Greco-Bactrian king Agathocles have been described as "little Rosetta stones", allowing to secure the first steps towards the decipherment of the Brahmi script by Christian Lassen, thus unlocking ancient Indian epigraphy. The Behistun inscription has also been compared to the Rosetta stone, as it links the translations of three ancient Middle-Eastern languages: Old Persian, Elamite, and Babylonian.

The term "Rosetta stone" has been also used idiomatically to represent a crucial key in the process of decryption of encoded information, especially when a small but representative sample is recognised as the clue to understanding a larger whole. According to the "Oxford English Dictionary", the first figurative use of the term appeared in the 1902 edition of the "EncyclopÃ¦dia Britannica" relating to an entry on the chemical analysis of glucose. Another use of the phrase is found in H. G. Wells' 1933 novel "The Shape of Things to Come", where the protagonist finds a manuscript written in shorthand that provides a key to understanding additional scattered material that is sketched out in both longhand and on typewriter.

Since then, the term has been widely used in other contexts. For example, Nobel laureate Theodor W. HÃ¤nsch in a 1979 "Scientific American" article on spectroscopy wrote that "the spectrum of the hydrogen atoms has proved to be the Rosetta Stone of modern physics: once this pattern of lines had been deciphered much else could also be understood". Fully understanding the key set of genes to the human leucocyte antigen has been described as "the Rosetta Stone of immunology". The flowering plant "Arabidopsis thaliana" has been called the "Rosetta Stone of flowering time". A Gamma ray burst (GRB) found in conjunction with a supernova has been called a Rosetta Stone for understanding the origin of GRBs. The technique of Doppler echocardiography has been called a Rosetta Stone for clinicians trying to understand the complex process by which the left ventricle of the human heart can be filled during various forms of diastolic dysfunction.

The name has also become used in various forms of translation software. Rosetta Stone is a brand of language-learning software published by Rosetta Stone Inc., headquartered in Arlington County, US. "Rosetta" is the name of a "lightweight dynamic translator" that enables applications compiled for PowerPC processors to run on Apple systems using an x86 processor. "Rosetta" is an online language translation tool to help localisation of software, developed and maintained by Canonical as part of the Launchpad project. Similarly, Rosetta@home is a distributed computing project for predicting protein structures from amino acid sequences (or "translating" sequence into structure). The Rosetta Project brings language specialists and native speakers together to develop a meaningful survey and near-permanent archive of 1,500 languages, intended to last from AD 2000 to 12,000. The European Space Agency's "Rosetta" spacecraft was launched to study the comet 67P/ChuryumovâGerasimenko in the hope that determining its composition will reveal the origin of the Solar System.





</doc>
<doc id="26429" url="https://en.wikipedia.org/wiki?curid=26429" title="Redshirt (stock character)">
Redshirt (stock character)

A "redshirt" is a stock character in fiction who dies soon after being introduced. The term originates from the original "" (NBC, 1966â69) television series in which the red-shirted security personnel frequently die during episodes. Redshirt deaths are often used to dramatize the potential peril that the main characters face.

In "Star Trek", red-uniformed security officers and engineers who accompany the main characters on landing parties often suffer quick deaths. The trope was allegedly debunked by statistics professor James Grime and Matthew Barsalou.

The first instance of what now is an established trope can be seen in the episode "What Are Little Girls Made Of?" (1966). Of the 59 crew members killed in the series, 43 (73%) were wearing red shirts. The "" book "Legends of the Ferengi" says Starfleet security personnel "rarely survive beyond the second act break". An episode of "" titled "" (1998) also references red as a sort of bad luck omen, in which the plot centers around a group of cadets calling themselves "Red Squad", almost all of whom die in the episode. The cinematic reboot of the franchise features a character named Olson (portrayed by Greg Ellis) who dies early on during a mission; he wears a red uniform in homage to the trope from the original series.

In other media, the term "redshirt" and images of characters wearing red shirts has come to represent disposable characters destined for suffering or death.

"Galaxy Quest" (1999), a comedy about actors from a defunct science-fiction television series serving on a real starship, includes an actor who is terrified that he's going to die on an away mission because his only appearance in the show was as an unnamed character who was killed early in the episode. "Redshirts" is a novel by John Scalzi that satirizes the trope, as is the PC game Redshirt.



</doc>
<doc id="26431" url="https://en.wikipedia.org/wiki?curid=26431" title="Receptor">
Receptor

Receptor may refer to:



</doc>
<doc id="26432" url="https://en.wikipedia.org/wiki?curid=26432" title="Resolution-class submarine">
Resolution-class submarine

The "Resolution" class was a class of four nuclear ballistic missile submarines (SSBN) built for the Royal Navy as part of the UK Polaris programme. Each submarine was armed with up to 16 UGM-27 Polaris A-3 nuclear missiles.

The class comprised , , and . They were built by Vickers Armstrong in Barrow-in-Furness and Cammell Laird in Birkenhead between 1964 and 1968. All four boats were based at HM Naval Base Clyde (HMS "Neptune"), west of Glasgow, Scotland.

The "Resolution" class was the launch platform for the United Kingdom's strategic nuclear deterrent from the late 1960s until 1996, when it was replaced by the carrying the Trident II.

During the 1950s and early 1960s, the United Kingdom's nuclear deterrent was based on the RAF's V-bombers. But in the early 1960s developments in radar and surface-to-air missiles made it clear that bombers were becoming vulnerable, and would be unlikely to penetrate Soviet airspace. Free-fall nuclear weapons would no longer be a credible deterrent.

To address this problem, in May 1960 the British Prime Minister, Harold Macmillan arranged a deal with US President Eisenhower to equip the V bombers with the US-designed AGM-48 Skybolt. The Skybolt was a range ballistic missile that allowed the launching bombers to remain well away from Soviet defences and launch attacks that would be basically invulnerable. With this range, the V bombers would have to fly only a few hundred miles from their bases before being in range for an attack on Moscow.

Under the agreement the UK's contribution to the programme was limited to developing suitable mounting points on the Avro Vulcan bomber, installing the required guidance systems that fed the missiles updated positioning information, and development of a British version of the US W47 warhead to arm it, the RE.179 .

The incoming Kennedy administration expressed serious doubts of both Skybolt and the US deterrent force in general. Robert McNamara was highly critical of the US bomber fleet, which he saw as obsolete in an age of ICBMs. Skybolt was seen simply as a means of continuing the existence of a system he no longer considered credible, and given the rapidly improving capabilities of ICBM inertial guidance systems, a precision strike capability with free-fall bombs would no longer be needed. McNamara was equally concerned about the UK also having its own nuclear force, and worried that the US could be drawn into a war by the UK. He wanted to bring the UK into a dual-key arrangement.

McNamara first broached the idea of cancelling Skybolt with the British in November 1962. When this was reported in the House of Commons, a storm of protest broke out. A meeting was arranged to settle the issue, and Macmillan stated in no uncertain terms that the UK would be retaining their independent deterrent capability, no matter what the cost. With development of their Polaris-derived warheads well along, a suitable launch platform would be developed, if need be.

Faced with a clear failure in policy terms, Kennedy gave up on the idea of strong-arming Britain into accepting a dual-key arrangement. By the end of the series of meetings, the UK had gained the much more impressive Polaris system, and would start development of a new submarine to launch it. The SSBNs would then take over the nuclear deterrent role from the RAF's V bombers from 1968 onwards.

Two pairs of the boats were ordered in May 1963 from Vickers Shipbuilding Ltd, Barrow in Furness and from Cammell Laird and Co. Ltd, Birkenhead. The option of buying a fifth unit, planned as , was cancelled in February 1965. Traditional battleship names were used, signifying that they were the capital ships of their time.

Vickers Armstrong in Barrow-in-Furness constructed "Resolution" and "Repulse" and Cammell Laird in Birkenhead constructed "Renown" and "Revenge". The construction was unusual in that the bow and stern were constructed separately before being assembled together with the American-designed missile compartment.

The design was a modification of the fleet submarine, but greatly extended to incorporate the missile compartment between the fin and the nuclear reactor. The length was , breadth , height and the displacement submerged and surfaced. A Rolls-Royce pressurised water reactor (PWR1) and English Electric Company turbines gave them a speed of and they could dive to depths of . Sixteen Polaris A3 missiles were carried, in two rows of eight. For emergencies there was a diesel generator and six torpedo tubes located at the bow, firing the Tigerfish wire-guided homing torpedoes. The submarines put to sea with a crew of 143.

According to former head of the Royal Corps of Naval Constructors R.J. Daniel, the "Resolution"-class SSBNs possessed five features that were envied by the United States Navy: the machinery loading hatch, automated hovering system, welded hull valves, standardised valves, and raft-mounted propulsion machinery.

The first to be completed was "Resolution", laid down in February 1964 and launched in September 1966. After commissioning in 1967 she underwent a long period of sea trials, culminating in the test firing of a Polaris missile from the USAF Eastern Test Range off Cape Kennedy at 11:15 on 15 February 1968. "Resolution" commenced her first operational patrol on 15 June 1968, beginning 28 years of Polaris patrols. The class were part of the 10th Submarine Squadron, all based at Faslane Naval Base, Scotland.

All four of the class underwent conversion during the 1980s so that they could be fitted with the Polaris A3TK missile which was fitted with the British-developed Chevaline MRV system.

As the newer "Vanguard"-class submarines entered service, the "Resolution" class was eventually retired and all boats laid up at Rosyth dockyard with their used nuclear fuel removed. All four will eventually be disposed of via MOD's Submarine Dismantling Project (SDP). This project will begin in 2016 with as the first submarine to prove the technique. The selected method will first remove all Low-level radioactive waste from the vessel, followed by the more radioactive intermediate-level waste. All non-radioactive material in the remainder of the vessel will be recycled for re-use by conventional ship-breaking techniques.

New methods of project management were used in the refits of the "Resolution" class, including:






</doc>
<doc id="26435" url="https://en.wikipedia.org/wiki?curid=26435" title="Roger Angell">
Roger Angell

Roger Sergeant Angell (born September 19, 1920) is an American essayist known for his writing on sports, especially baseball. He has been a regular contributor to "The New Yorker" and was its chief fiction editor for many years. He has written numerous works of fiction, non-fiction, and criticism, and for many years wrote an annual Christmas poem for "The New Yorker".

He received a number of awards for his writing, including the George Polk Award for Commentary in 1980, the "Kenyon Review" Award for Literary Achievement in 2005 along with Umberto Eco, and the inaugural PEN/ESPN Lifetime Achievement Award for Literary Sports Writing in 2011.

He was elected a Fellow of the American Academy of Arts and Sciences in 2007 and is a long-time ex-officio member of the council of the Authors Guild.

Angell was inducted into the Baseball Reliquary's Shrine of the Eternals in 2010. 

He was named the 2014 recipient of the J. G. Taylor Spink Award by the Baseball Writers' Association of America on December 10, 2013.

Angell is the son of Katharine Sergeant Angell White, "The New Yorker"âs first fiction editor, and the stepson of renowned essayist E.Â B. White, but was raised for the most part by his father, Ernest Angell, an attorney who became head of the American Civil Liberties Union.

Angell is a 1938 graduate of the Pomfret School and attended Harvard University. He served in the United States Army Air Forces during World War II.

Angell's earliest published works were pieces of short fiction and personal narratives. Several of these pieces were collected in "The Stone Arbor and Other Stories" (1960) and "A Day in the Life of Roger Angell" (1970).

He first contributed to "The New Yorker" in March 1944. His contributions have continued into 2018.

In 1948, Angell was employed at "Holiday Magazine", a travel magazine that featured literary writers.

He first wrote professionally about baseball in 1962, when William Shawn, editor of "The New Yorker", had him travel to Florida to write about spring training.

Angell has been called the "Poet Laureate of baseball" but dislikes the term. In a review of "Once More Around the Park" for the "Journal of Sport History", Richard C. Crepeau wrote that "Gone for Good", Angell's essay on the career of Steve Blass, "may be the best piece that anyone has ever written on baseball or any other sport". Angell contributed commentary to the Ken Burns series "Baseball", in 1994.

One of his most striking of essays was collected in "Season Ticket", about a spring training trip to see the Baltimore Orioles, where he interviewed Earl Weaver, then the Orioles manager, about Cal Ripken,Â Jr., who was starting his rookie season. Angell quoted Weaver as saying about Ripken that "his manager can just write his name into the lineup every day for the next fifteen years; that's how good he is". Ripken was written into lineups every day for more than fifteen years, setting the consecutive-games-played streak of 2,632 games.

Angell fathered three children: Callie, Alice, and John Henry. He had Alice and Callie with his first wife Evelyn, and John Henry with Carol. Callie Angell, who was an authority on the films of Andy Warhol, committed suicide on May 5, 2010, in Manhattan, where she worked as a curator at the Whitney Museum of American Art; she was 62. In a 2014 essay, Angell mentioned her death â "the oceanic force and mystery of that event" â and his struggle to comprehend that "a beautiful daughter of mine, my oldest child, had ended her life." Alice Angell lived in Portland, Maine and died from cancer on February 2, 2019, and John Henry Angell lives in Portland, Oregon.

His second wife, Carol Rogge Angell, to whom he was married for 48 years, died on April 10, 2012, of metastatic breast cancer at the age of 73. In 2014, he married Margaret Moorman, a writer and teacher.

In 2019, University of Nebraska Press published "No Place I Would Rather Be: Roger Angell and a Life in Baseball Writing", a book about Angell's career written by Joe Bonomo.



</doc>
<doc id="26436" url="https://en.wikipedia.org/wiki?curid=26436" title="Rent (musical)">
Rent (musical)

Rent is a rock musical with music, lyrics, and book by Jonathan Larson, loosely based on Giacomo Puccini's opera "La BohÃ¨me". It tells the story of a group of impoverished young artists struggling to survive and create a life in Lower Manhattan's East Village in the thriving days of Bohemian Alphabet City, under the shadow of HIV/AIDS.

The musical was first seen in a workshop production at New York Theatre Workshop in 1993. This same Off-Broadway theatre was also the musical's initial home following its official 1996 opening. The show's creator, Jonathan Larson, died suddenly of an aortic dissection, believed to have been caused by undiagnosed Marfan syndrome, the night before the Off-Broadway premiere. The musical moved to Broadway's larger Nederlander Theatre on April 29, 1996.

On Broadway, "Rent" gained critical acclaim and won several awards, including the Pulitzer Prize for Drama and the Tony Award for Best Musical. The Broadway production closed on September 7, 2008, after 12 years, making it one of the longest-running shows on Broadway. The production grossed over $280 million.

The success of the show led to several national tours and numerous foreign productions. In 2005, it was adapted into a motion picture featuring most of the original cast members.

In 1988, playwright Billy Aronson wanted to create "a musical based on Puccini's "La BohÃ¨me", in which the luscious splendor of Puccini's world would be replaced with the coarseness and noise of modern New York." In 1989, Jonathan Larson, a 29-year-old composer, began collaborating with Aronson on this project, and the two composed together "Santa Fe", "Splatter" (later re-worked into the song "Rent"), and "I Should Tell You". Larson suggested setting the play "amid poverty, homelessness, spunky gay life, drag queens and punk" in the East Village neighborhood of Manhattan, which happened to be down the street from his Greenwich Village apartment. He also came up with the show's ultimate title (a decision that Aronson was unhappy with, at least until Larson pointed out that "rent" also means torn apart). In 1991, he asked Aronson if he could use Aronson's original concept and make "Rent" his own. Larson had ambitious expectations for "Rent"; his ultimate dream was to write a rock opera "to bring musical theater to the MTV generation". Aronson and Larson made an agreement that if the show went to Broadway, Aronson would share in the proceeds and be given credit for "original concept & additional lyrics".

Jonathan Larson focused on composing "Rent" in the early 1990s, waiting tables at the Moondance Diner to support himself. Over the course of years, Larson wrote hundreds of songs and made many drastic changes to the show, which in its final incarnation contained 42 songs. In the fall of 1992, Larson approached James Nicola, artistic director of New York Theatre Workshop, with a tape and copy of "Rent"s script. When "Rent" had its first staged reading at New York Theatre Workshop in March 1993, it became evident that, despite its very promising material and moving musical numbers, many structural problems needed to be addressed, including its cumbersome length and overly complex plot.

As of 1994, the New York Theatre Workshop version of "Rent" featured songs that never made it into the final version, such as:


This workshop version of "Rent" starred Anthony Rapp as Mark and Daphne Rubin-Vega as Mimi. Larson continued to work on "Rent", gradually reworking its flaws and staging more workshop productions.

On January 24, 1996, after the musical's final dress rehearsal before its off-Broadway opening, Larson had his first (and only) newspaper interview with music critic Anthony Tommasini of "The New York Times", attracted by the coincidence that the show was debuting exactly 100 years after Puccini's opera. Larson would not live to see "Rent"s success; he died from an undiagnosed aortic aneurysm (believed to have resulted from Marfan syndrome) in the early morning of January 25, 1996. Friends and family gathered at the New York Theatre Workshop, and the first preview of "Rent" became a sing-through of the musical in Larson's memory.

The show premiered as planned and quickly gained popularity fueled by enthusiastic reviews and the recent death of its composer. It proved extremely successful during its Off-Broadway run, selling out all its shows at the 150-seat New York Theater Workshop. Due to such overwhelming popularity and a need for a larger theater, "Rent" moved to Broadway's then-under-renovation Nederlander Theatre on 41st Street on April 29, 1996. At the production's request, final touches on the theater's remodeling and renovation were put on hold before and during the run of "Rent" because the show's producers and creative team felt the unfinished look fit in well with the gritty setting of the show.

Larson's inspiration for "Rent"s content came from several different sources. Many of the characters and plot elements are drawn directly from Giacomo Puccini's opera "La BohÃ¨me", the world premiere of which was in 1896, a century before "Rent"s premiere. "La BohÃ¨me" was also about the lives of poor young artists. Tuberculosis, the plague of Puccini's opera, is replaced by HIV/AIDS in "Rent"; 1800s Paris is replaced by New York's East Village in the late 1980s or early 1990s. The names and identities of "Rent"s characters also heavily reflect Puccini's original characters, though they are not all direct adaptations. For example, Joanne in "Rent" represents the character of Alcindoro in "BohÃ¨me", but is also partially based on Marcello. Also, Joanne is the only "Rent" character whose predecessor in "La BohÃ¨me" is a different sex.

Other examples of parallels between Larson's and Puccini's work include Larson's song "Light My Candle", which draws melodic content directly from "Che gelida manina"; "Quando me'n vo'" ("Musetta's Waltz"), a melody taken directly from Puccini's opera; and "Goodbye Love", a long, painful piece that reflects a confrontation and parting between characters in both Puccini's and Larson's work. "Quando me'n vo'" is paralleled in the first verse of "Take Me or Leave Me", when Maureen describes the way people stare when she walks in the street. It is also directly referred to in the scene where the characters are celebrating their bohemian life. Mark says, "Roger will attempt to write a bittersweet, evocative song..." Roger plays a quick piece, and Mark adds, "...that "doesn't" remind us of 'Musetta's Waltz'." This part of "Musetta's Waltz" is also later used in "Your Eyes", a song Roger writes.

"Rent" is also a somewhat autobiographical work, as Larson incorporated many elements of his life into his show. Larson lived in New York for many years as a starving artist with an uncertain future. He sacrificed a life of stability for his art, and shared many of the same hopes and fears as his characters. Like his characters he endured poor living conditions, and some of these conditions (e.g. illegal wood-burning stove, bathtub in the middle of his kitchen, broken buzzer [his guests had to call from the pay phone across the street and he would throw down the keys, as in "Rent"]) made their way into the play. Part of the motivation behind the storyline in which Maureen leaves Mark for a woman (Joanne) is based on the fact that Larson's own girlfriend left him for a woman. The Mark Cohen character is based on Larson's friends, cinematographer and producer Jonathan Burkhart and documentary filmmaker Eddie Rosenstein.

Playwright Sarah Schulman alleged that "Rent" bore striking similarities to her novel "People in Trouble".

The line, "I'm more of a man than you'll ever be... and more of a woman than you'll ever get!", attributed to Angel Dumott Schunard at her funeral, was previously used by the character Hollywood Montrose, who appeared in the films "Mannequin" (1987) and "" (1991). Like Angel, Hollywood performs a song and dance number and sometimes wears women's clothing. This line was originally in the film "Car Wash" (1976), delivered by Antonio Fargas as a flamboyant homosexual cross dresser.

The earliest concepts of the characters differ largely from the finished products. Everyone except Mark had AIDS, including Maureen and Joanne; Maureen was a serious, angry character who played off Oedipus in her performance piece instead of Hey Diddle Diddle; Mark was, at one point, a painter instead of a filmmaker; Roger was named Ralph and wrote musical plays; Angel was a jazz philosopher, while Collins was a street performer; Angel and Collins were both originally described as Caucasian; and Benny had a somewhat enlarged role in the story, taking part in songs like "Real Estate", which was later cut.

Many actual locations and events are included in, or are the inspiration for, elements of the musical. Life CafÃ©, where the "La Vie BohÃ¨me" numbers are set, was an actual restaurant (closed 2013) on 10th Street and Avenue B in the East Village of New York City. The riot at the end of the first act is based on the East Village riot in 1988 that arose as a result of the city-imposed curfew in Tompkins Square Park.

"Will I?", a song which takes place during a Life Support meeting and expresses the pain and fear of living a life with AIDS, was inspired by a real event. Larson attended a meeting of Friends in Deed, an organization that helps people deal with illness and grief, much like Life Support. After that first time, Larson attended the meetings regularly. During one meeting, a man stood up and said that he was not afraid of dying. He did say, however, that there was one thing of which he was afraid: Would he lose his dignity? From this question stemmed the first line of this song. The people present at the Life Support meeting in the show, such as Gordon, Ali and Pam, carry the names of Larson's friends who died. In the Broadway show, the names of the characters in that particular scene (they introduce themselves) were changed nightly to honor the friends of the cast members who were living with or had died from AIDS.

The scene and song "Life Support" were also based on Friends in Deed, as well as on Gordon, Pam, and Ali. Originally, the members of Life Support had a solid block of the "forget regret" refrain, and they talked about remembering love. When Jonathan's HIV positive friends heard this scene, they told him that having AIDS was not so easy to accept: it made you angry and resentful too, and the song did not match that. Jonathan then added a part where Gordon says that he has a problem with this "credo...my T-cells are low, I regret that news, okay?" Paul, the leader of the meeting, replies, "Okay...but, Gordon, how do you feel today?" Gordon admits that he is feeling the best that he has felt all year. Paul asks, "Then why choose fear?" Gordon says, "I'm a New Yorker. Fear's my life."

Lynn Thomson was a dramaturg who was hired by New York Theatre Workshop to help rework "Rent". She claimed that between early May and the end of October 1995, she and Larson co-wrote a "new version" of the musical. She sued the Larson estate for $40 million and sought 16% of the show's royalties, claiming she had written a significant portion of the lyrics and the libretto of the "new version" of "Rent".

During the trial, Thomson could not recall the lyrics to the songs that she allegedly wrote, nor the structures of the libretto she claimed to have created. The judge ruled against her and gave the Jonathan Larson Estate full credit and right to "Rent". A federal appellate court upheld the original ruling on appeal. In August 1998, the case was settled out of court. The terms of the settlement were not disclosed.

On Christmas Eve in Manhattan's East Village, two roommatesâMark, a filmmaker, and Roger, a rock musicianâstruggle to stay warm and produce their art ("Tune Up #1"). Mark's mother leaves him a voicemail wishing him a merry Christmas and trying to comfort him since his ex-girlfriend Maureen dumped him ("Voice Mail #1"). Their friend Tom Collins, a gay anarchist professor at New York University, calls and plans to surprise them at their apartment, but is mugged before entering. At the same time, Mark and Roger's former roommate and friend Benny, who has since become their harsh new landlord, has reneged on an earlier agreement and now demands last year's rent, before shutting down their electrical power ("Tune Up #2"). However, Mark and Roger rebel and resolve not to pay the rent they cannot pay and which they were promised wouldn't be a problem ("Rent"). Meanwhile, Angel, a cross-dressing street drummer (presently out of drag), finds Collins wounded in an alley and tends to him ("You Okay Honey?") - the two are immediately attracted to each other, each learning that the other is HIV positive. It is revealed that Roger too has HIV which he contracted from his last girlfriend, who committed suicide after learning of her diagnosis, which has caused Roger to fall into depression. Mark leaves the loft while Roger stays home ("Tune Up #3"), trying to compose on his guitar without success; he wishes desperately to write one last song to be remembered by before he dies ("One Song Glory"). An exotic dancer, junkie, and neighbor, Mimi, shows up at their apartment asking for help with lighting her candle, flirting with Roger in the process; however, he is clearly hesitant to return her affections ("Light My Candle"). Meanwhile, Joanne, a lawyer and Maureen's girlfriend, receives a voicemail from her parents ("Voice Mail #2").

At last, the missing Collins enters the apartment, presenting Angel, who is now in full drag and shares the money she made and the amusing story of how she killed a dog to earn it ("Today 4 U"). Mark comes home, and Benny arrives, speaking of Maureen's upcoming protest against his plans to evict the homeless from a lot where he is hoping to build a cyber arts studio. Benny offers that, if they convince Maureen to cancel the protest, then Mark and Roger can officially remain rent-free tenants. However, the two rebuff Benny's offer and he leaves ("You'll See"). Mark leaves the loft again to go help Maureen with the sound equipment for the protest, unexpectedly meeting Joanne at the stage. Initially hesitant with each other, the two eventually bond over their shared distrust of Maureen's "gaslighting" and promiscuous behaviours ("Tango: Maureen"). Mark then joins Collins and Angel to film their HIV support group meeting ("Life Support"), while Mimi attempts to seduce Roger alone in his apartment ("Out Tonight"). Roger is extremely upset by Mimi's intrusion, demanding she leave him alone and resisting any romantic feelings he may harbour for her ("Another Day"). After Mimi leaves, Roger reflects on his fear of dying an undignified death from AIDS, while the Life Support group echoes his thoughts ("Will I").

Collins, Mark, and Angel protect a homeless woman from police harassment, but she chastises them ("On the Street"). To lighten the mood, Collins talks about his dream of escaping New York City to open a restaurant in Santa Fe ("Santa Fe"). Soon, Mark leaves to check up on Roger and while alone, Collins and Angel confess their love for each other ("I'll Cover You"). Joanne hectically prepares for Maureen's show, trying to balance all of the people calling her at once ("We're Okay"). Before the performance, Roger apologizes to Mimi, inviting her to come to the protest and the dinner party his friends are having afterwards. At the same time, police, vendors, and homeless people prepare for the protest ("Christmas Bells"). Maureen begins her avant-garde, if not over the top, performance based on "Hey Diddle Diddle" ("Over the Moon"). At the post-show party at the Life CafÃ©, Benny arrives, criticizing the protest and the group's bohemian lifestyle. In response, Mark and all the cafÃ©'s bohemian patrons defiantly rise up to celebrate their way of living ("La Vie BohÃ¨me"). Mimi and Roger each discover that the other is HIV-positive and hesitantly decide to move forward with their relationship ("I Should Tell You"). Joanne explains that Mark and Roger's building has been padlocked and a riot has broken out; Roger and Mimi, unaware, share their first kiss. The celebration continues ("La Vie BohÃ¨me B").

The cast lines up to sing together before the plot of the second act begins, affirming that one should measure life "in love" ("Seasons of Love"). Afterwards, Mark and Roger gather to break back into their locked apartment with their friends ("Happy New Year"). A new voicemail reveals that Mark's footage of the riot has earned him a job offering at a tabloid news company called Buzzline ("Voice Mail #3"). The others finally break through the door just as Benny arrives, saying he wants to call a truce and revealing that Mimiââwho used to be his girlfriendââconvinced him to change his mind. Mimi denies rekindling her relationship with Benny, but Roger is upset, and although they apologize to each other, Mimi goes to her drug dealer for a fix ("Happy New Year B").

Around Valentine's Day, Mark tells the audience that Roger and Mimi have been living together, but they are tentative with each other. It is also told that Maureen and Joanne are preparing another protest, and during rehearsal, Maureen criticizes Joanne's controlling behaviour and Joanne criticizes Maureen's promiscuous mannerisms. They break up dramatically following an ultimatum ("Take Me or Leave Me"). Time progresses to spring ("Seasons of Love B"), but Roger and Mimi's relationship is strained by Mimi's escalating heroin usage and Roger's lasting jealousy and suspicion of Benny. Each alone, Roger and Mimi sing of love and loneliness, telling each other how they feel, as they watch Collins nurse Angel, whose health is declining due to AIDS ("Without You"). By the end of the summer, Mark continues to receive calls offering a corporate job at Buzzline ("Voice Mail #4"). A dance is performed representing all the couples' sex lives ("Contact"). At the climax of the number, the two former couples break up, and Angel suddenly dies. At the funeral, the friends briefly come together to share their memories with Collins being the last to reminisce ("I'll Cover You [Reprise]"). Mark expresses his fear of being the only one left surviving when the rest of his friends die of AIDS, and he finally accepts the corporate job offer ("Halloween"). Roger reveals that he is leaving for Santa Fe, which sparks an argument about commitment between him and Mimi, and between Maureen and Joanne. Collins arrives and admonishes the entire group for fighting on the day of Angel's funeral, causing Maureen and Joanne to reconcile, but not Mimi and Roger. The group shares a sad moment, knowing that between deaths and leaving, their close-knit friendships will be breaking up. Everyone leaves except Mark and Roger, and so Mark tries to convince Roger to stay in New York. Roger, unable to handle Mimi's declining health, becomes angry with Mark and leaves. Mimi returns to say goodbye, overhears everything Roger says, and, terrified, agrees to go to rehab ("Goodbye Love"). Collins is forcibly removed from the church for being unable to pay for Angel's funeral; Benny shows compassion by paying and offering Mark and Collins drinks; Collins accepts, causing him and Collins to rekindle their old friendship, but Mark has to turn down the offer due to work commitments.

Some time later, both Mark and Roger simultaneously reach an artistic epiphany, as Roger finds his song in Mimi and Mark finds his film in Angel's memory; Roger decides to return to New York in time for Christmas, while Mark quits his job to devote his efforts to working on his own film ("What You Own"). The characters' parents, concerned and confused about their respective situations, leave several worried messages on their phones ("Voice Mail #5"). On Christmas Eve, exactly one year having passed, Mark prepares to screen his now-completed film to his friends. Roger has written his song, but no one can find Mimi for him to play it to. Benny's wife, discovering Benny's relationship with Mimi, has pulled Benny out of the East Village. The power suddenly blows and Collins enters with handfuls of cash, revealing that he reprogrammed an ATM at a grocery store to provide money to anybody with the code 'ANGEL'. Maureen and Joanne abruptly enter carrying Mimi, who had been homeless and is now weak and close to death. She begins to fade, telling Roger that she loves him ("Finale"). Roger tells her to hold on as he plays her the song he wrote for her, revealing the depth of his feelings for her ("Your Eyes"). Mimi appears to die, but abruptly awakens, claiming to have been heading into a white light before a vision of Angel appeared, telling her to go back and stay with Roger. The remaining friends gather together in a final moment of shared happiness and resolve to enjoy whatever time they have left with each other, affirming that there is "no day but today" ("Finale B").

Act 1
Act 2


There are also many other non-named roles such as Cops, Bohemians, Vendors, Homeless People.

"Rent" received several awards including a Pulitzer Prize and four Tony Awards.

Critical reception of "Rent" was positive not only for its acting and musical components, but for its representation of HIV-positive individuals. Many critics praised the portrayal of characters such as Angel and Collins as being happy, with positive outlooks on life, rather than being resigned to death. While critics and theatre patrons had largely positive reviews of the show, it was criticized for its stereotypically negative portrayal of lesbian characters and the "glamourization" of the East Village in the late 1980s.

Billy Aronson said, "For the record, although I was ambivalent about Jonathanâs ideas for "Rent" when we were working together on it, I have come to love the show. And as tragic as it is that he didnât live to see his work become a huge success, I believe he knew it would be. In our last conversation I asked how the show was going and he said, with complete assurance, that it was incredible."

The song "Seasons of Love" became a successful pop song and often is performed on its own. Because of its connection to New Years and looking back at times past, it is sometimes performed at graduations or school holiday programs.

"Rent" gathered a following of fans who refer to themselves as "RENT-heads." The name originally referred to people who would camp out at the Nederlander Theater for hours in advance for the discounted $20 rush tickets to each show, though it generally refers to anyone who is obsessed with the show. These discounted tickets were for seats in the first two rows of the theater reserved for sale by lottery two hours prior to each show. Other Broadway shows have followed "Rent"s example and now also offer cheaper tickets in efforts to make Broadway theater accessible to people who would otherwise be unable to afford the ticket prices.

The term originated in "Rent"s first months on Broadway. The show's producers offered 34 seats in the front two rows of the orchestra for $20 each, two hours before the performance. Fans and others interested in tickets would camp out for hours in front of the Nederlander Theater â which is on 41st Street, just outside Times Square â to buy these tickets.

The television series "The Simpsons", "Family Guy", "Friends", "Will and Grace", "Scrubs", "Glee", "The Big Bang Theory", "Gilmore Girls", "Felicity", "Saturday Night Live", "The Office", "Franklin & Bash", "2 Broke Girls", "Girls", "Seinfeld", "The Neighbors", "Modern Family", "Smash", "Supernatural", "Superstore", and "Bob's Burgers" have included references to the show.

The film "" includes a character who plays a lead role in "Lease", a Broadway musical parody of "Rent"; the finale song is "Everyone has AIDS!". 

Yitzhak in "Hedwig and the Angry Inch" wears a "Rent" T-shirt and speaks of his aspiration to play the role of Angel.
The off-Broadway musical revue "Forbidden Broadway Strikes Back" includes parodies of "Rent" songs such as "Rant" ("Rent"), "Ouch! They're Tight" ("Out Tonight"), "Season of Hype" ("Seasons of Love"), "Too Gay 4 U (Too Het'ro 4 Me)" ("Today 4 U"), "Pretty Voices Singing" ("Christmas Bells") and "This Ain't Boheme" ("La Vie BohÃ¨me").

In the film "Deadpool", Wade Wilson is seen wearing a "Rent" T-shirt. Stan Lee also referenced one of the songs ("Cover you") when he said as the DJ in the strip club "You can't buy love.." - "but you can rent it... " 

Lin-Manuel Miranda, the composer and writer of the Broadway show "Hamilton", has cited "Rent" as a main source of inspiration. He also referenced the show in a verse of the song "Wrote My Way Out" on "The Hamilton Mixtape" in the line "Running out of time like I'm Jonathan Larson's rent check".

"Rent" had its first staged reading at New York Theatre Workshop in March 1993. A further two-week New York Theatre Workshop version was performed in 1994 starring Anthony Rapp as Mark and Daphne Rubin-Vega as Mimi, and more workshops followed. The show opened on 1996, again at New York Theatre Workshop, and quickly gained popularity off-Broadway, receiving enthusiastic reviews. "The New York Times" theater critic Ben Brantley called it an "exhilarating, landmark rock opera" with a "glittering, inventive score" that "shimmers with hope for the future of the American musical." Another reviewer wrote, ""Rent" speaks to Generation X the way that the musical "Hair" spoke to the baby boomers or those who grew up in the 1960s," while the "New York Times" similarly called it "a rock opera for our time, a "Hair" for the 90s." The show proved extremely successful off-Broadway, selling out all of its performances at the 150-seat theatre.

Due to its overwhelming popularity and the need for a larger theater, "Rent" moved to Broadway's previously derelict Nederlander Theatre on 41st Street on April 29, 1996. On Broadway, the show achieved critical acclaim and word-of-mouth popularity. The production's ethnically diverse principal cast originally included Taye Diggs, Wilson Jermaine Heredia, Jesse L. Martin, Idina Menzel, Adam Pascal, Anthony Rapp, Daphne Rubin-Vega and Fredi Walker.

The production's controversial topics and innovative pricing, including same day-of-performance $20 tickets, helped to increase the popularity of musical theater amongst the younger generation. The production was nominated for ten Tony Awards in 1996 and won four: Best Musical, Best Book, Best Original Score and Best Performance by a Featured Actor in a Musical (Heredia)

On April 24, 2006, the original Broadway cast reunited for a one-night performance of the musical at the Nederlander Theatre. This performance raised over $2,000,000 for the Jonathan Larson Performing Arts Foundation, Friends In Deed and New York Theatre Workshop. Former cast members were invited, and many from prior tours and former Broadway casts appeared, performing an alternate version of "Seasons of Love" as the finale of the performance.

"Rent" closed on September 7, 2008, after a 12-year run and 5,123 performances, making it the eleventh-longest-running Broadway show. The production grossed over $280 million.

Original cast ensemble members Rodney Hicks and Gwen Stewart returned to the cast at the time of the Broadway closing. Hicks played Benny and Stewart played the role she created, the soloist in the song "Seasons of Love". In addition, actress Tracie Thoms joined the cast at the end of the run playing Joanne, the role she portrayed in the 2005 film version. The last Broadway performance was filmed and screened in movie theaters as "Rent: Filmed Live on Broadway" in September 2008. It was released on DVD and Blu-ray formats on February 3, 2009.

Successful United States national tours, the "Angel Tour" and the "Benny Tour", launched in the 1990s. Later, the non-Equity tour started its run. There was also a Canadian tour (often referred to as the "Collins Tour").

The Angel tour began in November 1996 in Boston. Anthony Rapp joined the cast for the Chicago run, and Daphne Rubin-Vega joined for the Los Angeles run. The tour finished in San Francisco in September 1999. Other members of the Angel cast included Carrie Hamilton, Amy Spanger, Luther Creek, Kristoffer Cusick, and Tony Vincent.

The Benny Tour began in July 1997 in San Diego, California, at the LaJolla Playhouse. Michael Grief, the original director of the Broadway show was also the artistic director of the LaJolla Playhouse and was instrumental in arranging for the Benny tour to begin in the smaller city of San Diego rather than Los Angeles, California. It originally featured Neil Patrick Harris in the role of Mark Cohen. The Benny tour generally played shorter stops and often-smaller markets than the Angel Tour did. Other cast members included Wilson Cruz and d'Monroe.

Tours ran each season from 2005 to 2008. Cast members throughout the run included Aaron Tveit, Ava Gaudet, Declan Bennett, Rebecca Naomi Jones, Constantine Maroulis, Dan Rosenbaum, Heinz Winckler, Anwar Robinson, Christine Dwyer and Karen Olivo. In 2009, a national tour starring Adam Pascal and Anthony Rapp, reprising their original Broadway roles, launched in Cleveland, Ohio. Original Broadway Cast member Gwen Steward also appeared, alongside Michael McElroy as Collins, The tour ended on February 7, 2010, in Sacramento, California. A 20th-anniversary non-Equity touring production of "Rent" began in Dallas on September 20, 2016, and is scheduled to run through May 10, 2020.

The show made its UK premiere on April 21, 1998, at the West End's Shaftesbury Theatre and officially opened on May 12, 1998. The original cast included Krysten Cummings as Mimi Marquez, Wilson Jermaine Heredia as Angel Schunard, Bonny Lockhart as Benny, Jesse L. Martin as Tom Collins, Adam Pascal as Roger Davis, Anthony Rapp as Mark Cohen, and Jessica Tezier as Maureen Johnson. The show closed on October 30, 1999, after one-and-a-half years. Limited revivals took place at the Prince of Wales Theatre from December 4, 2001, to January 6, 2002; December 6, 2002, to March 1, 2003 (featuring Adam Rickett as Mark and Caprice as Maureen). There was also a successful production for a limited run in Manchester in 2006 with an additional 'goodbye' performance in 2008 from the Manchester cast.

On October 16, 2007, the heavily revised production titled "Rent Remixed" opened at the Duke of York's Theatre in London's West End. Directed by William Baker, it was set in the present day. The cast included Oliver Thornton (Mark), Luke Evans (Roger), Craig Stein (Benny), Leon Lopez (Collins), Francesca Jackson (Joanne), Jay Webb (Angel), SiobhÃ¡n Donaghy (Mimi), and Denise Van Outen (Maureen). From December 24, 2007, the role of Maureen was played by Jessie Wallace. The production received generally unfavorable reviews. The Guardian gave it only one out of five stars, writing, "They call this 'Rent Remixed'. I'd dub it 'Rent Reduced', in that the late Jonathan Larson's reworking of La BohÃ¨me, while never a great musical, has been turned into a grisly, synthetic, pseudo pop concert with no particular roots or identity." The production closed on February 2, 2008.

The production radically altered elements of the musical including defining the characters of Mimi, Angel and Mark as British. Songs were reordered (including Maureen's first appearance as the Act I finale). The rehaul of the score was masterminded by Steve Anderson and featured radically rearranged versions of Out Tonight, Today 4 U, Over the Moon and Happy New Year.

A one-off Rent - The 20th Anniversary Concert was held at the Blackpool Opera house Monday November 11, 2013 
A 20th anniversary tour opened at Theatr Clwyd in October 2016 before playing a two-month run at the St James Theatre, London. The cast included Layton Williams as Angel and Lucie Jones as Maureen. The production then continued to tour the UK.

In 2018 an immersive production of RENT premiered at Frogmore Paper Mill in Apsley, Hemel Hempstead. The Cast included Aran Macrae (Roger), Connor Dyer (Mark) and Lizzie Emery (Mimi). The show opened on July 10, 2018, and ran until July 28th.

The show was revived Off-Broadway at Stage 1 of New World Stages with previews starting July 14, 2011 and a scheduled opening of August 11, 2011. This was the first New York Revival of the show since the original production closed less than three years earlier. The production was directed by "Rent"'s original director Michael Greif. Almost the entire show was different from the original yet the reinvention did not please the critics, who complained that the new actors did not have a feel for the characters they were playing and it made the show feel contrived. The Off-Broadway production of RENT closed on September 9, 2012.

In 1999, an Australian production featured Justin Smith as Mark, Rodger Corser as Roger and Christine Anu as Mimi. The tour began in Sydney and finished in Melbourne. A production in Perth, Western Australia was mounted in 2007 and featured Anthony Callea as Mark, Tim Campbell as Roger, Courtney Act as Angel and Nikki Webster as Maureen.

The Dublin production had an extended run at the Olympia Theatre, Dublin in 2000. It starred Sean Pol McGreevy as Mark, Rachel Tucker as Maureen and Allyson Brown as Mimi under the direction of Phil Willmot. The Swedish production premiered on May 15, 2002 at The GÃ¶teborg Opera in Gothenburg, Sweden, playing until June 8, 2003. Sarah Dawn Finer played Joanne.

"Rent" veteran Neil Patrick Harris directed a production at the Hollywood Bowl in Los Angeles, CA. The production played a three night engagement, August 6â8, 2010. The cast included Vanessa Hudgens as Mimi, Aaron Tveit as Roger, Skylar Astin as Mark, Wayne Brady as Collins, Telly Leung as Angel, Tracie Thoms as Joanne, Nicole Scherzinger as Maureen, Collins Pennie as Benny, and Gwen Stewart as Seasons of Love soloist (and additional roles).

In 2017, the first tour for the German speaking countries was mounted by Berlin theatrical producer . The production travelled Germany, Austria and Switzerland and was directed by the British opera director Walter Sutcliffe.

In 2007, an abridged edition of "Rent" was made available to five non-professional acting groups in the United States for production. Billed as , this version omits the song "Contact" and eliminates some of the coarse language and tones down some public displays of affection of the original. Shorewood High School in Shorewood, WI became the first high school to perform an early version of the adaptation in May 2006. The high school was selected to present a workshop performance as part of Music Theatre International's work to adapt the musical for younger actors and potentially more conservative audiences. As of 2008, Music Theatre International began licensing "Rent School Edition" for performances by schools and non-professional amateur theaters in the United States and around the world.

"Rent" has been performed in countries around the world, including Denmark, Estonia, Finland, Iceland, Norway, Sweden, Belgium, the Netherlands, Ireland, United Kingdom, France, Germany, Switzerland, Portugal, Spain, Italy, Hungary, Poland, Slovakia, Greece, Canada, the United States, Mexico, Panama, Bolivia, Brazil, Argentina, Russia, China, Hong Kong, South Korea, Taiwan, Japan, Philippines, Singapore, Thailand, South Africa, Australia, Guam, New Zealand, Israel, Puerto Rico, Austria, Peru, Trinidad and Tobago, Dominican Republic, Cuba, Czech Republic and Guatemala.

The musical has been performed in twenty-five languages: Danish, Estonian, Finnish, Icelandic, Norwegian, Swedish, Dutch, English, French, German, Portuguese, Spanish, Italian, Hungarian, Polish, Slovak, Greek, Russian, Mandarin Chinese, Cantonese Chinese, Korean, Japanese, Hebrew, Czech, and Catalan.

A cast recording of the original Broadway cast recording was released in 1996; it features all the music of the show on a double-disc "complete recording" collection along with a remixed version of the song "Seasons of Love" featuring Stevie Wonder.

The later 2005 film version (see below) also resulted in a double-disc cast recording of the complete score used in the movie There are also many foreign cast recordings of international productions of the show.

The final performance of the Broadway production of "Rent", which took place on September 7th 2008, was filmed live and, cut together with close-up footage from a day of filming in August of the same year, released as "Rent: Filmed Live on Broadway" in cinemas with high definition digital projection systems in the U.S. and Canada between September 24 and 28, 2008. "Rent: Filmed Live" on Broadway was released on February 3, 2009 on DVD & Blu-ray formats.

In 2005, "Rent" was adapted into a movie directed by Chris Columbus with a screenplay by Stephen Chbosky. With the exception of Daphne Rubin-Vega (who was pregnant at the time of filming) and Fredi Walker (who felt she was too old for her role), who played Mimi and Joanne respectively in the original Broadway cast, the original Broadway cast members reprised the principal roles. Released on November 23, 2005, the film remained in the box office top ten for three weeks, receiving mixed reviews. Several plot elements were changed slightly, and some songs were changed to spoken dialogue or cut entirely for the film. The soundtrack was produced by Rob Cavallo, engineered by Doug McKean and features renowned session musicians Jamie Muhoberac, Tim Pierce and Dorian Crozier.

In May 2017, Fox announced plans to air a of "Rent" in late 2018. However, on September 25, 2017, Fox announced the official air date for Rent Live! would be Sunday, January 27, 2019. Marc Platt is set to serve as executive producer along with the estate of Jonathan Larson.

Filmmaker and "Rent" alum Andy SeÃ±or, Jr. is currently producing a documentary, following his journey producing the musical in Cuba in late 2014. This production of "Rent" was the first Broadway musical to premiere in Cuba since diplomatic relations between the two countries became strained during the Cold War.



</doc>
<doc id="26437" url="https://en.wikipedia.org/wiki?curid=26437" title="Restaurant">
Restaurant

A restaurant (), or an eatery, is a business that prepares and serves food and drinks to customers. Meals are generally served and eaten on the premises, but many restaurants also offer take-out and food delivery services. Restaurants vary greatly in appearance and offerings, including a wide variety of cuisines and service models ranging from inexpensive fast food restaurants and cafeterias, to mid-priced family restaurants, to high-priced luxury establishments.

In Western countries, most mid- to high-range restaurants serve alcoholic beverages such as beer and wine. Some restaurants serve all the major meals, such as breakfast, lunch, and dinner (e.g., major fast food chains, diners, hotel restaurants, and airport restaurants). Other restaurants may serve only a single meal (for example, a pancake house may only serve breakfast) or they may serve two meals (e.g., lunch and dinner).

The word derives from the French verb "restaurer" ("to restore", "to revive") and, being the present participle of the verb, it literally means "that which restores". The term "restaurant" was defined in 1507 as a "restorative beverage", and in correspondence in 1521 to mean "that which restores the strength, a fortifying food or remedy". 

The first use of the word to refer to a public venue where one can order food is believed to be in the 18th century. In 1765, a French chef by the name of A. Boulanger established a business selling soups and other "restaurants" ("restoratives"). Additionally, while not the first establishment where one could order food, or even soups, it is thought to be the first to offer a menu of available choices 
The "first real restaurant" is considered to have been "La Grande Taverne de Londres" in Paris, founded by Antoine Beauviliers in either 1782 or 1786. According to Brillat-Savarin, this was "the first to combine the four essentials of an elegant room, smart waiters, a choice cellar, and superior cooking".
In 1802 the term was applied to an establishment where restorative foods, such as bouillon, a meat broth, were served ("Ã©tablissement de restaurateur").

Restaurants are classified or distinguished in many different ways. The primary factors are usually the food itself (e.g. vegetarian, seafood, steak); the cuisine (e.g. Italian, Chinese, Japanese, Indian, French, Mexican, Thai) or the style of offering (e.g. tapas bar, a sushi train, a tastet restaurant, a buffet restaurant or a yum cha restaurant). Beyond this, restaurants may differentiate themselves on factors including speed (see fast food), formality, location, cost, service, or novelty themes (such as automated restaurants).

Restaurants range from inexpensive and informal lunching or dining places catering to people working nearby, with modest food served in simple settings at low prices, to expensive establishments serving refined food and fine wines in a formal setting. In the former case, customers usually wear casual clothing. In the latter case, depending on culture and local traditions, customers might wear semi-casual, semi-formal or formal wear. Typically, at mid- to high-priced restaurants, customers sit at tables, their orders are taken by a waiter, who brings the food when it is ready. After eating, the customers then pay the bill. In some restaurants, such as workplace cafeterias, there are no waiters; the customers use trays, on which they place cold items that they select from a refrigerated container and hot items which they request from cooks, and then they pay a cashier before they sit down. Another restaurant approach which uses few waiters is the buffet restaurant. Customers serve food onto their own plates and then pay at the end of the meal. Buffet restaurants typically still have waiters to serve drinks and alcoholic beverages. Fast food restaurants are also considered a restaurant.

The travelling public has long been catered for with ship's messes and railway restaurant cars which are, in effect, travelling restaurants. Many railways, the world over, also cater for the needs of travellers by providing railway refreshment rooms, a form of restaurant, at railway stations. In the 2000s, a number of travelling restaurants, specifically designed for tourists, have been created. These can be found on trams, boats, buses, etc.

A restaurant's proprietor is called a "restaurateur", this derives from the French verb "restaurer", meaning "to restore". Professional cooks are called chefs, with there being various finer distinctions (e.g. sous-chef, chef de partie). Most restaurants (other than fast food restaurants and cafeterias) will have various waiting staff to serve food, beverages and alcoholic drinks, including busboys who remove used dishes and cutlery. In finer restaurants, this may include a host or hostess, a maÃ®tre d'hÃ´tel to welcome customers and to seat them, and a sommelier or wine waiter to help patrons select wines.
A new route to becoming a restauranter, rather than working one's way up through the stages, is to operate a food truck. Once a sufficient following has been obtained, a permanent restaurant site can be opened. This trend has become common in the UK and the US.

A chef's table is a table located in the kitchen of a restaurant, reserved for VIPs and special guests. Patrons may be served a themed tasting menu prepared and served by the head chef. Restaurants can require a minimum party and charge a higher flat fee. Because of the demand on the kitchen's facilities, chef's tables are generally only available during off-peak times.

Armenian cuisine is one of the main elements of the national culture. It's something that Armenians really love to present to the world. There are lots of restaurants in Armenia. And the majority is centralized in the capital city - Yerevan.

See also : "Best Restaurants in Yerevan"

The Armenian restaurants offer a wide variety of dishes, delicious food, sweets, and bakings. The most famous Armenian dishes are barbeque, dolma, fish, khash, harissa, aveluk, and many more. National Armenian restaurants try to bring back the right Armenian breakfast and the tradition of having breakfast. 

They say that once it was accepted to have a so-called "Brduchayin Nakhachash (Ô²ÖÕ¤Õ¸ÖÕ³Õ¡ÕµÕ«Õ¶ Õ¶Õ¡Õ­Õ¡Õ³Õ¡Õ·)" that means "A Sandwich Breakfast". This "brduch" is an Armenian sandwich made of Armenian lavash (Armenian bread), meal, and nut.
Besides national Armenian restaurants, there are also some foreign restaurants in Armenia which once more proves the Armenians' love towards the restaurant and food culture.

In China, food catering establishments that may be described as restaurants have been known since the 11th century in Kaifeng, China's capital during the first half of the Song dynasty (960â1279). Probably growing out of the tea houses and taverns that catered to travellers, Kaifeng's restaurants blossomed into an industry catering to locals as well as people from other regions of China. There is a direct correlation between the growth of the restaurant businesses and institutions of theatrical stage drama, gambling and prostitution which served the burgeoning merchant middle class during the Song dynasty. Restaurants catered to different styles of cuisine, price brackets, and religious requirements. Even within a single restaurant choice were available, and people ordered the entree from written menus. An account from 1275 writes of Hangzhou, the capital city for the last half of the dynasty:
The restaurants in Hangzhou also catered to many northern Chinese who had fled south from Kaifeng during the Jurchen invasion of the 1120s, while it is also known that many restaurants were run by families formerly from Kaifeng.

In Ancient Greece and Ancient Rome, thermopolia (singular "thermopolium") were small restaurant-bars that offered food and drinks to customers. A typical thermopolium had L-shaped counters in which large storage vessels were sunk, which would contain either hot or cold food. Their popularity was linked to the lack of kitchens in many dwellings and the ease with which people could purchase prepared foods. Furthermore, eating out was considered an important aspect of socializing.

In Pompeii, 158 thermopolia with service counters have been identified throughout the town. They were concentrated along the main axis of the town and the public spaces where they were frequented by the locals.

France has had a rich history with the development of various forms of inns and eateries, eventually to form many of the now-ubiquitous elements of the modern restaurant.

As far back as the thirteenth century, inns served a variety of food â bread, cheese, bacon, roasts, usually eaten at a common table. Parisians could buy what was essentially take-out food from "rÃ´tisseurs", who prepared roasted meat dishes, and pastry-cooks, who could prepare meat pies and often more elaborate dishes. Municipal statutes stated that the official prices per item were to be posted at the entrance; this was the first official mention of menus.

Taverns also served food, as did cabarets. A cabaret, however, unlike a tavern, served food at tables with tablecloths, provided drinks with the meal, and charged by the customers' choice of dish, rather than by the pot. Cabarets were reputed to serve better food than taverns and a few, such as the Petit Maure, became well-known. A few cabarets had musicians or singing, but most, until the late 19th century, were simply convivial eating places.

The first cafÃ© opened in Paris in 1672 at the Saint-Germain fair. By 1723 there were nearly four hundred cafÃ©s in Paris, but their menu was limited to simpler dishes or confectionaries, such as coffee, tea, chocolate, ice creams, pastries, and liqueurs.

At the end of the 16th century, the guild of cook-caterers (later known as "traiteurs") was given its own legal status. The "traiteurs" dominated sophisticated food service, delivering or preparing meals for the wealthy at their residences. Taverns and cabarets were limited to serving little more than roast or grilled meats. Towards the end of the seventeenth century, both inns and then traiteurs began to offer "host's tables" ("tables d'hÃ´te"), where one paid a set price to sit at a large table with other guests and eat a fixed menu meal.

The earliest modern-format "restaurants" to use that name in Paris were the establishments which served bouillon, a broth made of meat and egg which was said to restore health and vigor. The first restaurant of this kind opened in 1765 or 1766 by Mathurin Roze de Chantoiseau on rue des Poulies, now part of the Rue de Louvre. The name of the owner is sometimes given as Boulanger. Unlike earlier eating places, it was elegantly decorated, and besides meat broth offered a menu of several other "restorative" dishes, including macaroni. Chantoiseau and other chefs took the title "traiteurs-restaurateurs".

In June 1786 the Provost of Paris issued a decree giving the new kind of eating establishment official status, authorizing "restaurateurs" to receive clients and to offer them meals until eleven in the evening in winter and midnight in summer. Ambitious cooks from noble households began to open more elaborate eating places. The first luxury restaurant in Paris, the Taverne Anglaise, was opened at the Palais-Royal at the beginning of 1786, shortly before the French Revolution, by Antoine Beauvilliers, the former chef of the Count of Provence, It had mahogany tables, linen tablecloths, chandeliers, well-dressed and trained waiters, a long wine list and an extensive menu of elaborately prepared and presented dishes.

The French Revolution caused a mass emigration of nobles, and many of their cook chose to open restaurants. One restaurant was started in 1791 by MÃ©ot, the former chef of the Duke of Orleans, which offered a wine list with twenty-two choices of red wine and twenty-seven of white wine. By the end of the century there were a collection of luxury restaurants at the Grand-Palais: HurÃ©, the Couvert espagnol; FÃ©vrier; the Grotte flamande; VÃ©ry, Masse and the CafÃ© de Chartres (still open, now Le Grand Vefour)

In the early 19th century traiteurs and restaurateurs, became known simply as "restaurateurs". The use of the term "restaurant" for the establishment itself only became common in the nineteenth century).

The first restaurant guide, called "Almanach des Gourmandes", written by Grimod de La ReyniÃ©re, was published in 1804. During the French Restoration period, the most celebrated restaurant was the Rocher de Cancale, frequented by the characters of Balzac. In the middle of the century, Balzac's characters moved to the Cafe Anglais, which in 1867 also hosted the famous Three Emperors Dinner hosted by Napoleon III in honor of Tsar Alexander II, Kaiser Wilhelm I and Otto von Bismarck during the Exposition Universelle in 1867 Other restaurants that occupy a place in French history and literature include Maxim's and Fouquet's. The restaurant of Hotel Ritz Paris, opened in 1898, was made famous by its chef, Auguste Escoffier. The 19th century also saw the appearance of new kinds of more modest restaurants, including the bistrot. The brasserie featured beer and was made popular during the 1867 Paris Exposition.

In the United States, it was not until the late 18th century that establishments that provided meals without also providing lodging began to appear in major metropolitan areas in the form of coffee and oyster houses. The actual term "restaurant" did not enter into the common parlance until the following century. Prior to being referred to as "restaurants" these eating establishments assumed regional names such as "eating house" in New York City, "restorator" in Boston, or "victualing house" in other areas. Restaurants were typically located in populous urban areas during the 19th century and grew both in number and sophistication in the mid-century due to a more affluent middle class and to suburbanization. The highest concentration of these restaurants were in the West, followed by industrial cities on the Eastern Seaboard.
<ref name="http://digital.library.unlv.edu/collections/menus/early-restaurants-america">Early Restaurants in America.</ref>

In the 1970s, there was one restaurant for every 7,500 persons. In 2016, there were 1,000,000 restaurants; one for every 310 people. The average person eats out five to six times weekly. 10% of the nation's workforce is composed of restaurant workers.

In Brazil, restaurants varieties mirrors the multitude of nationalities that arrived in the country: Japanese, Arab, German, Italian, Portuguese and many more.

In Colombia, a "piqueteadero" is a type of casual or rustic eatery. Meals are often shared, and typical offerings include dishes such as chorizo, chicharrÃ³n, fried organs, fried yuca, maduro and corn on the cob. Customers order the foods they want and the prepared foods are served together on a platter to be shared. The word "piquete" can be used to refer to a common Colombian type of meal that includes meat, yuca and potatoes, which is a type of meal served at a piqueteaderos. The verb form of the word piquete, piquetear, means to participate in binging, liquor drinking, and leisure activities in popular areas or open spaces.

In Peru, many indigenous, Spanish, and Chinese dishes are frequently found. Because of recent immigration from places such as China, and Japan, there are many Chinese and Japanese restaurants around the country, especially in the capital city of Lima.

Restaurant guides review restaurants, often ranking them or providing information to guide consumers (type of food, handicap accessibility, facilities, etc.). One of the most famous contemporary guides is the Michelin series of guides which accord from 1 to 3 stars to restaurants they perceive to be of high culinary merit. Restaurants with stars in the Michelin guide are formal, expensive establishments; in general the more stars awarded, the higher the prices.

The main competitor to the Michelin guide in Europe is the guidebook series published by Gault Millau. Its ratings are on a scale of 1 to 20, with 20 being the highest.

In the United States, the Forbes Travel Guide (previously the Mobil travel guides) and the AAA rate restaurants on a similar 1 to 5 star (Forbes) or diamond (AAA) scale. Three, four, and five star/diamond ratings are roughly equivalent to the Michelin one, two, and three star ratings while one and two star ratings typically indicate more casual places to eat. In 2005, Michelin released a New York City guide, its first for the United States. The popular Zagat Survey compiles individuals' comments about restaurants but does not pass an "official" critical assessment. FreshNYC recommends plausible New York City restaurants for busy New Yorkers and visitors alike.

The "Good Food Guide," published by the Fairfax Newspaper Group in Australia, is the Australian guide listing the best places to eat. Chefs Hats are awarded for outstanding restaurants and range from one hat through three hats. The "Good Food Guide" also incorporates guides to bars, cafes and providers. "The Good Restaurant Guide" is another Australian restaurant guide that has reviews on the restaurants as experienced by the public and provides information on locations and contact details. Any member of the public can submit a review.

Nearly all major American newspapers employ food critics and publish online dining guides for the cities they serve. Some news sources provide customary reviews of restaurants, while others may provide more of a general listings service.

More recently Internet sites have started up that publish both food critic reviews and popular reviews by the general public.

Many restaurants are small businesses, and franchise restaurants are common. There is often a relatively large immigrant representation, reflecting both the relatively low start-up costs of the industry (thus making restaurant ownership an option for immigrants with relatively few resources) and the cultural importance of food.

Indian restaurant industry is highly fragmented with more than 1.5 million outlets of which only around 3000 of them are from the organized segment. Organized segment includes Quick Service Restaurants (QSRs), Casual Dining, Cafes, Fine Dining and Pubs, Bars, Clubs and Lounges.
There are 86,915 commercial foodservice units in Canada, or 26.4 units per 10,000 Canadians. By segment, there are:

Fully 63% of restaurants in Canada are independent brands. Chain restaurants account for the remaining 37%, and many of these are locally owned and operated franchises.

The EU-27 has an estimated 1.6m businesses involved in 'accommodation & food services', more than 75% of which are small and medium enterprises.

As of 2006, there are approximately 215,000 full-service restaurants in the United States, accounting for $298 billion in sales, and approximately 250,000 limited-service (fast food) restaurants, accounting for $260 billion. Starting in 2016, Americans spent more on restaurants than groceries.

In October 2017, "The New York Times" reported there are 620,000 eating and drinking places in the United States, according to the Bureau of Labor Statistics. They also reported that the number of restaurants are growing almost twice as fast as the population.

One study of new restaurants in Cleveland, Ohio found that 1 in 4 changed ownership or went out of business after one year, and 6 out of 10 did so after three years. (Not all changes in ownership are indicative of financial failure.) The three-year failure rate for franchises was nearly the same.

Restaurants employed 912,100 cooks in 2013, earning an average $9.83 per hour. The waiting staff numbered 4,438,100 in 2012, earning an average $8.84 per hour.

Jiaxi Lu of the "Washington Post" reports in 2014 that, "Americans are spending $683.4 billion a year dining out, and they are also demanding better food quality and greater variety from restaurants to make sure their money is well spent."

Dining in restaurants has become increasingly popular, with the proportion of meals consumed outside the home in restaurants or institutions rising from 25% in 1950 to 46% in 1990. This is caused by factors such as the growing numbers of older people, who are often unable or unwilling to cook their meals at home and the growing number of single-parent households. It is also caused by the convenience that restaurants can afford people; the growth of restaurant popularity is also correlated with the growing length of the work day in the US, as well as the growing number of single parent households. Eating in restaurants has also become more popular with the growth of higher income households. At the same time, less expensive establishments such as fast food establishments can be quite inexpensive, making restaurant eating accessible to many.

The restaurant industry in the United States is large and quickly growing, with 10 million workers. 1 in every 12 U.S. residents work in the business, and during the 2008 recession, the industry was an anomaly in that it continued to grow. Restaurants are known for having low wages, which they claim are due to thin profit margins of 4-5%. For comparison, however, Walmart has a 1% profit margin.

As a result of these low wages, restaurant employees suffer from three times the poverty rate as other U.S. workers, and use food stamps twice as much.

Restaurants also employ marginalized groups. They are the largest employer of people of color. Restaurants rank as the second largest employer of immigrants. These workers statistically are concentrated in the lowest paying positions in the restaurant industry. In the restaurant industry, 39% of workers earn minimum wage or lower.

China entire country have several millions cantings, according China Canyin Report 2019 , in 2018 year there have 4.27 trillion RMB total revenue. In 2017 year, the entire canyin industry employment person have reached 30 millions, or 30 millions person are working in the canyin industry.

In many countries, restaurants are subject to inspections by health inspectors to maintain standards for public health, such as maintaining proper hygiene and cleanliness. As part of these inspections, cooking and handling practices of ground beef are taken into account to protect against the spread of E coli poisoning. The most common kind of violations of inspection reports are those concerning the storage of cold food at appropriate temperatures, proper sanitation of equipment, regular hand washing and proper disposal of harmful chemicals. Simple steps can be taken to improve sanitation in restaurants. As sickness is easily spread through touch, restaurants are encouraged to regularly wipe down tables, door knobs and menus.

Depending on local customs, legislation and the establishment, restaurants may or may not serve alcoholic beverages. Restaurants are often prohibited from selling alcoholic beverages without a meal by alcohol sale laws; such sale is considered to be activity for bars, which are meant to have more severe restrictions. Some restaurants are licensed to serve alcohol ("fully licensed"), or permit customers to "bring your own" alcohol (BYO / BYOB). In some places restaurant licenses may restrict service to beer, or wine and beer.





</doc>
<doc id="26438" url="https://en.wikipedia.org/wiki?curid=26438" title="Rolf Nevanlinna">
Rolf Nevanlinna

Rolf Herman Nevanlinna (nÃ© Neovius; 22 October 1895 â 28 May 1980) was a Finnish mathematician who made significant contributions to complex analysis.

Nevanlinna was born Rolf Herman Neovius, becoming Nevanlinna in 1906 when his father changed the family name.

The Neovius-Nevanlinna family contained many mathematicians: Edvard Engelbert Neovius (Rolf's grandfather) taught mathematics and topography at a military academy; Edvard Rudolf Neovius (Rolf's uncle) was a professor of mathematics at the University of Helsinki from 1883â1900; Lars Theodor Neovius-Nevanlinna (Rolf's uncle) was an author of mathematical textbooks; and Otto Wilhelm Neovius-Nevanlinna (Rolf's father) was a physicist, astronomer and mathematician.

After Otto obtained his Ph.D. in physics from the University of Helsinki, he studied at the Pulkovo Observatory with the German astronomer Herman Romberg, whose daughter, Margarete Henriette Louise Romberg, he married in 1892. Otto and Margarete then settled in Joensuu, where Otto taught physics, and there their four children were born: Frithiof (born 1894; also a mathematician), Rolf (born 1895), Anna (born 1896) and Erik (born 1901).

Nevanlinna began his formal education at the age of 7. Having already been taught to read and write by his parents, he went straight into the second grade but still found the work boring and soon refused to attend the school. He was then homeschooled before being sent to a grammar school in 1903 when the family moved to Helsinki, where his father took up a new post as a teacher at Helsinki High School. At the new school, Nevanlinna studied French and German in addition to the languages he already spoke: Finnish and Swedish. He also attended an orchestra school and had a love of music, which was encouraged by his mother:

Nevanlinna then progressed onto the Helsinki High School, where his main interests were classics and mathematics. He was taught by a number of teachers during this time but the best of them all was his own father, who taught him physics and mathematics. He graduated in 1913 having performed very well, although he was not the top student of his year. He then went beyond the school syllabus in the summer of 1913 when he read Ernst Leonard LindelÃ¶f's "Introduction to Higher Analysis"; from that time on, Nevanlinna had an enthusiastic interest in mathematical analysis. (LindelÃ¶f was also a cousin of Nevanlinna's father, and so a part of the Neovius-Nevanlinna mathematical family.)

Nevanlinna began his studies at the University of Helsinki in 1913, and received his Master of Philosophy in mathematics in 1917. LindelÃ¶f taught at the university and Nevanlinna was further influenced by him. During his time at the University of Helsinki, World War I was underway and Nevanlinna wanted to join the 27th JÃ¤ger Battalion, but his parents convinced him to continue with his studies. He did however join the White Guard in the Finnish Civil War, but did not see active military action. In 1919, Nevanlinna presented his thesis, entitled "Ãber beschrÃ¤nkte Funktionen die in gegebenen Punkten vorgeschriebene Werte annehmen" ("On limited functions prescribed values at given points"), to LindelÃ¶f, his doctoral advisor. The thesis, which was on complex analysis, was of high quality and Nevanlinna was awarded his Doctor of Philosophy on 2 June 1919.

When Nevanlinna earned his doctorate in 1919, there were no university posts available so he became a school teacher. His brother, Frithiof, had received his doctorate in 1918 but likewise was unable to take up a post at a university, and instead began working as a mathematician for an insurance company. Frithiof recruited Rolf to the company, and Nevanlinna worked for the company and as a school teacher until he was appointed a Docent of Mathematics at the University of Helsinki in 1922. During this time, he had been contacted by Edmund Landau and requested to move to Germany to work at the University of GÃ¶ttingen, but did not accept.

After his appointment as Docent of Mathematics, he gave up his insurance job but did not resign his position as school teacher until he received a newly created full professorship at the university in 1926. Despite this heavy workload, it was between the years of 1922â25 that he developed what would become to be known as Nevanlinna theory.

From 1947 Nevanlinna had a chair in the University of Zurich, which he held on a half-time basis after receiving in 1948 a permanent position as one of the 12 salaried Academicians in the newly created Academy of Finland.

Rolf Nevanlinna's most important mathematical achievement is the "value distribution theory" of meromorphic functions. The roots of the theory go back to the result of Ãmile Picard in 1879, showing that a non-constant complex-valued function which is analytic in the entire complex plane assumes all complex values save at most one. In the early 1920s Rolf Nevanlinna, partly in collaboration with his brother Frithiof, extended the theory to cover meromorphic functions, i.e. functions analytic in the plane except for isolated points in which the Laurent series of the function has a finite number of terms with a negative power of the variable. Nevanlinna's value distribution theory or Nevanlinna theory is crystallised in its two "Main Theorems". Qualitatively, the first one states that if a value is assumed less frequently than average, then the function comes close to that value more often than average. The Second Main Theorem, more difficult than the first one, states roughly that there are relatively few values which the function assumes less often than average.

Rolf Nevanlinna's article "Zur Theorie der meromorphen Funktionen" which contains the Main Theorems was published in 1925 in the journal Acta Mathematica. Hermann Weyl has called it "one of the few great mathematical events of the [twentieth] century." Nevanlinna gave a fuller account of the theory in the monographs "Le thÃ©oreme de Picard â Borel et la thÃ©orie des fonctions mÃ©romorphes" (1929) and "Eindeutige analytische Funktionen" (1936).

Nevanlinna theory touches also on a class of functions called the Nevanlinna class, or functions of "bounded type".
When the Winter War broke out (1939), Nevanlinna was invited to join the Finnish Army's Ballistics Office to assist in improving artillery firing tables. These tables had been based on a calculation technique developed by General Vilho Petter Nenonen, but Nevanlinna now came up with a new method which made them considerably faster to compile. In recognition of his work he was awarded the Order of the Cross of Liberty, Second Class, and throughout his life he held this honour in especial esteem.

Among Rolf Nevanlinna's later interests in mathematics were the theory of Riemann surfaces (the monograph "Uniformisierung" in 1953) and functional analysis ("Absolute analysis" in 1959, written in collaboration with his brother Frithiof). Nevanlinna also published in Finnish a book on the foundations of geometry and a semipopular account of the Theory of Relativity. His Finnish textbook on the elements of complex analysis, "Funktioteoria" (1963), written together with Veikko Paatero, has appeared in German, English and Russian translations.

Rolf Nevanlinna supervised at least 28 doctoral theses. His first and most famous doctoral student was Lars Ahlfors, one of the first two Fields Medal recipients. The research for which Ahlfors was awarded the prize (proving the Denjoy Conjecture, now known as the DenjoyâCarlemanâAhlfors theorem) was strongly based on Nevanlinna's work.

Nevanlinna's work was recognised in the form of honorary degrees which he held from the universities of Heidelberg, the University of Bucharest, the University of Giessen, the Free University of Berlin, the University of Glasgow, the University of Uppsala, the University of Istanbul and the University of JyvÃ¤skylÃ¤. He was an honorary member of several learned societies, among them the London Mathematical Society and the Hungarian Academy of Sciences. â The 1679 Nevanlinna main belt asteroid is named after him.

From 1954, Rolf Nevanlinna chaired the committee which set about the first computer project in Finland. 

Rolf Nevanlinna served as President of the International Mathematical Union (IMU) from 1959 to 1963 and as President of the International Congress of Mathematicians (ICM) in 1962.

In 1964, Nevanlinna's connections with President Urho Kekkonen were instrumental in bringing about a total reorganization of the Academy of Finland.

From 1965 to 1970 Nevanlinna was Chancellor of the University of Turku.

Although Nevanlinna did not participate actively in politics, he was known to sympathise with the right-wing Patriotic People's Movement and, partly because of his half-German parentage, was also sympathetic towards Nazi Germany; with many mathematics professors fired in the 1930s due to the Nuremberg Laws, mathematicians sympathetic to the Nazi policies were sought as replacements, and Nevanlinna accepted a position as professor at the University of GÃ¶ttingen in 1936 and 1937. His sympathy towards the Nazis led to his removal from his position as Rector of the University of Helsinki after Finland made peace with the Soviet Union in 1944.

In the spring of 1941, Finland contributed a Volunteer Battalion to the Waffen-SS. At the time, the battalion was a symbolic bond between Germany and Finland as both fought against the Soviet Union but without a formal alliance between the two nations. In 1942, a committee was established for the Volunteer Battalion to take care of the battalion's somewhat strained relations with its German commanders, and Nevanlinna was chosen to be the chairman of the committee, as he was a person respected in Germany but loyal to Finland. He stated in his autobiography that he accepted this role due to a "sense of duty".

Nevanlinna's political activities did not colour his relationships with his mathematical contacts; after World War II, the Soviet mathematical community was isolated from the Western mathematical community and the International Colloquium on Function Theory in Helsinki in 1957, directed by Nevanlinna, was the first post-war occasion when Soviet mathematicians could contact their Western colleagues in person. In 1965, Nevanlinna was an honorary guest at a function theory congress in Soviet Armenia.

When the IMU in 1981 decided to create a prize, similar to the Fields Medal, in theoretical computer science and the funding for the prize was secured from Finland, the Union decided to give Nevanlinna's name to the prize; the Rolf Nevanlinna Prize is awarded every four years at the ICM. In 2018, the General Assembly of the IMU approved a resolution to remove Nevanlinna's name from the prize.




</doc>
<doc id="26441" url="https://en.wikipedia.org/wiki?curid=26441" title="Red panda">
Red panda

The red panda ("Ailurus fulgens") is a mammal native to the eastern Himalayas and southwestern China. It is listed as Endangered on the IUCN Red List because the wild population is estimated at fewer than 10,000 mature individuals and continues to decline due to habitat loss and fragmentation, poaching, and inbreeding depression.

The red panda has reddish-brown fur, a long, shaggy tail, and a waddling gait due to its shorter front legs; it is roughly the size of a domestic cat, though with a longer body, and is somewhat heavier. It is arboreal and feeds mainly on bamboo, but also eats eggs, birds, and insects. It is a solitary animal, mainly active from dusk to dawn, and is largely sedentary during the day. It is also called the lesser panda, the red bear-cat, and the red cat-bear.

The red panda is the only living species of the genus "Ailurus" and the family Ailuridae. It has been previously placed in the raccoon and bear families, but the results of phylogenetic analysis provide strong support for its taxonomic classification in its own family, Ailuridae, which is part of the superfamily Musteloidea, along with the weasel, raccoon and skunk families. Two subspecies are recognized. It is not closely related to the giant panda, which is a basal ursid.

The head and body length of a red panda measures , and its tail is . Males weigh and females . They have long, soft, reddish-brown fur on the upper parts, blackish fur on the lower parts, and a light face with tear markings and robust cranio-dental features. The light face has white badges similar to those of a raccoon, but each individual can have distinctive markings. Their roundish heads have medium-sized upright ears, black noses, and blackish eyes. Their long, bushy tails with six alternating transverse ochre rings provide balance and excellent camouflage against their habitat of moss- and lichen-covered trees. The legs are black and short with thick fur on the soles of the paws. This fur serves as thermal insulation on snow-covered or icy surfaces and conceals scent glands, which are also present on the anus.

The red panda is specialized as a bamboo feeder with strong, curved and sharp semi-retractile claws standing inward for grasping narrow tree branches, leaves, and fruit. Like the giant panda, it has a "false thumb", which is an extension of the wrist bone. When descending a tree head-first, the red panda rotates its ankle to control its descent, one of the few climbing species to do so.

The red panda is endemic to the temperate forests of the Himalayas, and ranges from the foothills of western Nepal to China in the east. Its easternmost limit is the Qinling Mountains of the Shaanxi Province in China. Its range includes southern Tibet, Sikkim and Assam in India, Bhutan, the northern mountains of Burma, and in south-western China, in the Hengduan Mountains of Sichuan and the Gongshan Mountains in Yunnan. It may also live in south-west Tibet and northern Arunachal Pradesh, but this has not been documented. Locations with the highest density of red pandas include an area in the Himalayas that has been proposed as having been a refuge for a variety of endemic species in the Pleistocene. The distribution range of the red panda should be considered disjunct, rather than continuous. A disjunct population inhabits the Meghalaya Plateau of north-eastern India.

The red panda lives between altitude, inhabiting areas of moderate temperature between with little annual change. It prefers mountainous mixed deciduous and conifer forests, especially with old trees and dense understories of bamboo.

During a survey in the 1970s, signs of red pandas were found in Nepal's Dhorpatan Hunting Reserve. Their presence was confirmed in spring 2007 when four red pandas were sighted at elevations ranging from . Its westernmost distribution is in Rara National Park. In 2018, red pandas were sighted at elevations of in Nepal's Lamjung District.

The red panda population in Sichuan Province is larger and more stable than the Yunnan population, suggesting a southward expansion from Sichuan into Yunnan in the Holocene.
The red panda has become extirpated from the Chinese provinces of Guizhou, Gansu, Shaanxi, and Qinghai.

The red panda is territorial; it is solitary except during mating season. The species is generally quiet except for some twittering, tweeting, and whistling communication sounds. It has been reported to be both nocturnal and crepuscular, sleeping on tree branches or in tree hollows during the day and increasing its activity in the late afternoon and early evening hours. It sleeps stretched out on a branch with legs dangling when it is hot, and curled up with its tail over the face when it is cold. This animal is very heat-sensitive, with an optimal "well-being" temperature between .
Shortly after waking, red pandas clean their fur somewhat like a cat would, licking their front paws and then rubbing their backs, torsos, and sides. They also rub their backs and bellies along the sides of trees or rocks. Then they patrol their territories, marking with urine and a weak musk-smelling secretion from their anal glands. They search for food running along the ground or through the trees. Red pandas may use their forepaws alternately to bring food to their mouths or place food directly into their mouths.

Predators of the red panda include the snow leopard, mustelids, and humans. If they feel threatened or sense danger, they may try to escape by climbing a rock column or tree. If they can no longer flee, they stand on their hind legs to make themselves appear larger and use the sharp claws on their front paws to defend themselves. A red panda, Futa, became a visitor attraction in Japan for his ability to stand upright for ten seconds at a time. (See also: facultative biped)

Red pandas are excellent climbers, and forage largely in trees. They eat mostly bamboo, and may eat small mammals, birds, eggs, flowers, and berries. In captivity, they were observed to eat birds, flowers, maple and mulberry leaves, and bark and fruits of maple, beech, and mulberry.

Like the giant panda, they cannot digest cellulose, so they must consume a large volume of bamboo to survive. Their diets consist of about two-thirds bamboo, but they also eat mushrooms, roots, acorns, lichens, and grasses. Occasionally, they supplement their diets with fish and insects. They do little more than eat and sleep due to their low-calorie diets.

Bamboo shoots are more easily digested than leaves, exhibiting the highest digestibility in summer and autumn, intermediate digestibility in the spring, and lowest digestibility in the winter. These variations correlate with the nutrient contents in the bamboo. Red pandas process bamboo poorly, especially the cellulose and cell wall components. This implies microbial digestion plays only a minor role in their digestive strategy. To survive on this poor-quality diet, they have to eat the high-quality sections of the bamboo plant, such as the tender leaves and shoots, in large quantities, over of fresh leaves and of fresh shoots daily. This food passes through the digestive tract fairly rapidly (about 2â4 hr) so as to maximize daily nutrient intake. Red pandas can taste artificial sweeteners, such as aspartame, and are the only nonprimates known to be able to do so.

Red pandas are able to reproduce at around 18 months of age, and are fully mature at two to three years. Adults rarely interact in the wild except to mate. Both sexes may mate with more than one partner during the mating season from mid-January to early March. A few days before birth, females begin to collect material, such as brushwood, grass, and leaves, to build a nest, which is normally located in a hollow tree or a rock crevice. After a gestation period of 112 to 158 days, the female gives birth in mid-June to late July to one to four (usually 1â2) blind and deaf cubs weighing each.

After birth, the mother cleans the cubs, and can then recognize each by its smell. At first, she spends 60% to 90% of her time with the cubs. After the first week, the mother starts spending more time outside the nest, returning every few hours to nurse and groom the cubs. She moves the young frequently among several nests, all of which she keeps clean. The cubs start to open their eyes at about 18 days of age. By about 90 days, they achieve full adult fur and coloring, and begin to venture out of the nest. They also start eating solid foods at this point, weaning at around six to eight months of age. The cubs stay with their mother until the next litter is born in the following summer. Males rarely help raise the young, and only if they live in pairs or in small groups.

A red panda's lifespan ranges between eight and 10 years, but individuals have been known to reach 15 years.

The primary threats to red pandas are direct harvest from the wild, live or dead, competition with domestic livestock resulting in habitat degradation, and deforestation resulting in habitat loss or fragmentation. The relative importance of these factors is different in each region, and is not well understood. For instance, in India, the biggest threat seems to be habitat loss followed by poaching, while in China, the biggest threat seems to be hunting and poaching. A 40% decrease in red panda populations has been reported in China over the last 50 years, and populations in western Himalayan areas are considered to be lower.

Deforestation can inhibit the spread of red pandas and exacerbate the natural population subdivision by topography and ecology, leading to severe fragmentation of the remaining wild population. Fewer than 40 animals in four separate groups share resources with humans in Nepal's Langtang National Park, where only 6% of is preferred red panda habitat. Although direct competition for food with domestic livestock is not significant, livestock can depress bamboo growth by trampling.

Small groups of animals with little opportunity for exchange between them face the risk of inbreeding, decreased genetic diversity, and even extinction. In addition, clearcutting for firewood or agriculture, including hillside terracing, removes old trees that provide maternal dens and decreases the ability of some species of bamboo to regenerate.

In south-west China, red pandas are hunted for their fur, especially for the highly valued bushy tails, from which hats are produced. In these areas, the fur is often used for local cultural ceremonies. In weddings, the bridegroom traditionally carries the hide. The "good-luck charm" red panda-tail hats are also used by local newly-weds. This practice may be quite old, as the red panda seems to be depicted in a 13th-century Chinese pen-and-ink scroll showing a hunting scene. Little or no mention of the red panda is made in the culture and folklore of Nepal.

In the past, red pandas were captured and sold to zoos. In an article appearing in the "International Zoo News" in 1969, one reported he personally had handled 350 red pandas in 17 years.

Due to CITES, this zoo harvest has decreased substantially in recent years, but poaching continues, and red pandas are often sold to private collectors at exorbitant prices. In some parts of Nepal and India, red pandas are kept as pets.

The red panda has a naturally low birth rate (usually one single or twin birth per year), and a high death rate in the wild.

The red panda is listed as endangered on the IUCN Red List since 2008 because the global population is estimated at about 10,000 individuals, with a decreasing population trend; only about half of the total area of potential habitat of is actually being used by the species. Due to its shy and secretive nature, and its largely nocturnal habits, observation of red pandas is difficult. Therefore, population figures in the wild are determined by population density estimates and not direct counts. It is protected in all range countries, and hunting is illegal. It is listed in CITES Appendix I.

Worldwide population estimates range from fewer than 2,500 to between 16,000 and 20,000 individuals. In 1999, the total population in China was estimated at between 3,000 and 7,000 individuals. In 2001, the wild population in India was estimated at between 5,000 and 6,000 individuals. Estimates for Nepal indicate only a few hundred individuals.
Reliable population numbers are hard to find, partly because other animals have been mistaken for the red panda. For instance, one report from Myanmar stated that red pandas were still fairly common in some areas; however, the accompanying photographic proof of the "red panda" was in fact a viverrid species.

Conservation efforts are highly variable between countries:

A community-managed forest in Ilam District of eastern Nepal is home to 15 red pandas which generate household income through tourism activities, including homestays. Villagers in the high-altitude areas of Arunachal Pradesh have formed the Pangchen Red Panda Conservation Alliance comprising five villages with a community-conserved forest area of at an altitude of to over .

The international red panda studbook is currently managed at Rotterdam Zoo in the Netherlands. In cooperation with the International Red Panda Management Group, they coordinate the Species Survival Plan in North America, the European Endangered Species Programme in Europe, and other captive-breeding programs in Australia, India, Japan, and China. As of 2006, more than 800 individuals were kept in zoos and parks around the world. Of these, 511 individuals of the Himalayan red panda were kept in 173 institutions and 306 individuals of Styan's red panda were kept in 81 institutions.
Since 2009, the North American Red Panda Species Survival Plan is coordinated at the Knoxville Zoo, which by 2011 had 101 red panda births. Only the Rotterdam Zoo has had more captive births worldwide.

The Padmaja Naidu Himalayan Zoological Park in Darjeeling successfully released four captive-bred red pandas to the wild in August and November 2003.<ref name="india/studbook"></ref>

The most often cited example of keeping red pandas as pets is the case of former Indian prime minister Indira Gandhi. Pandas were presented to her family as a gift, and they were then housed in "a special tree house".

"Ailurus fulgens" was the scientific name proposed by FrÃ©dÃ©ric Cuvier in 1825 who described a zoological specimen sent by Alfred Duvaucel "from the mountains north of India". He was the first to also use the vernacular name "panda". In the 19th and 20th centuries, the following specimens were described:
Pocock distinguished "A. f. styani" from "A. f. fulgens" by its longer winter coat and greater blackness of the pelage, bigger skull, more strongly curved forehead, and more robust teeth. His description is based on skulls and skins collected in Sichuan, Myitkyina District close to the border of Yunnan, and Upper Burma.
Styan's red panda is supposedly larger and darker in color than the Himalayan red panda, but with considerable variation in both subspecies, and some individuals may be brown or yellowish brown rather than red.

Two subspecies are considered valid:
The Brahmaputra River is often considered the natural division between the two subspecies, where it makes a curve around the eastern end of the Himalayas, although some authors suggest "A. f. fulgens" extends farther eastward into China.

The name "Ailurus fulgens refulgens" is sometimes incorrectly used for "A. f. styani". This stems from a lapsus made by Henri Milne-Edwards in 1874. making "A. f. refulgens" a "nomen nudum". This has been corrected in more recent publications.

At various times, the red panda was placed in the Procyonidae, Ursidae, with "Ailuropoda" (giant panda) in the Ailuropodinae (until this family was moved into the Ursidae), and into its own family, the Ailuridae. This uncertainty comes from difficulty in determining whether certain characteristics of "Ailurus" are phylogenetically conservative or are derived and convergent with species of similar ecological habits.

Evidence based on the fossil record, serology, karyology, behavior, anatomy, and reproduction reflect closer affinities with Procyonidae than Ursidae. However, ecological and foraging specializations and distinct geographical distribution in relation to modern procyonids support classification in the separate family Ailuridae.

Recent molecular systematic DNA research also places the red panda into its own family, Ailuridae, a part of the broad superfamily Musteloidea that also includes the mephitids (skunks), procyonids (raccoons), and mustelids (weasels). According to the most recent phylogenetic studies, the red panda's closest relatives within the Musteloidea superfamily are the procyonids and mustelids.

The red panda is considered a living fossil and only distantly related to the giant panda ("Ailuropoda melanoleuca"), as it is naturally more closely related to the other members of the superfamily Musteloidea to which it belongs. The common ancestor of both pandas (which also was an ancestor for all living bears; pinnipeds like seals and walruses; and members of the family Musteloidea like weasels and otters) can be traced back to the Paleogene period tens of millions of years ago, with a wide distribution across Eurasia.

Fossils of the extinct red panda "Parailurus anglicus" were excavated in China in the east to Britain in the west. In 1977, a single tooth of "Parailurus" was discovered in the Pliocene Ringold Formation of Washington. This first North American record is almost identical to European specimens and indicates the immigration of this species from Asia. In 2004, a tooth from a red panda species never before recorded in North America was discovered at the Gray Fossil Site in Tennessee. The tooth dates from 4.5â7 million years ago. This species, described as "Pristinailurus bristoli", indicates that a second, more primitive ailurine lineage inhabited North America during the Miocene. Cladistic analysis suggests that "Parailurus" and "Ailurus" are sister taxa. Additional fossils of "Pristinailurus bristoli" were discovered at the Gray Fossil Site in 2010 and in 2012.
The discovery in Spain of the postcranial remains of "Simocyon batalleri", a Miocene relative to the red panda, supports a sister-group relationship between red pandas and bears. The discovery suggests the red panda's "false thumb" was an adaptation to arboreal locomotionÂ â independent of the giant panda's adaptation to manipulate bambooÂ â one of the most dramatic cases of convergent evolution among vertebrates.

"Ailurus" is adopted from the ancient Greek word (""), meaning "cat". The specific epithet "fulgens" is Latin for "shining, bright".

Panda is a Roman goddess of peace and travellers, who was called upon before starting a difficult journey.

The Lepcha call it "sak nam". In Nepal, it is called "bhalu biralo" (bear-cat) and "habre". The Sherpa people of Nepal and Sikkim call it "ye niglva ponva" and "wah donka". The word "wáº­Ë" is Sunuwari meaning bear; in Tamang language, a small, red bear is called "tÄwÄm". In the Kanchenjunga region of eastern Nepal, the Limbu people know red pandas as "kaala" (literally "dark") because of their underside pelage; villagers of Tibetan origin call them "hoptongar".

Additionally, Pocock lists the vernacular names "ye" and ' (Nepal); "thokya" and "thongwa" (Limbu); "oakdonga" or "wakdonka" and "woker" (Bhotia); "saknam sunam" (Lepcha). ' may originate from the Nepali word ' or ', a small bamboo, "Arundinaria intermedia", but also refers to a kind of small leopard, or cat-bear. The word ' may originate from the Nepali ' ("claw") or "" ("paw").
'Poonya' also means "eater of bamboo". The name panda could originate from "panjÄ".

In modern Chinese, the red panda is called "xiÄoxiÃ³ngmÄo" ( and , lesser or small panda, or literally "little bear cat"), or / ("hÃ³ngxiÃ³ngmÄo", red panda or literally "red bear cat").

In English, the red panda is also called "lesser panda", "true panda" and "common panda".
The first known written record of the red panda occurs in a 13th-century Chinese scroll depicting a hunting scene between hunters and the red panda.

The red panda was recognized as the state animal of Sikkim in the early 1990s, and was the mascot of the Darjeeling Tea Festival.

In 2005, Babu, a male red panda at Birmingham Nature Centre in Birmingham, England, escaped and briefly became a media celebrity, before being recaptured. He was subsequently voted "Brummie of the Year", the first animal to receive this honor.
Rusty, a male red panda at the National Zoo in Washington, DC, similarly attracted media attention when he briefly escaped in 2013.

The name of the open-source Firefox web browser is said to have been derived from a nickname of the red panda: "fire fox".

An anthropomorphic red panda was featured as Master Shifu, the kung fu teacher, in the 2008 film "Kung Fu Panda", and its sequels "Kung Fu Panda 2" in 2011 and "Kung Fu Panda 3" in 2016. The red panda Futa inspired the character of Pabu, the so-called "fire ferret" animal companion (primarily of Bolin), in the U.S. animated TV series "The Legend of Korra".

Jetstar Japan uses a red panda mascot character named "Jetta" (ã¸ã§ãå¤ª).

An anthropomorphic red panda, Retsuko, is the main character of the TV anime and Netflix original series "Aggretsuko".




</doc>
<doc id="26442" url="https://en.wikipedia.org/wiki?curid=26442" title="Roppongi">
Roppongi

The name "Roppongi", which appears to have been coined around 1660, literally means "six trees". Six very old and large zelkova trees used to mark the area; the first three were cleared, and the last were destroyed during World War II. Another legend has it that the name comes from the fact that six "daimyÅs" lived nearby during the Edo period, each with the kanji character for "tree" or a kind of tree in their names. Roppongi was not extensively populated until after the Meiji Restoration, although the area was trafficked for centuries and served as the site of the cremation of ShÅgun Tokugawa Hidetada's wife in 1626.

In 1890, the Third Imperial Guard of the Imperial Japanese Army was moved to a site near Roppongi (now home to the Pacific bureau of "Stars and Stripes"). The influx of soldiers led to the area's rise as a nightlife district, briefly interrupted by the Great Kanto earthquake which flattened the area in 1923. Roppongi was administratively part of Azabu Ward from 1878 to 1947.

After World War II, during which the area was again destroyed, this time by aerial bombing raids, the United States Army and Allied government officials occupied several facilities in the area, beginning Roppongi's reputation as a neighborhood with large numbers of non-Japanese. Several large US military installations were located in the nearby area, with Hardy Barracks probably the most significant (the US Embassy Housing Compound and Akasaka Press Center including Hardy Barracks Recreational Lodging, "Stars and Stripes" office and heliport are still there). Surrounding the military installations were many Japanese-owned restaurants, pool halls, bars, and brothels which catered to US military personnel but were also often frequented by Japanese customers.

Starting in the late 1960s, Roppongi became popular among Japanese people and foreigners alike for its disco scene, which attracted many of Tokyo's entertainment elites. Contributing to the international scene was the location of several foreign embassies and foreign corporate offices in the Roppongi area. However, many dance clubs shut down in the recession following the market crash of 1989.

The Roppongi area received a major economic boost in 2002â2003 when the Izumi Garden Tower and the Roppongi Hills high-rise complexes were completed. These projects brought high-end office and condominium space to Roppongi for the first time. The Tokyo Midtown project in neighbouring Akasaka, which was completed in 2006, and includes the first Tokyo Ritz-Carlton Hotel, continued this trend.

The area features numerous bars, nightclubs, strip clubs, restaurants, hostess clubs, cabarets, and other forms of entertainment. Among the expatriate community, the area tends to be favored by business people, students, and off-duty US military personnel. Overall, the neighborhood caters to a younger crowd.
Clubs can range from large, multi-level establishments, to smaller one-room clubs located in upper levels of buildings.
In more recent times some of the larger venues with known Yakuza connections have closed.

Restaurants in Roppongi vary from upscale Japanese fare to popular international restaurants.

In the past, Roppongi had a reputation as an area with high Yakuza presence, whether as customers at Roppongi establishments, conducting business, or managing or owning clubs and bars in the area. Although still exerting some influence in Roppongi, in recent times they appear to have shifted much of their presence to other districts in the Tokyo area.

In 2006, Nigerian immigrants to Japan began opening a number of bars and nightclubs in the area, following an earlier group of innovators who had been in business in Roppongi for many years. The Nigerians were noted for using visible, high-pressure tactics to draw customers to their bars. In 2009 and 2010 a series of drink-spiking incidents, in which customers reported being drugged and robbed, were linked to Nigerian-owned bars. The incidents resulted in the United States embassy in Japan warning US citizens to avoid certain bars and clubs in Roppongi. An investigation by "The Japan Times" in July 2011 found that though drink spiking occurred, most of the incidents did not involve criminal activity. Many customers claimed unusually severe hangovers after nights spent in Nigerian-run establishments. Similar complaints are often made about non-Nigerian bars in Roppongi that offer unlimited drink packages and often lace drinks with hard liquor to minimize customer consumption and increase profit.

Mori Building Company and The PokÃ©mon Company have their headquarters in the Roppongi Hills Mori Tower.

Companies based in Roppongi include:


Public elementary and middle schools are operated by the Minato City (the Minato Ward) Board of Education. Roppongi Junior High School is located at Imoarai-Zaka, in Roppongi.

Public high schools are operated by the Tokyo Metropolitan Government Board of Education. Roppongi High School is located in Roppongi.

Toyo Eiwa Jogakuin is private girls school, also located at Torii-Zaka in the district.

The American School in Japan Early Learning Center is in Roppongi Hills.



</doc>
<doc id="26444" url="https://en.wikipedia.org/wiki?curid=26444" title="Robert Louis Stevenson">
Robert Louis Stevenson

Robert Louis Stevenson (13 November 1850 â 3 December 1894) was a Scottish novelist and travel writer, most noted for "Treasure Island", "Kidnapped", "Strange Case of Dr Jekyll and Mr Hyde", and "A Child's Garden of Verses".

Born and educated in Edinburgh, Stevenson suffered from serious bronchial trouble for much of his life, but continued to write prolifically and travel widely in defiance of his poor health. As a young man, he mixed in London literary circles, receiving encouragement from Andrew Lang, Edmund Gosse, Leslie Stephen and W.Â E.Â Henley, the last of whom may have provided the model for Long John Silver in "Treasure Island". In 1890, he settled in Samoa, where he died in 1894.

A celebrity in his lifetime, Stevenson's critical reputation has fluctuated since his death, though today his works are held in general acclaim. He is currently ranked as the 26th most translated author in the world.

Stevenson was born at 8 Howard Place, Edinburgh, Scotland on 13 November 1850 to Thomas Stevenson (1818â1887), a leading lighthouse engineer, and his wife Margaret Isabella (born Balfour, 1829â1897). He was christened Robert Lewis Balfour Stevenson. At about age 18, he changed the spelling of "Lewis" to "Louis", and he dropped "Balfour" in 1873.

Lighthouse design was the family's profession; Thomas's father (Robert's grandfather) was civil engineer Robert Stevenson, and Thomas's brothers (Robert's uncles) Alan and David were in the same field. Thomas's maternal grandfather Thomas Smith had been in the same profession. However, Robert's mother's family were gentry, tracing their lineage back to Alexander Balfour who had held the lands of Inchyra in Fife in the fifteenth century. His mother's father Lewis Balfour (1777â1860) was a minister of the Church of Scotland at nearby Colinton, and her siblings included physician George William Balfour and marine engineer James Balfour. Stevenson spent the greater part of his boyhood holidays in his maternal grandfather's house. "Now I often wonder what I inherited from this old minister," Stevenson wrote. "I must suppose, indeed, that he was fond of preaching sermons, and so am I, though I never heard it maintained that either of us loved to hear them."

Lewis Balfour and his daughter both had weak chests, so they often needed to stay in warmer climates for their health. Stevenson inherited a tendency to coughs and fevers, exacerbated when the family moved to a damp, chilly house at 1 Inverleith Terrace in 1851. The family moved again to the sunnier 17 Heriot Row when Stevenson was six years old, but the tendency to extreme sickness in winter remained with him until he was 11. Illness was a recurrent feature of his adult life and left him extraordinarily thin. Contemporaneous views were that he had tuberculosis, but more recent views are that it was bronchiectasis or even sarcoidosis.

Stevenson's parents were both devout Presbyterians, but the household was not strict in its adherence to Calvinist principles. His nurse Alison Cunningham (known as Cummy) was more fervently religious. Her mix of Calvinism and folk beliefs were an early source of nightmares for the child, and he showed a precocious concern for religion. But she also cared for him tenderly in illness, reading to him from John Bunyan and the Bible as he lay sick in bed and telling tales of the Covenanters. Stevenson recalled this time of sickness in "The Land of Counterpane" in "A Child's Garden of Verses" (1885), dedicating the book to his nurse.

Stevenson was an only child, both strange-looking and eccentric, and he found it hard to fit in when he was sent to a nearby school at age 6, a problem repeated at age 11 when he went on to the Edinburgh Academy; but he mixed well in lively games with his cousins in summer holidays at Colinton. His frequent illnesses often kept him away from his first school, so he was taught for long stretches by private tutors. He was a late reader, learning at age 7 or 8, but even before this he dictated stories to his mother and nurse, and he compulsively wrote stories throughout his childhood. His father was proud of this interest; he had also written stories in his spare time until his own father found them and told him to "give up such nonsense and mind your business." He paid for the printing of Robert's first publication at 16, entitled "The Pentland Rising: A Page of History, 1666". It was an account of the Covenanters' rebellion which was published in 1866, the 200th anniversary of the event.

In September 1857, Stevenson went to "Mr Henderson's School" in India Street, Edinburgh, but because of poor health stayed only a few weeks and did not return until October 1859. During his many absences he was taught by private tutors. In October 1861, he went to Edinburgh Academy, an independent school for boys, and stayed there sporadically for about fifteen months. In the autumn of 1863, he spent one term at an English boarding school at Spring Grove in Isleworth in Middlesex (now an urban area of West London). In October 1864, following an improvement to his health, he was sent to Robert Thomson's private school in Frederick Street, Edinburgh, where he remained until he went to university. In November 1867, Stevenson entered the University of Edinburgh to study engineering. He showed from the start no enthusiasm for his studies and devoted much energy to avoiding lectures. This time was more important for the friendships he made with other students in the Speculative Society (an exclusive debating club), particularly with Charles Baxter, who would become Stevenson's financial agent, and with a professor, Fleeming Jenkin, whose house staged amateur drama in which Stevenson took part, and whose biography he would later write. Perhaps most important at this point in his life was a cousin, Robert Alan Mowbray Stevenson (known as "Bob"), a lively and light-hearted young man who, instead of the family profession, had chosen to study art. Each year during vacations, Stevenson travelled to inspect the family's engineering worksâto Anstruther and Wick in 1868, with his father on his official tour of Orkney and Shetland islands lighthouses in 1869, and for three weeks to the island of Erraid in 1870. He enjoyed the travels more for the material they gave for his writing than for any engineering interest. The voyage with his father pleased him because a similar journey of Walter Scott with Robert Stevenson had provided the inspiration for Scott's 1822 novel "The Pirate". In April 1871, Stevenson notified his father of his decision to pursue a life of letters. Though the elder Stevenson was naturally disappointed, the surprise cannot have been great, and Stevenson's mother reported that he was "wonderfully resigned" to his son's choice. To provide some security, it was agreed that Stevenson should read Law (again at Edinburgh University) and be called to the Scottish bar. In his 1887 poetry collection "Underwoods", Stevenson muses on his having turned from the family profession:

Say not of me that weakly I declined
The labours of my sires, and fled the sea,
The towers we founded and the lamps we lit,
To play at home with paper like a child.
But rather say: "In the afternoon of time"
"A strenuous family dusted from its hands"
"The sand of granite, and beholding far"
"Along the sounding coast its pyramids"
"And tall memorials catch the dying sun,"
"Smiled well content, and to this childish task"
"Around the fire addressed its evening hours."

In other respects too, Stevenson was moving away from his upbringing. His dress became more Bohemian; he already wore his hair long, but he now took to wearing a velveteen jacket and rarely attended parties in conventional evening dress. Within the limits of a strict allowance, he visited cheap pubs and brothels. More importantly, he had come to reject Christianity and declared himself an atheist. In January 1873, his father came across the constitution of the LJR (Liberty, Justice, Reverence) Club, of which Stevenson and his cousin Bob were members, which began: "Disregard everything our parents have taught us". Questioning his son about his beliefs, he discovered the truth, leading to a long period of dissension with both parents:

What a "damned" curse I am to my parents! As my father said "You have rendered my whole life a failure". As my mother said "This is the heaviest affliction that has ever befallen me". O Lord, what a pleasant thing it is to have damned the happiness of (probably) the only two people who care a damn about you in the world.

Stevenson was visiting a cousin in England in late 1873 when he met two people who became very important to him: Sidney Colvin and Fanny (Frances Jane) Sitwell. Sitwell was a 34-year-old woman with a son, who was separated from her husband. She attracted the devotion of many who met her, including Colvin, who married her in 1901. Stevenson was also drawn to her, and they kept up a warm correspondence over several years in which he wavered between the role of a suitor and a son (he addressed her as "Madonna"). Colvin became Stevenson's literary adviser and was the first editor of his letters after his death. He placed Stevenson's first paid contribution in "The Portfolio", an essay entitled "Roads".

Stevenson was soon active in London literary life, becoming acquainted with many of the writers of the time, including Andrew Lang, Edmund Gosse, and Leslie Stephen, the editor of the "Cornhill Magazine" who took an interest in Stevenson's work. Stephen took Stevenson to visit a patient at the Edinburgh Infirmary named William Ernest Henley, an energetic and talkative man with a wooden leg. Henley became a close friend and occasional literary collaborator, until a quarrel broke up the friendship in 1888, and he is often considered to be the model for Long John Silver in "Treasure Island".

Stevenson was sent to Menton on the French Riviera in November 1873 to recuperate after his health failed. He returned in better health in April 1874 and settled down to his studies, but he returned to France several times after that. He made long and frequent trips to the neighborhood of the Forest of Fontainebleau, staying at Barbizon, Grez-sur-Loing, and Nemours and becoming a member of the artists' colonies there. He also traveled to Paris to visit galleries and the theatres. He qualified for the Scottish bar in July 1875, and his father added a brass plate to the Heriot Row house reading "R.L. Stevenson, Advocate". His law studies did influence his books, but he never practised law; all his energies were spent in travel and writing. One of his journeys was a canoe voyage in Belgium and France with Sir Walter Simpson, a friend from the Speculative Society, a frequent travel companion, and the author of "The Art of Golf" (1887). This trip was the basis of his first travel book "An Inland Voyage" (1878).

The canoe voyage with Simpson brought Stevenson to Grez in September 1876 where he met Fanny Van de Grift Osbourne (1840â1914), born in Indianapolis. She had married at age 17 and moved to Nevada to rejoin husband Samuel after his participation in the American Civil War. Their children were Isobel (or "Belle"), Lloyd, and Hervey (who died in 1875). But anger over her husband's infidelities led to a number of separations. In 1875, she had taken her children to France where she and Isobel studied art.

Stevenson returned to Britain shortly after this first meeting, but Fanny apparently remained in his thoughts, and he wrote the essay "On falling in love" for the "Cornhill Magazine". They met again early in 1877 and became lovers. Stevenson spent much of the following year with her and her children in France. In August 1878, she returned to San Francisco and Stevenson remained in Europe, making the walking trip that formed the basis for "Travels with a Donkey in the CÃ©vennes" (1879). But he set off to join her in August 1879, against the advice of his friends and without notifying his parents. He took second-class passage on the steamship "Devonia", in part to save money but also to learn how others traveled and to increase the adventure of the journey. He then traveled overland by train from New York City to California. He later wrote about the experience in "The Amateur Emigrant". It was good experience for his writing, but it broke his health. 

He was near death when he arrived in Monterey, California, where some local ranchers nursed him back to health. He stayed for a time at the French Hotel located at 530 Houston Street, now a museum dedicated to his memory called the "Stevenson House". While there, he often dined "on the cuff," as he said, at a nearby restaurant run by Frenchman Jules Simoneau, which stood at what is now Simoneau Plaza; several years later, he sent Simoneau an inscribed copy of his novel "Strange Case of Dr Jekyll and Mr Hyde" (1886), writing that it would be a stranger case still if Robert Louis Stevenson ever forgot Jules Simoneau. While in Monterey, he wrote an evocative article about "the Old Pacific Capital" of Monterey.

By December 1879, Stevenson had recovered his health enough to continue to San Francisco where he struggled "all alone on forty-five cents a day, and sometimes less, with quantities of hard work and many heavy thoughts," in an effort to support himself through his writing. But by the end of the winter, his health was broken again and he found himself at death's door. Fanny was now divorced and recovered from her own illness, and she came to his bedside and nursed him to recovery. "After a while," he wrote, "my spirit got up again in a divine frenzy, and has since kicked and spurred my vile body forward with great emphasis and success." When his father heard of his condition, he cabled him money to help him through this period.

Fanny and Robert were married in May 1880, although he said that he was "a mere complication of cough and bones, much fitter for an emblem of mortality than a bridegroom." He travelled with his new wife and her son Lloyd north of San Francisco to Napa Valley and spent a summer honeymoon at an abandoned mining camp on Mount Saint Helena. He wrote about this experience in "The Silverado Squatters". He met Charles Warren Stoddard, co-editor of the "Overland Monthly" and author of "South Sea Idylls", who urged Stevenson to travel to the South Pacific, an idea which returned to him many years later. In August 1880, he sailed with Fanny and Lloyd from New York to Britain and found his parents and his friend Sidney Colvin on the wharf at Liverpool, happy to see him return home. Gradually, his wife was able to patch up differences between father and son and make herself a part of the family through her charm and wit.

Stevenson searched in vain between 1880 and 1887 for a residence suitable to his health. He spent his summers at various places in Scotland and England, including Westbourne, Dorset, a residential area in Bournemouth. It was during his time in Bournemouth that he wrote the story "Strange Case of Dr Jekyll and Mr Hyde", naming the character Mr. Poole after the town of Poole which is situated next to Bournemouth. In Westbourne, he named his house "Skerryvore" after the tallest lighthouse in Scotland, which his uncle Alan had built (1838â44). In the wintertime, Stevenson travelled to France and lived at Davos Platz and the Chalet de Solitude at HyÃ¨res, where he was very happy for a time. "I have so many things to make life sweet for me," he wrote, "it seems a pity I cannot have that other one thingâhealth. But though you will be angry to hear it, I believe, for myself at least, what is is best." In spite of his ill health, he produced the bulk of his best-known work during these years. "Treasure Island" was published under the pseudonym "Captain George North" and became his first widely popular book; he wrote it during this time, along with "Kidnapped", "Strange Case of Dr Jekyll and Mr Hyde" (which established his wider reputation), "", "A Child's Garden of Verses", and "Underwoods". He gave a copy of "Kidnapped" to his friend and frequent Skerryvore visitor Henry James.

His father died in 1887 and Stevenson felt free to follow the advice of his physician to try a complete change of climate, so he headed for Colorado with his mother and family. But after landing in New York, they decided to spend the winter in the Adirondacks at a cure cottage now known as Stevenson Cottage at Saranac Lake, New York. During the intensely cold winter, Stevenson wrote some of his best essays, including "Pulvis et Umbra". He also began "The Master of Ballantrae" and lightheartedly planned a cruise to the southern Pacific Ocean for the following summer.

Stevenson believed in Conservatism for most of his life. His cousin and biographer Sir Graham Balfour said that "he probably throughout life would, if compelled to vote, have always supported the Conservative candidate." In 1866, Stevenson voted for Benjamin Disraeli, future Conservative Prime Minister of the United Kingdom, over Thomas Carlyle for the Lord Rectorship of the University of Edinburgh. During his college years, he briefly identified himself as a "red-hot socialist". He wrote at age 26: "I look back to the time when I was a Socialist with something like regretâ¦. Now I know that in thus turning Conservative with years, I am going through the normal cycle of change and travelling in the common orbit of men's opinions."

In June 1888, Stevenson chartered the yacht "Casco" and set sail with his family from San Francisco. The vessel "plowed her path of snow across the empty deep, far from all track of commerce, far from any hand of help." The sea air and thrill of adventure for a time restored his health, and for nearly three years he wandered the eastern and central Pacific, stopping for extended stays at the Hawaiian Islands, where he became a good friend of King KalÄkaua. He befriended the king's niece Princess Victoria Kaiulani, who also had Scottish heritage. He spent time at the Gilbert Islands, Tahiti, New Zealand, and the Samoan Islands. During this period, he completed "The Master of Ballantrae", composed two ballads based on the legends of the islanders, and wrote "The Bottle Imp". He preserved the experience of these years in his various letters and in his "In the South Seas" (which was published posthumously). He made a voyage in 1889 with Lloyd on the trading schooner "Equator", visiting Butaritari, Mariki, Apaiang, and Abemama in the Gilbert Islands. They spent several months on Abemama with tyrant-chief Tem Binoka, whom Stevenson described in "In the South Seas".

Stevenson left Sydney, Australia, on the "Janet Nicoll" in April 1890 for his third and final voyage among the South Seas islands. He intended to produce another book of travel writing to follow his earlier book "In the South Seas", but it was his wife who eventually published her journal of their third voyage. (Fanny misnames the ship in her account "The Cruise of the Janet Nichol".) A fellow passenger was Jack Buckland, whose stories of life as an island trader became the inspiration for the character of Tommy Hadden in "The Wrecker" (1892), which Stevenson and Lloyd Osbourne wrote together. Buckland visited the Stevensons at Vailima in 1894.

In 1890, Stevenson purchased a tract of about 400 acres (1.6Â kmÂ²) in Upolu, an island in Samoa where he established himself on his estate in the village of Vailima after two aborted attempts to visit Scotland. He took the native name Tusitala (Samoan for "Teller of Tales"). His influence spread among the Samoans, who consulted him for advice, and he soon became involved in local politics. He was convinced that the European officials who had been appointed to rule the Samoans were incompetent, and he published "" after many futile attempts to resolve the matter. This was such a stinging protest against existing conditions that it resulted in the recall of two officials, and Stevenson feared for a time that it would result in his own deportation. He wrote to Colvin, "I used to think meanly of the plumber; but how he shines beside the politician!"

He also found time to work at his writing, although he felt that "there was never any man had so many irons in the fire". He wrote "The Beach of Falesa", "Catriona" (titled "David Balfour" in the US), "The Ebb-Tide", and the "Vailima Letters" during this period.

Stevenson grew depressed and wondered if he had exhausted his creative vein, as he had been "overworked bitterly" and that the best he could write was "ditch-water". He even feared that he might again become a helpless invalid. He rebelled against this idea: "I wish to die in my boots; no more Land of Counterpane for me. To be drowned, to be shot, to be thrown from a horse â ay, to be hanged, rather than pass again through that slow dissolution." He then suddenly had a return of energy and he began work on "Weir of Hermiston". "It's so good that it frightens me," he is reported to have exclaimed. He felt that this was the best work he had done.

On 3 December 1894, Stevenson was talking to his wife and straining to open a bottle of wine when he suddenly exclaimed, "What's that?", asked his wife "does my face look strange?", and collapsed. He died within a few hours, probably of a cerebral haemorrhage. He was 44 years old. The Samoans insisted on surrounding his body with a watch-guard during the night and on bearing him on their shoulders to nearby Mount Vaea, where they buried him on a spot overlooking the sea on land donated by British Acting Vice Consul Thomas Trood. Stevenson had always wanted his Requiem inscribed on his tomb:

Stevenson was loved by the Samoans, and his tombstone epigraph was translated to a Samoan song of grief.

Half of Stevenson's original manuscripts are lost, including those of "Treasure Island", "The Black Arrow," and "The Master of Ballantrae". His heirs sold his papers during World War I, and many Stevenson documents were auctioned off in 1918.

Stevenson was a celebrity in his own time, being admired by many other writers, including Jorge Luis Borges, Bertolt Brecht, Marcel Proust, Arthur Conan Doyle, Henry James, Cesare Pavese, Emilio Salgari, Ernest Hemingway, Rudyard Kipling, Jack London, Vladimir Nabokov, J. M. Barrie, and G. K. Chesterton, who said that Stevenson "seemed to pick the right word up on the point of his pen, like a man playing spillikins."

Stevenson was seen for much of the 20th century as a second-class writer. He became relegated to children's literature and horror genres, condemned by literary figures such as Virginia Woolf (daughter of his early mentor Leslie Stephen) and her husband Leonard Woolf, and he was gradually excluded from the canon of literature taught in schools. His exclusion reached its nadir in the 1973 2,000-page "Oxford Anthology of English Literature" where he was entirely unmentioned, and "The Norton Anthology of English Literature" excluded him from 1968 to 2000 (1stâ7th editions), including him only in the 8th edition (2006).

The late 20th century brought a re-evaluation of Stevenson as an artist of great range and insight, a literary theorist, an essayist and social critic, a witness to the colonial history of the Pacific Islands, and a humanist. He was praised by Roger Lancelyn Green, one of the Oxford Inklings, as a writer of a consistently high level of "literary skill or sheer imaginative power" and a pioneer of the Age of the Story Tellers along with H. Rider Haggard. He is now evaluated as a peer of authors such as Joseph Conrad (whom Stevenson influenced with his South Seas fiction) and Henry James, with new scholarly studies and organisations devoted to him. Throughout the vicissitudes of his scholarly reception, Stevenson has remained popular worldwide. According to the Index Translationum, Stevenson is ranked the 26th most translated author in the world, ahead of Oscar Wilde and Edgar Allan Poe.

On the subject of Stevenson's modern reputation, American film critic Roger Ebert wrote in 1996,

The Writers' Museum near Edinburgh's Royal Mile devotes a room to Stevenson, containing some of his personal possessions from childhood through to adulthood.

The Stevenson House at 530 Houston Street in Monterey, California, formerly the French Hotel, memorializes Stevenson's 1879 stay in "the Old Pacific Capital", as he was crossing the United States to join his future wife, Fanny Osbourne. The Stevenson House museum is graced with a bas-relief depicting the sickly author writing in bed.

The Robert Louis Stevenson Museum in St. Helena, California, is home to over 11,000 objects and artifacts, the majority of which belonged to Stevenson. Opened in 1969, the museum houses such treasures as his childhood rocking chair, writing desk, toy soldiers, and personal writings among many other items. The museum is free to the public and serves as an academic archive for students, writers, and Stevenson enthusiasts.

Stevenson's former home in Vailima, Samoa is now a museum dedicated to the later years of his life. The museum collection includes several original items belonging to Stevenson and his family. The path to Stevenson's grave at the top of Mt Vaea commences from the museum.

A bronze relief memorial to Stevenson, designed by the American sculptor Augustus Saint-Gaudens in 1904, is mounted in the Moray Aisle of St Giles' Cathedral, Edinburgh. Saint-Gaudens' scaled-down version of this relief is in the collection of the Montclair Art Museum. Another small version depicting Stevenson with a cigarette in his hand rather than the pen he holds in the St. Giles memorial is displayed in the Nichols House Museum in Beacon Hill, Boston.

Another memorial in Edinburgh stands in West Princes Street Gardens below Edinburgh Castle; it is a simple upright stone inscribed with "RLS â A Man of Letters 1850â1894" by sculptor Ian Hamilton Finlay in 1987. In 2013, a statue of Stevenson as a child with his dog was unveiled by the author Ian Rankin outside Colinton Parish Church. The sculptor of the statue was Alan Herriot, and the money to erect it was raised by the Colinton Community Conservation Trust.

A plaque above the door of a house in Castleton of Braemar states "Here R.L. Stevenson spent the Summer of 1881 and wrote Treasure Island, his first great work".

A garden was designed by the Bournemouth Corporation in 1957 as a memorial to Stevenson, on the site of his Westbourne house, "Skerryvore", which he occupied from 1885 to 1887. A statue of the Skerryvore lighthouse is present on the site.

In 1966, the Canadian actor Lloyd Bochner played Stevenson in the episode "Jolly Roger and Wells Fargo" of the syndicated American television series, "Death Valley Days", hosted by Robert Taylor and directed by Denver Pyle. In an earlier "Death Valley Days" episode from 1958, "The Great Amulet", hosted by Stanley Andrews, the actor Don Reardon (died 2004) played the role of Stevenson. In the story line, Stevenson falls in love with Fanny Osbourne, played by Aline Towne (1919-1996), the mother of two children in a loveless marriage in San Francisco. The couple met in France where Stevenson was recuperating from health issues and moved to San Francisco, where Stevenson worked tirelessly despite lingering health matters in the production of his large volume of literary works. "The Great Amulet" is revealed at the conclusion of the episode.

In 1994, to mark the 100th anniversary of Stevenson's death, the Royal Bank of Scotland issued a series of commemorative Â£1 notes which featured a quill pen and Stevenson's signature on the obverse, and Stevenson's face on the reverse side. Alongside Stevenson's portrait are scenes from some of his books and his house in Western Samoa. Two million notes were issued, each with a serial number beginning "RLS". The first note to be printed was sent to Samoa in time for their centenary celebrations on 3 December 1994.

At least six US elementary schools are named after Stevenson, in the Upper West Side of New York City, in Fridley, Minnesota, in Burbank, California, in Grandview Heights, Ohio (suburb of Columbus), in San Francisco, California, and in Merritt Island, Florida. There is an R. L. Stevenson middle school in Honolulu, Hawaii and in Saint Helena, California. Stevenson School in Pebble Beach, California, was established in 1952 and still exists as a college preparatory boarding school. Robert Louis Stevenson State Park near Calistoga, California, contains the location where he and Fanny spent their honeymoon in 1880.

A street in Honolulu's Waikiki District, where Stevenson lived while in the Hawaiian Islands, was named after his Samoan moniker: Tusitala. This was also (until recently changed) the name of a restaurant on Comiston Road, Edinburgh on the route of a favourite walk that Stevenson often took to the village of Swanston in the Pentland Hills.

In 2011, Stevenson's open letter defending Father Damien from Rev. Dr. Charles McEwen Hyde influenced the founding of the Saint Damien Advocates in Hawaii.

The Chemin de Stevenson (GR 70) is a popular long-distance footpath in France that approximately follows Stevenson's route as described in "Travels with a Donkey in the CÃ©vennes". There are numerous monuments and businesses named after him along the route, including a fountain in the town of Saint-Jean-du-Gard where Stevenson sold his donkey Modestine and took a stagecoach to AlÃ¨s.

The Robert Louis Stevenson Memorial is an outdoor memorial in Portsmouth Square, San Francisco, California.

A memorial by Gutzon Borglum was unveiled, in 1915, at Baker Cottage, Saranac Lake, New York.



List of short stories sorted chronologically. Note: does not include collaborations with Fanny found in "More New Arabian Nights: The Dynamiter".



Although not well known, his island fiction and non-fiction is among the most valuable and collected of the 19th century body of work that addresses the Pacific area.



Literary works

Musical works

About

Websites


</doc>
<doc id="26446" url="https://en.wikipedia.org/wiki?curid=26446" title="Recreational mathematics">
Recreational mathematics

Recreational mathematics is mathematics carried out for recreation (entertainment) rather than as a strictly research and application-based professional activity. Although it is not necessarily limited to being an endeavor for amateurs, many topics in this field require no knowledge of advanced mathematics. Recreational mathematics involves mathematical puzzles and games, often appealing to children and untrained adults, inspiring their further study of the subject.

The Mathematical Association of America (MAA) includes Recreational Mathematics as one of its seventeen Special Interest Groups, commenting:
Mathematical competitions (such as those sponsored by mathematical associations) are also categorized under recreational mathematics.

Some of the more well-known topics in recreational mathematics are Rubik's Cubes, magic squares, fractals, logic puzzles and mathematical chess problems, but this area of mathematics includes the aesthetics and culture of mathematics, peculiar or amusing stories and coincidences about mathematics, and the personal lives of mathematicians.

Mathematical games are multiplayer games whose rules, strategies, and outcomes can be studied and explained using mathematics. The players of the game may not need to use explicit mathematics in order to play mathematical games. For example, Mancala is studied in the mathematical field of combinatorial game theory, but no mathematics is necessary in order to play it.

Mathematical puzzles require mathematics in order to solve them. They have specific rules, as do multiplayer games, but mathematical puzzles don't usually involve competition between two or more players. Instead, in order to solve such a puzzle, the solver must find a solution that satisfies the given conditions.

Logic puzzles and classical ciphers are common examples of mathematical puzzles. Cellular automata and fractals are also considered mathematical puzzles, even though the solver only interacts with them by providing a set of initial conditions.

As they often include or require game-like features or thinking, mathematical puzzles are sometimes also called mathematical games.

Other curiosities and pastimes of non-trivial mathematical interest include:

There are many online blogs devoted to recreational mathematics. Among them


Prominent practitioners and advocates of recreational mathematics have included:





</doc>
<doc id="26447" url="https://en.wikipedia.org/wiki?curid=26447" title="Resurrection">
Resurrection

Resurrection or anastasis is the concept of coming back to life after death. In a number of ancient religions, a dying-and-rising god is a deity which dies and resurrects.

The resurrection of the dead is a standard eschatological belief in the Abrahamic religions. As a religious concept, it is used in two distinct respects: a belief in the resurrection of individual souls that is current and ongoing (Christian idealism, realized eschatology), or else a belief in a singular resurrection of the dead at the end of the world. Some believe the soul is the actual vehicle by which people are resurrected.

The death and resurrection of Jesus is a central focus of Christianity. Christian theological debate ensues with regard to what kind of resurrection is factualÂ â either a "spiritual" resurrection with a spirit body into Heaven, or a material resurrection with a restored human body. While most Christians believe Jesus' resurrection from the dead and ascension to Heaven was in a material body, a very small minority believes it was spiritual.

Resurrection, from the Latin noun "resurrectio -onis", from the verb "rego", "to make straight, rule" + preposition "sub", "under", altered to "subrigo" and contracted to "surgo, surrexi, surrectum" ("to rise", "get up", "stand up") + preposition "re-", "again", thus literally "a straightening from under again".

The concept of resurrection is found in the writings of some ancient non-Abrahamic religions in the Middle East. A few extant Egyptian and Canaanite writings allude to dying and rising gods such as Osiris and Baal. Sir James Frazer in his book "The Golden Bough" relates to these dying and rising gods, but many of his examples, according to various scholars, distort the sources. Taking a more positive position, Tryggve Mettinger argues in his recent book that the category of rise and return to life is significant for Ugaritic Baal, Melqart, Adonis, Eshmun, Osiris and Dumuzi.

In ancient Greek religion a number of men and women became physically immortal as they were resurrected from the dead. Asclepius was killed by Zeus, only to be resurrected and transformed into a major deity. Achilles, after being killed, was snatched from his funeral pyre by his divine mother Thetis and resurrected, brought to an immortal existence in either Leuce, the Elysian plains or the Islands of the Blessed. Memnon, who was killed by Achilles, seems to have received a similar fate. Alcmene, Castor, Heracles, and Melicertes, were also among the figures sometimes considered to have been resurrected to physical immortality. According to Herodotus's "Histories", the seventh century BC sage Aristeas of Proconnesus was first found dead, after which his body disappeared from a locked room. Later he found not only to have been resurrected but to have gained immortality.

Many other figures, like a great part of those who fought in the Trojan and Theban wars, Menelaus, and the historical pugilist Cleomedes of Astupalaea, were also believed to have been made physically immortal, but without having died in the first place. Indeed, in Greek religion, immortality originally always included an eternal union of body and soul. The philosophical idea of an immortal soul was a later invention, which, although influential, never had a breakthrough in the Greek world. As may be witnessed even into the Christian era, not least by the complaints of various philosophers over popular beliefs, traditional Greek believers maintained the conviction that certain individuals were resurrected from the dead and made physically immortal and that for the rest of us, we could only look forward to an existence as disembodied and dead souls.

Greek philosophers generally denied this traditional religious belief in physical immortality. Writing his "Lives of Illustrious Men" (Parallel Lives) in the first century, the Middle Platonic philosopher Plutarch in his chapter on Romulus gave an account of the mysterious disappearance and subsequent deification of this first king of Rome, comparing it to traditional Greek beliefs such as the resurrection and physical immortalization of Alcmene and Aristeas the Proconnesian, "for they say Aristeas died in a fuller's work-shop, and his friends coming to look for him, found his body vanished; and that some presently after, coming from abroad, said they met him traveling towards Croton". Plutarch openly scorned such beliefs held in traditional ancient Greek religion, writing, "many such improbabilities do your fabulous writers relate, deifying creatures naturally mortal."

Alcestis undergoes resurrection over a three-day period of time,
but without achieving immortality.

The parallel between these traditional beliefs and the later resurrection of Jesus was not lost on the early Christians, as Justin Martyr argued: "when we say ... Jesus Christ, our teacher, was crucified and died, and rose again, and ascended into heaven, we propose nothing different from what you believe regarding those whom you consider sons of Zeus." ("1 Apol." 21).

There are stories in Buddhism where the power of resurrection was allegedly demonstrated in Chan or Zen tradition. One is the legend of Bodhidharma, the Indian master who brought the Ekayana school of India that subsequently became Chan Buddhism to China.

The other is the passing of Chinese Chan master Puhua (Japanese:Jinshu Fuke) and is recounted in the Record of Linji (Japanese: Rinzai Gigen). Puhua was known for his unusual behavior and teaching style so it is no wonder that he is associated with an event that breaks the usual prohibition on displaying such powers. Here is the account from Irmgard Schloegl's "The Zen Teaching of Rinzai".

In Christianity, resurrection most critically concerns the resurrection of Jesus, but also includes the resurrection of Judgment Day known as the resurrection of the dead by those Christians who subscribe to the Nicene Creed (which is the majority or mainstream Christianity), as well as the resurrection miracles done by Jesus and the prophets of the Old Testament.

In the New Testament, Jesus is said to have raised several persons from death. These resurrections included the daughter of Jairus shortly after death, a young man in the midst of his own funeral procession, and Lazarus of Bethany, who had been buried for four days.

During the Ministry of Jesus on earth, before his death, Jesus commissioned his Twelve Apostles to, among other things, raise the dead.

Similar resurrections are credited to the apostles and Catholic saints. In the Acts of the Apostles, Saint Peter raised a woman named Dorcas (also called Tabitha), and Paul the Apostle revived a man named Eutychus who had fallen asleep and fell from a window to his death. According to the Gospel of Matthew, after Jesus's resurrection, many of those previously dead came out of their tombs and entered Jerusalem, where they appeared to many. Following the Apostolic Age, many saints were said to resurrect the dead, as recorded in Orthodox Christian hagiographies. St Columba supposedly raised a boy from the dead in the land of Picts.

Christians regard the resurrection of Jesus as the central doctrine in Christianity. Others take the incarnation of Jesus to be more central; however, it is the miraclesÂ â and particularly his resurrectionÂ â which provide validation of his incarnation. According to Paul, the entire Christian faith hinges upon the centrality of the resurrection of Jesus and the hope for a life after death. The Apostle Paul wrote in his first letter to the Corinthians:

Christianity started as a religious movement within 1st-century Judaism (late Second Temple Judaism), and it retains what the New Testament itself claims was the Pharisaic belief in the afterlife and resurrection of the dead. Whereas this belief was only one of many beliefs held about the world to come in Second Temple Judaism, and was notably rejected by both the Sadducees and, according to Josephus, the Pharisees, this belief became dominant within Early Christianity and already in the Gospels of Luke and John included an insistence on the resurrection of the flesh. Most modern Christian churches continue to uphold the belief that there will be a final resurrection of the dead and world to come.

Belief in the resurrection of the dead, and Jesus' role as judge, is codified in the Apostles' Creed, which is the fundamental creed of Christian baptismal faith. The Book of Revelation also makes many references about the Day of Judgment when the dead will be raised.

The emphasis on the literal resurrection of the flesh remained strong in the medieval ages, and still remains so in Orthodox churches. In modern Western Christianity, especially "from the 17th to the 19th century, the language of popular piety no longer evoked the resurrection of the soul but everlasting life. Although theological textbooks still mentioned resurrection, they dealt with it as a speculative question more than as an existential problem."

In Platonic philosophy and other Greek philosophical thought, at death the soul was said to leave the inferior body behind. The idea that Jesus was resurrected spiritually rather than physically even gained popularity among some Christian teachers, whom the author of 1 John declared to be antichrists. Similar beliefs appeared in the early church as Gnosticism. However, in Luke 24:39, the resurrected Jesus expressly states "behold my hands and my feet, that it is I myself. Handle me and see, for a spirit does not have flesh and bones as you see I have."

There are folklore, stories, and extractions from certain holy texts that refer to resurrections. One major folklore is that of Savitri saving her husband's life from Yamraj. In the Ramayana, after Ravana was slayed by Rama in a great battle between good and evil, Rama requests the king of Gods, Indra, to restore the lives of all the monkeys who died in the great battle.

Belief in the "Day of Resurrection" ("Yawm al-QiyÄmah"; ) is also crucial for Muslims. They believe the time of "QiyÄmah" is preordained by God but unknown to man. The trials and tribulations preceding and during the "QiyÄmah" are described in the Qur'an and the hadith, and also in the commentaries of scholars. The Quran emphasizes bodily resurrection, a break from the pre-Islamic Arabian understanding of death.

There are three explicit examples in the Hebrew Bible of people being resurrected from the dead:

According to Herbert C. Brichto, writing in Reform Judaism's "Hebrew Union College Annual", the family tomb is the central concept in understanding biblical views of the afterlife. Brichto states that it is "not mere sentimental respect for the physical remains that is...the motivation for the practice, but rather an assumed connection between proper sepulture and the condition of happiness of the deceased in the afterlife".

According to Brichto, the early Israelites apparently believed that the graves of family, or tribe, united into one, and that this unified collectivity is to what the Biblical Hebrew term Sheol refers, the common grave of humans. Although not well defined in the Tanakh, Sheol in this view was a subterranean underworld where the souls of the dead went after the body died. The Babylonians had a similar underworld called Aralu, and the Greeks had one known as Hades. According to Brichto, other biblical names for Sheol were: Abaddon (ruin), found in Psalm 88:11, Job 28:22 and Proverbs 15:11; Bor (the pit), found in Isaiah 14:15, 24:22, Ezekiel 26:20; and Shakhat (corruption), found in Isaiah 38:17, Ezekiel 28:8.

During the Second Temple period, there developed a diversity of beliefs concerning the resurrection. The concept of resurrection of the physical body is found in 2 Maccabees, according to which it will happen through re-creation of the flesh. Resurrection of the dead also appears in detail in the extra-canonical books of Enoch, in Apocalypse of Baruch, and 2 Esdras. According to the British scholar in ancient Judaism Philip R. Davies, there is âlittle or no clear reference â¦ either to immortality or to resurrection from the deadâ in the Dead Sea scrolls texts. C.D. Elledge, however, argues that some form of resurrection may be referred to in the Dead Sea texts of the so-called Messianic Apocalypse, Pseudo-Ezekiel, and MÃ»sÄr LÄ MÄvÃ®n.

Both Josephus and the New Testament record that the Sadducees did not believe in an afterlife, but the sources vary on the beliefs of the Pharisees. The New Testament claims that the Pharisees believed in the resurrection, but does not specify whether this included the flesh or not. According to Josephus, who himself was a Pharisee, the Pharisees held that only the soul was immortal and the souls of good people will âpass into other bodies,â while âthe souls of the wicked will suffer eternal punishment.â Paul, who also was a Pharisee, said that at the resurrection what is "sown as a natural body is raised a spiritual body." Jubilees seems to refer to the resurrection of the soul only, or to a more general idea of an immortal soul.

Cryonics is the low-temperature freezing (usually at ) of a human corpse or severed head, with the speculative hope that resurrection may be possible in the future. Cryonics is a pseudoscience. It is regarded with skepticism within the mainstream scientific community and has been widely characterized as quackery.

Russian Cosmist Nikolai Fyodorovich Fyodorov advocated resurrection of the dead using scientific methods. Fedorov tried to plan specific actions for scientific research of the possibility of restoring life and making it infinite. His first project is connected with collecting and synthesizing decayed remains of dead based on "knowledge and control over all atoms and molecules of the world". The second method described by Fedorov is genetic-hereditary. The revival could be done successively in the ancestral line: sons and daughters restore their fathers and mothers, they in turn restore their parents and so on. This means restoring the ancestors using the hereditary information that they passed on to their children. Using this genetic method it is only possible to create a genetic twin of the dead person. It is necessary to give back the revived person his old mind, his personality. Fedorov speculates about the idea of "radial images" that may contain the personalities of the people and survive after death. Nevertheless, Fedorov noted that even if a soul is destroyed after death, Man will learn to restore it whole by mastering the forces of decay and fragmentation.

In his 1994 book "The Physics of Immortality", American physicist Frank J. Tipler, an expert on the general theory of relativity, presented his Omega Point Theory which outlines how a resurrection of the dead could take place at the end of the cosmos. He posits that humans will evolve into robots which will turn the entire cosmos into a supercomputer which will, shortly before the Big Crunch, perform the resurrection within its cyberspace, reconstructing formerly dead humans (from information captured by the supercomputer from the past light cone of the cosmos) as avatars within its metaverse.

David Deutsch, British physicist and pioneer in the field of quantum computing, agrees with Tipler's Omega Point cosmology and the idea of resurrecting deceased people with the help of quantum computers but he is critical of Tipler's theological views.

Italian physicist and computer scientist Giulio Prisco presents the idea of "quantum archaeology", "reconstructing the life, thoughts, memories, and feelings of any person in the past, up to any desired level of detail, and thus resurrecting the original person via 'copying to the future'".

In his book "Mind Children", roboticist Hans Moravec proposed that a future supercomputer might be able to resurrect long-dead minds from the information that still survived. For example, this information can be in the form of memories, filmstrips, medical records, and DNA.

Ray Kurzweil, American inventor and futurist, believes that when his concept of singularity comes to pass, it will be possible to resurrect the dead by digital recreation.

In their science fiction novel "The Light of Other Days", Sir Arthur Clarke and Stephen Baxter imagine a future civilization resurrecting the dead of past ages by reaching into the past, through micro wormholes and with nanorobots, to download full snapshots of brain states and memories.

Both the Church of Perpetual Life and the Terasem Movement consider themselves transreligions and advocate for the use of technology to indefinitely extend the human lifespan.

A zombie (Haitian French: "", ) is a fictional undead being created through the reanimation of a human corpse. Zombies are most commonly found in horror and fantasy genre works. The term comes from Haitian folklore, where a "zombie" is a dead body reanimated through various methods, most commonly magic.

As knowledge of different religions has grown, so have claims of bodily disappearance of some religious and mythological figures. In ancient Greek religion, this was a way the gods made some physically immortal, including such figures as Cleitus, Ganymede, Menelaus, and Tithonus. After his death, Cycnus was changed into a swan and vanished. In his chapter on Romulus from Parallel Lives, Plutarch criticises the continuous belief in such disappearances, referring to the allegedly miraculous disappearance of the historical figures Romulus, Cleomedes of Astypalaea, and Croesus. In ancient times, Greek and Roman pagan similarities were explained by the early Christian writers, such as Justin Martyr, as the work of demons, with the intention of leading Christians astray.

In the Buddhist Epic of King Gesar, also spelled as Geser or Kesar, at the end, chants on a mountain top and his clothes fall empty to the ground. The body of the first Guru of the Sikhs, Guru Nanak Dev, is said to have disappeared and flowers left in place of his dead body.

Lord Raglan's Hero Pattern lists many religious figures whose bodies disappear, or have more than one sepulchre. B. Traven, author of "The Treasure of the Sierra Madre", wrote that the Inca Virococha arrived at Cusco (in modern-day Peru) and the Pacific seacoast where he walked across the water and vanished. It has been thought that teachings regarding the purity and incorruptibility of the hero's human body are linked to this phenomenon. Perhaps, this is also to deter the practice of disturbing and collecting the hero's remains. They are safely protected if they have disappeared.

The first such case mentioned in the Bible is that of Enoch (son of Jared, great-grandfather of Noah, and father of Methuselah). Enoch is said to have lived a life where he "walked with God", after which "he was not, for God took him" (Genesis 5:1â18). In Deuteronomy (34:6) Moses is secretly buried. Elijah vanishes in a whirlwind 2 Kings (2:11). After hundreds of years these two earlier Biblical heroes suddenly reappear, and are seen walking with Jesus, then again vanish. Mark (9:2â8), Matthew (17:1â8) and Luke (9:28â33). The last time he is seen, Luke (24:51) alone tells of Jesus leaving his disciples by ascending into the sky.





</doc>
<doc id="26449" url="https://en.wikipedia.org/wiki?curid=26449" title="Resurrected">
Resurrected

Resurrected or The Resurrected may refer to:




</doc>
<doc id="26451" url="https://en.wikipedia.org/wiki?curid=26451" title="Robert Parr">
Robert Parr

Robert Ghormley Parr (September 22, 1921 â March 27, 2017) was an American theoretical chemist who was a Professor of Chemistry at the University of North Carolina at Chapel Hill.

Parr received an A. B. degree "magna cum laude" from Brown University in 1942, and then entered the University of Minnesota, receiving a Ph.D. in physical chemistry in 1947. He joined the faculty at Minnesota upon receiving his Ph.D. and remained there one year. In 1948 he moved to the Carnegie Institute of Technology (now Carnegie Mellon University) in Pittsburgh, Pennsylvania, becoming a full professor in 1957. In 1962 he moved to Johns Hopkins University in Baltimore, Maryland, and in 1974 to the University of North Carolina at Chapel Hill, where he received appointment to an endowed professorship in 1990 and where he last taught.

Working with DuPont chemist Rudolph Pariser, Parr developed a method of computing approximate molecular orbitals for pi electron systems, published in 1953. Since an identical procedure was derived by John A. Pople the same year, it is generally referred to as the PariserâParrâPople method or PPP method. The PPP method differed from existing structural chemistry thinking (which advocated "maximum overlap principle") by advancing the concept of "zero differential overlap approximation".

By 1978 Parr had realized that density functional theory (DFT) would be extremely useful in quantitative calculations of chemical and biological systems, especially those with high molecular weights. In 1988 Parr, Weitao Yang and Chengteh Lee produced an improved DFT method which could approximate the correlation energy of systems. The LYP functional theory is now one of the most-often cited papers in the chemical literature.

In 1963 Parr published "Quantum Theory of Molecular Electronic Structure", one of the first books to apply quantum theory to chemical systems.

In 1989 he and Yang published "Density Functional Theory of Atoms and Molecules", now considered the basic textbook on DFT.



</doc>
<doc id="26452" url="https://en.wikipedia.org/wiki?curid=26452" title="Riesz representation theorem">
Riesz representation theorem

There are several well-known theorems in functional analysis known as the Riesz representation theorem. They are named in honor of Frigyes Riesz.

This article will describe his theorem concerning the dual of a Hilbert space, which is sometimes called the FrÃ©chetâRiesz theorem. For the theorems relating linear functionals to measures, see RieszâMarkovâKakutani representation theorem.

This theorem establishes an important connection between a Hilbert space and its continuous dual space. If the underlying field is the real numbers, the two are isometrically isomorphic; if the underlying field is the complex numbers, the two are isometrically anti-isomorphic. The (anti-) isomorphism is a particular natural one as will be described next; a natural isomorphism.
Let "H" be a Hilbert space, and let "H*" denote its dual space, consisting of all continuous linear functionals from "H" into the field formula_1 or formula_2. 

If formula_3 is an element of "H", then the function formula_4 for all formula_5 in "H" defined by

formula_6

where formula_7 denotes the inner product of the Hilbert space, is an element of "H*". The Riesz representation theorem states that "every" element of "H*" can be written uniquely in this form. 

Proof. Let formula_14. Clearly formula_15 is closed subspace of formula_8. If formula_17, then we can trivially choose formula_18. Now assume formula_19. Then formula_20 is one-dimensional. Indeed, let formula_21 be nonzero vectors in formula_20. Then there is nonzero real number formula_23, such that formula_24. Observe that formula_25 and formula_26, so formula_27. This means that formula_28. Now let formula_29 be unit vector in formula_20. For arbitrary formula_11, let formula_32 be the orthogonal projection of formula_3 onto formula_20. Then formula_35 and formula_36 (from the properties of orthogonal projections), so that formula_37 and formula_38. Thus formula_39. Hence formula_40. We also see formula_41. From the Cauchy-Bunyakovsky-Schwartz inequality formula_42, thus for formula_3 with unit norm formula_44. This implies that formula_45.
Given any continuous linear functional "g" in "H*", the corresponding element formula_46 can be constructed uniquely by formula_47, where formula_48 is an orthonormal basis of "H", and the value of formula_49 does not vary by choice of basis. Thus, if formula_50, then formula_51 

Theorem. The mapping formula_52: "H" â "H*" defined by formula_53 = formula_54 is an isometric (anti-) isomorphism, meaning that:

The inverse map of formula_52 can be described as follows. Given a non-zero element formula_68 of "H*", the orthogonal complement of the kernel of formula_68 is a one-dimensional subspace of "H". Take a non-zero element "z" in that subspace, and set formula_70. Then formula_53 = formula_68.

Historically, the theorem is often attributed simultaneously to Riesz and FrÃ©chet in 1907 (see references).

In the mathematical treatment of quantum mechanics, the theorem can be seen as a justification for the popular braâket notation. The theorem says that, every bra formula_73 has a corresponding ket formula_74, and the latter is unique.



</doc>
<doc id="26455" url="https://en.wikipedia.org/wiki?curid=26455" title="Romano Scarpa">
Romano Scarpa

Romano Scarpa (September 27, 1927, Venice â April 23, 2005, MÃ¡laga) was one of the most famous Italian creators of Disney comics.

Growing up in Venice he developed a particular love for American cartoons and Disney comics, that, at the time, were published in the big format of the Topolino Giornale which was then printing now classic Floyd Gottfredson's stories. In the Forties he opened an Animation Studio in Venice in which he produced his first works: some commercials, a short titled "E poi venne il diluvio" and another one titled "La piccola fiammiferaia" (1953, based on Hans Christian Andersen's "The Little Match Girl"), distributed in Italy together with Robert Aldrich's "Attack!" (1956).

Right after that he stopped working in animation for a while and dedicated wholly to creating Disney comics. When in 1956 Italian editors had no more new Floyd Gottfredson's stories to reprint, he was given the responsibility to continue Gottfredson's stories about Mickey Mouse. Also influenced by Carl Barks in the late Fifties and up to about 1963 he wrote and penciled stories like "Topolino e la collana Chirikawa" (1960) or "The Flying Scot" (1957) that have, later, been translated in many different languages throughout the world. Many of these stories have their backgrounds in movies, for example "Topolino nel favoloso regno di Shan GrillÃ " (1961) is based upon Frank Capra's "Lost Horizon" (1937); not to talk about all the stories starring Snow White or the Seven Dwarfs, obviously based on "Snow White and the Seven Dwarfs" (1937). Sometimes the exact opposite happened; the Italian movie "Riusciranno i nostri eroi a ritrovare l'amico misteriosamente scomparso in Africa?" (1968) is based on Scarpa's story "Topolino e il Pippotarzan" (1957).

Around 1963, Scarpa stopped writing for 6 or 7 years. In the seventies, he moved to Spain and started working for a different publisher. Among the last things he made while he was still in Italy, at the end of the Eighties and at beginning of the Nineties, there are the so-called "Paperolimpiadi" (a long story about the 1988 Seoul Olympic games) and some strip stories, the same kind of stories that he loved when he was a child. One of these, "Topolino e l'enigma di Brigaboom" (1989) was partially based on "Brigadoon" (1954).

In the meanwhile he has had time enough for some more animation, so we have "Aihnoo degli Icebergs" (1972), "The Fourth King" (1977) and a new TV series, "The Adventures of Marco and Gina" ("Sopra i tetti di Venezia") (2001).

Mainly Scarpa worked on Disney comics, but many years ago he used to do something non-Disney once in a while, so he did one (Rolf Kauka's) "Lupo" story and one (Hannah and Barbera's) "Yogi Bear" story. In the 1950s he also drew some "Angelino" story, and Italian character.

Since 1988 some of his comic stories have been published in the USA by Gladstone Publishing; it was the first time that this happened to an Italian Disney author. Later, when Disney Comics took Gladstone's place, they published some more of his stories, and in 2003, the same happened with Gemstone Publishing, that is publishing his stories in the US at the moment.

He has influenced many younger creators (Giorgio Cavazzano was his inker during the Sixties) and many have attempted to imitate his style.

In his career Scarpa created many Disney characters that are now accepted by some as part of the Disney Universe. Those include, but are not limited to:


In 2017 Fantagraphics Books published a collection containing four stories of Scarpa's Snow White comics, titled "The Return of Snow-White and the Seven Dwarfs", ISBN .

In 2018 Fantagraphics Books began publishing a hardcover series titled "Disney Masters", in which Romano Scarpa has to date (October 2019) had three volumes dedicated to his Disney works.


This is an index of all Romano Scarpa comics published in the US. Only Duck universe and Mouse universe are listed. Chip and Dale comics are not listed.




</doc>
<doc id="26458" url="https://en.wikipedia.org/wiki?curid=26458" title="Rosa Parks">
Rosa Parks

Rosa Louise McCauley Parks (February 4, 1913Â â October 24, 2005) was an American activist in the civil rights movement best known for her pivotal role in the Montgomery bus boycott. The United States Congress has called her "the first lady of civil rights" and "the mother of the freedom movement".

On December 1, 1955, in Montgomery, Alabama, Parks rejected bus driver James F. Blake's order to relinquish her seat in the "colored section" to a white passenger, after the whites-only section was filled. Parks was not the first person to resist bus segregation, but the National Association for the Advancement of Colored People (NAACP) believed that she was the best candidate for seeing through a court challenge after her arrest for civil disobedience in violating Alabama segregation laws. Parks' prominence in the community and her willingness to become a controversial figure inspired the black community to boycott the Montgomery buses for over a year, the first major direct action campaign of the post-war civil rights movement. Her case became bogged down in the state courts, but the federal Montgomery bus lawsuit "Browder v. Gayle" succeeded in November 1956.

Parks' act of defiance and the Montgomery bus boycott became important symbols of the movement. She became an international icon of resistance to racial segregation. She organized and collaborated with civil rights leaders, including Edgar Nixon, president of the local chapter of the NAACP; and Martin Luther King Jr., a new minister in Montgomery who gained national prominence in the civil rights movement and went on to win a Nobel Peace Prize.

At the time, Parks was secretary of the Montgomery chapter of the NAACP. She had recently attended the Highlander Folk School, a Tennessee center for training activists for workers' rights and racial equality. She acted as a private citizen "tired of giving in". Although widely honored in later years, she also suffered for her act; she was fired from her job as a seamstress in a local department store, and received death threats for years afterwards.

Shortly after the boycott, she moved to Detroit, where she briefly found similar work. From 1965 to 1988, she served as secretary and receptionist to John Conyers, an African-American US Representative. She was also active in the Black Power movement and the support of political prisoners in the US.

After retirement, Parks wrote her autobiography and continued to insist that the struggle for justice was not over and there was more work to be done. In her final years, she suffered from dementia. Parks received national recognition, including the NAACP's 1979 Spingarn Medal, the Presidential Medal of Freedom, the Congressional Gold Medal, and a posthumous statue in the United States Capitol's National Statuary Hall. Upon her death in 2005, she was the first woman to lie in honor in the Capitol Rotunda, becoming the third of only four Americans to ever receive this honor. California and Missouri commemorate Rosa Parks Day on her birthday February 4, while Ohio and Oregon commemorate the occasion on the anniversary of the day she was arrested, December 1.

Rosa Parks was born Rosa Louise McCauley in Tuskegee, Alabama, on February 4, 1913, to Leona (nÃ©e Edwards), a teacher, and James McCauley, a carpenter. In addition to African ancestry, one of Parks' great-grandfathers was Scots-Irish and one of her great-grandmothers a part-Native American slave. She was small as a child and suffered poor health with chronic tonsillitis. When her parents separated, she moved with her mother to Pine Level, just outside the state capital, Montgomery. She grew up on a farm with her maternal grandparents, mother, and younger brother Sylvester. They all were members of the African Methodist Episcopal Church (AME), a century-old independent black denomination founded by free blacks in Philadelphia, Pennsylvania, in the early nineteenth century.

McCauley attended rural schools until the age of eleven. As a student at the Industrial School for Girls in Montgomery, she took academic and vocational courses. Parks went on to a laboratory school set up by the Alabama State Teachers College for Negroes for secondary education, but dropped out in order to care for her grandmother and later her mother, after they became ill.

Around the turn of the 20th century, the former Confederate states had adopted new constitutions and electoral laws that effectively disenfranchised black voters and, in Alabama, many poor white voters as well. Under the white-established Jim Crow laws, passed after Democrats regained control of southern legislatures, racial segregation was imposed in public facilities and retail stores in the South, including public transportation. Bus and train companies enforced seating policies with separate sections for blacks and whites. School bus transportation was unavailable in any form for black schoolchildren in the South, and black education was always underfunded.

Parks recalled going to elementary school in Pine Level, where school buses took white students to their new school and black students had to walk to theirs:

I'd see the bus pass every day ... But to me, that was a way of life; we had no choice but to accept what was the custom. The bus was among the first ways I realized there was a black world and a white world.

Although Parks' autobiography recounts early memories of the kindness of white strangers, she could not ignore the racism of her society. When the Ku Klux Klan marched down the street in front of their house, Parks recalls her grandfather guarding the front door with a shotgun. The Montgomery Industrial School, founded and staffed by white northerners for black children, was burned twice by arsonists. Its faculty was ostracized by the white community.

Repeatedly bullied by white children in her neighborhood, Parks often fought back physically. She later said: "As far back as I remember, I could never think in terms of accepting physical abuse without some form of retaliation if possible."

In 1932, Rosa married Raymond Parks, a barber from Montgomery. He was a member of the NAACP, which at the time was collecting money to support the defense of the Scottsboro Boys, a group of black men falsely accused of raping two white women. Rosa took numerous jobs, ranging from domestic worker to hospital aide. At her husband's urging, she finished her high school studies in 1933, at a time when less than 7% of African Americans had a high-school diploma.

In December 1943, Parks became active in the civil rights movement, joined the Montgomery chapter of the NAACP, and was elected secretary at a time when this was considered a woman's job. She later said, "I was the only woman there, and they needed a secretary, and I was too timid to say no." She continued as secretary until 1957. She worked for the local NAACP leader Edgar Nixon, even though he maintained that "Women don't need to be nowhere but in the kitchen." When Parks asked, "Well, what about me?", he replied: "I need a secretary and you are a good one."

In 1944, in her capacity as secretary, she investigated the gang-rape of Recy Taylor, a black woman from Abbeville, Alabama. Parks and other civil rights activists organized "The Committee for Equal Justice for Mrs. Recy Taylor", launching what the "Chicago Defender" called "the strongest campaign for equal justice to be seen in a decade."

Although never a member of the Communist Party, she attended meetings with her husband. The notorious Scottsboro case had been brought to prominence by the Communist Party.

In the 1940s, Parks and her husband were members of the Voters' League. Sometime soon after 1944, she held a brief job at Maxwell Air Force Base, which, despite its location in Montgomery, Alabama, did not permit racial segregation because it was federal property. She rode on its integrated trolley. Speaking to her biographer, Parks noted, "You might just say Maxwell opened my eyes up." Parks worked as a housekeeper and seamstress for Clifford and Virginia Durr, a white couple. Politically liberal, the Durrs became her friends. They encouragedâand eventually helped sponsorâParks in the summer of 1955 to attend the Highlander Folk School, an education center for activism in workers' rights and racial equality in Monteagle, Tennessee. There Parks was mentored by the veteran organizer Septima Clark. In 1945, despite the Jim Crow laws and discrimination by registrars, she succeeded in registering to vote on her third try.

In August 1955, black teenager Emmett Till was brutally murdered after reportedly flirting with a young white woman while visiting relatives in Mississippi. On November 27, 1955, four days before she would make her stand on the bus, Rosa Parks attended a mass meeting at Dexter Avenue Baptist Church in Montgomery that addressed this case, as well as the recent murders of the activists George W. Lee and Lamar Smith. The featured speaker was T. R. M. Howard, a black civil rights leader from Mississippi who headed the Regional Council of Negro Leadership. Howard brought news of the recent acquittal of the two men who had murdered Till. Parks was deeply saddened and angry at the news, particularly because Till's case had garnered much more attention than any of the cases she and the Montgomery NAACP had worked onâand yet, the two men still walked free.

In 1900, Montgomery had passed a city ordinance to segregate bus passengers by race. Conductors were empowered to assign seats to achieve that goal. According to the law, no passenger would be required to move or give up their seat and stand if the bus was crowded and no other seats were available. Over time and by custom, however, Montgomery bus drivers adopted the practice of requiring black riders to move when there were no white-only seats left.

The first four rows of seats on each Montgomery bus were reserved for whites. Buses had "colored" sections for black people generally in the rear of the bus, although blacks composed more than 75% of the ridership. The sections were not fixed but were determined by placement of a movable sign. Black people could sit in the middle rows until the white section filled; if more whites needed seats, blacks were to move to seats in the rear, stand, or, if there was no room, leave the bus. Black people could not sit across the aisle in the same row as white people. The driver could move the "colored" section sign, or remove it altogether. If white people were already sitting in the front, black people had to board at the front to pay the fare, then disembark and reenter through the rear door.

For years, the black community had complained that the situation was unfair. Parks said, "My resisting being mistreated on the bus did not begin with that particular arrest. I did a lot of walking in Montgomery."

One day in 1943, Parks boarded a bus and paid the fare. She then moved to her seat, but driver James F. Blake told her to follow city rules and enter the bus again from the back door. When Parks exited the vehicle, Blake drove off without her. Parks waited for the next bus, determined never to ride with Blake again.

After working all day, Parks boarded the Cleveland Avenue bus, a General Motors Old Look bus belonging to the Montgomery City Lines, around 6Â p.m., Thursday, December 1, 1955, in downtown Montgomery. She paid her fare and sat in an empty seat in the first row of back seats reserved for blacks in the "colored" section. Near the middle of the bus, her row was directly behind the ten seats reserved for white passengers. Initially, she did not notice that the bus driver was the same man, James F. Blake, who had left her in the rain in 1943. As the bus traveled along its regular route, all of the white-only seats in the bus filled up. The bus reached the third stop in front of the Empire Theater, and several white passengers boarded.
Blake noted that two or three white passengers were standing, as the front of the bus had filled to capacity. He moved the "colored" section sign behind Parks and demanded that four black people give up their seats in the middle section so that the white passengers could sit. Years later, in recalling the events of the day, Parks said, "When that white driver stepped back toward us, when he waved his hand and ordered us up and out of our seats, I felt a determination cover my body like a quilt on a winter night."

By Parks' account, Blake said, "Y'all better make it light on yourselves and let me have those seats." Three of them complied. Parks said, "The driver wanted us to stand up, the four of us. We didn't move at the beginning, but he says, 'Let me have these seats.' And the other three people moved, but I didn't." The black man sitting next to her gave up his seat.

Parks moved, but toward the window seat; she did not get up to move to the redesignated colored section. Parks later said about being asked to move to the rear of the bus, "I thought of Emmett Till and I just couldn't go back." Blake said, "Why don't you stand up?" Parks responded, "I don't think I should have to stand up." Blake called the police to arrest Parks. When recalling the incident for "Eyes on the Prize", a 1987 public television series on the Civil Rights Movement, Parks said, "When he saw me still sitting, he asked if I was going to stand up, and I said, 'No, I'm not.' And he said, 'Well, if you don't stand up, I'm going to have to call the police and have you arrested.' I said, 'You may do that.'"

During a 1956 radio interview with Sydney Rogers in West Oakland several months after her arrest, Parks said she had decided, "I would have to know for once and for all what rights I had as a human being and a citizen."

In her autobiography, "My Story", she said:
When Parks refused to give up her seat, a police officer arrested her. As the officer took her away, she recalled that she asked, "Why do you push us around?" She remembered him saying, "I don't know, but the law's the law, and you're under arrest." She later said, "I only knew that, as I was being arrested, that it was the very last time that I would ever ride in humiliation of this kind. ... "

Parks was charged with a violation of Chapter 6, Section 11 segregation law of the Montgomery City code, although technically she had not taken a white-only seat; she had been in a colored section. Edgar Nixon, president of the Montgomery chapter of the NAACP and leader of the Pullman Porters Union, and her friend Clifford Durr bailed Parks out of jail that evening.

Parks did not originate the idea of protesting segregation with a bus sit-in. Those preceding her included Bayard Rustin in 1942, Irene Morgan in 1946, Lillie Mae Bradford in 1951, Sarah Louise Keys in 1952, and the members of the ultimately successful "Browder v. Gayle" 1956 lawsuit (Claudette Colvin, Aurelia Browder, Susie McDonald, and Mary Louise Smith) who were arrested in Montgomery for not giving up their bus seats months before Parks.

Nixon conferred with Jo Ann Robinson, an Alabama State College professor and member of the Women's Political Council (WPC), about the Parks case. Robinson believed it important to seize the opportunity and stayed up all night mimeographing over 35,000 handbills announcing a bus boycott. The Women's Political Council was the first group to officially endorse the boycott.

On Sunday, December 4, 1955, plans for the Montgomery bus boycott were announced at black churches in the area, and a front-page article in the "Montgomery Advertiser" helped spread the word. At a church rally that night, those attending agreed unanimously to continue the boycott until they were treated with the level of courtesy they expected, until black drivers were hired, and until seating in the middle of the bus was handled on a first-come basis.

The next day, Parks was tried on charges of disorderly conduct and violating a local ordinance. The trial lasted 30 minutes. After being found guilty and fined $10, plus $4 in court costs, Parks appealed her conviction and formally challenged the legality of racial segregation. In a 1992 interview with National Public Radio's Lynn Neary, Parks recalled:

On the day of Parks' trialÂ â December 5, 1955Â â the WPC distributed the 35,000 leaflets. The handbill read,
We are ... asking every Negro to stay off the buses Monday in protest of the arrest and trial ... You can afford to stay out of school for one day. If you work, take a cab, or walk. But please, children and grown-ups, don't ride the bus at all on Monday. Please stay off the buses Monday.

It rained that day, but the black community persevered in their boycott. Some rode in carpools, while others traveled in black-operated cabs that charged the same fare as the bus, 10 cents. Most of the remainder of the 40,000 black commuters walked, some as far as .

That evening after the success of the one-day boycott, a group of 16 to 18 people gathered at the Mt. Zion AME Zion Church to discuss boycott strategies. At that time, Parks was introduced but not asked to speak, despite a standing ovation and calls from the crowd for her to speak; when she asked if she should say something, the reply was, "Why, you've said enough."

The group agreed that a new organization was needed to lead the boycott effort if it were to continue. Rev. Ralph Abernathy suggested the name "Montgomery Improvement Association" (MIA). The name was adopted, and the MIA was formed. Its members elected as their president Martin Luther King Jr., a relative newcomer to Montgomery, who was a young and mostly unknown minister of the Dexter Avenue Baptist Church.

That Monday night, 50 leaders of the African-American community gathered to discuss actions to respond to Parks' arrest. Edgar Nixon, the president of the NAACP, said, "My God, look what segregation has put in my hands!" Parks was considered the ideal plaintiff for a test case against city and state segregation laws, as she was seen as a responsible, mature woman with a good reputation. She was securely married and employed, was regarded as possessing a quiet and dignified demeanor, and was politically savvy. King said that Parks was regarded as "one of the finest citizens of Montgomeryânot one of the finest Negro citizens, but one of the finest citizens of Montgomery."

Parks' court case was being slowed down in appeals through the Alabama courts on their way to a Federal appeal and the process could have taken years. Holding together a boycott for that length of time would have been a great strain. In the end, black residents of Montgomery continued the boycott for 381 days. Dozens of public buses stood idle for months, severely damaging the bus transit company's finances, until the city repealed its law requiring segregation on public buses following the US Supreme Court ruling in "Browder v. Gayle" that it was unconstitutional. Parks was not included as a plaintiff in the Browder decision because the attorney Fred Gray concluded the courts would perceive they were attempting to circumvent her prosecution on her charges working their way through the Alabama state court system.

Parks played an important part in raising international awareness of the plight of African Americans and the civil rights struggle. King wrote in his 1958 book "Stride Toward Freedom" that Parks' arrest was the catalyst rather than the cause of the protest: "The cause lay deep in the record of similar injustices." He wrote, "Actually, no one can understand the action of Mrs. Parks unless he realizes that eventually the cup of endurance runs over, and the human personality cries out, 'I can take it no longer.'"

After her arrest, Parks became an icon of the Civil Rights Movement but suffered hardships as a result. Due to economic sanctions used against activists, she lost her job at the department store. Her husband quit his job after his boss forbade him to talk about his wife or the legal case. Parks traveled and spoke extensively about the issues.

In 1957, Raymond and Rosa Parks left Montgomery for Hampton, Virginia; mostly because she was unable to find work. She also disagreed with King and other leaders of Montgomery's struggling civil rights movement about how to proceed, and was constantly receiving death threats. In Hampton, she found a job as a hostess in an inn at Hampton Institute, a historically black college.

Later that year, at the urging of her brother and sister-in-law in Detroit, Sylvester and Daisy McCauley, Rosa and Raymond Parks and her mother moved north to join them. The City of Detroit attempted to cultivate a progressive reputation, but Parks encountered numerous signs of discrimination against African-Americans. Schools were effectively segregated, and services in black neighborhoods substandard. In 1964, Parks told an interviewer that, "I don't feel a great deal of difference here ... Housing segregation is just as bad, and it seems more noticeable in the larger cities." She regularly participated in the movement for open and fair housing.

Parks rendered crucial assistance in the first campaign for Congress by John Conyers. She persuaded Martin Luther King (who was generally reluctant to endorse local candidates) to appear with Conyers, thereby boosting the novice candidate's profile. When Conyers was elected, he hired her as a secretary and receptionist for his congressional office in Detroit. She held this position until she retired in 1988. In a telephone interview with CNN on October 24, 2005, Conyers recalled, "You treated her with deference because she was so quiet, so sereneÂ â just a very special personÂ ... There was only one Rosa Parks." Doing much of the daily constituent work for Conyers, Parks often focused on socio-economic issues including welfare, education, job discrimination, and affordable housing. She visited schools, hospitals, senior citizen facilities, and other community meetings and kept Conyers grounded in community concerns and activism.

Parks participated in activism nationally during the mid-1960s, traveling to support the Selma-to-Montgomery Marches, the Freedom Now Party, and the Lowndes County Freedom Organization. She also befriended Malcolm X, who she regarded as a personal hero.

Like many Detroit blacks, Parks remained particularly concerned about housing issues. She herself lived in a neighborhood, Virginia Park, which had been compromised by highway construction and urban renewal. By 1962, these policies had destroyed 10,000 structures in Detroit, displacing 43,096 people, 70 percent of them African-American. Parks lived just a mile from the epicenter of the riot that took place in Detroit in 1967, and she considered housing discrimination a major factor that provoked the disorder.

In the aftermath Parks collaborated with members of the League of Revolutionary Black Workers and the Republic of New Afrika in raising awareness of police abuse during the conflict. She served on a "people's tribunal" on August 30, 1967, investigating the killing of three young men by police during the 1967 Detroit uprising, in what came to be known as the Algiers Motel incident. She also helped form the Virginia Park district council to help rebuild the area. The council facilitated the building of the only black-owned shopping center in the country. Parks took part in the black power movement, attending the Philadelphia Black Power conference, and the Black Political Convention in Gary, Indiana. She also supported and visited the Black Panther school in Oakland.

In the 1970s, Parks organized for the freedom of political prisoners in the United States, particularly cases involving issues of self-defense. She helped found the Detroit chapter of the Joann Little Defense Committee, and also worked in support of the Wilmington 10, the RNA 11, and Gary Tyler. Following national outcry around her case, Little succeeded in her defense that she used deadly force to resist sexual assault and was acquitted. Gary Tyler was finally released in April 2016 after 41 years in prison.

The 1970s were a decade of loss for Parks in her personal life. Her family was plagued with illness; she and her husband had suffered stomach ulcers for years and both required hospitalization. In spite of her fame and constant speaking engagements, Parks was not a wealthy woman. She donated most of the money from speaking to civil rights causes, and lived on her staff salary and her husband's pension. Medical bills and time missed from work caused financial strain that required her to accept assistance from church groups and admirers.

Her husband died of throat cancer on August 19, 1977, and her brother, her only sibling, died of cancer that November. Her personal ordeals caused her to become removed from the civil rights movement. She learned from a newspaper of the death of Fannie Lou Hamer, once a close friend. Parks suffered two broken bones in a fall on an icy sidewalk, an injury which caused considerable and recurring pain. She decided to move with her mother into an apartment for senior citizens. There she nursed her mother Leona through the final stages of cancer and geriatric dementia until she died in 1979 at the age of 92.

In 1980, Parksâwidowed and without immediate familyârededicated herself to civil rights and educational organizations. She co-founded the Rosa L. Parks Scholarship Foundation for college-bound high school seniors, to which she donated most of her speaker fees. In February 1987, she co-founded, with Elaine Eason Steele, the Rosa and Raymond Parks Institute for Self Development, an institute that runs the "Pathways to Freedom" bus tours which introduce young people to important civil rights and Underground Railroad sites throughout the country. Parks also served on the Board of Advocates of Planned Parenthood. Though her health declined as she entered her seventies, Parks continued to make many appearances and devoted considerable energy to these causes.

In 1992, Parks published "Rosa Parks: My Story", an autobiography aimed at younger readers, which recounts her life leading to her decision to keep her seat on the bus. A few years later, she published "Quiet Strength" (1995), her memoir, which focuses on her faith.

At age 81, Parks was robbed and assaulted in her home in central Detroit on August 30, 1994. The assailant, Joseph Skipper, broke down the door but claimed he had chased away an intruder. He requested a reward and when Parks paid him, he demanded more. Parks refused and he attacked her. Hurt and badly shaken, Parks called a friend, who called the police. A neighborhood manhunt led to Skipper's capture and reported beating. Parks was treated at Detroit Receiving Hospital for facial injuries and swelling on the right side of her face. Parks said about the attack on her by the African-American man, "Many gains have been made ... But as you can see, at this time we still have a long way to go." Skipper was sentenced to 8 to 15 years and was transferred to prison in another state for his own safety.

Suffering anxiety upon returning to her small central Detroit house following the ordeal, Parks moved into Riverfront Towers, a secure high-rise apartment building. Learning of Parks' move, Little Caesars owner Mike Ilitch offered to pay for her housing expenses for as long as necessary.

In 1994, the Ku Klux Klan applied to sponsor a portion of United States Interstate 55 in St. Louis County and Jefferson County, Missouri, near St. Louis, for cleanup (which allowed them to have signs stating that this section of highway was maintained by the organization). Since the state could not refuse the KKK's sponsorship, the Missouri legislature voted to name the highway section the "Rosa Parks Highway". When asked how she felt about this honor, she is reported to have commented, "It is always nice to be thought of."

In 1999, Parks filmed a cameo appearance for the television series "Touched by an Angel". It was her last appearance on film; Parks began to suffer from health problems due to old age.

In 2002, Parks received an eviction notice from her $1,800 per month apartment for non-payment of rent. Parks was incapable of managing her own financial affairs by this time due to age-related physical and mental decline. Her rent was paid from a collection taken by Hartford Memorial Baptist Church in Detroit. When her rent became delinquent and her impending eviction was highly publicized in 2004, executives of the ownership company announced they had forgiven the back rent and would allow Parks, by then 91 and in extremely poor health, to live rent-free in the building for the remainder of her life. Elaine Steele, manager of the nonprofit Rosa and Raymond Parks Institute, told the newspaper that Parks got proper care, and that eviction notices were sent in error in 2002. Her heirs and various interest organizations alleged at the time that her financial affairs had been mismanaged.

In 2016, Parks's former residence in Detroit was threatened with demolition. A Berlin-based American artist, Ryan Mendoza, arranged to have the house disassembled, moved to his garden in Germany, and partly restored. It currently serves as a museum honoring Rosa Parks.

Parks died of natural causes on October 24, 2005, at the age of 92, in her apartment on the east side of Detroit. She and her husband never had children and she outlived her only sibling. She was survived by her sister-in-law (Raymond's sister), 13 nieces and nephews and their families, and several cousins, most of them residents of Michigan or Alabama.

City officials in Montgomery and Detroit announced on October 27, 2005, that the front seats of their city buses would be reserved with black ribbons in honor of Parks until her funeral. Parks' coffin was flown to Montgomery and taken in a horse-drawn hearse to the St. Paul African Methodist Episcopal (AME) church, where she lay in repose at the altar on October 29, 2005, dressed in the uniform of a church deaconess. A memorial service was held there the following morning. One of the speakers, United States Secretary of State Condoleezza Rice, said that if it had not been for Parks, she would probably have never become the Secretary of State. In the evening the casket was transported to Washington, D.C. and transported by a bus similar to the one in which she made her protest, to lie in honor in the rotunda of the U.S. Capitol.

Since the founding of the practice in 1852, Parks was the 31st person, the first American who had not been a U.S. government official, and the second private person (after the French planner Pierre L'Enfant) to be honored in this way. She was the first woman and the second black person to lie in honor in the Capitol. An estimated 50,000 people viewed the casket there, and the event was broadcast on television on October 31, 2005. A memorial service was held that afternoon at Metropolitan AME Church in Washington, DC.

With her body and casket returned to Detroit, for two days, Parks lay in repose at the Charles H. Wright Museum of African American History. Her funeral service was seven hours long and was held on November 2, 2005, at the Greater Grace Temple Church in Detroit. After the service, an honor guard from the Michigan National Guard laid the U.S. flag over the casket and carried it to a horse-drawn hearse, which was intended to carry it, in daylight, to the cemetery. As the hearse passed the thousands of people who were viewing the procession, many clapped, cheered loudly and released white balloons. Parks was interred between her husband and mother at Detroit's Woodlawn Cemetery in the chapel's mausoleum. The chapel was renamed the Rosa L. Parks Freedom Chapel in her honor. Parks had previously prepared and placed a headstone on the selected location with the inscription "Rosa L. Parks, wife, 1913â."






Multimedia and interviews

Others


</doc>
<doc id="26459" url="https://en.wikipedia.org/wiki?curid=26459" title="Ringworld (role-playing game)">
Ringworld (role-playing game)

The Ringworld science fiction role-playing game was published by Chaosium in 1984, using the Basic Role-Playing system for its rules and Larry Niven's "Ringworld" novels as a setting.

The setting is a distant future based on extrapolation of as much hard science as Niven had available. Specifically, it's the 29th century. "Known Space" (also the commonly used title for Larry Niven's future history science fiction series) is about 80 light years in diameter with 10,000 stars, including Human Space (40 light years diameter, 524 stars in 357 systems, 30 billion humans, â on Earth), as well as neighbouring Alien civilisations. Important Alien civilisations include the Puppeteers, paranoid pacifist herbivore centaurs, and the Kzinti, carnivorous warlike felines, who fought multiple wars over hundreds of years against the Humans, being defeated each time. Human allies include intelligent dolphins and orcas.

"Known Space" only serves as a background for the game. The game is intended to be set on the Ringworld itself, an enormous single world discovered at the far reaches of Known Space, a ring around a sun at approximately the orbit of the Earth. It is 997,000 miles wide, about 125 Earth-diameters. The total inner surface of the ring is equal to that of 3 million Earths. The ring is spun at a speed to provide 0.992G of gravity on the innerside, while 20 giant shadow squares at about the orbit of Mercury occlude the Sun to provide night. It was constructed by the Pak Protectors, now mostly extinct, who had a common origin with humans. The Ringworld is home to some 30 trillion sentient inhabitants from up to 2000 hominid species. The world is described in a series of novels by Niven, "Ringworld", "The Ringworld Engineers", and, after the game's publication, "The Ringworld Throne" and "Ringworld's Children".

The role-playing game contains a great deal of technical details about the setting, more than the fiction the setting is based on. The game setting details are complete enough that some "Ringworld" fans not interested in role-playing buy the game just for the background material.

Information from the RPG, along with notes composed by RPG author Hewitt with Niven, were later used to form the "Bible" given to authors writing in the "Man-Kzin Wars" series. Niven himself recommended that Hewitt write one of the stories for the original two MKW books, although this never came to pass.

The players initially play explorers from Known Space, sent as scouts to the Ringworld. They can be anthropologists, artists, doctors, police, or even zealots, who will explore the mysteries of this huge artificial world and its inhabitants. Basic characters can be humans from a dozen planets of Human Space, Puppeteers, or Kzin. Later play can see characters from Ringworld species, such as the (so-called) Ghouls, Vampires, Giants, Sea People, and others.

This Ringworld focus has been a criticism of the game. The "Ringworld" role-playing game is not a 'full' science fiction RPG, like "Traveller", including, for example, rules for starship construction, space combat, travel to different planets and systems, and so forth. Instead, the game and rules focused on parties of characters exploring the Ringworld itself, and, despite its vast size (with a surface area larger than that of all of Known Space's inhabited planets put together), many who bought the game felt limited by this one world setting.

A character is initially defined by his species or world of origin, which affects characteristics (for example, by determining the gravity to which it is accustomed). Then the players roll randomly for a certain number of defects, character age, and characteristics. The system used is Chaosium's Basic Role-Playing, with eight basic characteristics: Strength, Constitution, Mass (equivalent to Size in other BRP games), Intelligence, Power, Dexterity, Appearance and Education determining secondary attributes such damage modifiers, hit points, and skill rolls.

At creation, each character gets to spend a number of points (based mainly on age, Education, and Intelligence) on skills determined by interests or career choice. Each of the three playable races has specific tables for the creation of characters. Character Skills are based on percentages. To succeed in a skill, the player must roll under the relevant skill with modifiers on percentile dice.

Another critique of the game system has been the large effect of character age on skills, usually considered the most important character attributes. In Niven's future world, the deterioration of age has been largely reversed, so humans live hundreds of years. Therefore, a 200-year-old character will have vastly more skill points than a 20-year-old, with little compensatory advantages for the younger one.

Only two publications were published, the "Ringworld" role-playing game box set itself, and the "Ringworld Companion", both in 1984 by Chaosium. The magazine "Different Worlds", issue 37, featured a "Ringworld" adventure, "Louis Wu & His Motley Crew." The article "The Dolphins of Known Space: A new race for the Ringworld Game" appeared in "Dragon Magazine" issue 95.

The "Ringworld" role-playing game box set was titled "Larry Niven's Ringworld: Roleplaying Adventure Beneath the Great Arch", referring to the way the Ringworld looked from its interior surface. The authors are credited as Greg Stafford, John Hewitt, Sherman Kahn, Lynn Willis, Sandy Petersen, Rudy Kraft, Charlie Krank, Ed Gore, and Jeff Okamoto. It came in a box set with four books: the Explorer Book, Technology Book, Gamemasters Book, and Creatures Book, a sheet of cardboard miniatures, reference and character sheets, and a set of dice: 2d20 (actually dice with two sets of digits 0 to 9), 1d8, and 2d6.

This book begins with a character sheet. It introduces role-playing games, then covers character creation, skill use, and combat. It presents a detailed history of humanity between the 20th and 29th centuries. It then describes eleven human worlds: Belt (the asteroid belt), Canyon, Down, Gummidgy, Home, Jinx, Margrave, Plateau, Silvereyes, "We Made It" and Wunderland. Finally, it gives rules for non-human, Kzin or Puppeteer, player characters, and a glossary.

The Gamemaster Book begins with technical essays on the Ringworld, from physical construction, to life on the ring, with diagrams. There is a section on the "City Builders"âa Ringworld race that dominated the Ringworld, built floating cities, and sent spaceships to explore other worlds, until a mysterious technological virus destroyed their empire. Another section lists unanswered questions about the Ringworld. There are suggestions for creating scenarios and campaigns, and information on technology of various humanoid species of the Ringworld, and additional rules, including gravity, Credit Rating, and psionics. There is also an introductory scenario ("The Journey of the Catseye") intended to begin a Ringworld campaign. The characters are hired by Captain Gregor Lopez, famous explorer, for a journey to the Ringworld that does not go completely as planned.

The Technology Book gives rules and descriptions of the equipment employed by the explorers of the 29th century, categorized into generators, computers, medical equipment, tools, vehicles, weapons and defenses.

The Creature Book gives rules and descriptions for creatures, divided into Aliens, Humanoids, Animals and Plants. Many races get specialized hit location tables, characteristic maxima and minima, skills and traits.

This supplement was published not long after the box set. The authors are credited as Greg Stafford, John Hewitt, Sherman Kahn, Lynn Willis, Sandy Petersen, Rudy Kraft, and Charlie Krank.

The book starts with a diagram of the Ringworld and its star, EC-1752, new humanoids, aliens, plants and animals, technological objects, and original errata. There is some information on spaceships (Human and City Builder), hyperspace, a map of Human Space, and statistics for vehicles used on the Ringworld. Then there is a new race, the "Agamans", desert nomads, and a scenario involving them, "The Sand Eaters". Finally, there is a three part scenario named "The Kaladians", about the defense of travelling merchants. Both scenarios can be integrated into the campaign given in the basic set. None of these three additional races appear in any of the "Ringworld" novels.

Steve Peterson reviewed "Ringworld" in "Space Gamer" No. 71. Peterson commented that "Niven fans should buy it for the essays and background materials. Role-players should be prepared to do some work on scenarios; but if you do, you'll have some terrific roleplaying in a beautifully detailed world. Science-fiction gamers who want to use it for source material probably won't get their money's worth."




</doc>
<doc id="26461" url="https://en.wikipedia.org/wiki?curid=26461" title="Risus">
Risus

Risus: The Anything RPG is a rules-light generic role-playing game (RPG) written, designed and illustrated by S. John Ross of Cumberland Games and Diversions. "Risus" is available free on the web. It was first published online in 1993. Earlier versions of the game were titled GUCS: The Generic Universal Comedy System (a parody of "GURPS") and were distributed privately beginning in 1989.

"Risus" (Latin for âlaughterâ) is a comedy game (often described by its creator as a "joke game") and uses a clichÃ© (character class) system inspired by the broad "career scale" skills in Greg Gorden's "DC Heroes RPG" (Mayfair Games), and later influenced by Atlas Games' "Over the Edge". The core systems of "Risus" owe their largest debt to the "Ghostbusters RPG" published by West End Games, and to "Tunnels and Trolls" by Ken St. Andre. The game itself also cites "GURPS" as an influence, along with "FUDGE", another free RPG released to the web a year earlier. Several more recent games have been, in turn, influenced by "Risus."

Despite the game's small size and admittedly joking nature, there are more than 30 fan-authored websites devoted to "Risus," some including several rules variants, simple worldbooks, and wholly rewritten adaptations of the game. "Risus" itself has been translated into Chinese, Croatian, Czech, Danish, Dutch, Esperanto, French, German, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, and Spanish. In December, 2003, Cumberland Games began to support the free game with commercial supplements, beginning with the "Risus Companion" and the founding of the International Order of Risus. An example of another commercial product is "A Kringle in Time", "an adventure about saving Christmas from ancient evil."

"New Risus" won the 2001 inaugural RPGnet award for "Best Free RPG".

"The Risus Companion" is the first commercial supplement of "Risus". S. John Ross wrote and published the "Risus Companion" on the 10th anniversary of "Risus" on the World Wide Web, in order to provide a foundation for "Risus" as a commercial venture. "Risus" itself remains free of charge, allowing "Risus" fans the option to support "Risus" if they choose and be materially rewarded for doing so. The "Risus Companion" is an electronic document in PDF form, made available to all members of the International Order of Risus.

The International Order of Risus is the official fan club of "Risus". The Order is, according to their own charter, dedicated to promoting "Risus" and to "imposing our iron will upon an unsuspecting universe."



</doc>
<doc id="26463" url="https://en.wikipedia.org/wiki?curid=26463" title="Rigel">
Rigel

Rigel , designated Î² Orionis (Latinized to Beta Orionis, abbreviated Beta Ori, Î² Ori), is generally the seventh-brightest star in the night sky and the brightest star in the constellation of Orion. Its brightness varies slightly, and it is occasionally outshone by Betelgeuse, itself a semi-regular variable star. Rigel looks blue-white to the naked eye, contrasting with orange-red Betelgeuse. Although appearing as a single star to the naked eye, Rigel is a multiple star system composed of at least four stars.

The name "Rigel" strictly refers to only the primary star (A), although it is commonly applied to the whole system. The primary has a companion star away with an apparent magnitude of 6.7, 400 times fainter than the primary. The companion, often referred to as Rigel B or Rigel BC, is actually a triple star system. Two stars, components B and C, can be resolved by very large telescopes. The brighter of the two is a spectroscopic binary, the components designated Ba and Bb. A fainter star, separated from the others by nearly an arc minute, might also be part of the same star system.

Rigel is a massive blue supergiant calculated to be anywhere from 61,500 to 363,000 times as luminous as the Sun, depending on the method used to calculate its properties and assumptions about its distance, estimated to be about . Rigel's radius is over 70 times that of the Sun, and its surface temperature is . Pulsations cause Rigel's small intrinsic brightness variations, and it is classified as an Alpha Cygni variable.

The traditional name "Rigel" is derived from Arabic, meaning leg or foot; the star represents the foot of Orion. In 2016, the International Astronomical Union (IAU) included the name Rigel in the IAU Catalog of Star Names.

Rigel was designated "Î² Orionis" (Latinized to "Beta Orionis") by Johann Bayer in 1603. The "beta" designation is commonly given to the second-brightest star in each constellation, but Rigel is almost always brighter than Alpha Orionis (Betelgeuse). Astronomer James B. Kaler has speculated that Rigel was designated by Bayer during a rare period when it was outshone by the variable star Betelgeuse, resulting in the latter star being designated "alpha" and Rigel designated "beta". However, Bayer did not strictly order the stars by brightness; rather he grouped them by magnitude class and then ordered the stars within each class according to a different scheme. Rigel and Betelgeuse were both considered to be of the first class, and in Orion the stars of each class are thought to have been ordered north to south. Rigel is included in the General Catalogue of Variable Stars, but since it already has a Bayer designation, Î² Orionis, it has no separate variable star designation.

Rigel has several alternative stellar designations taken from various catalogues, including the Flamsteed designation 19Â Orionis (19 Ori), the Bright Star Catalogue entry HRÂ 1713, and the Henry Draper Catalogue number HDÂ 34085. These designations appear in the scientific literature, but rarely in popular writing.

The naked-eye star Rigel is actually a multiple star system. According to the IAU Catalog of Star Names, the proper name "Rigel" applies only to the supergiant primary component Î² Orionis A. In historical astronomical catalogs, the system is listed variously as HÂ IIÂ 33, Î£Â 668, Î²Â 555, or ADSÂ 3823. For simplicity, Rigel's companions can be referred to as Rigel B, C, and D; the IAU describes such names as "useful nicknames" that are "unofficial". In modern comprehensive catalogues, the whole multiple star system is known as WDS 05145-0812 or CCDM 05145-0812. 

Rigel is an intrinsic variable star with an apparent magnitude ranging from 0.05 to 0.18. It is typically the seventh-brightest star in the celestial sphere excluding the Sun, although occasionally fainter than Betelgeuse. It is usually fainter than Capella, which also varies slightly in brightness. Rigel appears slightly blue-white and has a B-V color index of â0.06. It contrasts strongly with reddish Betelgeuse.

Culminating at midnight on 12 December, and at 9 PM on 24 January, Rigel is visible on winter evenings in the northern hemisphere and on summer evenings in the southern hemisphere. In the southern hemisphere, Rigel is the first bright star of Orion visible as the constellation rises. The star is a vertex of the "Winter Hexagon", an asterism that includes Aldebaran, Capella, Pollux, Procyon, and Sirius. Rigel is a prominent equatorial navigation star, being easily located and readily visible in all the world's oceans (the exception is the area within 8Â° of the North Pole).

Rigel's spectral type is a defining point of the classification sequence for supergiants. The overall spectrum is typical for a late B class star, with strong absorption lines of the hydrogen Balmer series together with neutral helium lines and some of heavier elements such as oxygen, calcium, and magnesium. The luminosity class for B8 stars is estimated from the strength and narrowness of the hydrogen spectral lines, and Rigel is assigned to the bright supergiant class Ia.

As early as 1888, the radial velocity of Rigel, as estimated from the Doppler shifts of its spectral lines, was seen to vary. This was confirmed and interpreted at the time as being due to a spectroscopic companion with a period of about 22 days. The radial velocity has since been measured to vary by about around a mean of .

In 1933, the HÎ± spectral line was seen to be unusually weak and shifted towards shorter wavelengths, while there was a narrow emission spike about to the long wavelength side of the main absorption line. This is now known as a P Cygni profile after a star that shows this feature strongly in its spectrum. It is associated with mass loss where there is simultaneously emission from a dense wind close to the star and absorption from circumstellar material expanding away from the star.

The unusual HÎ± line profile is observed to vary unpredictably: around a third of the time it is a normal absorption line; about a quarter of the time it is a double-peaked line, that is an absorption line with an emission core or an emission line with an absorption core; about a quarter of the time it has a P Cygni profile; most of the rest of the time the line has an inverse P Cygni profile, where the emission component is on the short wavelength side of the line; rarely there is a pure emission HÎ± line. The line profile changes are interpreted as variations in the quantity and velocity of material being expelled from the star. Occasional very high-velocity outflows have been inferred, and, more rarely, infalling material. The overall picture is one of large looping structures arising from the photosphere and driven by magnetic fields.

Variations in the spectrum have resulted in the assignment of different classes to Rigel, such as B8 Ia, B8 Iab, and B8 Iae.

Rigel has been known to vary in brightness since at least 1930. The small amplitude of Rigel's brightness variation requires photoelectric or CCD photometry to be detected. These brightness changes have no obvious period. Observations over 18 nights in 1984 showed variations at red, blue, and yellow wavelengths of up to 0.13 magnitudes on timescales of a few hours to several days, but again no clear period. Rigel's colour index varies slightly but is not strongly correlated with its brightness variations.

From analysis of Hipparcos satellite photometry, Rigel is identified as belonging to the Alpha Cygni class of variable stars, defined as "non-radially pulsating supergiants of the BepâAepIa spectral types". The 'e' indicates that it displays emission lines in its spectrum, while the 'p' means it has an unspecified spectral peculiarity. Alpha Cygni type variables are generally considered to be irregular or have quasi-periods. Rigel was added to the General Catalogue of Variable Stars in the 74th name-list of variable stars on the basis of the Hipparcos photometry, which showed variations with a photographic amplitude of 0.039 magnitudes and a possible period of 2.075 days. Rigel was observed with the Canadian MOST satellite for nearly 28 days in 2009. Milli-magnitude variations were observed, and gradual changes in flux suggest the presence of long-period pulsation modes.

From observations of the variable HÎ± spectral line, Rigel is estimated to lose solar masses per year (/yr), around 10 million times more than the mass loss rate from the Sun. More detailed optical and K band infrared spectroscopic observations, together with VLTI interferometry, were taken from 2006 to 2010. Analysis of the HÎ± and HÎ³ line profiles, and measurement of the regions producing the lines, show that Rigel's stellar wind varies greatly in structure and strength. Loop and arm structures were also detected within the wind. Calculations of mass loss from the HÎ³ line give in 2006-7 and in 2009â10. Calculations using the HÎ± line give lower results, around . The terminal wind velocity is . It is estimated that Rigel has lost around 3 solar masses () since beginning life as a star of 7 to 9 million years ago.

Rigel's distance from the Sun is somewhat uncertain, with different distance estimates obtained with different methods. The 2007 Hipparcos reduction of Rigel's parallax is , giving a distance of with a margin of error of about 9%. A companion star to Rigel, usually considered to be physically associated and at the same distance, has a Gaia Data Release 2 parallax of , suggesting a distance around . However, the measurements for this object may be unreliable, possibly because it is a close double star.

Indirect distance estimation methods have also been employed. For example, Rigel is believed to be in a region of nebulosity, with its radiation illuminating several nearby clouds. Most notable of these is the 5Â°âlong IC 2118 (Witch Head Nebula), located at an angular separation of 2.5Â° from the star, or a distance of away. From measures of other nebula-embedded stars, ICÂ 2118's distance is estimated to be .

Rigel is an outlying member of the Orion OB1 Association, which is located at a distance of up to from Earth. It is a member of the loosely-defined Taurus-Orion R1 Association, somewhat closer at . Rigel is thought to be considerably closer than most of the members of Orion OB1 and the Orion Nebula. Betelgeuse and Saiph lie at a similar distance to Rigel, although Betelgeuse is a runaway star with a complex history and might have originally formed in the main body of the association.

The Rigel star system has at least four components. The blue supergiant primary has a visual companion, which is likely a close triple star system. A fainter star at wider separation might also be a component of the Rigel system.

William Herschel discovered Rigel to be a visual double star on 1 October 1781, cataloguing it as star 33 in the "second class of double stars" in his Catalogue of Double Stars, usually abbreviated to HÂ IIÂ 33, or as HÂ 2Â 33 in the Washington Double Star Catalogue. Friedrich Georg Wilhelm von Struve first measured the relative position of the companion in 1822, cataloguing the visual pair as Î£ 668. The secondary star is often referred to as Rigel B or Î² Orionis B. The angular separation of Rigel B from the primary star is 9.5 arc seconds to its south along position angle 204Â°. Although not particularly faint at visual magnitude 6.7, the overall difference in brightness from the primary (about 6.6 magnitudes or 440 times fainter) makes it a challenging target for telescope apertures smaller than .

At Rigel's estimated distance, Rigel B's projected separation from its primary is over 2,200Â AU. Since its discovery, there has been no sign of orbital motion, although both stars share similar common proper motion. The pair would have a minimum orbital period of around 18,000Â years. Gaia Data Release 2 (DR2) contains a somewhat unreliable parallax for Rigel B, placing it at about , further away than the Hipparcos distance for Rigel, but similar to the Taurus-Orion R1 association. There is no parallax for Rigel in Gaia DR2. The Gaia DR2 proper motions for Rigel B and the Hipparcos proper motions for Rigel are both small, although not quite the same.

In 1871, Sherburne Wesley Burnham suspected Rigel B to be double, and in 1878, he resolved it into two components. This visual companion is designated as component C (Rigel C), with a measured separation from component B that varies from less than to around . In 2009, speckle interferometry showed the two almost identical components separated by 0.124", with visual magnitudes of 7.5 and 7.6 respectively. Their estimated orbital period is 63Â years. Burnham listed the Rigel multiple system as Î²Â 555 in his double star catalogue or BUÂ 555 in modern use.

Component B is a double-lined spectroscopic binary system, which shows two sets of spectral lines combined within its single stellar spectrum. Periodic changes observed in relative positions of these lines indicate an orbital period of 9.86Â days. The two spectroscopic components Rigel Ba and Rigel Bb cannot be resolved in optical telescopes but are known to both be hot stars of spectral type around B9. This spectroscopic binary, together with the close visual component Rigel C, is likely a physical triple star system, although Rigel C cannot be detected in the spectrum which is inconsistent with its observed brightness.

In 1878, Burnham found another possibly associated star of approximately 13th magnitude. He listed it as component D of Î²Â 555. Its 2017 separation from Rigel was almost due north at a position angle of 1Â°, although it is unclear whether it is physically related or a coincidental alignment. Gaia DR2 finds it to be a 12th magnitude sunlike star at approximately the same distance as Rigel. Likely an orange dwarf, this star would have an orbital period of around 250,000 years, if it is part of the Rigel system.

A spectroscopic companion to Rigel was reported on the basis of radial velocity variations, and its orbit was even calculated, but subsequent work suggests that the star does not exist and that observed pulsations are intrinsic to Rigel itself.

Estimation of many physical characteristics of Rigel and other blue supergiant stars are difficult due to their rarity and uncertainty about how far they are from the Sun. As such, much of our understanding about their characteristics is based on theoretical stellar evolution models.

Although Rigel is often considered the most luminous star within 1,000 light-years of the Sun, its energy output is poorly known. For example, using the Hipparcos distance of , the estimated relative luminosity for Rigel is about 120,000 times that of the Sun (), but another recently published distance of suggests an even higher luminosity of . Other calculations based on theoretical stellar evolutionary models of Rigel's atmosphere give luminosities anywhere between and , while summing the spectral energy distribution from historical photometry with the Hipparcos distance suggests a luminosity as low as .

A 2018 study using the Navy Precision Optical Interferometer measured the angular diameter as . After correcting for limb darkening, the angular diameter is found to be , yielding a radius of . An older measurement of the angular diameter gives , equivalent to a radius at .

A mass of at an age of million years has been determined by comparing evolutionary tracks, while atmospheric modelling from the spectrum gives a mass of . From the spectral type and colour, Rigel's surface temperature is estimated to be about 12,100Â K.

Rigel is a blue supergiant that has exhausted the hydrogen fuel in its core, expanded and cooled as it moved away from the main sequence across the upper part of the HertzsprungâRussell diagram. When it was on the main sequence, its temperature would have been around . Rigel's pulsation properties suggest it may have already passed through a red supergiant phase and then increased its temperature to become a blue supergiant for a second time, something that is expected for some sufficiently massive stars. The surface abundances seen in the spectrum are compatible with this only if its internal convection zones are modelled using non-homogeneous chemical conditions known as the Ledoux Criteria. Rigel is expected to eventually end its stellar life as a Type II supernova. It is one of the closest known potential supernova progenitors to Earth, and would be expected to have a maximum apparent magnitude of around (about the same brightness as a quarter Moon or around 300 times brighter than Venus ever gets.)

Rigel's complex variability at visual wavelengths is caused by stellar pulsations similar to those of Deneb. Additional observations of radial velocity variations indicate that it simultaneously oscillates in at least 19 non-radial modes with periods ranging from about 1.2 to 74 days. Recent stellar evolution models suggest the pulsations are powered by nuclear reactions in a hydrogen-burning shell that is at least partially non-convective. The star may also be fusing helium in its core.

Due to their closeness to each other and ambiguity of the spectrum, little is known about the individual intrinsic properties of the members of the Rigel BC triple system. All three stars seem to be near equally hot B-type main-sequence stars that are 3 to 4 times as massive as the Sun.

The earliest known recording of the modern name "Rigel" is in the Alfonsine Tables of 1521. It is derived from the Arabic name ', "the left leg (foot) of Jauzah" (i.e. "rijl" meaning "leg, foot"), which can be traced to the 10th century. "Jauzah" was a proper name of the Orion figure, an alternative Arabic name was ', "the foot of the great one", which is the source of the rarely used variant names "Algebar" or "Elgebar". The "Alphonsine Tables" saw its name split into "Rigel" and "Algebar", with the note, "et dicitur Algebar. Nominatur etiam Rigel." Alternate spellings from the 17th century include "Regel" by Italian astronomer Giovanni Battista Riccioli, "Riglon" by German astronomer Wilhelm Schickard, and "Rigel Algeuze" or "Algibbar" by English scholar Edmund Chilmead.

In the constellation of Orion as the mythological Greek huntsman, Rigel represents his knee or (as its name suggests) foot; with the nearby star Beta Eridani marking Orion's footstool. Rigel is presumably the star known as "Aurvandil's toe" in Norse mythology. In the Caribbean, Rigel represented the severed leg of the folkloric figure "Trois Rois", himself represented by the three stars of Orion's Belt. The leg had been severed with a cutlass by the maiden "BÄ¯hi" (Sirius). The Lacandon people of southern Mexico knew it as "tunsel" ("little woodpecker").

Rigel was known as "Yerrerdet-kurrk" to the Wotjobaluk koori of southeastern Australia, and held to be the mother-in-law of "Totyerguil" (Altair). The distance between them signified the taboo preventing a man from approaching his mother-in-law. The indigenous Boorong people of northwestern Victoria named Rigel as "Collowgullouric Warepil". The Wardaman people of northern Australia know Rigel as the Red Kangaroo Leader "Unumburrgu" and chief conductor of ceremonies in a songline when Orion is high in the sky. Eridanus, the river, marks a line of stars in the sky leading to it, and the other stars of Orion are his ceremonial tools and entourage. Betelgeuse is "Ya-jungin" "Owl Eyes Flicking", watching the ceremonies.

The MÄori people of New Zealand named Rigel as "Puanga", said to be a daughter of "Rehua" (Antares), the chief of all stars. Its heliacal rising presages the appearance of "Matariki" (the Pleiades) in the dawn sky, marking the MÄori New Year in late May or early June. The Moriori people of the Chatham Islands, as well as some Maori groups in New Zealand, mark the start of their New Year with Rigel rather than the Pleiades. "Puaka" is a local variant used in the South Island. In Japan, the Minamoto or Genji clan chose Rigel and its white color as its symbol, calling the star "Genji-boshi" (), while the Taira or Heike clan adopted Betelgeuse and its red color. The two powerful families fought the Genpei War; the stars were seen as facing off against each other and only kept apart by the three stars of Orion's Belt. Rigel was also known as "Gin-waki", (), "the Silver (Star) beside ("Mitsu-boshi")".

The MS Rigel was originally a Norwegian ship, built in Copenhagen in 1924. It was requisitioned by the Germans during World War II and sunk in 1944 while being used to transport prisoners of war. Two US Navy ships have borne the name USS Rigel.

The SSM-N-6 Rigel was a cruise missile program for the US Navy that was cancelled in 1953 before reaching deployment.

The Rigel Skerries are a chain of small islands in Antarctica, renamed after originally being called Utskjera. They were given their current name as Rigel was used as an astrofix. Mount Rigel, elevation 1,910 m, is in Antarctica.


</doc>
<doc id="26464" url="https://en.wikipedia.org/wiki?curid=26464" title="Afghan Civil War (1928â1929)">
Afghan Civil War (1928â1929)

The Afghan Civil War was fought from 14 November 1928 to 13 October 1929. Rebelling, and subsequently governing Saqqawist forces under HabibullÄh KalakÄni fought against various opposing tribes and rival monarchs in the Kingdom of Afghanistan, among whom Mohammed NÄdir KhÄn eventually achieved a preponderant role. Despite early successes, such as the capture of Kabul and defeat of Amanullah Khan on 17 January 1929 or the capture of Kandahar on 3 June, the Saqqawists were eventually deposed by anti-Saqqawist forces led by Nadir on 13 October 1929, leading to Nadir's ascension as King of Afghanistan, who ruled until his assassination on 3 November 1933.

The war began when the Shinwari tribe revolted in Jalalabad and drew a manifesto of 10 grievances, 5 of which related to Amanullah's meddling with the status of women. Although this revolt was quelled by a force led by Ali Ahmad Khan, a concurrent Saqqawist uprising in the north managed to capture the besieged city of Jabal al-Siraj, before attacking Kabul on 14 December 1928. Although the first Saqqawist assault on Kabul was repulsed, the second Saqqawist assault succeeded at capturing Kabul on 17 January 1929. The government at that time was focused on social reforms, such as the expansion of women's rights and the adoption of a military draft, which had earlier led to the unsuccessful Alizai rebellion and Khost rebellion. Kalakani denounced his opponents as kuffar, while his forces committed acts of rape and looting.

After capturing Kabul, the Saqqawists defeated a rival government in Jalalabad led by Ali Ahmad Khan on 9 February. Despite a setback in the Battle of Shaykhabad in early March, the Saqqawists managed to extend their control to Kandahar in June after a short siege. However, they were unable to defeat Nadir Khan in the Logar valley, who had entered the area together with Amanullah in March, although the latter left the country on 23 May. After a months-long stalemate, Nadir Khan eventually managed to force the Saqqawists to retreat into Kabul in October 1929, and subsequently into the Arg. The capture of the Arg on 13 October 1929 marked the end of the civil war, although Saqqawist activity continued until 1931. The civil war was fought concurrently with a to fight the Basmachi movement.

During the anti-Saqqawist capture of Kabul, Nadir's forces sacked the city against his orders. After the civil war, Nadir did not cede control of the Afghan throne back to Amanullah, and this led to several rebellions, including the Shinwari rebellion, the Kuhistan rebellion, the Ghilzai rebellion, and Mazrak's revolt. During World War II, Amanullah would unsuccessfully try to regain the throne with Axis help.
AmÄnullÄh KhÄn reigned in Afghanistan from 1919, achieving full independence from the British Empire shortly afterwards. Before the Treaty of Rawalpindi was concluded in 1921, Afghanistan had already begun to establish its own foreign policy, including diplomatic relations with the Russian Soviet Federative Socialist Republic in 1919. During the 1920s, Afghanistan established diplomatic relations with most major countries.

The second round of AngloâAfghan negotiations for final peace were inconclusive. Both sides were prepared to agree on Afghan independence in foreign affairs, as provided for in the previous agreement. The two nations disagreed, however, on the issue that had plagued Anglo-Afghan relations for decades and would continue to cause friction for many more â authority over Pashtun tribes on both sides of the Durand Line. The British refused to concede Afghan control over the tribes on the British side of the line while the Afghans insisted on it. The Afghans regarded the 1921 agreement as only an informal one.

The rivalry of the great powers in the region might have remained subdued had it not been for the dramatic change in government in Moscow brought about by the Bolshevik Revolution of 1917. In their efforts to placate Muslims within their borders, the new Soviet leaders were eager to establish cordial relations with neighboring Muslim states. In the case of Afghanistan, the Soviets could achieve a dual purpose: by strengthening relations with the leadership in Kabul, they could also threaten Britain, which was one of the Western states supporting counterrevolution in the Soviet Union. In his attempts to end British control of Afghan foreign policy, Amanullah sent an emissary to Moscow in 1919; Vladimir Lenin received the envoy warmly and responded by sending a Soviet representative to Kabul to offer aid to AmÄnullÄh's government.

Throughout AmÄnullÄh's reign, Soviet-Afghan relations fluctuated according to Afghanistan's value to the Soviet leadership at a given time; Afghanistan was either viewed as a tool for dealing with Soviet Muslim minorities or for threatening the British. Whereas the Soviets sought Amanullah's assistance in suppressing anti-Bolshevik elements in Central Asia in return for help against the British, the Afghans were more interested in regaining lands across the Amu Darya lost to Russia in the nineteenth century. Afghan attempts to regain the oases of Merv and Panjdeh were easily subdued by the Soviet Red Army.

In May 1921, the Afghans and the Soviets signed a Treaty of Friendship, Afghanistan's first international agreement since gaining full independence in 1919. The Soviets provided Amanullah with aid in the form of cash, technology and military equipment. Despite this, Amanullah grew increasingly disillusioned with the Soviets, especially as he witnessed the widening oppression of his fellow Muslims across the border.

Anglo-Afghan relations soured over British fear of an Afghan-Soviet friendship, especially with the introduction of a few Soviet planes into Afghanistan. British unease increased when Amanullah maintained contacts with Indian nationalists and gave them asylum in Kabul, and also when he sought to stir up unrest among the Pashtun tribes across the border. The British responded by refusing to address Amanullah as "Your Majesty," and imposing restrictions on the transit of goods through India.

AmÄnullÄh's domestic reforms were no less dramatic than his foreign policy initiatives, but those reforms could not match his achievement of complete, lasting independence. Mahmud Tarzi, Amanullah's father-in-law and Foreign Minister, encouraged the monarch's interest in social and political reform but urged that it be gradually built upon the basis of a strong central government, as had occurred in Turkey under Kemal AtatÃ¼rk. Socially, Amanullah enjoyed many of Mahmud Tarzi's thoughts at the time, such as giving women more rights and allowing freedom of press through publishing. Tarzi, being heavily influenced by the West, brought this influence to Afghanistan - Amanullah enjoyed Western dress and etiquette. His wife, Queen Soraya Tarzi, became the face of Amanullah Khan's reforms in regard to women.

AmÄnullÄh's reforms touched on many areas of Afghan life. In 1921 he established an air force, albeit with only a few Soviet planes and pilots; Afghan personnel later received training in France, Italy and Turkey. Although he came to power with army support, Amanullah alienated many army personnel by reducing both their pay and size of the forces and by altering recruiting patterns to prevent tribal leaders from controlling who joined the service. Amanullah's Turkish advisers suggested the king retire the older officers, men who were set in their ways and might resist the formation of a more professional army. Amanullah's minister of war, General Muhammad Nadir Khan, a member of the Musahiban branch of the royal family, opposed these changes, preferring instead to recognize tribal sensitivities. The king rejected Nadir Khan's advice and an anti-Turkish faction took root in the army; in 1924 Nadir Khan left the government to become ambassador to France.

If fully enacted, AmÄnullÄh's reforms would have totally transformed Afghanistan. Most of his proposals, however, died with his abdication. His transforming social and educational reforms included: adopting the solar calendar, requiring Western dress in parts of Kabul and elsewhere, discouraging the veiling and seclusion of women, abolishing slavery and forced labor, introducing secular education (for girls as well as boys); adult-education classes and educating nomads. His economic reforms included restructuring, reorganizing and rationalizing the entire tax structure, anti-smuggling and anti-corruption campaigns, a livestock census for taxation purposes, the first budget (in 1922), implementing the metric system (which did not take hold), establishing the Bank-i-Melli (National Bank) in 1928, and introducing the Afghani as the new unit of currency in 1923. The political and judicial reforms AmÄnullÄh proposed were equally radical for the time and included the creation of Afghanistan's first constitution (in 1923), the guarantee of civil rights (first by decree and later constitutionally), national registration and identity cards for the citizenry, the establishment of a legislative assembly, a court system to enforce new secular penal, civil and commercial codes, prohibition of blood money, and abolition of subsidies and privileges for tribal chiefs and the royal family.

Although sharia (Islamic law) was to be the residual source of law, it regained prominence after the Khost rebellion of 1924-25. Religious leaders, who had gained influence under Habibullah Khan, were unhappy with AmÄnullÄh's extensive religious reforms. Conventional wisdom holds that the tribal revolt that overthrew Amanullah grew out of opposition to his reform program, although those people most affected by his reforms were urban dwellers not universally opposed to his policies, rather than the tribes. Nevertheless, the king had managed to alienate religious leaders and army members.

According to a later British ambassador in Afghanistan, William Kerr Fraser-Tytler, the British empire, though officially neutral, was very concerned about the situation in Afghanistan and they "made up a set of rules to govern the situation. It was unneutral to refuse an Afghan entry into Afghanistan, but once he was in he became a contestant, and it would be unneutral to allow him to recross the border, seeking a brief asylum before plunging again into the fray. And so in a mixture of the rules of cricket and football it was ordained that a player might go on the field once, and play for the crown. But if he was forced into touch, and recrossed the line, whether voluntarily or not, he was 'out' and the referee would not let him back into the game."

Many commentators in Afghanistan and elsewhere hold the belief that Britain played a part in the fall of Amanullah in January 1929, and this is supported by Soviet Historiography. According to EncyclopÃ¦dia Iranica, "While it can not be dismissed out of hand, the fact remains that no evidence to support it can be found in the copious British Indian archives pertaining to this period. There can be no doubt, however, that behind the stance of official neutrality which the British maintained throughout the crisis of 1929 lay an unwillingness to help AmÄn-AllÄh to reconquer his throne and a benevolence toward the moves of NÄder Khan. While the Soviet authorities favored AmÄn-AllÄh (though reluctantly) and aided a foray on his behalf by á¸ olÄm NabÄ« Äará¸µÄ« in the Balá¸µ region, the British authorities allowed NÄder Khan to reenter Afghanistan through India and to obtain a decisive addition of strength through his recruitment of thousands of armed WazÄ«r and MasÊ¿Å«d frontier tribesmen. Also helpful was their decision to lift a restriction order, imposing residence at a fixed address in India, on FaÅ¼l Ê¿Omar MojaddedÄ«, who was to play an apparently decisive role in persuading the NaqÅ¡bandÄ« "mollÄ"s of Afghanistan to change sides and later was to become NÄder Shahâs first minister of justice. In short, while all the evidence indicates that BaÄÄa-ye SaqqÄ (Kalakani)âs rise was due solely to the internal disintegration of King AmÄn-AllÄhâs rÃ©gime, there can be no doubt that British policy, tacit rather than explicit, helped to bring about BaÄÄa-ye SaqqÄâs fall".

After coming to power in Afghanistan, the Saqqawists allowed Basmachi insurgents to operate in northern Afghanistan, who then had established themselves in Imanseiide, Khanabad, Rustake, Talikane, Faizabad by the end of March 1929. Repeated Basmachi incursions into Soviet territory eventually prompted the start of the .

The Iranian military attache, Colonel Ali Khan, was under instruction by the Iranian Shah to protect the Shiite community of Afghanistan to the greatest possible extent that would not invite a Saqqawist attack on Iran.

While Germany itself was uninvolved in the war, the Afghan-German Trading Company was requested by Kalakani to assassinate Amanullah Khan on 15 April 1929, and were promised a large reward if they did so.

The unraveling began when Shinwari Pashtun tribesmen revolted and besieged Jalalabad on 14 
November 1928, cutting telegraph wires and cutting the road to the capital, after which they drew a manifesto of ten grievances, five of which related to what they saw as Amanullah's unsupportable meddling with the status of women. However, during the Shinwari rebellion two years later, the Shinwari claimed that this revolt was "not so much anti-Amanullah as against the local tax-collectors at Jelalabad". The initial response of the government was to send a small contingent to relieve Jalalabad, which was halted at Nimla, 20 miles (32Â km) west of Jalalabad, before that force found itself surrounded and destroyed shortly after. Thereafter, Amanullah sent two representatives to suppress the uprising - His foreign minister, Ghulam Siddiq Khan, and the head of the National Council, Shayr Ahmad Khan. However, In late November, they had a falling out, and according to Fayz Muhammad, were negotiating separately with the tribes. Ghulam Siddiq is said to have incited some of the Shinwari to attack Shayr Ahmad Khan, the main consequence of which was that the Shinwari burned the Emir's winter palace in Jalalabad to the ground.

On 3 December 1928, Amanullah then decided to send his brother-in-law, Ali Ahmad Khan Luynab, to deal with the problem, and sent him off with regular troops, militia levies, and a sizable treasury with which to conciliate the tribal leaders. Ghulam Siddiq and Shayr Ahmad were ordered back to Kabul.

In the meantime, calls had gone out for tribal levies to assist the regular army in dealing with the Shinwari uprising, and armed tribesmen from the east, south and west, which included Waziri, Wardak, Ghilzai and Tajik tribesmen, but also more recently the Mangal tribesmen (who recently were at war with Amanullah's government) trickled into the capital to help. These men had no particular loyalty to the government and saw the situation simply as an opportunity for enrichment. As it turned out, there was no need to send them to Jalalabad, Ali Ahmad managed to conciliate the Shinwari leaders and put an end to the uprising, but as it took a while for this news to spread through the countryside, the levy tribesmen continued to arrive in the capital.

Amanullah presumably welcomed the news of the reconciliation. However, any feeling of relief would have been very temporary - forces led by a Tajik leader, Habibullah Kalakani, were moving toward Kabul from the north. Kalakani was an native of Kalakan, a village thirty kilometers north of Kabul. In late November, they besieged Jabal al-Siraj, north of Kabul, and on either 11 or 12 December, after 18 days of siege, Ahmad Ali Lodi peacefully surrendered the citadel, handing over all government funds as well as 18 machine guns, and an unspecified number of heavy weapons and rifles.

Emboldened by the victory, Kalakani attacked Kabul with 2000 men (only 200 of which were armed with rifles, and the rest armed with sticks and axes) on 14 December 1928. He and his forces entered the Murad Beg Fort on the northern slopes of the Kuh-i Kutal, nearby the village of Khayr Khanah. The rebels, feeling that deposing an emir would be against the shariah, performed a ritual and declared Kalakani the new emir, and then passed through the village of Dih-i Kupak at 3:00 PM. Around 3:15 PM, they reached the Bagh-i Bala park. They also occupied Bagh-i Bala palace, formerly the summer residence of Abdur Rahman Khan, which had now been turned into a military hospital for the Emir's personal guard and the residence of a Turkish physician, Bahjet Beg. After disarming and dismissing the guards and the embassy, they stationed their own guards, reassuring the employees of the embassy that they were guests of the nation and as such no harm would come to them.

The rebels also managed to enter the house and fortress tower of Shahr Ara, which was defended by Shawkat Beg, a Turkish officer who was the son of Muhammad Akbar Khan. His small force, as well as a group of cavalry officers, managed to prevent the Rebels from entering the old city.

As the battle continued, the whole city was filled the sounds of artillery and gunfire. However, only the cavalry of the Emir's personal guard and a few other loyal soldiers actually put up a fight against Kalakani's forces. The rest of the army was in a mutinous mood, as their officers had been appropriating the soldier's rations. Holding their commanders rather than the rebels to blame for the trouble, when ordered to shoot, the soldiers simply fired their weapons in the air. Tumult and confusion were now widespread. The emir was furious when he heard of the mutiny, and ordered all the weapons to be distributed to the residents of Kabul and to the tribesmen who had come into the city but had not yet left for Jalalabad to fight the Shinwari. However, the near-universal loathing of the Afghans for Amanullah led to the majority of them refusing to take up arms against Kalakani. To make matters worse for Amanullah, Some Waziri, Mangal and Ahmadzai tribesmen defected to Kalakani, took up positions on the Asmai Hill in the center of Kabul, and fired on the Emir's troops.

Ghulam Ghaws, Whose father, Malik Jahandad Ahmadzai, had been executed following a rebellion, headed towards his hometown costs, carrying with him more than 300 rifles, armed the people there, and rose up against the government. Other tribes acted similarly because there was no control over the distribution of weapons.

The battle took a drastic turn on 25 December, when Kalakani was wounded in the shoulder from an aerial bomb, causing him to retreat 20 kilometers north, to Murad Beg Fort, in the Kuhdaman region.

Kalakani's retreat gave Amanullah a chance to regroup. In late December, he began shelling Murad Beg Fort, and this shelling lasted until 13 January. However, the shelling failed to provide any results, and this disheartened the king. In the early morning of 14 January, Amanullah abdicated the throne to his oldest brother, Inayatullah Khan, who ruled for only three days before escaping into exile in British-India. Amanullah's efforts to recover power by leading a small, ill-equipped force toward Kabul failed. The deposed king crossed the border into British-India and went into exile in Italy and remained in Europe until his 1960 death in ZÃ¼rich, Switzerland. At the time of his abdication, Amanullah's troops were fighting in the Khayr Khanah (Khirskhanah) pass, seven miles (11Â km) north of Kabul.

After accessing to the Afghan throne, Inayatullah Khan sent a peace envoy to Kalakani. The envoys informed Kalakani that Inayatullah's accession had been illegal in accordance to the shariah, since Kalakani had ascended the throne in the Islamic month of Rajab, and Inayatullah's accession had taken place in the Islamic month of Sha'ban. Rejoiced, Kalakani and 28 armed men, accompanied by a group of unarmed Kuhdamanis passed through the village of Dih-i Afghanan and attacked the capital, shouting "ya chahar yar" and firing guns at the air. On the very first day of his reign, Inayatullah was forced to barricade himself in the Arg with several of his ministers.

On the 16th of January, while 80 Hazaras from Bihsud were defending the Qalah-i Buland Fortress, as well as the arsenal at Kulula Pasha, some officials declared their allegiance to Kalakani. These included Shayr Ahmad, head of the national council, Fayz Muhammad Khan, former minister of trade, Abd al-Hadi Khan, the minister of finance, and the sons of Abdur Rahman Khan: Mir Hashim, Sardar Amin Allah Khan, Muhammad Umar Khan, as well as a number of deputy ministers and heads of state bureaus.

On the 17th of January, Inayatullah, unnerved by the lack of support from the Kabulis, surrendered to Kalakani and abdicated the throne. Kalakani allowed him to peacefully leave Kabul with his family and 3000 rupees.

Having become King of Afghanistan, Kalakani appointed a number of people into office, including:


On 9 May, Kalakani passed a decree in Kabul which forbade citizens of Kabul from moving out of the city without permission, even into the government-controlled Bandar-i Arghandah, Charasya, Bini Hisar, Butkhak, Kutal-i Pay Manar, Kutal-i Khayr Khanah, Maydan, Jalriz, Logar, Khurd Kabul, Tangi Gharu or Dih Sabz.

On 31 May, Kalakani paid a visit to the shrine at Mazar-i Khwajah Musafir, which lies near the village of Chihil Tan above the village of Shaykh Muhammad Riza-yi Khurasani which lies in the Paghman District, 6 miles (9.6Â km) west of Kabul

Following his takeover, Kalakani, fearful of a counterattack by the Amanullah loyalists, swiftly moved the treasury to Kudhaman.

The first concerted opposition to Kalakani came from Ali Ahmad Khan, who was still stationed in Jalalabad after suppressing the Shinwari revolt. There, the locals proclaimed Ali as the new Emir upon receiving the news of Kalakani's accession. Ali then marched his troops on Samucha-i Mulla Omar, Tangi Khurd Kabul, and Chanri, and took up positions there. At the head of a 2,000 men strong army and a tribal militia, he marched to Jagdalak, where he waited for a force of Mohmands who had promised to join him. Over the course of 23 to 29 January, Ali sent out proclamations of his new emirate to Kabul, Logar, the Hazarahjat, the Southern province, and elsewhere, and called on people to join him.
Malik Qays of the Khugyani tribe, who had initially allied himself with Ali, defected to Kalakani, captured Ali and brought Ali to Kalakani in exchange for 17,000 rupees and the rank of lieutenant general, ending Ali's reign on 9 February.

Sometime before 13 March, the Battle of Shaykhabad took place, 46 miles (74Â km) from Kabul and halfway across the Kabul-Ghazni road.

It was here where Karim Khan Wardak, who refused to pledge allegiance to Kalakani, had made defensive preparations. Around this time, Abd al-Wakil Khan, who had earlier been appointed field marshal by Kalakani, was dispatched to Ghazni and Qandahar with a force of 3,000 men. When Abd al-Wakil reached the village of Bini Badam and Qalah-i Durrani, 30 miles (48 kilometres) from Kabul, he halted there to deal with Karim Khan Wardak's forces, only then to proceed. But Karim Khan, along with Wazir and Hazara leaders who had gathered in support of Aman Allah, sent a joint message to the field marshal that said:

Abd al-Wakil accepted this message at face value, and he sent the Model Battalion, which at the time numbered 1,800 men and was stationed at Qal ah-yi Durrani, to march on Shaykhabad along with 400 royal cavalry and 800 Kuhistani and Kuhdamani infantry militia which had halted near the village of Bini Badam. After an exhausting march through snow-covered hills, Abd al-Wakil's forces were ambushed near Zarani, at the edge of the Daht-i Tup waste land by Wardak tribesmen, who came thundering down the hills after a soldier's shot at a bird alerted them that Kalakani's troops were nearby. Many of Abd al-Wakil's troops were killed in the ambush; only 20 of 400 cavalrymen survived.

The people of Maydan, Jalriz and Sanglakh refused to offer allegiance to Kalakani, and formed an alliance with the Wardak and surrounded Kalakani's armies in Maydan, and defeated them in Qalah-i Durrani, before advancing to Arghandah, 14 miles (22.5 kilometres) west of Kabul, where some Kalakani's forced decided to retreat toward Qalah-i Qazi, Chardihi and Kuhdaman.

On 5:30 on 22 March, Kalakani personally headed from Kabul to Arghandah to bolster the spirits of his soldiers and managed to convince the soldiers to advance on Kutal-i Shaykh, a small village near the intersection with the road west to the Unay Pass. They accepted, and the battle of Kutal-i Shaykh lasted until the evening with a victory for Kalakani.

In the morning of 23 March, Kalakani ordered 500 militamen to be brought back to Kabul from Najrab, because they had been fighting against the Tagabis and Kalakani was worried that Najrab might defect. On 24 March, Kalakani ordered some Kuhdamanis, Kuhistanis, and people from the villages of Dih-i Nur, Maydan and Arghandah to cover the army rear which was then at Qalah-i Durrani and Pul-i Maydan and so deny those awaiting its defeat the chance to march on Chardihi and Kuhdaman. Later that same day, Kalakani's Field Marshall, Purdil Khan (who had since been named as Minister of Defense) shelled Maydan, which strengthened the resolve among the Maydan, Arghandah and Sanglakjh to fight Kalakani. On the 25th, Purdil Khan managed to capture Maydan, but the great casualties inflicted prevented him from advancing towards Wardak and Ghazni, and he withdrew to Arghandah and Qalah the following day.

At this time, Amanullah had supposedly returned to Afghanistan and was marching from Qandahar with an army made of Durrani, Khattak, Ghilzai, and Hazara fighters. Four days after entering Afghanistan, Amanullah learnt of a Saqqawist uprising in Herat. On 27 March, Habibullah Kalakani ordered his brother, Hamid Allah Kalakani, to lead a force of Panjshiris backed by 14 siege guns, to Maydan. At Kutal-i Shaykh, this force won a major victory which allowed it to continue advancing towards Maydan, where it took 25 prisoners and destroyed several forts. On the night of the 28th, anti-Saqqawist tribesmen ambushed Hamid Allah's force, and while they were able to deal vast casualties and capture many field guns and rifles, they were unable to disloge Hamid from his position. On the 30th, the anti-Saqqawist tribes renewed the battle, and this time they managed to almost completely expel Hamid Allah's forces from Maydan, except for a few detachments which were surrounded in the fortress known as Qalah-i Abd al-Ghani Khan Beg Samandi, about 14 miles (22.5Â km) west of Shaykhabad. A large part of Hamid's defeated army retreated to Arghandah and Qalah-i Qazi.

On the 31st, Kalakani started another offensive on Maydan and made some progress. On 2 April, a force from Bihsuf occupied the Unay Pass and reached an agreement with the militias of the Surkh-i Parsa, Turkman, Bamyan, Balkhab and Shaykh Ali Hazarah for them to attack Kuhistan and Kuhdaman via the Ghurband Valley road while it simultaneously attacks Kabul via the road through Maydan. On 3 April, Kalakani's forces clashed in Shash Gaw, 13 miles (20.9Â km) north of Ghazni. On 7 April they were defeated while advancing along the road not so far from Ghazni near Shiniz in Wardak. On 7 April, they clashed at Shiniz, and on the 9th they clashed in Shaykhabad and Jaghatu, northwest of Ghazni. Fayz Muhammed reports that Kalakani suffered a major defeat near Ghazni on 9 April and that his forces fled to Qalah-i Durrani, but historian Robert D. McChesney believes this to be false. By the 12th, there were rumours in Kabul that Ghazni was surrounded by anti-Saqqawist forces. In mid-March, Mohammed Nadir Shah, who had departed from France in January, arrived in Jalalabad to centralize the opposition to Kalakani. It was reported on 16 April that Ghazni had fallen to anti-Saqqawist forces, and that Kalakani's forces had been defeated at Shaykh Amir, near the Majid Pass. By the 20th, there were reports that anti-Saqqawist forces were on the doorstep of Paghman, just west of Kabul, and that Hazarah forces from Bihsud crossed the Unay pass and were heading on their way to Ghurband, while another force occupied positions there to prevent Kalakani from using it to cross to Hazarahjat. The force sent to Ghazni retreated to Shiniz-i Wardak. On the 21st, soldiers loyal to Kalakani left Kabul to reinforce Ghazni. At this time, Kalakani decided to reinforce the Qalah-i Durrani fort to prevent rebel tribes from advancing past it. On the 24th, Kalakani's forces were clashing in Shash Gaw, 13 miles (21Â km) northwest of Ghazni. On the 26th, while laying siege to Ghazni, Amanullah had inexplicably given the order to retreat to Qandahar. On the 28th, it was reported that Kalakani's army had captured Ghazni. On 30 April, anti-Saqqawist forces re-entered Ghazni, renewing the battle. On the same day, a large anti-Saqqawist offensive managed to dislodge Kalakani's forces from positions in Shaykhabad, Takiya and Shash Gaw, forcing some of them to retreat towards Daht-i Tup. On 1 May, anti-Saqqawist forces continued their offensive, clashing in Dasht-i Tup and Shaykhabad, and on 2 May fighting was taking place in Shaykhabad, Dasht-i Tup and Qalah-yi Durrani. On 7 May, units were sent from Kabul to Mahtab and Arghandah to prepare defenses there. On 8 May, while there was fighting in Dasht-i Tup and Bini Badam, Saqqawist forces under Purdil Khan departed for Charikar. One of Kalakani's generals, Muhammad Unar Khan, died on 14 May. The next day, Kalakani sent units to Kuh-i Asmai and Shayr Darwazah. On the 19th, Amanullah was rumoured to be besieged at Kalat, 80 miles (128.75Â km) north of Qandahar. On the 23rd, Amanullah Khan fled Afghanistan into the British Raj, leaving his brother Inayatullah Khan in charge of anti-Saqqawist resistance. By that time, Kalakani held control of the entire Ghazni region, and the road south of Ghazni as open. By 1 June, anti-Saqqawist forces who at this time were at Qarabagh decided to retreat Qandahar, while Kalakani's armies were able to take Kalat and had surrounded the city of Qandahar, which duly fell on 3 June or 31 May.

On 8 March, Nadir Khan crossed into Afghanistan just east of Matun in the Kurram Valley. On 16 March, Kalakani dispatched troops in two directions: along the route to Maydan through Qalah-i Mahtab Bagh, Qalah-i Durrani, Qalah-i Qazi, and Arghandah, and via Charasya and Musai to Logar. 129 troops were also dispatched by Kalakani to the Logar Valley, which were defeated at Waghjan Gorge (Between Kushi in Kulangar and Shikar Qalah), forcing them to retreat to Rishkhur, south of Kabul. On 23 March, heavy fighting was occurring in Najrab, north of Kabul. The next day, 500 of Kalakani's troops who were marching from Charasya to Kulangar were ambushed, with many killed or wounded.
By 31 March, there were some reversals for Kalakani on the Maydan front. On 23 March, 6000 Mangal tribesmen joined Nadir Khan at Khost. Four days later, he left for Urgun, which he reached on 5 April. A few days later, he took Baladah, and on the 15th he captured Gardez. On the 23rd, Nadir was residing in Safid Qalah, at the southern entrance of the Altamur (or Tirah) pass. On the 24th, he continued through the pass to Charkh, where he was confronted by a force sent by Kalakani. After initial success in capturing the village of Dabar in Charkh, he was ultimately forced to retreat to Sijinak, east of Gardiz on the 27th. On the 22nd, Kalakani sent troops to Logar to defend it against Nadir, whose forces captured Dubandi and the village of Kushi that same day. On the 23rd there were rumours in Kabul that Kalakani's armies had been defeated and forced to retreat to Qalah-i Durrani on the Maydan-Ghazni road. On the 23rd, Nadir reached the Waghjan Gorge. On the 24th, there were rumours in Kabul that Nadir's forces had entered the village of Aghujan, 22 miles (35.4Â km) south of Kabul. On the 25th, Nadir reached Hisarak in the Logar valley, and on that same day it was rumoured that he had suffered a defeat in a battle at Tirah Pass. On 1 May, while a battle in the Southern province had been going on for three days, Kalakani's forces carried out a raid on Khushi in Logar and plundered its inhabitants. By 3 May, Nadir had established a fort at Surkhab and was harassing Kalakani's troops to prevent Kalakani from advancing into the Southern province. On the sixth, Kalakani sent new troops to Charikar. On the 11th there were rumours that Nadir arrived in Charkh in Logar. Robert D. McChesney believes this to be false, and says this was just wishful thinking. On 8 May, Hashim (Nadir Khan's brother) persuaded tribes of the Eastern province to unite against Kalakani, who agreed to raise 40,000 troops who would advance in three formations through Tagab, Tangi Gharu, Ghakari and Lataband to attack Kuhdaman, Kuhistan and Kabul. That same day, Nadir's forces reached the region of Pul-i Hashim Khayl in Gandamak and at Tagab made plans to continue further down the road. On 11 May pro-Nadir tribes moved on Kabul but were stopped by Saqqawist Shinwari at Surkhrud. On 12 May there were rumours that Nadir had inflicted a defeat upon Kalakani at Bidak. On 15 May Nadir crossed the Tirah Pass and began an incursion into the Logar valley, which continued the 16th, when Nadir pursued the local Saqqawist forces as far as Kulangar, Kutti Khayl and Muhammad Aghah, and was fighting over control of the Ghurband Valley. Also on the 16th, Nadir reached Khak-i Jabbar via the road through Hisarak. On the 23rd, when peace negotiations were ongoing, Kalakani sent a force of 300 men to Logar. On 26 June, Kalakani's forces recaptured Gardiz.

On 14 July, Nadir Khan's forces entered the Logar valley, won a victory at Padkhwab-i Rughani and from there, advanced on Surkhab where they surrounded and besieged one of Kalakani's forces at Kariz-i Darwish, which surrendered the next day. On 18 July, Kalakani's forces fought a battle with the Khugyani near Khurd Kabul. In order to get the upper hand in the battle, Kalakani confiscated all automobiles and horse carriages in Kabul, so that reinforcements could arrive more quickly. This plan worked, and on 19 July the situation was stabilized. On 18 August, Nadir moved his headquarters to Ali Khayl with the Jaji tribe, which had assured him of their unswerving loyalty.

Sometime before 17 March, anti-Saqqawist tribes from Tagab launched a surprise attack on Sarubi and Gugamandan, opening what Robert D. McChesney labels the "The Tagab Front". This attack took the local garrisons by surprise and allowed the Tagabis to capture two cannons, weaponry and other military supplies. After this success, the Tagabis planned a northward assault on Jamal Afgha in Kuhistan, which was successfully undertaken on the 18th. On 23 March, people of Durnama, Sujnan and Bulaghin attacked the Tagabis, defeated them, and occupied their positions, which stabilized the Tagab Front. On 1 April, prisoners arrived from Tagab.

On 2 August, the Tagab front was re-opened following a local uprising. On 12 August, after days of skirimishes, Kalakani's forces launched a large counteroffensive and forced the Tagabis to surrender the next day. This ended the Tagab front.

On 2 April, there were rumours in Kabul that anti-Saqqawist Hazaras had occupied positions in Balkh, while others were able to march on Aqchah, Andkhuy, Maymanah and Mazar-i Sharif. On the 7th, anti-Saqqawist forces arrived in Siyahgird in Ghurband. On the 17th, Sayyid Hussayn had left for Charikar. anti-Saqqawist forces closed the road in the Ghurband valley leading to Kuhistan and Kuhdaman, and on the 18th they had reportedly reached Ghurband. On the 19th, a counteroffensive by Ghulam Rasul Khan against the Hazaras was called off by Kalakani, who wanted instead to focus on Charikar, which was reportedly under attack by local anti-Saqqawist partisans. On the 20th, Sayyid Husayn left for Charikar, where he ambushed and killed Ata Muhammed, whose fianceÃ© Sayyid had taken as a wife in the years prior, for which Ata Muhammed had sworn to kill him. On the 26th, word spread in Kabul that Hazara units had reached Katan mountain, west of Shakar Dara, and from there captured the Khudamani villages of Shakar Darra, Farza, Ghaza, Saray Khwajah, and Charikar. On the 27th, anti-Saqqawist Hazaras reportedly attacked Farza, Shakar Darra and Istalif (towns in Kuhdaman). That same day, in response to Hazara advances, Kalakani sent Hamid Allah on a counteroffensive which succeeded forcing the Hazaras to pull back. However, the situation remained dire for Kalakani and on 3 May he withdrew troops and munitions from other fronts to reinforce the Ghurband front. On 4 May, on the same day which Amanullah withdrew from Wardak, there were rumours in Kabul that Sayyid Husayn, one of Kalakani's generals, had made a breakthrough in Ghurband, and on the next day he reportedly was marching on Mazar-i Sharif via the road through Qunduz. Before this march on Mazar-i Sharif, the city had earlier been the site of a mutiny by Kuhistani and Kuhdamani forces, which began in January after Kalakani captured Kabul, and was ended on 30 April by anti-Saqqawist forces. It's after this point that sources disagree - Faiz Mohammad records the mutineers of Mazar-i Sharif retreating to Herat and capturing it sometime before 15 May, while Ademec says that Herat was captured sometime after Saqqawist forces captured Mazar-i Sharif in June, after Amanullah left Afghanistan for the British Raj, and then gradually extended their control to Maymanah and then Herat.

On 10 May, word had spread in Kabul that Ghulam Jalani Khan had occupied Andarab and Khanabad in Qataghan and the governor of Qataghan-Badakhshan Province, Mir Baba Sahib Charikari had been killed. On 9 or 10 May, Sayyid Husayn suffered a severe setback in the Battle of Shibar Pass, where his 12,000-man strong army was routed by a local Hazara militia which sought revenge for destruction of cattle, which ended Sayyid's hopes of taking Mazar-i Sharif. On May 12, Sayyid found himself besieged in Kuhistan and was reportedly wounded. On that same day, one of Kalakani's generals, Abd Al-Wakil Khan captured Fayzabad in Badakhshan while some of his units reached Farjaghan (at the head of the Alishang Valley near Tagab and Najrab). Also on 12 May anti-Saqqawist forces won a victory in a battle near Pul-i Matak after marching on Tagab. On 13 May 900 men from Kalakani's army were captured in Ghurband after a brief battle. On the 14th, another 2000 Saqqawist soldiers were defeated and their weapons, materiel and ammunition was seized.

On the 15th, Sayyid Husayn began another offensive against anti-Saqqawist forces, but after taking the Pansjir Valley on the 15th he was stopped at the Khawak Pass on the 16th. On the 19th, there were rumours in Kabul that Sayyid Husayn had died and that anti-Saqqawist forces had marched on Charikar, which Robert D. McChesney believes to be either highly exaggerated or completely false. On the 26th, Sayyid Husayn returned to Kabul alive and well, dispelling rumours about his death, and by the beginning of June, the routes via the Ghurband Valley and the Salang and Khawak Passes were firmly in the hands of Kalakani. On 29 May, 2300 men were sent north to reinforce the Ghurband front. On 31 May, Kalakani's army, which had advanced as far as Bamyan en route to Mazar-i Sharif had been routed and forced to retreat to Jabal al-Siraj. On 11 April, Nadir arrived in Khushi in the Logar valley. On 20 April, the son of Abd Allah Khan Tukhi, whose brother, Ata Muhammad, had raised a rebellion in Mazar-i Sharif against Kalakani, was hanged in Kabul. On 7 May 12,000 anti-Saqqawist forces occupied the Unay Pass and the Safidkhak Pass, while others were positioned on the lower slopes of the Paghman and Shakar Darrah. At this time, various anti-Saqqawist tribes planned a coordinated assault on Kabul. However, ethnic differences and poor communications led the attack to never materialize. On 2 June, Kalakani sent troops to Sar-i Chashmah, where they were ordered not to fire on anyone who swears allegiance as long as they were unarmed. On 23 May, Amanullah left Afghanistan for a last time, never to return. That same day, Kalakani sent 6500 men to conquer the Hazarahjat. Sometime before 19 June, Kalakani's forces won a victory at Bamyan, where they at first found themselves encircled, before the commander of the besieging Hazara forces was bribed to retreat. As of 23 June, anti-Saqqawist forces continued to occupy the Unay pass. As of 27 June, anti-Saqqawist forces had advanced two miles (3.2Â km) from the Unay Pass towards Sar-i Chashmah. On 28 June, Kalakani's forces fought an offensive battle against Hazara militias at Qalah-i Karim. After capturing the village and burning it to the ground, they were ambushed by Hazaras who were subsequently driven off with artillery fire and forced to retreat into the mountains. After this victory, Kalakani's forces took control of the Unay Pass. On 29 June, Kalakani's forces advanced on Qalah-i Yurt. On 30 June, Kalakani's forces advanced toward Qalah-i Yurt after getting as far as Jawqul. On 1 July, Kalakani's forces looted homes in Takana, Jalriz and Kuhnah Khumar. On 2 July, Kalakani's forces suffered a defeat at Jawqul and were forced to retreat to Sar-i Chashmah, and then to Chandawul through Bazar-i Sar-i Chawk, Baghban Kuchan and Chandawul. On 3 July, Hamid Allah renewed the offensive, but was beaten back after being surrounded by Hazara militias, and was then pursued on his retreat as far as Sar-i Chashmah. As of 8 July, the Hazaras continued to fight Kalakani's forces and attacked a 5,000-man regiment, forcing them to retreat to Jalriz. However, the Hazarahs did not follow up this victory with an attack on Kabul - Robert D. McChesney points out that politics in Afghanistan tend to be extremely local, and once the Hazarahs secured control of their own regions, they had little interest in fighting for more territory.

On 10 July, Umar Khan promised to Kalakani that he would take upon himself the task of either forcing the Hazara's submission or crushing them. Faiz Mohammad quotes him as saying "Purdil Khan has taken Ghazni and captured Qandahar; Major General Muhammad Siddiq and Abd al-Qayyum Ibrahim Khayl Paghmani have advanced toward the Southern Province; and Muhammad Umar General Sur-i Satranj has defeated the opposition in numerous battles. I don't want to lose ground to my peers here. I should be able to make short work of the Hazarahs and extract their submission." He set off for Jalriz and linked up with soldiers who had been recently been skirmishing with Hazaras, won a victory at Sar-i Chasmah and conducted operations as far as Takana. However, success soon stalled, and after being wounded in the leg, Umar was forced to retreat, leaving the Hazaras to re-occupy all lost territory.

On 17 July, Hazara militias attacked Saqqawist forces in the Unay Pass and Qalah-i Safid, routing them and pursuing them to Takana and Jalriz. From 25 to 29 July, fighting took place in Jalriz, however on 30 July the Hazaras withdrew from Jalriz back to the Hazarahjat after they heard that Nadir suffered setbacks in the eastern province, which, for the time being, ended the Hazara's hopes of joining Nadir in a multi-pronged offensive toward Kabul. On 1 August the Hazaras began another offensive, attacking Qalah-i Majid (near Siyah Baghal) and Qalah-i Safid, a fort in the Unay Pass, chasing Kalakani's forces as far as Jalriz once again. On 3 August, Kalakani's forces were reportedly defeated at Jalriz once again. On 15 August, Hazara militias launched an offensive against Turkmen tribes who had pledged allegiance to Kalakani on 31 July, occupying positions in Darrah-i Suf, Kuh-i Shadyan and Marmal, and besieged fort Dih Dadi (a garrison village midway between the site of ancient balkh and Mazar-i Sharif).

On 18 August, an anti-Saqqawist uprising took place in Bamyan, Ghuri and Baghlan, blocking Kalakani's forces's route to Turkistan and forcing them to retreat to Ghurband. On 21 August, the Sayyid of Shaykh Ali launched an offensive against Kalakani, advancing as far as Khanabad, Andarab and Ghurband. On 26 August, there were rumours in Kabul that Hazara settlers successfully attacked Mazar-i Sharif. In early September, the Saqqawists won their last victory by taking Jalalabad. On 23 September, an pro-Nadir uprising in Kandahar succeeded at driving out Kalakani's forces from the city. On 29 September, a pro-Nadir force under Shah Wali crossed the Durand Line and occupied Khushi. On the 30th, he sent a 1000-man force ahead to Tangi Waghjan, the gorge on the road to the Logar Valley. On 3 October, after an intense battle, anti-Saqqawist forces captured the town of Muhammad Aghah, placing themselves within striking distance of Kabul. Kalakani himself took part in this battle, trying to lift his soldier's spirits, to no avail. anti-Saqqawist forces continued to slowly push towards Kabul, seizing Charasya, Chihil Tan and Chihil Sutun on 5 October. By 7 October, Kalakani's forces had retreated from almost all territory outside Kabul, and prepared for their last stand. On 9 October, after dozens of hours of street fighting in Kabul, the Arg was put under siege. On 13 October, after several days of bombardment, Nadir's forces entered the Arg, and after a short but fierce battle, captured it, ending the civil war. Upon hearing this news, a small contingent of Kalakani's army which was besieged at Jabal al-Siraj resolved to surrender that same day.

On 15 October, Mohammed Nadir Shah arrived in Kabul after hearing word of Kalakani's defeat. He considered pardoning Kalakani, but pressure from loyal tribes led him to execute Kalakani on 1 November 1929. Kalakani, his brother, and 9 others were lined up against the west wall of the arg and shot. During the reign of Nadir, the Saqqawists attempted another uprising, the Kuhistan rebellion (July 1930), which was crushed within a week. Saqqawist activity continued until 1930 in Kuhdaman, and 1931 in Herat.

Upon winning the civil war, Nadir did not cede control of the Afghan throne to Amanullah, and this led to several rebellions. The first of these, the Shinwari rebellion and the Kuhistan rebellion (FebruaryâApril 1930), occurred in 1930. In 1938, there was also the Ghilzai rebellion. In the 1940s, Mohammed Zahir Shah faced several tribal revolts, and the leader of the Zadran revolt, Mazrak Zadran, sought to restore Amanullah. During World War II, western press reported that Amanullah Khan was working as an agent for Nazi Germany in Berlin. It is believed he was involved in plans to regain his throne with Axis help.

According to "Resort to war: a data guide to inter-state, extra-state, intra-state, and non-state wars, 1816-2007", both sides suffered 7500 combat deaths during the civil war.

During the Afghan Civil War, there were incidents of rape and looting among Saqqawist troops. One such incident took place on 28 June 1929, when Saqqawists attacked the Hazara settlement of Qalah-i Karim, looting anything movable and driving off livestock. Another incident, which took place on 23 July 1929, was described by contemporary Afghan historian Fayz Muhammad as follows:

Following the anti-Saqqawist capture of Kabul in October 1929, Kabul was sacked by Nadir's forces. Some sources state that this sacking had been authorized by Nadir, but this contested by historian Vartan Gregorian.




</doc>
<doc id="26469" url="https://en.wikipedia.org/wiki?curid=26469" title="Î-recursive function">
Î-recursive function

In mathematical logic and computer science, the general recursive functions (often shortened to recursive functions) or Î¼-recursive functions are a class of partial functions from natural numbers to natural numbers that are "computable" in an intuitive sense. In computability theory, it is shown that the Î¼-recursive functions are precisely the functions that can be computed by Turing machines(this is one of the theorems that supports the ChurchâTuring thesis). The Î¼-recursive functions are closely related to primitive recursive functions, and their inductive definition (below) builds upon that of the primitive recursive functions. However, not every Î¼-recursive function is a primitive recursive functionâthe most famous example is the Ackermann function.

Other equivalent classes of functions are the Î»-recursive functions and the functions that can be computed by Markov algorithms.

The subset of all "total" recursive functions with values in is known in computational complexity theory as the complexity class R.

The Î¼-recursive functions (or general recursive functions) are partial functions that take finite tuples of natural numbers and return a single natural number. They are the smallest class of partial functions that includes the initial functions and is closed under composition, primitive recursion, and the Î¼ operator.

The smallest class of functions including the initial functions and closed under composition and primitive recursion (i.e. without minimisation) is the class of primitive recursive functions. While all primitive recursive functions are total, this is not true of partial recursive functions; for example, the minimisation of the successor function is undefined. The primitive recursive functions are a subset of the total recursive functions, which are a subset of the partial recursive functions. For example, the Ackermann function can be proven to be total recursive, and to be non-primitive.

Primitive or "basic" functions:

Operators (the domain of a function defined by an operator is the set of the values of the arguments such that every function application that must be done during the computation provides a well-defined result):

The strong equality operator formula_31 can be used to compare partial Î¼-recursive functions. This is defined for all partial functions "f" and "g" so that
holds if and only if for any choice of arguments either both functions are defined and their values are equal or both functions are undefined.

In the equivalence of models of computability, a parallel is drawn between Turing machines that do not terminate for certain inputs and an undefined result for that input in the corresponding partial recursive function.
The unbounded search operator is not definable by the rules of primitive recursion as those do not provide a mechanism for "infinite loops" (undefined values).

A normal form theorem due to Kleene says that for each "k" there are primitive recursive functions formula_33 and formula_34 such that for any Î¼-recursive function formula_35 with "k" free variables there is an "e" such that 
The number "e" is called an index or GÃ¶del number for the function "f". A consequence of this result is that any Î¼-recursive function can be defined using a single instance of the Î¼ operator applied to a (total) primitive recursive function.

Minsky (1967) observes (as does Boolos-Burgess-Jeffrey (2002) pp.Â 94â95) that the U defined above is in essence the Î¼-recursive equivalent of the universal Turing machine:

A number of different symbolisms are used in the literature. An advantage to using the symbolism is a derivation of a function by "nesting" of the operators one inside the other is easier to write in a compact form. In the following we will abbreviate the string of parameters x, ..., x as x: 





Example: Kleene gives an example of how to perform the recursive derivation of f(b, a) = b + a (notice reversal of variables a and b). He starts with 3 initial functions 

He arrives at:






</doc>
<doc id="26470" url="https://en.wikipedia.org/wiki?curid=26470" title="Rex Ingram (director)">
Rex Ingram (director)

Rex Ingram (born Reginald Ingram Montgomery Hitchcock, 15 January 1892 â 21 July 1950) was an Irish film director, producer, writer and actor. Director Erich von Stroheim once called him "the world's greatest director".

Born in Dublin, Ireland, Ingram was educated at Saint Columba's College, near Rathfarnham, County Dublin. He spent much of his adolescence living in the Old Rectory, Kinnitty, Birr, County Offaly where his father was the Church of Ireland rector. He emigrated to the United States in 1911.

His brother Francis joined the British Army and fought during World War I where he was awarded the Military Cross and rose to the rank of Colonel. 

Ingram studied sculpture at the Yale University School of Art, where he contributed to campus humour magazine "The Yale Record". He soon moved into film, first taking acting work from 1913 and then writing, producing and directing. His first work as producer-director was in 1916 on the romantic drama "The Great Problem". He worked for Edison Studios, Fox Film Corporation, Vitagraph Studios, and then MGM, directing mainly action or supernatural films.

In 1920, he moved to Metro, where he was under supervision of executive June Mathis. Mathis and Ingram would go on to make four films together: "Hearts are Trump", "The Four Horsemen of the Apocalypse", "The Conquering Power", and "Turn to the Right". It is believed the two were romantically involved. Ingram and Mathis had begun to grow distant when her new find, Rudolph Valentino, began to overshadow his own fame. Their relationship ended when Ingram eloped with Alice Terry in 1921.

Ingram married twice, first to actress Doris Pawn in 1917; this ended in divorce in 1920. He then married Alice Terry in 1921, with whom he remained for the rest of his life. Both marriages were childless. He and Terry relocated to the French Riviera in 1923. They formed a small studio in Nice and made several films on location in North Africa, Spain, and Italy for MGM and others.

Amongst those who worked for Ingram at MGM on the Riviera during this period was the young Michael Powell, who later went on to direct (with Emeric Pressburger) "The Red Shoes" and other classics, and technician Leonti Planskoy. By Powell's own account, Ingram was a major influence on him, especially in its themes in illusion, dreaming, magic and the surreal. David Lean said he was indebted to Ingram. MGM studio chief Dore Schary listed the top creative people in Hollywood as D. W. Griffith, Ingram, Cecil B. DeMille and Erich von Stroheim (in declining order of importance).

Carlos Clarens writes: "As Rex Ingram's films became more esoteric, his career declined. The coming of sound forced him to relinquish his studios in Nice. Rather than equip them for talking pictures, he chose instead to travel and pursue a writing career." 
Rex Ingram made only one talkie, "Baroud", filmed for Gaumont British Pictures in Morocco. The film was a not a commercial success and Ingram left the film business, returning to Los Angeles to work as a sculptor and writer.

Interested in Islam as early as 1927, he converted to the faith in 1933.

For his contribution to the motion picture industry he has a star on the Hollywood Walk of Fame at 1651 Vine Street.

Ingram died of a cerebral hemorrhage in North Hollywood on 21 July 1950, aged 58. He was interred in the Forest Lawn Memorial Park Cemetery in Glendale, California.

Critic Carlos Clarens wrote of Ingram: "A full-blown Irishman fascinated by the bizarre and the grotesque (he once employed a dwarf as a valet), Ingram was also a writer of some talent. Frequently pedestrian and pretentious, Ingram's films nevertheless contain splendid flashes of macabre fantasy, such as the ride of the Four Horsemen in the Valentino epic, or the 'ghoul visions' that bring about the death of the miser in "The Conquering Power". His more or less mystical bent was apparent in "Mare Nostrum" and "The Garden of Allah", which he filmed in the Mediterranean and North Africa, respectively."

Ingram's complete filmography as a director:



</doc>
<doc id="26471" url="https://en.wikipedia.org/wiki?curid=26471" title="Rat">
Rat

Rats are various medium-sized, long-tailed rodents. Species of rats are found throughout the order Rodentia, but stereotypical rats are found in the genus "Rattus". Other rat genera include "Neotoma" (pack rats), "Bandicota" (bandicoot rats) and "Dipodomys" (kangaroo rats).

Rats are typically distinguished from mice by their size. Generally, when someone discovers a large muroid rodent, its common name includes the term "rat", while if it is smaller, its name includes the term "mouse". The common terms "rat" and "mouse" are not taxonomically specific.

The best-known rat species are the black rat ("Rattus rattus") and the brown rat ("Rattus norvegicus"). This group, generally known as the Old World rats or true rats, originated in Asia. Rats are bigger than most Old World mice, which are their relatives, but seldom weigh over in the wild.

The term "rat" is also used in the names of other small mammals that are not true rats. Examples include the North American pack rats (aka wood rats) and a number of species loosely called kangaroo rats. Rats such as the bandicoot rat ("Bandicota bengalensis") are murine rodents related to true rats but are not members of the genus "Rattus".

Male rats are called "bucks"; unmated females, "does", pregnant or parent females, "dams"; and infants, "kittens" or "pups". A group of rats is referred to as a "mischief".

The common species are opportunistic survivors and often live with and near humans; therefore, they are known as commensals. They may cause substantial food losses, especially in developing countries. However, the widely distributed and problematic commensal species of rats are a minority in this diverse genus. Many species of rats are island endemics, some of which have become endangered due to habitat loss or competition with the brown, black, or Polynesian rat.

Wild rodents, including rats, can carry many different zoonotic pathogens, such as "Leptospira", "Toxoplasma gondii", and "Campylobacter". The Black Death is traditionally believed to have been caused by the microorganism "Yersinia pestis", carried by the tropical rat flea ("Xenopsylla cheopis"), which preyed on black rats living in European cities during the epidemic outbreaks of the Middle Ages; these rats were used as transport hosts. Another zoonotic disease linked to the rat is foot-and-mouth disease.
Rats become sexually mature at age 6 weeks, but reach social maturity at about 5 to 6 months of age. The average lifespan of rats varies by species, but many only live about a year due to predation.

The rat's larynx has been used in experimentations that involve inhalation toxicity, allograft rejection, and irradiation responses. One experiment described four features of the rat's larynx. The first being the location and attachments of the thyroarytenoid muscle, the alar cricoarytenoid muscle, and the superior cricoarytenoid muscle, the other of the newly named muscle that ran from the arytenoid to a midline tubercle on the cricoid. The newly named muscles were not seen in the human larynx. In addition, the location and configuration of the laryngeal alar cartilage was described. The second feature was that the way the newly named muscles appear to be familiar to those in the human larynx. The third feature was that a clear understanding of how MEPs are distributed in each of the laryngeal muscles was helpful in understanding the effects of botulinum toxin injection. The MEPs in the posterior
cricoarytenoid muscle, lateral cricoarytenoid muscle, cricothyroid muscle, and superior
cricoarytenoid muscle were focused mostly at the midbelly. In addition, the medial thyroarytenoid muscle were focused at the midbelly while the lateral thyroarytenoid muscle MEPs were focused at the anterior third of the belly. The fourth and final feature that was cleared up was how the MEPs were distributed in the thyroarytenoid muscle. 

The black and brown rats diverged from other Old World rats in the forests of Asia during the beginning of the Pleistocene.

The characteristic long tail of most rodents is a feature that has been extensively studied in various rat species models, which suggest three primary functions of this structure: thermoregulation, minor proprioception, and a nocifensive-mediated degloving response. Rodent tailsâparticularly in rat modelsâhave been implicated with a thermoregulation function that follows from its anatomical construction. This particular tail morphology is evident across the family Muridae, in contrast to the bushier tails of Sciuridae, the squirrel family. The tail is hairless and thin skinned but highly vascularized, thus allowing for efficient countercurrent heat exchange with the environment. The high muscular and connective tissue densities of the tail, along with ample muscle attachment sites along its plentiful caudal vertebrae, facilitate specific proprioceptive senses to help orient the rodent in a three-dimensional environment. Lastly, murids have evolved a unique defense mechanism termed "degloving" that allows for escape from predation through the loss of the outermost integumentary layer on the tail. However, this mechanism is associated with multiple pathologies that have been the subject of investigation.

Multiple studies have explored the thermoregulatory capacity of rodent tails by subjecting test organisms to varying levels of physical activity and quantifying heat conduction via the animals' tails. One study demonstrated a significant disparity in heat dissipation from a rat's tail relative to its abdomen. This observation was attributed to the higher proportion of vascularity in the tail, as well as its higher surface-area-to-volume ratio, which directly relates to heat's ability to dissipate via the skin. These findings were confirmed in a separate study analyzing the relationships of heat storage and mechanical efficiency in rodents that exercise in warm environments. In this study, the tail was a focal point in measuring heat accumulation and modulation.

On the other hand, the tail's ability to function as a proprioceptive sensor and modulator has also been investigated. As aforementioned, the tail demonstrates a high degree of muscularization and subsequent innervation that ostensibly collaborate in orienting the organism. Specifically, this is accomplished by coordinated flexion and extension of tail muscles to produce slight shifts in the organism's center of mass, orientation, etc., which ultimately assists it with achieving a state of proprioceptive balance in its environment. Further mechanobiological investigations of the constituent tendons in the tail of the rat have identified multiple factors that influence how the organism navigates its environment with this structure. A particular example is that of a study in which the morphology of these tendons is explicated in detail. Namely, cell viability tests of tendons of the rat's tail demonstrate a higher proportion of living fibroblasts that produce the collagen for these fibers. As in humans, these tendons contain a high density of golgi tendon organs that help the animal assess stretching of muscle in situ and adjust accordingly by relaying the information to higher cortical areas associated with balance, proprioception, and movement.

The characteristic tail of murids also displays a unique defense mechanism known as "degloving" in which the outer layer of the integument can be detached in order to facilitate the animal's escape from a predator. This evolutionary selective pressure has persisted despite a multitude of pathologies that can manifest upon shedding part of the tail and exposing more interior elements to the environment. Paramount among these are bacterial and viral infection, as the high density of vascular tissue within the tail becomes exposed upon avulsion or similar injury to the structure. The degloving response is a nocifensive response, meaning that it occurs when the animal is subjected to acute pain, such as when a predator snatches the organism by the tail.

Specially bred rats have been kept as pets at least since the late 19th century. Pet rats are typically variants of the species brown rat - but black rats and giant pouched rats are also sometimes kept. Pet rats behave differently from their wild counterparts depending on how many generations they have been kept as pets. Pet rats do not pose any more of a health risk than pets such as cats or dogs. Tamed rats are generally friendly and can be taught to perform selected behaviors.

In 1895, Clark University in Worcester, Massachusetts, established a population of domestic albino brown rats to study the effects of diet and for other physiological studies. Over the years, rats have been used in many experimental studies, adding to our understanding of genetics, diseases, the effects of drugs, and other topics that have provided a great benefit for the health and wellbeing of humankind.

The aortic arches of the rat are among the most commonly studied in murine models due to marked anatomical homology to the human cardiovascular system. Both rat and human aortic arches exhibit subsequent branching of the brachiocephalic trunk, left common carotid artery, and left subclavian artery, as well as geometrically similar, nonplanar curvature in the aortic branches. Aortic arches studied in rats exhibit abnormalities similar to those of humans, including altered pulmonary arteries and double or absent aortic arches. Despite existing anatomical analogy in the inthrathoracic position of the heart itself, the murine model of the heart and its structures remains a valuable tool for studies of human cardiovascular conditions.

Laboratory rats have also proved valuable in psychological studies of learning and other mental processes (Barnett 2002), as well as to understand group behavior and overcrowding (with the work of John B. Calhoun on behavioral sink). A 2007 study found rats to possess metacognition, a mental ability previously only documented in humans and some primates.

Domestic rats differ from wild rats in many ways. They are calmer and less likely to bite; they can tolerate greater crowding; they breed earlier and produce more offspring; and their brains, livers, kidneys, adrenal glands, and hearts are smaller (Barnett 2002).

Brown rats are often used as model organisms for scientific research. Since the publication of the rat genome sequence, and other advances, such as the creation of a rat SNP chip, and the production of knockout rats, the laboratory rat has become a useful genetic tool, although not as popular as mice. When it comes to conducting tests related to intelligence, learning, and drug abuse, rats are a popular choice due to their high intelligence, ingenuity, aggressiveness, and adaptability. Their psychology seems in many ways similar to that of humans.

Entirely new breeds or "lines" of brown rats, such as the Wistar rat, have been bred for use in laboratories. Much of the genome of "Rattus norvegicus" has been sequenced.

Early studies found evidence both for and against measurable intelligence using the "g factor" in rats. Part of the difficulty of understanding animal cognition generally, is determining what to measure. One aspect of intelligence is the ability to learn, which can be measured using a maze like the T-maze. Experiments done in the 1920s showed that some rats performed better than others in maze tests, and if these rats were selectively bred, their offspring also performed better, suggesting that in rats an ability to learn was heritable in some way.

Rat meat is a food that, while taboo in some cultures, is a dietary staple in others.

Rats have been used as working animals. Tasks for working rats include the sniffing of gunpowder residue, demining, acting and animal-assisted therapy.

Rats have a keen sense of smell and are easy to train. These characteristics have been employed, for example, by the Belgian non-governmental organization APOPO, which trains rats (specifically African giant pouched rats) to detect landmines and diagnose tuberculosis through smell.

Rats can serve as zoonotic vectors for certain pathogens and thus spread disease, such as bubonic plague, Lassa fever, leptospirosis, and Hantavirus infection.

They are also associated with human dermatitis because they are frequently infested with blood feeding rodent mites such as the tropical rat mite ("Ornithonyssus bacoti") and spiny rat mite ("Laelaps echidnina"), which will opportunistically bite and feed on humans, where the condition is known as rat mite dermatitis"."

Rats have long been considered deadly pests. Once considered a modern myth, the rat flood in India occurs every fifty years, as armies of bamboo rats descend upon rural areas and devour everything in their path. Rats have long been held up as the chief villain in the spread of the Bubonic Plague; however, recent studies show that rats alone could not account for the rapid spread of the disease through Europe in the Middle Ages. Still, the Center for Disease Control does list nearly a dozen diseases directly linked to rats.

Most urban areas battle rat infestations. A 2015 study by the American Housing Survey (AHS) found that eighteen percent of homes in Philadelphia showed evidence of rodents. Boston, New York City, and Washington, D.C., also demonstrated significant rodent infestations. Indeed, rats in New York City are famous for their size and prevalence. The urban legend that the rat population in Manhattan equals that of its human population was definitively refuted by Robert Sullivan in his book "Rats" but illustrates New Yorkers' awareness of the presence, and on occasion boldness and cleverness, of the rodents. New York has specific regulations for eradicating rats; multifamily residences and commercial businesses must use a specially trained and licensed rat catcher.

Rats have the ability to swim up sewer pipes into toilets. Rats will infest any area that provides shelter and easy access to sources of food and water, including under sinks, near garbage, and inside walls or cabinets. 

When introduced into locations where rats previously did not exist, they can wreak an enormous degree of environmental degradation. "Rattus rattus", the black rat, is considered to be one of the world's worst invasive species. Also known as the ship rat, it has been carried worldwide as a stowaway on seagoing vessels for millennia and has usually accompanied men to any new area visited or settled by human beings by sea. The similar species "Rattus norvegicus", the brown rat or wharf rat, has also been carried worldwide by ships in recent centuries.

The ship or wharf rat has contributed to the extinction of many species of wildlife, including birds, small mammals, reptiles, invertebrates, and plants, especially on islands. True rats are omnivorous, capable of eating a wide range of plant and animal foods, and have a very high birth rate. When introduced to a new area, they quickly reproduce to take advantage of the new food supply. In particular, they prey on the eggs and young of forest birds, which on isolated islands often have no other predators and thus have no fear of predators. Some experts believe that rats are to blame for between forty percent and sixty percent of all seabird and reptile extinctions, with ninety percent of those occurring on islands. Thus man has indirectly caused the extinction of many species by accidentally introducing rats to new areas.

Rats are found in nearly all areas of Earth which are inhabited by human beings. The only rat-free continent is Antarctica, which is too cold for rat survival outdoors, and its lack of human habitation does not provide buildings to shelter them from the weather. However, rats have been introduced to many of the islands near Antarctica, and because of their destructive effect on native flora and fauna, efforts to eradicate them are ongoing. In particular, Bird Island (just off rat-infested South Georgia Island), where breeding seabirds could be badly affected if rats were introduced, is subject to special measures and regularly monitored for rat invasions.

As part of island restoration, some islands' rat populations have been eradicated to protect or restore the ecology. Hawadax Island, Alaska was declared rat free after 229 years and Campbell Island, New Zealand after almost 200 years. Breaksea Island in New Zealand was declared rat free in 1988 after an eradication campaign based on a successful trial on the smaller Hawea Island nearby.

In January 2015, an international "Rat Team" set sail from the Falkland Islands for the British Overseas Territory of South Georgia and the South Sandwich Islands on board a ship carrying three helicopters and 100 tons of rat poison with the objective of "reclaiming the island for its seabirds". Rats have wiped out more than 90% of the seabirds on South Georgia, and the sponsors hope that once the rats are gone, it will regain its former status as home to the greatest concentration of seabirds in the world. The South Georgia Heritage Trust, which organized the mission describes it as "five times larger than any other rodent eradication attempted worldwide". That would be true if it were not for the rat control program in Alberta (see below).

The Canadian province of Alberta is notable for being the largest inhabited area on Earth which is free of true rats due to very aggressive government rat control policies. It has large numbers of native pack rats, also called bushy-tailed wood rats, but they are forest-dwelling vegetarians which are much less destructive than true rats.

Alberta was settled relatively late in North American history and only became a province in 1905. Black rats cannot survive in its climate at all, and brown rats must live near people and in their structures to survive the winters. There are numerous predators in Canada's vast natural areas which will eat non-native rats, so it took until 1950 for invading rats to make their way over land from Eastern Canada. Immediately upon their arrival at the eastern border with Saskatchewan, the Alberta government implemented an extremely aggressive rat control program to stop them from advancing further. A systematic detection and eradication system was used throughout a control zone about long and wide along the eastern border to eliminate rat infestations before the rats could spread further into the province. Shotguns, bulldozers, high explosives, poison gas, and incendiaries were used to destroy rats. Numerous farm buildings were destroyed in the process. Initially, tons of arsenic trioxide were spread around thousands of farm yards to poison rats, but soon after the program commenced the rodenticide and medical drug warfarin was introduced, which is much safer for people and more effective at killing rats than arsenic.

Forceful government control measures, strong public support and enthusiastic citizen participation continue to keep rat infestations to a minimum. The effectiveness has been aided by a similar but newer program in Saskatchewan which prevents rats from even reaching the Alberta border. Alberta still employs an armed rat patrol to control rats along Alberta's borders. About ten single rats are found and killed per year, and occasionally a large localized infestation has to be dug out with heavy machinery, but the number of permanent rat infestations is zero.

Ancient Romans did not generally differentiate between rats and mice, instead referring to the former as "mus maximus" (big mouse) and the latter as "mus minimus" (little mouse).

On the Isle of Man, there is a taboo against the word "rat".

The rat (sometimes referred to as a mouse) is the first of the twelve animals of the Chinese zodiac. People born in this year are expected to possess qualities associated with rats, including creativity, intelligence, honesty, generosity, ambition, a quick temper and wastefulness. People born in a year of the rat are said to get along well with "monkeys" and "dragons", and to get along poorly with "horses".
In Indian tradition, rats are seen as the vehicle of Ganesha, and a rat's statue is always found in a temple of Ganesh. In the northwestern Indian city of Deshnoke, the rats at the Karni Mata Temple are held to be destined for reincarnation as Sadhus (Hindu holy men). The attending priests feed milk and grain to the rats, of which the pilgrims also partake.

European associations with the rat are generally negative. For instance, "Rats!" is used as a substitute for various vulgar interjections in the English language. These associations do not draw, "per se", from any biological or behavioral trait of the rat, but possibly from the association of rats (and fleas) with the 14th-century medieval plague called the Black Death. Rats are seen as vicious, unclean, parasitic animals that steal food and spread disease. However, some people in European cultures keep rats as pets and conversely find them to be tame, clean, intelligent, and playful.

Rats are often used in scientific experiments; animal rights activists allege the treatment of rats in this context is cruel. The term "lab rat" is used, typically in a self-effacing manner, to describe a person whose job function requires them to spend a majority of their work time engaged in bench-level research (such as postgraduate students in the sciences).

Rats are frequently blamed for damaging food supplies and other goods, or spreading disease. Their reputation has carried into common parlance: in the English language, "rat" is often an insult or is generally used to signify an unscrupulous character; it is also used, as a synonym for the term "nark", to mean an individual who works as a police informant or who has turned state's evidence. Writer/director Preston Sturges created the humorous alias "Ratskywatsky" for a soldier who seduced, impregnated, and abandoned the heroine of his 1944 film, "The Miracle of Morgan's Creek". It is a term (noun and verb) in criminal slang for an informant â "to rat on someone" is to betray them by informing the authorities of a crime or misdeed they committed. Describing a person as "rat-like" usually implies he or she is unattractive and suspicious.

Among trade unions, the word "rat" is also a term for nonunion employers or breakers of union contracts, and this is why unions use inflatable rats.

Depictions of rats in fiction are historically inaccurate and negative. The most common falsehood is the squeaking almost always heard in otherwise realistic portrayals (i.e. nonanthropomorphic). While the recordings may be of actual squeaking rats, the noise is uncommon â they may do so only if distressed, hurt, or annoyed. Normal vocalizations are very high-pitched, well outside the range of human hearing. Rats are also often cast in vicious and aggressive roles when in fact, their shyness helps keep them undiscovered for so long in an infested home.

The actual portrayals of rats vary from negative to positive with a majority in the negative and ambiguous. The rat plays a villain in several mouse societies; from Brian Jacques's "Redwall" and Robin Jarvis's "The Deptford Mice", to the roles of Disney's Professor Ratigan and Kate DiCamillo's Roscuro and Botticelli. They have often been used as a mechanism in horror; being the titular evil in stories like "The Rats" or H.P. Lovecraft's "The Rats in the Walls" and in films like "Willard" and "Ben". Another terrifying use of rats is as a method of torture, for instance in Room 101 in George Orwell's "Nineteen Eighty-Four" or "The Pit and the Pendulum" by Edgar Allan Poe.

Selfish helpfulness âthose willing to help for a priceâ has also been attributed to fictional rats. Templeton, from E. B. White's "Charlotte's Web", repeatedly reminds the other characters that he is only involved because it means more food for him, and the cellar-rat of John Masefield's "The Midnight Folk" requires bribery to be of any assistance.

By contrast, the rats appearing in the Doctor Dolittle books tend to be highly positive and likeable characters, many of whom tell their remarkable life stories in the Mouse and Rat Club established by the animal-loving doctor.

Some fictional works use rats as the main characters. Notable examples include the society created by O'Brien's "Mrs. Frisby and the Rats of NIMH", and others include "Doctor Rat", and Rizzo the Rat from The Muppets. Pixar's 2007 animated film "Ratatouille" is about a rat described by Roger Ebert as "earnest... lovable, determined, [and] gifted" who lives with a Parisian garbage-boy-turned-chef.

"Mon oncle d'AmÃ©rique" (""My American Uncle""), a 1980 French film, illustrates Henri Laborit's theories on evolutionary psychology and human behaviors by using short sequences in the storyline showing lab rat experiments.

In Harry Turtledove's science fiction novel "Homeward Bound", humans unintentionally introduce rats to the ecology at the home world of an alien race which previously invaded Earth and introduced some of its own fauna into its environment. A. Bertram Chandler pitted the space-bound protagonist of a long series of novels, Commodore Grimes, against giant, intelligent rats who took over several stellar systems and enslaved their human inhabitants. "The Stainless Steel Rat" is nickname of the (human) protagonist of a series of humorous science fiction novels written by Harry Harrison.

One of the oldest and most historic stories about rats is "The Pied Piper of Hamelin", in which a rat-catcher leads away an infestation with enchanted music. The piper is later refused payment, so he in turn leads away the town's children. This tale, traced to Germany around the late 13th century, has inspired adaptations in film, theatre, literature, and even opera. The subject of much research, some theories have intertwined the tale with events related to the Black Plague, in which black rats played an important role. Fictional works based on the tale that focus heavily on the rat aspect include Pratchett's "The Amazing Maurice and his Educated Rodents", and Belgian graphic novel "" ("The Ball of the Dead Rat").





</doc>
<doc id="26472" url="https://en.wikipedia.org/wiki?curid=26472" title="Adobe RoboHelp">
Adobe RoboHelp

Adobe RoboHelp is a help authoring tool (HAT) developed and published by Adobe Inc. for Windows. RoboHelp was created by Gen Kiyooka, and Blue Sky Software released version 1.0 in January 1992.

Blue Sky Software was founded in 1990 and changed its name to eHelp Corporation on 4 April 2000. Macromedia acquired eHelp Corporation on 24 October 2003. Macromedia was, in turn, acquired by Adobe Systems on 3 December 2005. Adobe Systems has developed and released nine successive versions of RoboHelp since 2007.

Adobe RoboHelp can generate help files in the following file formats:

The version numbering systems used by Blue Sky Software/eHelp Corporation, Macromedia, and Adobe Systems induced some head-scratching, especially among longtime RoboHelp users. For example, the first version of RoboHelp released by Adobe Systems in January 2007 was the 14th version of the software, but Adobe Systems decided to continue the numbering convention from Macromedia and thus gave this version the number 6...and dropped the X used in the previous version, RoboHelp X5. This decision caused confusion because Blue Sky Software released RoboHelp 6.0 in 1998. Adobe Systems continued with that numbering system and used versions 7 through 11 for successive versions of RoboHelp released from September 2007 to January 2014. With the introduction of Adobe RoboHelp 2015 in June 2015, Adobe Systems used a new numbering system with the release year instead of a version number and continues to use this convention with successive versions. This new version numbering system has removed any uncertainty about which version is the most recent. The current version, Adobe RoboHelp 2019, is the 22nd version of the software released in RoboHelp's 26-year history.
The network-specific version of Adobe RoboHelp, Adobe RoboHelp Server, is released on a separate schedule. Adobe RoboHelp Server, formerly RoboSource Control, provides version control for and deployment of online help systems on a network. The current version of Adobe RoboHelp Server, version 10, was released on 12 April 2016.


</doc>
<doc id="26473" url="https://en.wikipedia.org/wiki?curid=26473" title="Replicant">
Replicant

A replicant is a fictional bioengineered being in the 1982 film "Blade Runner", and in its 2017 sequel "Blade Runner 2049". The Nexus-series of replicants are virtually identical to adult humans but have superior strength, speed, agility, resilience, and intelligence, to varying degrees depending on the model. A replicant can only be detected by means of the fictional Voight-Kampff test, in which emotional responses are provoked and a replicant's nonverbal responses differ from those of a human. A version of the test, referred to as a Baseline, is taken by K in "Blade Runner 2049" to detect any mental or empathic damage, for which failure means retirement. Throughout the franchise the euphemism "retire" is used when referring to killing Replicants.

Nexus-6 replicants (e.g. Roy Batty) have a safety mechanism, namely a four-year lifespan, to prevent them from developing empathic abilities (and, therefore, immunity to the test). Nexus-7 replicants (e.g. Rachael) were limited experimental models by Tyrell Corporation with a capability to reproduce. Nexus-8 replicants (e.g. Sapper Morton, Freysa), also by Tyrell Corporation, have an open-ended lifespan; however, a rebellion resulting in the "Blackout of 2022" led them to be discontinued and hunted down for retirement. Nexus-9 replicants (e.g. K), by Wallace Corporation, are also open-ended but have increased compliance which makes them incapable of not following human orders, and are thus full slaves. Replicants are sometimes referred to by the slur "skin-job".

In his novel "Do Androids Dream of Electric Sheep?" (the inspiration for "Blade Runner"), Philip K. Dick used the term android (or "andy"), but director Ridley Scott wanted a new term that the audience would not have any preconceptions about. As David Peoples was re-writing the screenplay, he consulted his daughter, who was involved in microbiology and biochemistry. She suggested the term "replicating", the biological process of a cell making a copy of itself. From that, either Peoples or Scottâeach would later recall it was the otherâcame up with "replicant", and it was inserted into Hampton Fancher's screenplay.

Prior to the events of the film, replicants became illegal on Earth after a bloody off-world mutiny by a band of Nexus-6 models. Two weeks before the starting point of the film, six Nexus-6 replicants escaped the off-world colonies, killing 23 people and taking a shuttle to Earth; the film focuses on the pursuit of the replicants by Rick Deckard, a category of police-officer bounty-hunter called a "Blade Runner", who investigates, tests, and retires replicants found on Earth.

Nexus-6 replicants had been designed to copy humans in every way except for their emotions. The Tyrell Corporation "began to recognize in them a strange obsession", and in order to be able to control them better, started to implant false memories into the replicants in order to give them the years of experiences that humans take for granted; these memories created "a cushion or pillow for their emotions".

Early in the film, Captain Bryant tells Deckard that the Nexus-6 units' possible eventual development of emotional responses was the reason the Tyrell Corporation designed them with a four-year lifespan. Dr Eldon Tyrell states in the film they were made as well as they could be with a limited lifespan. All attempts to increase a replicant's lifespan has resulted in death. The unsuccessful procedures they tried were:

Deckard had no experience with Nexus-6 replicants at the beginning of the film; he and Captain Bryant are puzzled as to why they have risked coming back to Earth and Deckard is unsure how effective the Voight-Kampff test would be on them, as they appeared to have developed human emotion.

Escaped replicants (all Nexus-6 models):

Other replicants (possible Nexus-7 models):

According to Deckard, a normal replicant can usually be discovered using the Voight-Kampff test, after being given 20â30 questions. Rachael answers over 100 questions before Deckard determines she is a replicant. The theatrical cut's voice-over ending said that, as an experimental replicant, Rachael did not have the four-year life but the Director's Cut did not address this. Scott said that he had wanted to cast a young actress in the role to emphasise Rachael's naivety and unworldliness.

The second film further developed Rachael's origin, and gave significantly more details about its radical design. It revealed most significantly that it was an experimental reproductive model of replicant (which ultimately produced a daughter with Deckard) with a high percentage of human organs in comparison to replicant parts. It has an internal human bone structure, natural eyes, hair, skin and reproductive organs, which explains its ability to pass as human. Thus, the film suggests it was only its brain and possibly other vital organs which were the replicant parts. As Rachael died during childbirth, its possible survival beyond the four years was undetermined. 

In "Do Androids Dream of Electric Sheep?", the android manufacturer, known as the Rosen Corporation, did not know how to manufacture an android capable of living beyond four years. The super-soldiers in "Soldier"âthe spiritual successor to "Blade Runner"âare intended to be replicants in the film.

The dark, paranoid atmosphere of "Blade Runner", and its multiple versions, add fuel to the speculation and debate over this issue. In "Do Androids Dream of Electric Sheep?", Rick Deckard (the protagonist) is at one point tricked into following an android, whom he believes to be a police officer, to a fake police station. Deckard then escapes and retires some androids there before returning to his own police station. Deckard takes the Voight-Kampff test and passes, confirming that he is a human.

Harrison Ford, who played Deckard in the film, has said that he did not think Deckard is a replicant, and has said that he and director Ridley Scott had discussions that ended in the agreement that the character was human. According to several interviews with Scott, Deckard "is" a replicant. Deckard collects photographs which are seen on his piano, yet has no obvious family beyond a reference to his ex-wife (who called him a "cold fish"). The film's Supervising Editor Terry Rawlings remembers that Scott "purposefully put Harrison in the background of the shot, and slightly out of focus, so that you'd only notice his eyes were glowing if you were paying attention... Ridley himself may have definitely felt that Deckard is a replicant, but still, by the end of the picture, he intended to leave it up to the viewer."

Author Will Brooker has written that the dream may not be unique to Deckard and that unicorn dreams may be a personal touch added to some or all of the Nexus-6 replicants' brains. Since we are not privy to the dreams of the other replicants, this is unknown. From this, one could also derive that Gaff is a replicant and may share the same embedded memory.

Paul Sammon, author of "Future Noir: The Making of Blade Runner", has suggested in interviews that Deckard may be a Nexus-7, a new generation of replicant who possesses no superhuman strength or intelligence but does have neurological features that complete the illusion of humanity. Scott has mentioned Nexus-7 and Nexus-8 replicants as possibilities in a sequel to the film. Sammon also suggests that Nexus-7 replicants may not have a set lifespan (i.e., they could be immortal).

Sammon wrote that Scott thought it would be more provocative to imply that Deckard is a replicant. This ties back into the theme of "what is it to be human?" What is important is not whether Deckard is a replicant but that the ambiguity blurs the line between humans and replicants.

When Scott was asked about the possibility of a "Blade Runner" sequel in October 2012, he said, "It's not a rumorâit's happening. With Harrison Ford? I don't know yet. Is he too old? Well, he was a Nexus-6, so we don't know how long he can live. And that's all I'm going to say at this stage". 

The sequel "Blade Runner 2049" was released in October 2017, which revisited the question while leaving the answer deliberately ambiguous. The film reveals that Deckard was able to naturally conceive a child with Rachael, and this was possible because she was an experimental prototype (designated Nexus-7), the first and only attempt to design a replicant model capable of procreating on its own. The Tyrell Corporation eventually went bankrupt after several replicant rebellions and was bought out by Wallace Corporation, which took over replicant production, but it could not duplicate Tyrell's success with Rachael. Niander Wallace, the sinister CEO of the company, captures Deckard and muses to him about how he met her and fell in love: Wallace thinks it sounds too perfect, and ponders if Deckard himself was designed to fall in love with Rachael, as part of Tyrell's experiment to develop replicants that can procreate (in which case Deckard is a replicant) - but Wallace also admits that with Tyrell dead and the records destroyed, he'll never know, and it is equally possible that Tyrell never planned for Rachael and Deckard to fall in love (in which case, Deckard is probably human).

Although the press kit for the film explicitly defines a replicant as "A genetically engineered creature composed entirely of organic substance", the physical make-up of the replicants themselves is not clear. In the films's preamble, it is noted that replicants are said to be the result of "advanced robot evolution." The preamble also states that replicants were created by genetic engineers. Characters mention that replicants have eyes and brains like humans, and they are seen to bleed when injured. The only way of telling a replicant from a human is to ask a series of questions and analyze emotional responses, suggesting they might be entirely, or almost entirely, organic. The film also shows that at least certain body parts of a replicant are separately engineered and assembled, as shown with Hannibal Chew, a genetic engineer who specifically made replicant eyes. In a deleted scene, J.F. Sebastian was stated to have made replicant hands along with his own personal robotic toys.

During the creation process of a replicant, their physical and mental capacities are separately ranked on a A to C system and designated for each replicant with the C level representing below normal human ability, B level being equal to a normal human and A being above normal human ability, the latter of which leads to superhuman physicality or genius level intelligence. 

"Do Androids Dream of Electric Sheep?" makes mention of the biological components of the androids, but also alludes to mechanical aspects commonly found in other material relating to robots. It states that the bone marrow can be tested to prove whether it is from a human or replicant.

In May 2012, Scott confirmed that the replicants were biological in nature, and contrasted them to the androids in the "Alien" series: Roy Batty was an evolved... He wasn't an engine. If I cut him open, there wasn't metal, he was grown... and then within twenty years you get the first bill not passed in the Senate where they applied for replication of animals, sheep and goats and cattle and animals and they turned it down, but if you can do that, then you can do human beings. If you go deeper into it and say 'Yeah, but if you are going to grow a human being, does he start that big and I've got to see him through everything?'

I don't want to answer the question, because of course he does... Ash in "Alien" had nothing to do with Roy Batty, because Roy Batty is more humanoid, whereas Ash was more metal.

The sequel "Blade Runner 2049" was released in October 2017. In the intervening 30 years, several major events occurred and new replicant lines were introduced.

The sequel retroactively establishes that Rachael was part of a short-lived prototype line of replicants designated Nexus-7, which was not only intended as a test to make replicants more mentally stable with implanted memories, but to develop replicants capable of naturally conceiving children on their own (all other models before or since are sterile). Rachael died in childbirth in 2021, and the child was hidden by the replicant underground.

In 2020, Tyrell Corporation introduced the Nexus-8 replicant, built with open lifespans not limited to only four years. Tyrell himself had been killed during the events of the first movie in November 2019, and the secret of producing replicants that can procreate died with him. The Nexus-8 went into mass production, but a new wave of replicant rebellions occurred, culminating in rogue Nexus-8's detonating a nuclear weapon in orbit over the western United States, to create an electromagnetic pulse that wiped out all of the electronic records. The Blackout destroyed most records about replicants, making it difficult for humans to track them down on Earth, but the terrorist attack led to mass purges and complete shutdown of Nexus-8 production (though many existing units were able to go into hiding in the chaos). 

In 2036, however, genetic engineer Niander Wallace designed a new line of Nexus-9 replicants. They also have an open lifespan, but were designed to be unable to resist orders given by a human, even if that order is to commit suicide. Wallace Corporation had solved a global food crisis with genetically modified crops, which combined with the demonstrated effectiveness of Nexus-9 programming, allowed him to successfully push for the ban on replicant production to be lifted. 

By 2049, Nexus-9 replicants are extensively used across Earth and the off-world colonies, but they also necessitate special police units tasked with tracking down any that might go rogue, and any remaining Nexus-8's still in hiding (Nexus-7 was never mass-produced, and all the older models like Nexus-6 simply died of old age decades before). These police units are once again called Blade Runners, but are now openly composed of self-aware replicants (such as officer KD6-3.7), who are fully aware that they are replicants themselves. Like Nexus-7, Nexus-9 models also have implanted memories to aid their mental stability, though they are aware that these memories are fabrications.



</doc>
<doc id="26474" url="https://en.wikipedia.org/wiki?curid=26474" title="Roman Jakobson">
Roman Jakobson

Roman Osipovich Jakobson (; October 11, 1896 â July 18, 1982) was a Russian-American linguist and literary theorist.

A pioneer of structural linguistics, Jakobson was one of the most celebrated and influential linguists of the twentieth century. With Nikolai Trubetzkoy, he developed revolutionary new techniques for the analysis of linguistic sound systems, in effect founding the modern discipline of phonology. Jakobson went on to extend similar principles and techniques to the study of other aspects of language such as syntax, morphology and semantics. He made numerous contributions to Slavic linguistics, most notably two studies of Russian case and an analysis of the categories of the Russian verb. Drawing on insights from C. S. Peirce's semiotics, as well as from communication theory and cybernetics, he proposed methods for the investigation of poetry, music, the visual arts, and cinema.

Through his decisive influence on Claude LÃ©vi-Strauss and Roland Barthes, among others, Jakobson became a pivotal figure in the adaptation of structural analysis to disciplines beyond linguistics, including philosophy, anthropology, and literary theory; his development of the approach pioneered by Ferdinand de Saussure, known as "structuralism", became a major post-war intellectual movement in Europe and the United States. Meanwhile, though the influence of structuralism declined during the 1970s, Jakobson's work has continued to receive attention in linguistic anthropology, especially through the ethnography of communication developed by Dell Hymes and the semiotics of culture developed by Jakobson's former student Michael Silverstein. Jakobson's concept of underlying linguistic universals, particularly his celebrated theory of distinctive features, decisively influenced the early thinking of Noam Chomsky, who became the dominant figure in theoretical linguistics during the second half of the twentieth century.

Jakobson was born in Russia on 11 October 1896 to a well-to-do family of Jewish descent, the industrialist Osip Jakobson and chemist Anna Volpert Jakobson, and he developed a fascination with language at a very young age. He studied at the Lazarev Institute of Oriental Languages and then at the Historical-Philological Faculty of Moscow University. As a student he was a leading figure of the Moscow Linguistic Circle and took part in Moscow's active world of avant-garde art and poetry. The linguistics of the time was overwhelmingly neogrammarian and insisted that the only scientific study of language was to study the history and development of words across time (the diachronic approach, in Saussure's terms). Jakobson, on the other hand, had come into contact with the work of Ferdinand de Saussure, and developed an approach focused on the way in which language's structure served its basic function (synchronic approach) â to communicate information between speakers. Jakobson was also well known for his critique of the emergence of sound in film. Jakobson received a master's degree from Moscow University in 1918.

Although he was initially an enthusiastic supporter of the Bolshevik revolution, Jakobson soon became disillusioned as his early hopes for an explosion of creativity in the arts fell victim to increasing state conservatism and hostility. He left Moscow for Prague in 1920, where he worked as a member of the Soviet diplomatic mission while continuing with his doctoral studies. Then, in 1933, he took up a chair at Brno. Living in Czechoslovakia meant that Jakobson was physically close to the linguist who would be his most important collaborator during the 1920s and 1930s, Prince Nikolai Trubetzkoy, who fled Russia at the time of the Revolution and took up a chair at Vienna in 1922. In 1926 the Prague school of linguistic theory was established by the professor of English at Charles University, VilÃ©m Mathesius, with Jakobson as a founding member and a prime intellectual force (other members included Nikolai Trubetzkoy, RenÃ© Wellek and Jan MukaÅovskÃ½). Jakobson immersed himself in both the academic and cultural life of pre-World War II Czechoslovakia and established close relationships with a number of Czech poets and literary figures. Jakobson received his Ph.D. from Charles University in 1930. He became a professor at Masaryk University in Brno in 1933. He also made an impression on Czech academics with his studies of Czech verse.

Jakobson escaped from Prague in early March 1939 via Berlin for Denmark, where he was associated with the Copenhagen linguistic circle, and such intellectuals as Louis Hjelmslev. He fled to Norway on 1 September 1939, and in 1940 walked across the border to Sweden, where he continued his work at the Karolinska Hospital (with works on aphasia and language competence). When Swedish colleagues feared a possible German occupation, he managed to leave on a cargo ship, together with Ernst Cassirer (the former rector of Hamburg University) to New York City in 1941 to become part of the wider community of intellectual Ã©migrÃ©s who fled there.

In New York, he began teaching at The New School, still closely associated with the Czech Ã©migrÃ© community during that period. At the Ãcole libre des hautes Ã©tudes, a sort of Francophone university-in-exile, he met and collaborated with Claude LÃ©vi-Strauss, who would also become a key exponent of structuralism. He also made the acquaintance of many American linguists and anthropologists, such as Franz Boas, Benjamin Whorf, and Leonard Bloomfield. When the American authorities considered "repatriating" him to Europe, it was Franz Boas who actually saved his life. After the war, he became a consultant to the International Auxiliary Language Association, which would present Interlingua in 1951.

In 1949 Jakobson moved to Harvard University, where he remained until his retirement in 1967. His universalizing structuralist theory of phonology, based on a markedness hierarchy of distinctive features, achieved its canonical exposition in a book published in the United States in 1951, jointly authored by Roman Jakobson, C. Gunnar Fant and Morris Halle. In the same year, Jakobson's theory of 'distinctive features' made a profound impression on the thinking of a young American linguist named Noam Chomsky, in this way decisively shaping the development of linguistics for the remainder of the twentieth century.

In his last decade, Jakobson maintained an office at the Massachusetts Institute of Technology, where he was an honorary Professor Emeritus. In the early 1960s Jakobson shifted his emphasis to a more comprehensive view of language and began writing about communication sciences as a whole. He converted to Eastern Orthodox Christianity in 1975.

Jakobson died in Cambridge, Massachusetts on 18 July 1982. His widow died in 1986. His first wife, who was born in 1908, died in 2000.

According to Jakobson's own personal reminiscences, the most decisive stage in the development of his thinking was the period of revolutionary anticipation and upheaval in Russia between 1912 and 1920, when, as a young student, he fell under the spell of the celebrated Russian futurist wordsmith and linguistic thinker Velimir Khlebnikov.
Offering a slightly different picture, the preface to the second edition of "The Sound Shape of Language" argues that this book represents the fourth stage in "Jakobson's quest to uncover the function and structure of sound in language." The first stage was roughly the 1920s to 1930s where he collaborated with Trubetzkoy, in which they developed the concept of the phoneme, and elucidated the structure of phonological systems. The second stage, from roughly the late 1930s to the 1940s, during which he developed the notion that "binary distinctive features" were the foundational element in language, and that such distinctiveness is "mere otherness" or differentiation. In the third stage in Jakobson's work, from the 1950s to 1960s, he worked with the acoustician C. Gunnar Fant and Morris Halle (a student of Jakobson's) to consider the acoustic aspects of distinctive features.

Influenced by the Organon-Model by Karl BÃ¼hler, Jakobson distinguishes six communication functions, each associated with a dimension or factor of the communication process [n.b. â Elements from BÃ¼hler's theory appear in the diagram below in yellow and pink, Jakobson's elaborations in blue]:

One of the six functions is always the dominant function in a text and usually related to the type of text. In poetry, the dominant function is the poetic function: the focus is on the message itself. The true hallmark of poetry is according to Jakobson "the projection of the principle of equivalence from the axis of selection to the axis of combination". Very broadly speaking, it implies that poetry successfully combines and integrates form and function, that poetry turns the poetry of grammar into the grammar of poetry, so to speak. Jakobson's theory of communicative functions was first published in "Closing Statements: Linguistics and Poetics" (in Thomas A. Sebeok, "Style In Language", Cambridge Massachusetts, MIT Press, 1960, pp.Â 350â377). Despite its wide adoption, the six-functions model has been criticized for lacking specific interest in the "play function" of language that, according to an early review by Georges Mounin, is "not enough studied in general by linguistics researchers".

Jakobson's three principal ideas in linguistics play a major role in the field to this day: linguistic typology, markedness, and linguistic universals. The three concepts are tightly intertwined: typology is the classification of languages in terms of shared grammatical features (as opposed to shared origin), markedness is (very roughly) a study of how certain forms of grammatical organization are more "optimized" than others, and linguistic universals is the study of the general features of languages in the world. He also influenced Nicolas Ruwet's paradigmatic analysis.

Jakobson has also influenced Friedemann Schulz von Thun's four sides model, as well as Michael Silverstein's metapragmatics, Dell Hymes's ethnography of communication and ethnopoetics, the psychoanalysis of Jacques Lacan, and philosophy of Giorgio Agamben.

Jakobson's legacy among researchers specializing in Slavics, and especially Slavic linguistics in North America, has been enormous, for example, Olga Yokoyama.






</doc>
<doc id="26475" url="https://en.wikipedia.org/wiki?curid=26475" title="Rudolph Pariser">
Rudolph Pariser

Rudolph Pariser (born December 8, 1923) is a physical and polymer chemist. He was born in Harbin, China to merchant parents, Ludwig Jacob Pariser and Lia Rubinstein. He attended the Von Hindenburg Schule in Harbin, an American Missionary School in Beijing and American School in Japan in Tokyo. He left for the United States just before World War II broke out.

He received his Bachelor of Science degree from the University of California, Berkeley in 1944, and his Ph. D. degree from the University of Minnesota in physical chemistry in 1950. From 1944 to 1946, during World War II and shortly afterward, he served in the United States Army. He became a naturalized citizen of the United States of America in 1944.

He spent most of his career as a polymer chemist working for DuPont in the Central Research Department at the Experimental Station. He rose to the level of Director of Polymer Sciences, leading it during a time of great innovation. After retiring from DuPont, he formed his own consulting company.

Pariser is best known for his work with Robert G. Parr on the method of molecular orbital computation now known (because it was independently developed by John A. Pople) as the PariserâParrâPople method (PPP method), published both by Pariser and Parr and by Pople in almost simultaneous papers in 1953.

He married Margaret Louise Marsh on July 31, 1972.


</doc>
<doc id="26476" url="https://en.wikipedia.org/wiki?curid=26476" title="Rendezvous with Rama">
Rendezvous with Rama

Rendezvous with Rama is a science fiction novel by British writer ArthurÂ C. Clarke first published in 1973. Set in the 2130s, the story involves a cylindrical alien starship that enters the Solar System. The story is told from the point of view of a group of human explorers who intercept the ship in an attempt to unlock its mysteries. The novel won both the Hugo and Nebula awards upon its release, and is regarded as one of the cornerstones in Clarke's bibliography. The concept was later extended with several sequels.

After an asteroid falls in Northeast Italy in 2077, creating a major disaster, the government of Earth sets up the Spaceguard system as an early warning of arrivals from deep space.

The "Rama" of the title is an alien starship, initially mistaken for an asteroid categorised as "31/439". It is detected by astronomers in the year 2131 while it is still outside the orbit of Jupiter. Its speed (100,000Â km/h - 62,137Â m/h) and the angle of its trajectory clearly indicate it is not on a long orbit around the sun, but comes from interstellar space. The astronomers' interest is further piqued when they realise the asteroid has an extremely rapid rotation period of four minutes and is exceptionally large. It is named Rama after the Hindu god, and an unmanned space probe dubbed "Sita" is launched from the Mars moon Phobos to intercept and photograph it. The resulting images reveal that Rama is a perfect cylinder, in diameter and long, and almost completely featureless, making this humankind's first encounter with an alien spacecraft.

The solar survey vessel "Endeavour" is sent to study Rama, as it is the only ship close enough to do so in the brief period Rama will spend in the solar system. "Endeavour" manages to rendezvous with Rama one month after it first comes to Earth's attention, when the alien ship is already inside Venus's orbit. The crew, led by Commander Bill Norton, enters Rama through a dual safety system consisting of two sets of triple airlocks, and explores the 16-km wide by 50-km long cylindrical world of its interior, but the nature and purpose of the starship and its creators remain enigmatic throughout the book. Rama's inner surfaces hold "cities" of geometric structures that resemble buildings and are separated by streets with shallow trenches. A band of water, dubbed the Cylindrical Sea, stretches around Rama's central circumference. Massive cones, which are theorised as part of Rama's propulsion system, stand at its "southern" end. They also find that Rama's atmosphere is breathable.

One of the crew members, Jimmy Pak, who has experience with low gravity skybikes, rides a smuggled skybike along Rama's axis to the far end, otherwise inaccessible due to the cylindrical sea and the 500m high cliff on the opposite shore. Once at the massive metal cones on the southern end of Rama, Jimmy detects magnetic and electric fields coming from the cones, which increase, resulting in lightning. Due to his proximity to the spires, the concussion from a discharge damages his skybike causing him to crash on the isolated southern continent.

When Pak wakes up, he sees a crab-like creature picking up his skybike and chopping it into pieces. He cannot decide whether it is a robot or a biological alien, and keeps his distance while radioing for help. As Pak waits, Norton sends a rescue party across the cylindrical sea, using a small, improvised craft, constructed earlier for exploration of the sea's central island. The creature dumps the remains of the skybike into a pit, but ignores Pak himself, who explores the surrounding fields while waiting for the rescue party to arrive. Amongst the strange geometric structures, he sees an alien flower growing through a cracked tile in the otherwise sterile environment, and decides to take it as both a curiosity and for scientific research.

Pak jumps off the 500m cliff, his descent slowed by the low gravity and using his shirt as a drogue parachute, and is quickly rescued by the waiting boat. As they ride back, tidal waves form in the cylindrical sea, created by the movements of Rama itself as it makes course corrections. When the crew arrives at base, they see a variety of odd creatures inspecting their camp. When one is found damaged and apparently lifeless, the team's doctor/biologist Surgeon-Commander Laura Ernst inspects it, and discovers it to be a hybrid biological entity and robotâeventually termed a "biot". It, and by assumption the others, are powered by internal batteries (much like those of terrestrial electric eels) and possess some intelligence. They are believed to be the drones of Rama's still-absent builders.

The members of the Rama Committee and the United Planets, both based on the Moon, have been monitoring events inside Rama and giving feedback. The Hermian colonists have concluded that Rama is a potential threat and send a rocket-mounted nuclear bomb to destroy it should it prove to pose a threat. Lt. Boris Rodrigo takes advantage of the five minute transmission delay and uses a pair of wire cutters to defuse the bomb and its control.

As Rama approaches perihelion, and on their final expedition, the crew decide to visit the city closest to their point of entry, christened "London", and use a laser to cut open one of the "buildings" to see what it houses. They discover transparent pedestals containing holograms of various artefacts, which they theorise are used by the Ramans as templates for creating tools and other objects. One hologram appears to be a uniform with bandoliers, straps and pockets that suggests the size and shape of the Ramans. As the crew photographs some of the holograms, the biots begin returning to the cylindrical sea, where they are recycled by aquatic biots ('sharks') and the six striplights that illuminate Rama's interior start to dim, prompting the explorers to leave Rama and to re-board "Endeavour".

With "Endeavour" a safe distance away, Rama reaches perihelion and utilizes the Sun's gravitational field, and its mysterious "space drive", to perform a slingshot manoeuvre which flings it out of the solar system and towards an unknown destination in the direction of the Large Magellanic Cloud.

The book was meant to stand alone, although its final sentence suggests otherwise:

Clarke denied that this sentence was a hint that the story might be continued. In his foreword to the book's sequel, he stated that it was just a good way to end the first book, and that he added it during a final revision.

John Leonard of "The New York Times", while finding Clarke "benignly indifferent to the niceties of characterization," praised the novel for conveying "that chilling touch of the alien, the not-quite-knowable, that distinguishes sci-fi at its most technically imaginative." Other reviewers have also commented on Clarke's lack of character development and overemphasis on realism.

The novel was awarded the following soon after publication

The interior of Rama is essentially a large cylindrical landscape, dubbed "The Central Plain" by the crew, 16 kilometres in diameter and nearly 50 long, with artificial gravity provided by its 0.25 rpm spin. It is split into the "northern" and "southern" plains, divided in the middle by a 10-km wide expanse of water the astronauts dub the "Cylindrical Sea". In the center of the Cylindrical Sea is an island of unknown purpose covered in tall, skyscraper-like structures, which the astronauts name "New York" due to an imagined similarity to Manhattan. At each end of the ship are North and South "Poles". The North Pole is effectively the bow and the South Pole the stern, as Rama accelerates in the direction of the north pole and its drive system is at the South Pole.

The North Pole contains Rama's airlocks, and is where the "Endeavour" lands. The airlocks open into the hub of the massive bowl shaped cap at the North Pole, with three 8-kilometre long stair systems, called Alpha, Beta, and Gamma by the crew, leading to the plain.

The Northern plain contains several small "towns" interconnected by roads, dubbed London, Paris, Peking, Tokyo, Rome, and Moscow. The South Pole has a giant cone-shaped protrusion surrounded by six smaller ones, which are thought to be part of Rama's reactionless space drive.

Both ends of Rama are lit by giant trenches (three in the northern plain and three in the south), equidistantly placed around the cylinder, effectively functioning as giant strip lighting.

Clarke paired up with Gentry Lee for the remainder of the series. Lee did the actual writing, while Clarke read and made editing suggestions. The focus and style of the last three novels are quite different from those of the original with an increased emphasis on characterisation and more clearly portrayed heroes and villains, rather than Clarke's dedicated professionals. These later books did not receive the same critical acclaim and awards as the original.


Gentry Lee also wrote two further novels set in the same "Rama" Universe.


A graphic adventure computer game of the same name with a text parser based on the book was made in 1984 by Trillium (later known as Telarium) and ported to other systems such as the Apple II, Commodore 64. Despite its primitive graphics, it had highly detailed descriptions, and it followed the book very closely along with having puzzles to solve during the game.
In Spain there was an official adaptation from the 2nd generation MSX computers, which were known by their high quality graphics, with 256 colour palette screens. It was called "Cita con Rama".

It was adapted from the Clarke novel in 1983 by Ron Martinez, who went on to design the massively multiplayer online game "10Six", also known as "Project Visitor".

Sierra Entertainment created "Rama" in 1996 as a point and click adventure game in the style of "Myst". Along with highly detailed graphics, Arthur C. Clarke also appeared in the game as the guide for the player. This game featured characters from the sequel book "Rama II".

In 2009, BBC Radio 4 produced a two-part radio adaptation of the book as part of a science-fiction season. It was adapted by Mike Walker, and was broadcast on 1 March 2009 (Part 1) and 8 March 2009 (Part 2).

In the early 2000s, actor Morgan Freeman expressed his desire to produce a film based on "Rendezvous with Rama." The film has been stuck in "development hell" for many years. In 2003, after initial problems procuring funding, it appeared the project would go into production. The film was to be produced by Freeman's production company, Revelations Entertainment. David Fincher, touted on Revelations' "Rama" web page as far back as 2001, stated in a late 2007 interview that he was still attached to helm.

By late 2008, David Fincher stated the movie was unlikely to be made. "It looks like it's not going to happen. There's no script and as you know, Morgan Freeman's not in the best of health right now. We've been trying to do it but it's probably not going to happen."

In 2010, Freeman stated in an interview that he was still planning to make the project but that it has been difficult to find the right script. He also stated that it should be made in 3D. In January 2011, Fincher stated in an interview with MTV that he was still planning to make the film after he had completed work on his planned remake of "20,000 Leagues Under the Sea" (which was scheduled to begin production in 2012 but has since been scrapped). He also reiterated Freeman's concerns about the difficulty of finding the right script.

In an interview with Neil deGrasse Tyson in February 2012, Freeman indicated an interest in playing the role of Commander Norton for the film, stating that "my fantasy of commanding a starship is commanding Endeavour". Tyson then asked, "So is this a pitch to be ... that person if they ever make that movie?" to which Freeman reaffirmed, "We ARE going to make that movie." In response to a plea to "make that come out sooner rather than later", Freeman reiterated that difficulty in authoring a high quality script is the primary barrier for the film, stating "... the only task you have that's really really hard in making movies, harder than getting money, is getting a script ... a good script".

Clarke invented the space study program which detects Rama, Project Spaceguard, as a method of identifying near-Earth objects on Earth-impact trajectories; in the novel it was initiated in 2077. A real project named Spaceguard was initiated in 1992, named after Clarke's fictional project and "with the permission and encouragement of Clarke". After interest in the dangers of asteroid strikes was heightened by a series of Hollywood disaster films, the United States Congress gave NASA authorisation and funding to support Spaceguard. By 2017, there were a number of different efforts to detect potentially dangerous asteroids - see figure on right. 

On 19th October 2017 an incoming interstellar object was discovered by Pan-STARRS, a system similar to Spaceguard. Like Rama, the object had an unusually elongated shape. Before the official Hawaiian name 'Oumuamua was selected by the International Astronomical Union, a popular choice was "Rama".




</doc>
<doc id="26477" url="https://en.wikipedia.org/wiki?curid=26477" title="Rust">
Rust

Rust is an iron oxide, a usually red oxide formed by the redox reaction of iron and oxygen in the presence of water or air moisture. Several forms of rust are distinguishable both visually and by spectroscopy, and form under different circumstances. Rust consists of hydrated iron(III) oxides FeOÂ·"n"HO and iron(III) oxide-hydroxide (FeO(OH), Fe(OH)).

Given sufficient time, oxygen, and water, any iron mass will eventually convert entirely to rust and disintegrate. Surface rust is flaky and
friable, and it provides no protection to the underlying iron, unlike the formation of patina on copper surfaces. Rusting is the common term for corrosion of iron and its alloys, such as steel. Many other metals undergo similar corrosion, but the resulting oxides are not commonly called rust.

Other forms of rust exist, like the result of reactions between iron and chloride in an environment deprived of oxygen. Rebar used in underwater concrete pillars, which generates green rust, is an example. Although rusting is generally a negative aspect of iron, a particular form of rusting, known as "stable rust," causes the object to have a thin coating of rust over the top, and if kept in low relative humidity, makes the "stable" layer protective to the iron below, but not to the extent of other oxides, such as aluminum.

Rust is another name for iron oxide, which occurs when iron or an alloy that contains iron, like steel, is exposed to oxygen and moisture for a long period of time. Over time, the oxygen combines with the metal at an atomic level, forming a new compound called an oxide and weakening the bonds of the metal itself. Although some people refer to rust generally as "oxidation", that term is much more general and describes a vast number of processes involving the loss of electrons or increased oxidation state, as part of a reaction. The best-known of these reactions involve oxygen, hence the name "oxidation". The terms "rust" and "rusting" only mean oxidation of iron and its resulting products. Many other oxidation reactions exist which do not involve iron or produce rust. But only iron or alloys that contain iron can rust. However, other metals can corrode in similar ways.

The main catalyst for the rusting process is water. Iron or steel structures might appear to be solid, but water molecules can penetrate the microscopic pits and cracks in any exposed metal. The hydrogen atoms present in water molecules can combine with other elements to form acids, which will eventually cause more metal to be exposed. If chloride ions are present, as is the case with saltwater, the corrosion is likely to occur more quickly. Meanwhile, the oxygen atoms combine with metallic atoms to form the destructive oxide compound. As the atoms combine, they weaken the metal, making the structure brittle and crumbly.

When impure (cast) iron is in contact with water, oxygen, other strong oxidants, or acids, it rusts. If salt is present, for example in seawater or salt spray, the iron tends to rust more quickly, as a result of electrochemical reactions. Iron metal is relatively unaffected by pure water or by dry oxygen. As with other metals, like aluminium, a tightly adhering oxide coating, a passivation layer, protects the bulk iron from further oxidation. The conversion of the passivating ferrous oxide layer to rust results from the combined action of two agents, usually oxygen and water.

Other degrading solutions are sulfur dioxide in water and carbon dioxide in water. Under these corrosive conditions, iron hydroxide species are formed. Unlike ferrous oxides, the hydroxides do not adhere to the bulk metal. As they form and flake off from the surface, fresh iron is exposed, and the corrosion process continues until either all of the iron is consumed or all of the oxygen, water, carbon dioxide, or sulfur dioxide in the system are removed or consumed.

When iron rusts, the oxides take up more volume than the original metal; this expansion can generate enormous forces, damaging structures made with iron. See "economic effect" for more details.

The rusting of iron is an electrochemical process that begins with the transfer of electrons from iron to oxygen. The iron is the reducing agent (gives up electrons) while the oxygen is the oxidising agent (gains electrons). The rate of corrosion is affected by water and accelerated by electrolytes, as illustrated by the effects of road salt on the corrosion of automobiles. The key reaction is the reduction of oxygen:
Because it forms hydroxide ions, this process is strongly affected by the presence of acid. Indeed, the corrosion of most metals by oxygen is accelerated at low pH. Providing the electrons for the above reaction is the oxidation of iron that may be described as follows:

The following redox reaction also occurs in the presence of water and is crucial to the formation of rust:

In addition, the following multistep acidâbase reactions affect the course of rust formation:

as do the following dehydration equilibria:

From the above equations, it is also seen that the corrosion products are dictated by the availability of water and oxygen. With limited dissolved oxygen, iron(II)-containing materials are favoured, including FeO and black lodestone or magnetite (FeO). High oxygen concentrations favour ferric materials with the nominal formulae Fe(OH)O. The nature of rust changes with time, reflecting the slow rates of the reactions of solids.

Furthermore, these complex processes are affected by the presence of other ions, such as Ca, which serve as electrolytes which accelerate rust formation, or combine with the hydroxides and oxides of iron to precipitate a variety of Ca, Fe, O, OH species.

Onset of rusting can also be detected in laboratory with the use of ferroxyl indicator solution. The solution detects both Fe ions and hydroxyl ions. Formation of Fe ions and hydroxyl ions are indicated by blue and pink patches respectively.

Because of the widespread use and importance of iron and steel products, the prevention or slowing of rust is the basis of major economic activities in a number of specialized technologies. A brief overview of methods is presented here; for detailed coverage, see the cross-referenced articles.
Rust is permeable to air and water, therefore the interior metallic iron beneath a rust layer continues to corrode. Rust prevention thus requires coatings that preclude rust formation.

Stainless steel forms a passivation layer of chromium(III) oxide. Similar passivation behavior occurs with magnesium, titanium, zinc, zinc oxides, aluminium, polyaniline, and other electroactive conductive polymers.

Special "weathering steel" alloys such as Cor-Ten rust at a much slower rate than normal, because the rust adheres to the surface of the metal in a protective layer. Designs using this material must include measures that avoid worst-case exposures, since the material still continues to rust slowly even under near-ideal conditions.

Galvanization consists of an application on the object to be protected of a layer of metallic zinc by either hot-dip galvanizing or electroplating. Zinc is traditionally used because it is cheap, adheres well to steel, and provides cathodic protection to the steel surface in case of damage of the zinc layer. In more corrosive environments (such as salt water), cadmium plating is preferred. Galvanization often fails at seams, holes, and joints where there are gaps in the coating. In these cases, the coating still provides some partial cathodic protection to iron, by acting as a galvanic anode and corroding itself instead of the underlying protected metal. The protective zinc layer is consumed by this action, and thus galvanization provides protection only for a limited period of time.

More modern coatings add aluminium to the coating as "zinc-alume"; aluminium will migrate to cover scratches and thus provide protection for a longer period. These approaches rely on the aluminium and zinc oxides reprotecting a once-scratched surface, rather than oxidizing as a sacrificial anode as in traditional galvanized coatings. In some cases, such as very aggressive environments or long design life, both zinc and a coating are applied to provide enhanced corrosion protection.

Typical galvanization of steel products which are to subject to normal day to day weathering in an outside environment consists of a hot dipped 85Â Âµm zinc coating. Under normal weather conditions, this will deteriorate at a rate of 1Â Âµm per year, giving approximately 85 years of protection.

Cathodic protection is a technique used to inhibit corrosion on buried or immersed structures by supplying an electrical charge that suppresses the electrochemical reaction. If correctly applied, corrosion can be stopped completely. In its simplest form, it is achieved by attaching a sacrificial anode, thereby making the iron or steel the cathode in the cell formed. The sacrificial anode must be made from something with a more negative electrode potential than the iron or steel, commonly zinc, aluminium, or magnesium. The sacrificial anode will eventually corrode away, ceasing its protective action unless it is replaced in a timely manner.

Cathodic protection can also be provided by using a special-purpose electrical device to appropriately induce an electric charge.

Rust formation can be controlled with coatings, such as paint, lacquer, varnish, or wax tapes that isolate the iron from the environment. Large structures with enclosed box sections, such as ships and modern automobiles, often have a wax-based product (technically a "slushing oil") injected into these sections. Such treatments usually also contain rust inhibitors. Covering steel with concrete can provide some protection to steel because of the alkaline pH environment at the steelâconcrete interface. However, rusting of steel in concrete can still be a problem, as expanding rust can fracture or slowly "explode" concrete from within.

As a closely related example, iron bars were used to reinforce stonework of the Parthenon in Athens, Greece, but caused extensive damage by rusting, swelling, and shattering the marble components of the building.

When only temporary protection is needed for storage or transport, a thin layer of oil, grease, or a special mixture such as Cosmoline can be applied to an iron surface. Such treatments are extensively used when "mothballing" a steel ship, automobile, or other equipment for long-term storage.

Special antiseize lubricant mixtures are available, and are applied to metallic threads and other precision machined surfaces to protect them from rust. These compounds usually contain grease mixed with copper, zinc, or aluminium powder, and other proprietary ingredients.

Bluing is a technique that can provide limited resistance to rusting for small steel items, such as firearms; for it to be successful, a water-displacing oil is rubbed onto the blued steel and other steel.

Corrosion inhibitors, such as gas-phase or volatile inhibitors, can be used to prevent corrosion inside sealed systems. They are not effective when air circulation disperses them, and brings in fresh oxygen and moisture.

Rust can be avoided by controlling the moisture in the atmosphere. An example of this is the use of silica gel packets to control humidity in equipment shipped by sea.

Rust removal from small iron or steel objects by electrolysis can be done in a home workshop using simple materials such as a plastic bucket filled with an electrolyte consisting of washing soda dissolved in tap water, a length of rebar suspended vertically in the solution to act as an anode, another laid across the top of the bucket to act as a support for suspending the object, baling wire to suspend the object in the solution from the horizontal rebar, and a battery charger as a power source in which the positive terminal is clamped to the anode and the negative terminal is clamped to the object to be treated which becomes the cathode.

Rust may be treated with commercial products known as rust converter which contain tannic acid or phosphoric acid which combines with rust; removed with organic acids like citric acid and vinegar or the stronger hydrochloric acid; or removed with chelating agents as in some commercial formulations or even a solution of molasses.

Rust is associated with degradation of iron-based tools and structures. As rust has a much higher volume than the originating mass of iron, its buildup can also cause failure by forcing apart adjacent parts â a phenomenon sometimes known as "rust packing". It was the cause of the collapse of the Mianus river bridge in 1983, when the bearings rusted internally and pushed one corner of the road slab off its support.

Rust was an important factor in the Silver Bridge disaster of 1967 in West Virginia, when a steel suspension bridge collapsed in less than a minute, killing 46 drivers and passengers on the bridge at the time. The Kinzua Bridge in Pennsylvania was blown down by a tornado in 2003, largely because the central base bolts holding the structure to the ground had rusted away, leaving the bridge anchored by gravity alone.

Reinforced concrete is also vulnerable to rust damage. Internal pressure caused by expanding corrosion of concrete-covered steel and iron can cause the concrete to spall, creating severe structural problems. It is one of the most common failure modes of reinforced concrete bridges and buildings.

Rust is a commonly used metaphor for slow decay due to neglect, since it gradually converts robust iron and steel metal into a soft crumbling powder. A wide section of the industrialized American Midwest and American Northeast, once dominated by steel foundries, the automotive industry, and other manufacturers, has experienced harsh economic cutbacks that have caused the region to be dubbed the "Rust Belt".

In music, literature, and art, rust is associated with images of faded glory, neglect, decay, and ruin.




</doc>
<doc id="26478" url="https://en.wikipedia.org/wiki?curid=26478" title="Real analysis">
Real analysis

In mathematics, real analysis is the branch of mathematical analysis that studies the behavior of real numbers, sequences and series of real numbers, and real functions. Some particular properties of real-valued sequences and functions that real analysis studies include convergence, limits, continuity, smoothness, differentiability and integrability.

Real analysis is distinguished from complex analysis, which deals with the study of complex numbers and their functions.

The theorems of real analysis rely intimately upon the structure of the real number line. The real number system consists of an uncountable set (formula_1), together with two binary operations denoted and , and an order denoted . The operations make the real numbers a field, and, along with the order, an ordered field. The real number system is the unique "complete ordered field", in the sense that any other complete ordered field is isomorphic to it. Intuitively, completeness means that there are no 'gaps' in the real numbers. In particular, this property distinguishes the real numbers from other ordered fields (e.g., the rational numbers formula_2) and is critical to the proof of several key properties of functions of the real numbers. The completeness of the reals is often conveniently expressed as the "least upper bound property" (see below).

There are several ways of formalizing the definition of the real numbers. Modern approaches consist of providing a list of axioms, and a proof of the existence of a model for them, which has above properties. Moreover, one may show that any two models are isomorphic, which means that all models have exactly the same properties, and that one may forget how the model is constructed for using real numbers. Some of these constructions are described in the main article.

The real numbers have various lattice-theoretic properties that are absent in the complex numbers. Also, the real numbers form an ordered field, in which sums and products of positive numbers are also positive. Moreover, the ordering of the real numbers is total, and the real numbers have the least upper bound property: "Every nonempty subset of formula_1 that has an upper bound has a least upper bound that is also a real number." These order-theoretic properties lead to a number of fundamental results in real analysis, such as the monotone convergence theorem, the intermediate value theorem and the mean value theorem.

However, while the results in real analysis are stated for real numbers, many of these results can be generalized to other mathematical objects. In particular, many ideas in functional analysis and operator theory generalize properties of the real numbers â such generalizations include the theories of Riesz spaces and positive operators. Also, mathematicians consider real and imaginary parts of complex sequences, or by pointwise evaluation of operator sequences.

Many of the theorems of real analysis are consequences of the topological properties of the real number line. The order properties of the real numbers described above are closely related to these topological properties. As a topological space, the real numbers has a "standard topology", which is the order topology induced by order formula_4. Alternatively, by defining the "metric" or "distance function" formula_5 using the absolute value function as formula_6, the real numbers become the prototypical example of a metric space. The topology induced by metric formula_7 turns out to be identical to the standard topology induced by order formula_4. Theorems like the intermediate value theorem that are essentially topological in nature can often be proved in the more general setting of metric or topological spaces rather than in formula_1 only. Often, such proofs tend to be shorter or simpler compared to classical proofs that apply direct methods.

A sequence is a function whose domain is a countable, totally ordered set. The domain is usually taken to be the natural numbers, although it is occasionally convenient to also consider bidirectional sequences indexed by the set of all integers, including negative indices.

Of interest in real analysis, a real-valued sequence, here indexed by the natural numbers, is a map formula_10. Each formula_11 is referred to as a term (or, less commonly, an element) of the sequence. A sequence is rarely denoted explicitly as a function; instead, by convention, it is almost always notated as if it were an ordered â-tuple, with individual terms or a general term enclosed in parentheses: formula_12.A sequence that tends to a limit (i.e., formula_13 exists) is said to be convergent; otherwise it is divergent. ("See the section on limits and convergence for details.") A real-valued sequence formula_14 is bounded if there exists formula_15 such that formula_16 for all formula_17. A real-valued sequence formula_14 is monotonically increasing or decreasing if formula_19 or formula_20holds, respectively. If either holds, the sequence is said to be monotonic. The monotonicity is strict if the chained inequalities still hold with formula_21 or formula_22 replaced by < or >.

Given a sequence formula_14, another sequence formula_24 is a subsequence of formula_14 if formula_26 for all positive integers formula_27 and formula_28 is a strictly increasing sequence of natural numbers.

Roughly speaking, a limit is the value that a function or a sequence "approaches" as the input or index approaches some value. (This value can include the symbols formula_29 when addressing the behavior of a function or sequence as the variable increases or decreases without bound.) The idea of a limit is fundamental to calculus (and mathematical analysis in general) and its formal definition is used in turn to define notions like continuity, derivatives, and integrals. (In fact, the study of limiting behavior has been used as a characteristic that distinguishes calculus and mathematical analysis from other branches of mathematics.)

The concept of limit was informally introduced for functions by Newton and Leibniz, at the end of 17th century, for building infinitesimal calculus. For sequences, the concept was introduced by Cauchy, and made rigorous, at the end of 19th century by Bolzano and Weierstrass, who gave the modern Îµ-Î´ definition, which follows.

Definition. Let formula_30 be a real-valued function defined on formula_31. We say that formula_32 tends to formula_33 as formula_34 approaches formula_35, or that the limit of formula_32 as formula_34 approaches formula_35 is formula_33 if, for any formula_40, there exists formula_41 such that for all formula_42, formula_43 implies that formula_44. We write this symbolically as formula_45, or formula_46.Intuitively, this definition can be thought of in the following way: We say that formula_47 as formula_48, when, given any positive number formula_49, no matter how small, we can always find a formula_50, such that we can guarantee that formula_32 and formula_33 are less than formula_49 apart, as long as formula_34 (in the domain of formula_30) is a real number that is less than formula_50 away from formula_35 but distinct from formula_35. The purpose of the last stipulation, which corresponds to the condition formula_59 in the definition, is to ensure that formula_46 does not imply anything about the value of formula_61 itself. Actually, formula_35 does not even need to be in the domain of formula_30 in order for formula_64 to exist.

In a slightly different but related context, the concept of a limit applies to the behavior of a sequence formula_14 when formula_66 becomes large.

Definition. Let formula_14 be a real-valued sequence. We say that formula_14 converges to formula_69 if, for any formula_40, there exists a natural number formula_71 such that formula_72 implies that formula_73. We write this symbolically as formula_74, or formula_75;if formula_14 fails to converge, we say that formula_14 diverges.

Generalizing to a real-valued function of a real variable, a slight modification of this definition (replacement of sequence formula_14 and term formula_79 by function formula_30 and value formula_32 and natural numbers formula_71 and formula_66 by real numbers formula_84 and formula_34, respectively) yields the definition of the limit of formula_32 as formula_34 increases without bound, notated formula_88. Reversing the inequality formula_89 to formula_90 gives the corresponding definition of the limit of formula_32 as formula_34 "decreases" "without bound", formula_93.

Sometimes, it is useful to conclude that a sequence converges, even though the value to which it converges is unknown or irrelevant. In these cases, the concept of a Cauchy sequence is useful.

Definition. Let formula_14 be a real-valued sequence. We say that formula_14 is a Cauchy sequence if, for any formula_40, there exists a natural number formula_71 such that formula_98 implies that formula_99.

It can be shown that a real-valued sequence is Cauchy if and only if it is convergent. This property of the real numbers is expressed by saying that the real numbers endowed with the standard metric, formula_100, is a complete metric space. In a general metric space, however, a Cauchy sequence need not converge.

In addition, for real-valued sequences that are monotonic, it can be shown that the sequence is bounded if and only if it is convergent.

In addition to sequences of numbers, one may also speak of "sequences of functions" "on" formula_101, that is, infinite, ordered families of functions formula_102, denoted formula_103, and their convergence properties. However, in the case of sequences of functions, there are two kinds of convergence, known as "pointwise convergence" and "uniform convergence", that need to be distinguished.

Roughly speaking, pointwise convergence of functions formula_104 to a limiting function formula_105, denoted formula_106, simply means that given any formula_42, formula_108 as formula_109. In contrast, uniform convergence is a stronger type of convergence, in the sense that a uniformly convergent sequence of functions also converges pointwise, but not conversely. Uniform convergence requires members of the family of functions, formula_104, to fall within some error formula_40 of formula_30 for "every value of formula_42", whenever formula_72, for some integer formula_71. For a family of functions to uniformly converge, sometimes denoted formula_116, such a value of formula_71 must exist for any formula_40 given, no matter how small. Intuitively, we can visualize this situation by imagining that, for a large enough formula_71, the functions formula_120 are all confined within a 'tube' of width formula_121 about formula_30 (i.e., between formula_123 and formula_124) "for every value in their domain" formula_125.

The distinction between pointwise and uniform convergence is important when exchanging the order of two limiting operations (e.g., taking a limit, a derivative, or integral) is desired: in order for the exchange to be well-behaved, many theorems of real analysis call for uniform convergence. For example, a sequence of continuous functions (see below) is guaranteed to converge to a continuous limiting function if the convergence is uniform, while the limiting function may not be continuous if convergence is only pointwise. Karl Weierstrass is generally credited for clearly defining the concept of uniform convergence and fully investigating its implications.

Compactness is a concept from general topology that plays an important role in many of the theorems of real analysis. The property of compactness is a generalization of the notion of a set being "closed" and "bounded". (In the context of real analysis, these notions are equivalent: a set in Euclidean space is compact if and only if it is closed and bounded.) Briefly, a closed set contains all of its boundary points, while a set is bounded if there exists a real number such that the distance between any two points of the set is less than that number. In formula_1, sets that are closed and bounded, and therefore compact, include the empty set, any finite number of points, closed intervals, and their finite unions. However, this list is not exhaustive; for instance, the set formula_127 is a compact set; the Cantor ternary set formula_128 is another example of a compact set. On the other hand, the set formula_129 is not compact because it is bounded but not closed, as the boundary point 0 is not a member of the set. The set formula_130 is also not compact because it is closed but not bounded.

For subsets of the real numbers, there are several equivalent definitions of compactness.

Definition. A set formula_31 is compact if it is closed and bounded.

This definition also holds for Euclidean space of any finite dimension, formula_132, but it is not valid for metric spaces in general. The equivalence of the definition with the definition of compactness based on subcovers, given later in this section, is known as the Heine-Borel theorem.

A more general definition that applies to all metric spaces uses the notion of a subsequence (see above).

Definition. A set formula_125 in a metric space is compact if every sequence in formula_125 has a convergent subsequence.

This particular property is known as "subsequential compactness". In formula_1, a set is subsequentially compact if and only if it is closed and bounded, making this definition equivalent to the one given above. Subsequential compactness is equivalent to the definition of compactness based on subcovers for metric spaces, but not for topological spaces in general.

The most general definition of compactness relies on the notion of "open covers" and "subcovers", which is applicable to topological spaces (and thus to metric spaces and formula_1 as special cases). In brief, a collection of open sets formula_137 is said to be an "open cover" of set formula_138 if the union of these sets is a superset of formula_138. This open cover is said to have a "finite subcover" if a finite subcollection of the formula_137 could be found that also covers formula_138.

Definition. A set formula_138 in a topological space is compact if every open cover of formula_138 has a finite subcover.

Compact sets are well-behaved with respect to properties like convergence and continuity. For instance, any Cauchy sequence in a compact metric space is convergent. As another example, the image of a compact metric space under a continuous map is also compact.

A function from the set of real numbers to the real numbers can be represented by a graph in the Cartesian plane; such a function is continuous if, roughly speaking, the graph is a single unbroken curve with no "holes" or "jumps".

There are several ways to make this intuition mathematically rigorous. Several definitions of varying levels of generality can be given. In cases where two or more definitions are applicable, they are readily shown to be equivalent to one another, so the most convenient definition can be used to determine whether a given function is continuous or not. In the first definition given below, formula_144 is a function defined on a non-degenerate interval formula_145 of the set of real numbers as its domain. Some possibilities include formula_146, the whole set of real numbers, an open interval formula_147 or a closed interval formula_148 Here, formula_69 and formula_150 are distinct real numbers, and we exclude the case of formula_145 being empty or consisting of only one point, in particular.

Definition. If formula_152 is a non-degenerate interval, we say that formula_144 is continuous at formula_154 if formula_155. We say that formula_30 is a continuous map if formula_30 is continuous at every formula_154.

In contrast to the requirements for formula_30 to have a limit at a point formula_160, which do not constrain the behavior of formula_30 at formula_160 itself, the following two conditions, in addition to the existence of formula_163, must also hold in order for formula_30 to be continuous at formula_160: (i) formula_30 must be defined at formula_160, i.e., formula_160 is in the domain of formula_30; "and" (ii) formula_170 as formula_171. The definition above actually applies to any domain formula_125 that does not contain an isolated point, or equivalently, formula_125 where every formula_174 is a limit point of formula_125. A more general definition applying to formula_176 with a general domain formula_177 is the following:

Definition. If formula_138 is an arbitrary subset of formula_1, we say that formula_176 is continuous at formula_181 if, for any formula_40, there exists formula_41 such that for all formula_184, formula_185 implies that formula_186. We say that formula_30 is a continuous map if formula_30 is continuous at every formula_181.

A consequence of this definition is that formula_30 is "trivially continuous at any isolated point" formula_181. This somewhat unintuitive treatment of isolated points is necessary to ensure that our definition of continuity for functions on the real line is consistent with the most general definition of continuity for maps between topological spaces (which includes metric spaces and formula_1 in particular as special cases). This definition, which extends beyond the scope of our discussion of real analysis, is given below for completeness.

Definition. If formula_138 and formula_194 are topological spaces, we say that formula_195 is continuous at formula_181 if formula_197 is a neighborhood of formula_160 in formula_138 for every neighborhood formula_200 of formula_201 in formula_194. We say that formula_30 is a continuous map if formula_204 is open in formula_138 for every formula_206 open in formula_194.

Definition. If formula_138 is a subset of the real numbers, we say a function formula_176 is uniformly continuous on formula_138 if, for any formula_40, there exists a formula_41 such that for all formula_216, formula_217 implies that formula_218"."

Explicitly, when a function is uniformly continuous on formula_138, the choice of formula_50 needed to fulfill the definition must work for "all of" formula_138 for a given formula_49. In contrast, when a function is continuous at every point formula_181 (or said to be continuous on formula_138), the choice of formula_50 may depend on both formula_49 "and" formula_160. In contrast to simple continuity, uniform continuity is a property of a function that only makes sense with a specified domain; to speak of uniform continuity at a single point formula_160 is meaningless.

On a compact set, it is easily shown that all continuous functions are uniformly continuous. If formula_125 is a bounded noncompact subset of formula_1, then there exists formula_105 that is continuous but not uniformly continuous. As a simple example, consider formula_232 defined by formula_233. By choosing points close to 0, we can always make formula_234 for any single choice of formula_41, for a given formula_40.

Definition. Let formula_237 be an interval on the real line. A function formula_238 is said to be absolutely continuous on formula_145 if for every positive number formula_49, there is a positive number formula_50 such that whenever a finite sequence of pairwise disjoint sub-intervals formula_242 of formula_145 satisfies
then
Absolutely continuous functions are continuous: consider the case "n" = 1 in this definition. The collection of all absolutely continuous functions on "I" is denoted AC("I"). Absolute continuity is a fundamental concept in the Lebesgue theory of integration, allowing the formulation of a generalized version of the fundamental theorem of calculus that applies to the Lebesgue integral.

The notion of the "derivative" of a function or "differentiability" originates from the concept of approximating a function near a given point using the "best" linear approximation. This approximation, if it exists, is unique and is given by the line that is tangent to the function at the given point formula_69, and the slope of the line is the derivative of the function at formula_69.

A function formula_248 is differentiable at formula_69 if the limit

exists. This limit is known as the derivative of formula_30 at formula_69, and the function formula_253, possibly defined on only a subset of formula_1, is the derivative (or derivative function) of formula_30. If the derivative exists everywhere, the function is said to be differentiable.

As a simple consequence of the definition, formula_30 is continuous at formula_69 if it is differentiable there. Differentiability is therefore a stronger regularity condition (condition describing the "smoothness" of a function) than continuity, and it is possible for a function to be continuous on the entire real line but not differentiable anywhere (see Weierstrass's nowhere differentiable continuous function). It is possible to discuss the existence of higher-order derivatives as well, by finding the derivative of a derivative function, and so on.

One can classify functions by their differentiability class. The class formula_258 (sometimes formula_259 to indicate the interval of applicability) consists of all continuous functions. The class formula_260 consists of all differentiable functions whose derivative is continuous; such functions are called continuously differentiable. Thus, a formula_260 function is exactly a function whose derivative exists and is of class formula_258. In general, the classes "formula_263" can be defined recursively by declaring formula_258 to be the set of all continuous functions and declaring "formula_263" for any positive integer formula_27 to be the set of all differentiable functions whose derivative is in formula_267. In particular, "formula_263" is contained in formula_267 for every formula_27, and there are examples to show that this containment is strict. Class formula_271 is the intersection of the sets "formula_263" as "formula_27" varies over the non-negative integers, and the members of this class are known as the smooth functions. Class formula_274 consists of all analytic functions, and is strictly contained in formula_271 (see bump function for a smooth function that is not analytic).

A series formalizes the imprecise notion of taking the sum of an endless sequence of numbers. The idea that taking the sum of an "infinite" number of terms can lead to a finite result was counterintuitive to the ancient Greeks and led to the formulation of a number of paradoxes by Zeno and other philosophers. The modern notion of assigning a value to a series avoids dealing with the ill-defined notion of adding an "infinite" number of terms. Instead, the finite sum of the first formula_66 terms of the sequence, known as a partial sum, is considered, and the concept of a limit is applied to the sequence of partial sums as formula_66 grows without bound. The series is assigned the value of this limit, if it exists.

Given an (infinite) sequence formula_14, we can define an associated series as the formal mathematical object formula_279, sometimes simply written as formula_280. The partial sums of a series formula_280 are the numbers formula_282. A series formula_280 is said to be convergent if the sequence consisting of its partial sums, formula_284, is convergent; otherwise it is divergent. The sum of a convergent series is defined as the number formula_285.

The word "sum" is used here in a metaphorical sense as a shorthand for taking the limit of a sequence of partial sums and should not be interpreted as simply "adding" an infinite number of terms. For instance, in contrast to the behavior of finite sums, rearranging the terms of an infinite series may result in convergence to a different number (see the article on the "Riemann rearrangement theorem" for further discussion).

An example of a convergent series is a geometric series which forms the basis of one of Zeno's famous paradoxes:

In contrast, the harmonic series has been known since the Middle Ages to be a divergent series:

A series formula_280 is said to converge absolutely if formula_290 is convergent. A convergent series formula_280 for which formula_290 diverges is said to converge conditionally (or nonabsolutely). It is easily shown that absolute convergence of a series implies its convergence. On the other hand, an example of a conditionally convergent series is

The Taylor series of a real or complex-valued function "Æ"("x") that is infinitely differentiable at a real or complex number "a" is the power series

which can be written in the more compact sigma notation as

where "n"! denotes the factorial of "n" and "Æ"("a") denotes the "n"th derivative of "Æ" evaluated at the point "a". The derivative of order zero "Æ" is defined to be "Æ" itself and and 0! are both defined to beÂ 1. In the case that , the series is also called a Maclaurin series.

A Taylor series of "f" about point "a" may diverge, converge at only the point "a", converge for all "x" such that formula_296 (the largest such "R" for which convergence is guaranteed is called the "radius of convergence"), or converge on the entire real line. Even a converging Taylor series may converge to a value different from the value of the function at that point. If the Taylor series at a point has a nonzero radius of convergence, and sums to the function in the disc of convergence, then the function is analytic. The analytic functions have many fundamental properties. In particular, an analytic function of a real variable extends naturally to a function of a complex variable. It is in this way that the exponential function, the logarithm, the trigonometric functions and their inverses are extended to functions of a complex variable.

Fourier series decomposes periodic functions or periodic signals into the sum of a (possibly infinite) set of simple oscillating functions, namely sines and cosines (or complex exponentials). The study of Fourier series typically occurs and is handled within the branch mathematics > mathematical analysis > Fourier analysis.

Integration is a formalization of the problem of finding the area bound by a curve and the related problems of determining the length of a curve or volume enclosed by a surface. The basic strategy to solving problems of this type was known to the ancient Greeks and Chinese, and was known as the "method of exhaustion". Generally speaking, the desired area is bounded from above and below, respectively, by increasingly accurate circumscribing and inscribing polygonal approximations whose exact areas can be computed. By considering approximations consisting of a larger and larger ("infinite") number of smaller and smaller ("infinitesimal") pieces, the area bound by the curve can be deduced, as the upper and lower bounds defined by the approximations converge around a common value.

The spirit of this basic strategy can easily be seen in the definition of the Riemann integral, in which the integral is said to exist if upper and lower Riemann (or Darboux) sums converge to a common value as thinner and thinner rectangular slices ("refinements") are considered. Though the machinery used to define it is much more elaborate compared to the Riemann integral, the Lebesgue integral was defined with similar basic ideas in mind. Compared to the Riemann integral, the more sophisticated Lebesgue integral allows area (or length, volume, etc.; termed a "measure" in general) to be defined and computed for much more complicated and irregular subsets of Euclidean space, although there still exist "non-measurable" subsets for which an area cannot be assigned.

The Riemann integral is defined in terms of Riemann sums of functions with respect to tagged partitions of an interval. Let formula_297 be a closed interval of the real line; then a tagged partition formula_298 of formula_297 is a finite sequence

This partitions the interval formula_297 into formula_66 sub-intervals formula_303 indexed by formula_304, each of which is "tagged" with a distinguished point formula_305. For a function formula_30 bounded on formula_297, we define the Riemann sum of formula_30 with respect to tagged partition formula_298 as

where formula_311 is the width of sub-interval formula_312. Thus, each term of the sum is the area of a rectangle with height equal to the function value at the distinguished point of the given sub-interval, and width the same as the sub-interval width. The mesh of such a tagged partition is the width of the largest sub-interval formed by the partition, formula_313. We say that the Riemann integral of formula_30 on formula_297 is formula_316 if for any formula_40 there exists formula_41 such that, for any tagged partition formula_298 with mesh formula_320, we have

This is sometimes denoted formula_322. When the chosen tags give the maximum (respectively, minimum) value of each interval, the Riemann sum is known as the upper (respectively, lower) Darboux sum. A function is Darboux integrable if the upper and lower Darboux sums can be made to be arbitrarily close to each other for a sufficiently small mesh. Although this definition gives the Darboux integral the appearance of being a special case of the Riemann integral, they are, in fact, equivalent, in the sense that a function is Darboux integrable if and only if it is Riemann integrable, and the values of the integrals are equal. In fact, calculus and real analysis textbooks often conflate the two, introducing the definition of the Darboux integral as that of the Riemann integral, due to the slightly easier to apply definition of the former.

The fundamental theorem of calculus asserts that integration and differentiation are inverse operations in a certain sense.

Lebesgue integration is a mathematical construction that extends the integral to a larger class of functions; it also extends the domains on which these functions can be defined. The concept of a measure, an abstraction of length, area, or volume, is central to Lebesgue integral probability theory. 

Distributions (or generalized functions) are objects that generalize functions. Distributions make it possible to differentiate functions whose derivatives do not exist in the classical sense. In particular, any locally integrable function has a distributional derivative.

Real analysis is an area of analysis that studies concepts such as sequences and their limits, continuity, differentiation, integration and sequences of functions. By definition, real analysis focuses on the real numbers, often including positive and negative infinity to form the extended real line. Real analysis is closely related to complex analysis, which studies broadly the same properties of complex numbers. In complex analysis, it is natural to define differentiation via holomorphic functions, which have a number of useful properties, such as repeated differentiability, expressability as power series, and satisfying the Cauchy integral formula.

In real analysis, it is usually more natural to consider differentiable, smooth, or harmonic functions, which are more widely applicable, but may lack some more powerful properties of holomorphic functions. However, results such as the fundamental theorem of algebra are simpler when expressed in terms of complex numbers.

Techniques from the theory of analytic functions of a complex variable are often used in real analysis â such as evaluation of real integrals by residue calculus.

Important results include the BolzanoâWeierstrass and HeineâBorel theorems, the intermediate value theorem and mean value theorem, Taylor's theorem, the fundamental theorem of calculus, the ArzelÃ -Ascoli theorem, the Stone-Weierstrass theorem, Fatou's lemma, and the monotone convergence and dominated convergence theorems.

Various ideas from real analysis can be generalized from the real line to broader or more abstract contexts. These generalizations link real analysis to other disciplines and subdisciplines. For instance, generalization of ideas like continuous functions and compactness from real analysis to metric spaces and topological spaces connects real analysis to the field of general topology, while generalization of finite-dimensional Euclidean spaces to infinite-dimensional analogs led to the concepts of Banach spaces and Hilbert spaces and, more generally to functional analysis. Georg Cantor's investigation of sets and sequence of real numbers, mappings between them, and the foundational issues of real analysis gave birth to naive set theory. The study of issues of convergence for sequences of functions eventually gave rise to Fourier analysis as a subdiscipline of mathematical analysis. Investigation of the consequences of generalizing differentiability from functions of a real variable to ones of a complex variable gave rise to the concept of holomorphic functions and the inception of complex analysis as another distinct subdiscipline of analysis. On the other hand, the generalization of integration from the Riemann sense to that of Lebesgue led to the formulation of the concept of abstract measure spaces, a fundamental concept in measure theory. Finally, the generalization of integration from the real line to curves and surfaces in higher dimensional space brought about the study of vector calculus, whose further generalization and formalization played an important role in the evolution of the concepts of differential forms and smooth (differentiable) manifolds in differential geometry and other closely related areas of geometry and topology.





</doc>
<doc id="26479" url="https://en.wikipedia.org/wiki?curid=26479" title="Richie Benaud">
Richie Benaud

Richard Benaud (; 6 October 1930 â 10 April 2015) was an Australian cricketer who, after his retirement from international cricket in 1964, became a highly regarded commentator on the game.

Benaud was a Test cricket all-rounder, blending leg spin bowling with lower-order batting aggression. Along with fellow bowling all-rounder Alan Davidson, he helped restore Australia to the top of world cricket in the late 1950s and early 1960s after a slump in the early 1950s. In 1958 he became Australia's Test captain until his retirement in 1964. He became the first player to reach 200 wickets and 2,000 runs in Test cricket, arriving at that milestone in 1963.

Gideon Haigh described him as "perhaps the most influential cricketer and cricket personality since the Second World War." In his review of Benaud's autobiography "Anything But", Sri Lankan cricket writer Harold de Andrado wrote: "Richie Benaud possibly next to Sir Don Bradman has been one of the greatest cricketing personalities as player, researcher, writer, critic, author, organiser, adviser and student of the game."

Benaud was born in Penrith, New South Wales, in 1930. He came from a cricket family, with his younger brother John Benaud also going on to become an Australian Test cricketer. His father Louis, a third generation Australian of French Huguenot descent, was a leg spinner who played for Penrith District Cricket Club in Sydney Grade Cricket, gaining attention for taking all twenty wickets in a match against St. Marys for 65 runs. Lou later moved to Parramatta region in western Sydney, and played for Cumberland. Benaud also used to live in Coraki, NSW.

It was here that Richie Benaud grew up, learning how to bowl leg breaks, googlies and topspinners under his father's watch. Educated at Parramatta High School, Benaud made his first grade debut for Cumberland at age 16, primarily as a batsman.

In November 1948, at the age of 18, Benaud was selected for the New South Wales Colts, the state youth team. He scored 47 not out and took 3/37 in an innings win over Queensland. As a specialist batsman, he made his first class debut for New South Wales at the Sydney Cricket Ground against Queensland in the New Year's match of the 1948â49 season. On a green pitch which was struck by a downpour on the opening day, Benaud's spin was not used by Arthur Morris and he failed to make an impression with the bat in his only innings, scoring only two. New South Wales were the dominant state at the time, and vacancies in the team were scarce, particularly as there were no Tests that season and all of the national team players were available for the whole summer.

Relegated to the Second XI after this match, he was struck in the head above the right eye by a ball from Jack Daniel while batting against Victoria in Melbourne, having missed an attempted hook. After 28 X-rays showed nothing, it was finally diagnosed that the crater in his forehead had resulted in a skull fracture and he was sidelined for the remainder of the season, since a second impact could have been fatal. He spent two weeks in hospital for the surgery. This was the only match he played for the second-string state team that summer.

In his early career, Benaud was a batting all-rounder, marked by a looping backlift which made him suspect against fast bowling but allowed him to have a wide attacking stroke range. At the start of the 1949â50 season, he was still in the Second XI, but when the Test players departed for a tour of South Africa soon afterwards, vacancies opened up. Benaud was recalled to the New South Wales First XI in late December for the Christmas and New Year's fixtures. With Ray Lindwall, Keith Miller and Ernie Toshack, three of Australia's leading four bowlers from the 1948 "Invincibles" tour of England unavailable, Benaud bowled heavily in some matches. However, he did not have much success in his five games, taking only five wickets at 54.00.

He took the wicket of Queensland batsman Bill Brown in his third match of the season. Benaud erroneously recalled in an autobiography that this was his maiden wicketâit was his fourthâand described the ball as "the worst I ever bowled". He had more success with the bat, scoring 93 and narrowly missing a century against South Australia. He added another fifty and ended with 250 runs at 31.25.

The next season, England toured Australia, and with the Test players back, Benaud was initially forced out of the team. He was recalled for a match against the Englishmen. He was attacked by the touring batsmen, taking 1/75 from 16.5 overs in his first outing against an international outfit. His only wicket was that of the all-rounder Trevor Bailey. He scored 20 not out and was not called on to bowl in the second innings.

In the next Shield match against Victoria, led by Australian captain Lindsay Hassett, Benaud came in for attack. Hassett was known for his prowess against spin bowling, being the only batsman to score centuries in a match against the leg-spin of Bill O'Reilly, regarded as the finest bowler of his age. Hassett struck 179 in four hours, and took 47 runs from Benaud's seven overs. The young leg spinner claimed Hassett in the second innings when a ball landed in a crack and skidded through onto his foot. He ended with 3/56, the first time he had taken three wickets in a match.

In the next match against South Australia, he made 48, took 4/93 and 1/29 and suffered three dropped catches by the wicketkeeper in successive balls. Benaud was cementing his position and was in the senior team for four consecutive matches even with the Test players available. He was selected for an Australian XI match against England, in what was effectively a trial for Test selection, but suffered a chipped bone in his thumb. This put him out of action until the last match of the season, leaving him with little opportunity to impress the national selectors for his rise to international cricket. Benaud returned and scored 37 and took a total of 2/68 in the final match, ending the season with 184 runs at 36.80 and 11 wickets at 34.63.

The 1951â52 season saw a tour to Australia by the West Indies. Benaud was given a chance against the visiting team when New South Wales played them in Sydney after the First Test. On a green pitch, Benaud came in at 7/96 and featured in a century partnership in only an hour, making 43 himself. The Caribbeans were skittled for 134 in reply and went on to lose the match, although they attacked the young leg-spinner, who took 1/130 in total from 36 overs. Benaud scored his maiden first-class century, 117 against South Australia, in the next match, two years after falling short of the milestone by seven runs. In the next four matches, Benaud passed 15 only once, scoring a 34, and took only seven wickets. Up to this point, in seven matches for the season, the young all-rounder had only scored 307 runs at 27.90 and taken ten wickets at 64.80.

Despite this, Benaud was chosen for his Test debut in the Fifth Test against the West Indies in 1951â52 in Sydney. At this point, Australia had already taken an unassailable 3â1 series lead and decided to try out some young players. Selected as a batsman, he scored 3 and 19. Hassett allowed him to bowl only in the second innings, when nine West Indian wickets had fallen and Australia were on the verge of an inevitable victory. Leading opposition batsman Everton Weekes edged Benaud in his first over, but Gil Langley dropped the catch. Benaud went on to dismiss tail-ender Alf Valentine for his first Test wicket, conceding 14 runs from 4.3 overs. Benaud ended his season with 97 and a total of 3/39 in an innings win over South Australia.

The following Australian season in 1952â53, Benaud started modestly and in the five first-class matches before the Tests, scored 208 runs at 26.00 including a 63 and 69, and 14 wickets at 38.64. This included figures of 2/70 and 4/90 against the touring
South Africa. However, this was not enough to ensure his selection in the First Test, where he was made 12th man. After scoring 60 and 37 and taking 1/60 in an Australian XI against the South Africans following the Test, he was selected for the Second Test. He suffered a smashed gum and a severely cut top lip when a square cut by John Waite in the Third Test against South Africa at the Sydney Cricket Ground hit him in the face while he was fielding at short gully.

Doctors told him he was lucky: it could have broken his cheekbones, jaw or removed his eyesight if it had hit any of the surrounding areas. It could have killed him if it had struck him where his skull was previously fractured. He married after the match and had to mumble his wedding vows through a swathe of bandages. Benaud went on to play in the final four Tests. He made 124 runs at 20.66, making double figures in four of seven innings, but was unable to capitalise on his starts, with a top score of 45. His leg spin yielded ten wickets at 30.60, with a best of 4/118 in the Fourth Test in Adelaide when he was given a heavy workload, totalling 58 overs, when Ray Lindwall and Keith Miller broke down during the match. In another match for New South Wales against the touring team, he took a total of 5/95.

Up to this point, his first-class batting average was below 30 and his bowling average close to 40, and he had never taken more than four wickets in an innings or six in a match.

The selectors persisted in Benaud despite his unproductive Test performances, selecting him for the squad for the 1953 Ashes tour of England. He had been seventh and eighth in the domestic runscoring and wicket-taking aggregates for the season, but was yet to convert this into international performance. He justified their decision prior to the team's departure, scoring 167 not out and taking match figures of 7/137 for the touring team against a Tasmania Combined XI, his wickets including Test batsmen Miller, Ian Craig and Neil Harvey. He also put on 167 in a partnership with Alan Davidson, the first collaboration between the pair, who would later go on to lead Australia's bowling in the last five years of their career. Benaud struck an unbeaten 100 and totalled 1/64 in the next match against Western Australia before the Australians departed for England.

On arrival in the British Isles, Benaud quickly made an impression with both bat and ball. After scoring 44 and taking 2/66 in the opening first-class match against Worcestershire, the all-rounder starred in his next match, against Yorkshire. He scored 97 in Australia's only innings and then took 7/46 in the hosts' first innings as the Australians took an innings win. Although his form with the willow dropped off in his remaining six matches before the Testsâa 35 was his only score beyond 20 in seven attemptsâBenaud continued to strike regularly with the ball. He took 18 wickets in these matches, including 3/20 and 3/37 against Oxford University, 5/13 against Minor Counties and 4/38 against Hampshire. This was enough for him to gain selection for the start of the Tests.

He managed only eight runs in four innings in the first two Tests, and having taken only two wickets for 136 runs was dropped for the Third. This was part of a month-long run in which he made only 123 runs in eight innings and took only seven wickets in four matches. He was recalled immediately for the Fourth Test, but was dropped for the Fifth after managing seven runs in his only innings and going wicketless. He ended the Test series with 15 runs at 3.00 and two wickets at 87.00. It was thought that the surface at the Oval would favour pacemen, but Australia's selection proved to be a blunder as England's spinners took them to the only win of the series, allowing them to regain the Ashes.

He also showed his hitting ability in a tour match against T.N. Pearce's XI at Scarborough. Opening the batting, he struck 135 in 110 minutes in the second innings, including an Australian record of eleven sixes, four of them in one over. In eight first-class matches after his Test campaign was over, Benaud added a further half-century in addition to the century against Pearce's XI, and took 22 more wickets, including 4/20 against the Gentlemen of England.

After returning home from his first overseas tour, Benaud was prolific during the 1953â54 Australian season, which was purely domestic with no touring Test team. He contributed significantly with both bat and ball in New South Wales' Sheffield Shield triumph, the first of nine consecutive titles. In the opening match of the season, he struck 158 and took 5/88 and 1/65 against Queensland. He made another century in the return match, striking 144 not out and taking a total of 2/55. Midway through the season, he played in Morris's XI in a testimonial match for Hassett, who captained the other team. Benaud scored 78 and 68 and took a total of 5/238, his dismissals being Davidson and frontline Test batsmen in a 121-run win. He then finished the summer strongly, and ended the season with 811 runs at 62.38 and 35 wickets at 30.54. Benaud was the only bowler selected for all five Tests of the 1954â55 series when England visited Australia. He secured his place after scoring 125 against Queensland at the start of the season, although his lead-up form in two matches against England for his state and an Australian XI was not encouraging.

At this stage of his career, he had played 13 Tests with mediocre results. Selected as a batsman who could bowl, he had totalled 309 runs at 15.45 without passing 50, and taken 23 wickets at 37.87 with only two four-wicket innings hauls. Even so, he was promoted to vice-captain above several senior players when Ian Johnson and Keith Miller missed the 2nd Test at Sydney through injury and Arthur Morris was made temporary captain. He also made 113 against the touring side for the Prime Minister's XI.

Australia's selectors persisted and selected him for the squad to tour the West Indies in 1954â55. Their faith was rewarded by an improvement in performances. Benaud contributed 46 and match figures of 2/73 in a First Test victory at Kingston. After a draw in the Second Test, he took three wickets in four balls to end with 4/15 in the first innings at Georgetown, Guyana, before scoring 68 (his first Test half century) as Australia moved to a 2â0 series lead. In the Fifth Test at Kingston, he struck a century in 78 minutes, despite taking 15 minutes to score his first run. He ended with 121 and took four wickets in the match as Australia won by an innings and took the series 3â0. Benaud had contributed 246 runs at 41 and taken wickets steadily to total 18 at 26.94.

During the 1956 tour to England, he helped Australia to its only victory in the Lord's Test, when he scored a rapid 97 in the second innings in 143 minutes from only 113 balls. His fielding, in particular at gully and short leg, was consistently of a high standard, in particular his acrobatic catch to dismiss Colin Cowdrey. He was unable to maintain the standards he had set in the West Indies, contributing little apart from the Lord's Test. He ended the series with 200 runs at 25 and eight wickets at 42.5.

Benaud's bowling reached a new level on the return leg of Australia's overseas tour, when they stopped in the Indian subcontinent in 1956â57 en route back to Australia. In a one-off Test against Pakistan in Karachi, he scored 56 and took 1/36 as Australia fell to defeat. He claimed his Test innings best of 7/72 in the first innings of the First Test in Madras, allowing Australia to build a large lead and win by an innings. It was his first five-wicket haul in a Test innings. After taking four wickets in the drawn Second Test in Bombay,

Benaud bowled Australia to victory in the Third Test in Calcutta, sealing the series 2â0. He took 6/52 and 5/53, his best-ever match analysis, ending the series with 113 runs at 18.83 and 24 wickets at 17.66.

It was the first of his successes against India, against whom he took his wickets at an average of 18. This put him in a small group of spinners whose career averages were inferior to their performances against India, generally regarded as the best players of spin in the world. At this stage of his career, he had yet to perform consistently with bat and ball simultaneously, apart from his breakthrough series in the Caribbean. He had managed, in the 14 Tests since then, 559 runs at 27.95 and 67 wickets at 24.98.

After a break in the international calendar of a year, the 1957â58 tour to South Africa heralded the start of a phase of three international seasons when Benaud was at his peak. The tour saw his bowling talents come to the fore when he took 106 wickets, surpassing the previous record of 104 by England's Sydney Barnes. He scored 817 runs including four centuries, two of them in Test matches. The first of these came in the First Test at Johannesburg, where after conceding 1/115, Benaud struck 122, his highest Test score, to see Australia reach a draw.

In the Second Test at Cape Town, Benaud took 4/95 and then 5/49 in the second innings to secure an innings victory after the home team were forced to follow on. He followed this with 5/114 in a drawn Third Test, before a match-winning all round performance in the Fourth Test in Johannesburg. Benaud struck exactly 100 in the first innings, before taking 4/70 in South Africa's reply. When South Africa followed on, Benaud took 5/84, which left Australia needing only one run to win. He took 5/82 in the second innings of the Fifth Test, the fourth consecutive match in which he had taken five wickets in an innings, as Australia took a 3â0 series win. He had been a major contributor to the series win, scoring 329 runs at 54.83 and taking 30 wickets at 21.93, establishing himself as one of the leading leg spinners of the modern era.

When Ian Craig fell ill at the start of the 1958â59 season, Benaud was promoted to the captaincy ahead of vice-captain Neil Harvey. Harvey and Benaud had been captains of their respective states until Harvey moved in the same season for employment purposes from Victoria to New South Wales and became Benaud's deputy. Benaud had little prior leadership experience, and faced the task of recovering the Ashes from an England team which had arrived in Australia as favourites. He led from the front with his bowling, taking match figures of 7/112 in his debut as captain as Australia claimed the First Test in Brisbane. Benaud's men won the Second Test, before he took 5/83 and 4/94 in the drawn Third Test. Benaud produced an all-round performance of 46, 5/91 and 4/82 in the Fourth Test in Adelaide to take an unassailable 3â0 series lead and regain the Ashes, before scoring 64 and match figures of 5/57 to help take the Fifth Test and a 4â0 series result. Benaud contributed 132 runs at 26.4 and 31 wickets at the low average of 18.83, as well as his shrewd and innovative captaincy. According to Neil Harvey, he also was the first captain who started hosting team meetings, a procedure now followed by his successors after he retired.

Benaud then led Australia on its first full tour of the Indian subcontinent, playing three and five Tests against Pakistan and India respectively. Benaud took 4/69 and 4/42 in the First Test in Dacca (now in Bangladesh), sealing Australia's first win in Pakistan. He took four wickets in a Second Test in Lahore that sealed the series 2â0, the last time Australia would win a Test in Pakistan until Mark Taylor's men in 1998, 37 years later. Six further wickets in the drawn Third Test saw Benaud end the series with 84 runs at 28 and 18 wickets at 21.11. Benaud made a strong start to the series against India, taking 3/0 in the first innings of the First Test in Delhi, before a 5/76-second innings haul secured an innings victory. Benaud had less of an impact on the next two Tests, which Australia lost and drew, totaling 6/244. He returned to form with 5/43 and 3/43 as India were defeated by an innings after being forced to follow on in the Fourth Test in Madras. A further seven wickets from the captain in the Fifth Test saw Australia secure a draw and the series 2â1.

Benaud had contributed 91 runs at 15.16 and 29 wickets at 19.59. The first two seasons of the Benaud captaincy had been a resounding success, with Australia winning eight, drawing four and losing only one Test. Benaud's personal form was a major factor in this success. In the previous seasons when he and his team were at their peak, he had scored 636 runs at 31.8 with taken 108 wickets at 20.27 in eighteen Tests, averaging six wickets a match.

Benaud took over when Australian cricket was in a low phase with a young team. His instinctive, aggressive captaincy and daring approach to cricket â and his charismatic nature and public relations ability â revitalised cricket interest in Australia. This was exhibited in the 1960â61 Test series against the visiting West Indians, in which the grounds were packed to greater levels than they are today despite Australia's population doubling since then.

The First Test in Brisbane ended in the first tie in Test history, which came about after Benaud and Alan Davidson, rather than settle for a draw, decided to risk defeat and play an attacking partnership, which took Australia to the brink of victory. Australia had fallen to 6/92 on the final day chasing a target of 233 with Benaud and Davidson at the crease. Australia's chances of winning looked remote when they reached tea at 6/109 with 124 runs still required with only the tailenders to follow. Despite this, Benaud told chairman of selectors Don Bradman that he would still be going for an improbable victory in accordance with his policy of aggression. With an attacking partnership, the pair took Australia to within sight of the target.

Both men were noted for their hitting ability and viewed attack as their most effective chance of survival. Regular boundaries and quickly-run singles took the score to 226, a seventh-wicket partnership of 134. Only seven runs were required with four wickets in hand as time was running short. Benaud hit a ball into the covers and the pair attempted a quick single when a direct hit from Joe Solomon saw Davidson run out. Australia needed six runs from the final over, in which Benaud was caught and the last two wickets fell to run outs while attempting the winning run.

The Test was tied when Solomon ran out Ian Meckiff with a direct hit. Benaud had an unpenetrative match with the ball, taking 1/162. He took 4/107 in a seven-wicket victory in Melbourne, before the West Indies levelled the series with a 22-run win in Sydney. Benaud had a heavy load in the match taking 8/199 after Davidson tore a hamstring mid-match. In Adelaide, with Davidson absent, Benaud bowled long spells to take match figures of 7/207 in addition to a score of 77 in the first innings. With Davidson back, Australia won the final Test by two wickets, after a controversial incident in which Australian wicketkeeper Wally Grout was not given out hit wicket when a bail was dislodged and the umpires did not notice. Australia won the series 2â1, and although Benaud was below his best, scoring at 21.77 and taking 23 wickets at 33.87, the series was a success for cricket. The unprecedented public interest saw the Caribbean touring party farewelled with a ticker-tape parade by the Australian public. Along with the West Indian captain Frank Worrell, Benaud's bold leadership enlivened interest in Test cricket among a public who had increasingly regarded it as boring.

On his third and final tour to England in 1961, he was hampered by damaged tendons in his right shoulder, which forced him to miss the Second Test at Lord's known as the "Battle of the Ridge". In all he missed a third of the matches due to injury. Despite this impairment to his bowling shoulder, his team played with an aggressive strategy leading them to lose only one Test match and no other matches during the tour, honouring his pre-series pledge. The First Test at Edgbaston was drawn with Benaud taking three wickets. After Harvey led the team to victory at Lord's, Benaud had an unhappy return in the Third at Headingley scoring two runs in two innings and taking match figures of 2/108 as Australia lost within three days. With the series balanced at 1â1, the Fourth Test at Old Trafford initially brought no improvement, with Benaud scoring 2 and taking 0/80 in the first innings. He made 1 in the second before a last-wicket partnership between Davidson and Graham McKenzie of 98 yielded a defendable target.

During England's chase on the final afternoon it became apparent that, with Ted Dexter scoring quickly, Australia would lose the Test unless England were bowled out. Benaud went around the wicket and bowled into the footmarks, having Dexter caught behind and then Peter May bowled around his legs. Benaud's 5/13 in 25 balls instigated an English collapse which saw Australia retain the Ashes. He finished the innings with 6/70. Benaud then took four wickets in the drawn Fifth Test to end the series 2â1. Benaud had a poor series with the bat, scoring 45 runs at 9 and taking 15 wickets at 32.53. He finished the first-class tour with 627 runs and 61 wickets at 23.54. He was appointed an OBE in that year and in 1962 was named as one of the "Wisden" Cricketers of the Year.

The 1961â62 Australian season was purely a domestic one, with no touring international team. Benaud led New South Wales throughout a dominant season, winning the Sheffield Shield with 64 of the 80 possible points. Benaud was the leading wicket-taker of the season with 47 at 17.97. His aggressive tactical style brought large crowds throughout the season, with almost 18,000 watching one match against South Australia.

In another match against Victoria, he ordered his team to attempt to score 404 on the final day to take an unlikely victory in accordance with a promise to score at 400 per day. At one stage, New South Wales were six wickets down with less than 150 runs scored, but Benaud refused to attempt to defend for a draw. He made 140, in a seventh-wicket partnership of 255 in just 176 minutes, an Australian record that still stands.

1962â63 saw an English team under Dexter visit Australia. Fred Trueman with 216 Test wickets and Brian Statham with 229 were poised to overtake the record of 236 Test wickets set by the assistant-manager Alec Bedser. Benaud was another contender with 219 wickets, but it was Statham who broke the record (only to be overtaken by Trueman in New Zealand) and Benaud had to be content with breaking Ray Lindwall's Australian record of 228 Test wickets. In an early tour match Benaud took his best first class innings haul of 18â10â18â7 for New South Wales against the MCC, which lost by an innings and 80 runs, the state's biggest win against the English team. Benaud started the series with seven wickets and a half century as the First Test in Brisbane was drawn. This was followed by three unproductive Tests which yielded only 5/360 and a win apiece. Benaud returned to form with match figures of 5/142 and 57 in the Fifth Test at Sydney, which ended in a draw when Benaud ordered Bill Lawry and Peter Burge to play out the last afternoon for a draw that would retain the Ashes. They were booed and heckled as they left the field and Benaud's reputation as a "go ahead" cricket captain was badly tanished. The draw meant that the series was shared 1â1, the first time he had drawn a series after five successive wins. It was another lean series with the ball, Benaud's 17 wickets costing 40.47, the third consecutive series where his wickets cost more than 30. His batting was reliable, with 227 runs at 32.47.

At the start of the 1963â64 season, Benaud announced that it would be his last at first-class level. The first Test of the season, against the touring South Africans, saw high drama as Australia's left arm paceman Ian Meckiff was called for throwing by Colin Egar and removed from the attack by Benaud after one over. Benaud did not bowl Meckiff from the other end, and at the end of the match Meckiff announced his retirement. Benaud took 5/72 and scored 43 in the First Test, but then injured himself in a grade match, so Bob Simpson captained the team for the Second Test and won the match in Benaud's absence.

Upon his return, Benaud advised the Australian Cricket Board that it would be in the better interests of the team if Simpson continued as captain for the remainder of the season. Benaud took 3/116 to complement scores of 43 and 90 on his return in the Third Test in Sydney. His final two Tests saw no fairytale finish, yielding only four wickets and 55 runs. His batting had been steady though with 231 runs at 33, but his bowling unpenetrative with 12 wickets at 37.42.

Benaud was awarded life membership by the New South Wales Cricket Association, but he returned it in protest in 1970 when his younger brother John was removed from the captaincy. In 1967â68 he captained a Commonwealth team against Pakistan, playing in his last five first-class fixtures.

During Benaud's captaincy, Australia did not lose a series, and became the dominant team in world cricket. His success was based on his ability to attack, his tactical boldness and his ability to extract more performance from his players, in particular Davidson. He was known for his unbuttoned shirt, and raised eyebrows with his on-field exuberance. Benaud embraced his players when opposition wickets fell, something that was uncommon at the time. Benaud's bold leadership coupled with his charismatic nature and public relations ability enlivened interest in Test cricket among a public who had increasingly regarded it as boring.

Benaud was not a large spinner of the ball, but he was known for his ability to extract substantial bounce from the surface. In addition to his accurate probing consistency, he possessed a well-disguised googly and topspinner which tricked many batsmen and yielded him many wickets. In his later career, he added the flipper, a combination of the googly and top spinner which was passed to him by Bruce Dooland. Coupled with his subtle variations in flight and angle of the delivery, he kept the batsman under constant pressure. Benaud had the tendency to bowl around the wicket at a time when he was one of the first players to do so; it had an influence on spin bowlers like Shane Warne and Ashley Giles. Benaud was regarded as one of the finest close-fielders of his era, either at gully or in a silly position. As a batsman, he was tall and lithe, known for his hitting power, in particular his lofted driving ability from the front foot.

Johnnie Moyes said "Certainly Benaud received a little help from the roughened patches, but he could do what the off-spinners could not do: he could turn the ball, mostly slowly, sometimes with more life. His control was admirable, and when Benaud gets a batsman in trouble he rarely if ever gives him a loose one. He keeps him pinned down, probing and probing until the victim is well and truly enmeshed."


After the 1956 England tour, Benaud stayed behind in London to take a BBC presenter training course. He took up a journalism position with the "News of the World", beginning as a police roundsman before becoming a sports columnist. In 1960 he made his first radio commentary in the United Kingdom at the BBC, after which he moved into television.

After retiring from playing in 1964, Benaud turned to full-time cricket journalism and commentary, dividing his time between Britain (where he worked for the BBC for many years before joining Channel 4 in 1999), and Australia (for the Nine Network). Overall he played in or commentated on approximately 500 Test matches, as he himself noted in one of his final interviews in Britain when asked if he would miss Test cricket.

He openly criticized the actions by the Chappell brothers (Trevor and Greg) in the post-match reaction to the underarm bowling incident of 1981, proving his moral integrity far outweighed his unconditional patriotism for Australia. He vacated the commentary booth when New Zealand was about to clinch a test victory at Lord's in 1999, allowing former New Zealand captain-turned-commentator Ian Smith to call the famous victory of his compatriots. Some of his other memorable moments he commentated on included Shane Warne's "Ball of the Century", Ian Botham's dominant all-round display during 1981 Ashes, Dennis Lillee overtaking Benaud's record for most wickets, and subsequent 300th and 310th wickets, and Andrew Symonds' tackle on a streaker.

The idea for what became his trademarkâwearing a cream or white jacket during live commentaryâcame from Channel 9 owner Kerry Packer, who suggested the look to help Benaud stand out from the rest of the commentary team.

He also helped to design a computer-based parody of himself available for download off Channel 4's website called "Desktop Richie". It was developed by the software company Turtlez Ltd. Having downloaded this, cricket fans would be treated to live Test match updates and weather reports from a cartoon version of Benaud with real voice samples such as "Got 'im!" and "That's stumpsÂ ... and time for a glass of something chilled". On Channel 4's live commentary, Benaud often made sarcastic comments regarding the advertisement of Desktop Richie.

In 2004, Benaud starred in a series of television advertisements for the Australian Tourism Commission, aimed at promoting Australia as a tourist destination. Benaud's ad featured him in various scenic locations uttering his signature comment, "Marvellous!". It was also emulated by New Zealand broadcaster John Campbell. He appeared in "Richie Benaud's Greatest XI", a video in which he chooses his own team.

Benaud became a staunch advocate of cricket being available on free-to-view TV. He chose to end his British commentary career, which spanned more than 42 years, when the rights to broadcast live Test match cricket were lost by Channel 4 to the subscription broadcaster British Sky Broadcasting. Thus, the 2005 Ashes series was the last that Benaud commentated on in Britain. His final commentary came near the end of the final day of the Fifth Test at the Oval. His last goodbye was interrupted by Glenn McGrath taking Kevin Pietersen's wicket; Benaud simply wove his description of the dismissal into what he was already saying. Benaud stated he would spend the Northern Hemisphere summer in Britain writing, and would continue working for the Nine Network in Australia.

Benaud commentated for the BBC TV highlights of the 2006â07 Ashes in Australia as part of his continuing commentary work for Australia's Nine Network.

Benaud's distinctive speaking style has been frequently parodied on the Australian comedy series "Comedy Inc." and "The Twelfth Man". In the case of the latter, comedian Billy Birmingham's impersonations of Benaud on The Twelfth Man comedy recordings have become very successful, spanning more than twenty years. Chris Barrie of "Red Dwarf" fame incorporated impressions of Benaud into his stand-up repertoire.

On 18 February 2009, during a radio interview, Benaud announced that he would be retiring from television commentary. Benaud said: "I'll be doing Australian cricket next yearâ2010âbut I don't do any television at all anywhere else now and when I finish next year, then I'll be doing other thingsÂ ... But that'll be no more television commentary".

It was announced on 15 November 2009, that Benaud had signed a three-year contract with the Nine Network to continue being part of their cricket coverage until 2013, although his role would change from that of ball-by-ball commentary. Benaud said: "I won't be doing live commentary any more." Someone asked me, "Does that mean you'll never again go into the commentary box?", "Well, the answer to that", Benaud replied, "If there is, as there always can be, some emergency or a sensational happening on or off the field where it would be quite ridiculous not to go into the commentary box, of course I'll be in there doing my job and doing it as professionally as I can. But I won't be on the live commentary roster. But I will be doing all sorts of, what I regard as, interesting things for Channel Nine on the cricketâspecial features on the cricketÂ ...". Richie commentated regularly during the 2011â12 season and was part of Nine's commentating team/roster.

Benaud married Marcia Lavender in 1953 and had two sons, Greg and Jeffery, from this marriage; he divorced Marcia in 1967. In 1967, he married his second wife, Daphne Surfleet, who had worked for the English cricket writer E. W. Swanton. Benaud and Daphne often stayed at their holiday home in Beaulieu-sur-Mer on the French Riviera.

On 29 October 2008, Benaud's mother, Irene, died, aged 104. He said of her, "She improved my love of vegetables by introducing the phrase, 'You can't go out and play cricket until you have eaten all your vegetables.'"

In October 2013 Benaud crashed his vintage 1965 Sunbeam Alpine into a wall while driving near his home in Coogee, a beachside suburb in Sydney's east. He sustained a cracked sternum and shoulder injuries. Slow recovery meant he was unable to commentate for Australia's Channel Nine during the 2013â14 Ashes series.

Benaud had last handed "Baggy green" caps to Simon Katich and Mitchell Starc when they made their test debut, but Benaud's own was lost early in his test career, and former captain, now commentator and Director of Cricket Australia, Mark Taylor was to present the replacement cap to him at the semi-final of the 2015 Cricket World Cup between Australia and India at the SCG, but Benaud was too unwell to attend, and when the cap arrived at Channel 9 headquarters, it was the day before Benaud died. It was presented to his wife.

In November 2014, at age 84, Benaud announced that he had been diagnosed with skin cancer. He died in his sleep on 10 April 2015.
Prime Minister Tony Abbott offered his family a state funeral but his widow, Daphne, declined, respecting his wishes for a private funeral.

Benaud was buried on 15 April, in a private funeral ceremony attended only by his immediate family. Later that same day, there was a commemoration service officiated by former teammate turned lay preacher Brian Booth; attendees included his family and close friends, among them former players Shane Warne and Ian Chappell, and then Australian Test captain Michael Clarke.

Benaud was made an Officer of the Order of the British Empire (OBE) in 1961 for services to cricket. He was inducted into the Sport Australia Hall of Fame in 1985. In 1999 he was awarded a Logie Award for Most Outstanding Sports Broadcaster.

In 2007, he was inducted into the Australian Cricket Hall of Fame at the Allan Border Medal award evening and in 2009 he was inducted into the ICC Cricket Hall of Fame.

In November 2015, Benaud became an honouree at Bradman Foundation, having been a long-serving patron in his life. After rain interrupted the 2016 SCG Test against West Indies, the second day unofficially became Richie Benaud Day as 501 Benaud impersonators stayed at the SCG, which is a day before the annual Jane McGrath Day for Breast Cancer awareness and fundraising, which was again rained out.ã

In 2017, the Australian Mint issued a 50-cent coin commemorating Benaud.

In October 2018, Benaud became the 40th Legend in the Sport Australia Hall of Fame.

Benaud wrote a number of books:




</doc>
<doc id="26480" url="https://en.wikipedia.org/wiki?curid=26480" title="Radio Research Project">
Radio Research Project

The Radio Research Project was a social research project funded by the Rockefeller Foundation to look into the effects of mass media on society.

In 1937, the Rockefeller Foundation started funding research to find the effects of new forms of mass media on society, especially radio. Several universities joined up and a headquarters was formed at the School of Public and International Affairs at Princeton University. 

Among the subjects of the Project's first studies were soap operas, known as radio dramas at the time.

The Radio Project also conducted research on the infamous Halloween broadcast of "The War of the Worlds" in 1938. Of the estimated 6 million people who heard this broadcast, they found that 25% accepted the program's reports of mass destruction. The majority of these did not think they were hearing a literal invasion from Mars, but rather an attack by Germany. The researchers determined that radio broadcasts from the Munich Crisis may have lent credence to this supposition.

A third research project was that of listening habits. Because of this, a new method was developed to survey an audience â this was dubbed the Little Annie Project. The official name was the Stanton-Lazarsfeld Program Analyzer. This allowed one not only to find out if a listener liked the performance, but how they felt at any individual moment, through a dial which they would turn to express their preference (positive or negative). This has since become an essential tool in focus group research.

Theodor Adorno produced numerous reports on the effects of "atomized listening" which radio supported and of which he was highly critical. However, because of profound methodological disagreements with Lazarsfeld over the use of techniques such as listener surveys and "Little Annie" (Adorno thought both grossly simplified and ignored the degree to which expressed tastes were the result of commercial marketing), Adorno left the project in 1941.


</doc>
<doc id="26484" url="https://en.wikipedia.org/wiki?curid=26484" title="Religious pluralism">
Religious pluralism

Religious pluralism is an attitude or policy regarding the diversity of religious belief systems co-existing in society. It can indicate one or more of the following:


Religious pluralism, to paraphrase the title of a recent academic work, goes beyond mere toleration. Chris Beneke, in "Beyond Toleration: The Religious Origins of American Pluralism", explains the difference between religious tolerance and religious pluralism by pointing to the situation in the late 18th century United States. By the 1730s, in most colonies religious minorities had obtained what contemporaries called religious toleration: "The policy of toleration relieved religious minorities of some physical punishments and some financial burdens, but it did not make them free from the indignities of prejudice and exclusion. Nor did it make them equal. Those 'tolerated' could still be barred from civil offices, military positions, and university posts." In short, religious toleration is only the absence of religious persecution, and does not necessarily preclude religious discrimination. However, in the following decades something extraordinary happened in the Thirteen Colonies, at least if one views the events from "a late eighteenth-century perspective". Gradually the colonial governments expanded the policy of religious toleration, but then, between the 1760s and the 1780s, they replaced it with "something that is usually called religious liberty". Mark Silka, in "Defining Religious Pluralism in America: A Regional Analysis", states that Religious pluralism "enables a country made up of people of different faiths to exist without sectarian warfare or the persecution of religious minorities. Understood differently in different times and places, it is a cultural construct that embodies some shared conception of how a country's various religious communities relate to each other and to the larger nation whole."

Religious pluralism can be defined as "respecting the otherness of others". Freedom of religion encompasses all religions acting within the law in a particular region. Exclusivist religions teach that theirs is the only way to salvation and to religious truth, and some of them would even argue that it is necessary to suppress the falsehoods taught by other religions. Some Protestant sects argue fiercely against Roman Catholicism, and fundamentalist Christians of all kinds teach that religious practices like those of Paganism and witchcraft are pernicious. This was a common historical attitude prior to the Enlightenment, and has appeared as governmental policy into the present day under systems like Afghanistan's Taliban regime, which destroyed the ancient Buddhas of Bamyan.

Giving one religion or denomination special rights that are denied to others can weaken religious pluralism. This situation was observed in Europe through the Lateran Treaty and Church of England. In modern era, many Islamic countries have laws that criminalize the act of leaving Islam to someone born in Muslim family, forbid entry to non-Muslims into Mosques, and forbid construction of Church, Synagogue or Temples inside their countries.

Relativism, the belief that all religions are equal in their value and that none of the religions give access to absolute truth, is an extreme form of inclusivism. Likewise, syncretism, the attempt to take over creeds of practices from other religions or even to blend practices or creeds from different religions into one new faith is an extreme form of inter-religious dialogue. Syncretism must not be confused with ecumenism, the attempt to bring closer and eventually reunite different denominations of one religion that have a common origin but were separated by a schism.

Cultural and religious pluralism has a long history and development that reaches from antiquity to contemporary trends in post-modernity.

German philosophers of religion Ludwig Feuerbach and Ernst Troeltsch concluded that Asian religious traditions, in particular Hinduism and Buddhism, were the earliest proponents of religious pluralism and granting of freedom to the individuals to choose their own faith and develop a personal religious construct within it (see also Relationship between Buddhism and Hinduism); Jainism, another ancient Indian religion, as well as Daoism have also always been inclusively flexible and have long favored religious pluralism for those who disagree with their religious viewpoints. The Age of Enlightenment in Europe triggered a sweeping transformation about religion after the French Revolution (liberalism, democracy, civil and political rights, freedom of thought, separation of Church and State, secularization), with rising acceptance of religious pluralism and decline of Christianity. According to Chad Meister, these pluralist trends in the Western thought, particularly since the 18th century, brought mainstream Christianity and Judaism closer to the Asian traditions of philosophical pluralism and religious tolerance.

BahÃ¡'u'llÃ¡h, founder of BahÃ¡'Ã­ Faith, a religion that developed in Persia, though not a sect of Islam, urged the elimination of religious intolerance. He taught that God is one, and has manifested himself to humanity through several historic messengers. BahÃ¡'u'llÃ¡h taught that BahÃ¡'Ã­s must associate with peoples of all religions, showing the love of God in relations with them, whether this is reciprocated or not.

BahÃ¡'Ã­'s refer to the concept of Progressive revelation, which means that God's will is revealed to mankind progressively as mankind matures and is better able to comprehend the purpose of God in creating humanity. In this view, God's word is revealed through a series of messengers: Abraham, Krishna, Moses, Buddha, Jesus, Muhammad, and BahÃ¡'u'llÃ¡h (the founder of the BahÃ¡'Ã­ Faith) among them. In the "KitÃ¡b-i-ÃqÃ¡n" ("Book of Certitude"), BahÃ¡'u'llÃ¡h explains that messengers of God have a twofold station, one of divinity and one of an individual. According to BahÃ¡'Ã­ writings, there will not be another messenger for many hundreds of years. There is also a respect for the religious traditions of the native peoples of the planet who may have little other than oral traditions as a record of their religious figures.

The earliest reference to Buddhist views on religious pluralism in a political sense is found in the Edicts of Emperor Ashoka:

All religions should reside everywhere, for all of them desire self-control and purity of heart. Rock Edict Nb7 (S. Dhammika)

Contact (between religions) is good. One should listen to and respect the doctrines professed by others. Beloved-of-the-Gods, King Piyadasi, desires that all should be well-learned in the good doctrines of other religions. Rock Edict Nb12 (S. Dhammika)

When asked, "Donât all religions teach the same thing? Is it possible to unify them?" the Dalai Lama said:
People from different traditions should keep their own, rather than change. However, some Tibetan may prefer Islam, so he can follow it. Some Spanish prefer Buddhism; so follow it. But think about it carefully. Donât do it for fashion. Some people start Christian, follow Islam, then Buddhism, then nothing.

In the United States I have seen people who embrace Buddhism and change their clothes! Like the New Age. They take something Hindu, something Buddhist, something, somethingâ¦ That is not healthy.

For individual practitioners, having one truth, one religion, is very important. Several truths, several religions, is contradictory.

I am Buddhist. Therefore, Buddhism is the only truth for me, the only religion. To my Christian friend, Christianity is the only truth, the only religion. To my Muslim friend, [Islam] is the only truth, the only religion. In the meantime, I respect and admire my Christian friend and my Muslim friend. If by unifying you mean mixing, that is impossible; useless.
For the Romans, religion was part of the daily life. Each home had a household shrine at which prayers and libations to the family's domestic deities were offered. Neighborhood shrines and sacred places such as springs and groves dotted the city. The Roman calendar was structured around religious observances; in the Imperial Era, as many as 135 days of the year were devoted to religious festivals and games ("ludi)". Women, slaves, and children all participated in a range of religious activities. Some public rituals could be conducted only by women, and women formed what is perhaps Rome's most famous priesthood, the state-supported Vestal Virgins, who tended Rome's sacred hearth for centuries, until disbanded under Christian persecution and domination.

The Romans are known for the great number of deities they honored. The presence of Greeks on the Italian peninsula from the beginning of the historical period influenced Roman culture, introducing some religious practices that became as fundamental as the cult of Apollo. The Romans looked for common ground between their major gods and those of the Greeks, adapting Greek myths and iconography for Latin literature and Roman art. Etruscan religion was also a major influence, particularly on the practice of augury, since Rome had once been ruled by Etruscan kings.

Mystery religions imported from the Near East (Ptolemaic Egypt, Persia and Mesopotamia), which offered initiates salvation through a personal God and eternal life after the death, were a matter of personal choice for an individual, practiced in addition to carrying on one's family rites and participating in public religion. The mysteries, however, involved exclusive oaths and secrecy, conditions that conservative Romans viewed with suspicion as characteristic of "magic", conspiracy ("coniuratio"), and subversive activity. Sporadic and sometimes brutal attempts were made to suppress religionists who seemed to threaten traditional Roman morality and unity, as with the Senate's efforts to restrict the Bacchanals in 186 BC.
As the Romans extended their dominance throughout the Mediterranean world, their policy in general was to absorb the deities and cults of other peoples rather than try to eradicate them, since they believed that preserving tradition promoted social stability.

One way that Rome incorporated diverse peoples was by supporting their religious heritage, building temples to local deities that framed their theology within the hierarchy of Roman religion. Inscriptions throughout the Empire record the side-by-side worship of local and Roman deities, including dedications made by Romans to local Gods. By the height of the Empire, numerous international deities were cultivated at Rome and had been carried to even the most remote provinces (among them Cybele, Isis, Osiris, Serapis, Epona), and Gods of solar monism such as Mithras and Sol Invictus, found as far north as Roman Britain. Because Romans had never been obligated to cultivate one deity or one cult only, religious tolerance was not an issue in the sense that it is for competing monotheistic religions. The monotheistic rigor of Judaism posed difficulties for Roman policy that led at times to compromise and the granting of special exemptions, but sometimes to intractable conflict.

Some Christians have argued that religious pluralism is an invalid or self-contradictory concept.

Maximal forms of religious pluralism claim that all religions are equally true, or that one religion can be true for some and another for others. Most Christians hold this idea to be logically impossible from the Principle of contradiction. The two largest Christian branches, the Catholic Church and the Orthodox Church, both claim to be the "one true church" and that "outside the true Church there is no salvation"; Protestantism however, which has many different denominations, has no consistent doctrine in this regard, and has a variety of different positions regarding religious pluralism.

Other Christians have held that there can be truth value and salvific value in other faith traditions. John Macquarrie, described in the "Handbook of Anglican Theologians" (1998) as "unquestionably Anglicanism's most distinguished systematic theologian in the second half of the twentieth century", wrote that "there should be an end to proselytizing but that equally there should be no syncretism of the kind typified by the Baha'i movement" (p.Â 2). In discussing 9 founders of major faith traditions (Moses, Zoroaster, Lao-zu, Buddha, Confucius, Socrates, Krishna, Jesus, and Muhammad), which he called "mediators between the human and the divine", Macquarrie wrote that:

I do not deny for a moment that the truth of God has reached others through other channels - indeed, I hope and pray that it has. So while I have a special attachment to one mediator, I have respect for them all. (p. 12)
The Church of Jesus Christ of Latter-day Saints also teaches a form of religious pluralism, that there is at least some truth in almost all religions and philosophies.

Before the Great Schism, mainstream Christianity confessed "one holy catholic and apostolic church", in the words of the Nicene Creed. Roman Catholics, Orthodox Christians, Episcopalians and most Protestant Christian denominations still maintain this belief. Furthermore, the Catholic Church makes the claim that it alone is the one and only true Church founded by Jesus Christ, but the Eastern Orthodox and Oriental Orthodox Churches also make this claim in respect to themselves.

Church unity for these groups, as in the past, is something very visible and tangible, and schism was just as serious an offense as heresy. Following the Great Schism, Roman Catholicism sees and recognizes the Orthodox Sacraments as valid but illicit and without canonical jurisdiction. Eastern Orthodoxy does not have the concept of "validity" when applied to Sacraments, but it considers the "form" of Roman Catholic Sacraments to be acceptable, and there is some recognition of Catholic sacraments among some, but not all, Orthodox. Both generally mutually regard each other as "heterodox" and "schismatic", while continuing to recognize each other as Christian, at least secundum quid. (See ecumenicism).

Some other Protestants hold that only believers who believe in certain fundamental doctrines know the true pathway to salvation. The core of this doctrine is that Jesus Christ was a perfect man, is the Son of God and that he died and rose again for the wrongdoing of those who will accept the gift of salvation. They continue to believe in "one" church, an "invisible church" which encompasses different types of Christians in different sects and denominations, believing in certain issues they deem fundamental, while disunited on a variety of doctrines they deem non-fundamental. Some evangelical Protestants are doubtful if Roman Catholics or Eastern Orthodox can possibly be members of this "invisible church", and usually they reject religious (typically restorationist) movements rooted in 19th century American Christianity, such as Mormonism, Christian Science, or Jehovah's Witnesses as not distinctly Christian.

Hinduism is naturally pluralistic. A well-known "Rig Vedic" hymn says: "Truth is One, though the sages know it variously" ("Ãkam sat vipra bahudÄ vadanti"). Similarly, in the "Bhagavad GÄ«tÄ" (4:11), God, manifesting as an incarnation, states: "As people approach me, so I receive them. All paths lead to me" ("ye yathÄ mÄá¹ prapadyante tÄá¹s tathÄiva bhajÄmyaham mama vartmÄnuvartante manuá¹£yÄá¸¥ pÄrtha sarvaÅaá¸¥"). The Hindu religion has no theological difficulties in accepting degrees of truth in other religions. According to Swami Bhaskarananda, Hinduism emphasizes that everyone actually worships the same God, whether one knows it or not.

While some claim that religious pluralism is controversial in Islam, Islamic civilizations have been characterized as one of the most religiously pluralist. The primary sources that guide Islam, namely Quran and hadiths, promote the fundamental right to practise an individual's belief, even though it may be a false belief. The acceptability of religious pluralism within Islam remains a topic of active debate, however the vast majority of Islamic scholars and historical evidences reveal Islam's commitment to no coercion in religion, supporting pluralism.

In several Surah, Quran asks Muslims to remain steadfast with Islam, and not yield to the vain desires of other religions and unbelievers. These verses have been interpreted to imply pluralism in religions. For example, Surah Al-Ma'idah verses 47 through 49 state:
Surah Al-Ankabut verse 45 through 47 state:
Surah Al-E-Imran verses 62 through 66 state:
Surah Al-Kafiroon verse 1 through 6 state:
Several verses of the Quran state that Islam rejects religious pluralism. For example, Surah Al-Tawba verse 1 through 5 seems to command the Muslim to slay the pagans (with verse 9.5 called the 'sword verse'): 
However, this verse has been explained.
Bernard Lewis presents some of his conclusions about Islamic culture, Shari'a law, jihad, and the modern day phenomenon of terrorism in his text, "Islam: The Religion and the People". He writes of jihad as a distinct "religious obligation", but suggests that "it is a pity" that people engaging in terrorist activities are not more aware of their own religion:
In Surah Al-Tawba, verse 29 demands Muslims to fight all those who do not believe in Islam, including Christians and Jews (People of the Book), until they pay the Jizya, a tax, with willing submission.
Some people have concluded from verse 9:29, that Muslims are commanded to attack all non-Muslims until they pay money, but Shaykh Jalal Abualrub writes:
In Surah Al-Nisa, verse 89 has been misquoted to seem that it says to slay the apostates. In actuality, it only commands Muslims to fight those who practice oppression or persecution, or attack the Muslims.

The Sufis were practitioners of the esoteric mystic traditions within an Islam at a certain point. Sufism is defined by the Sufi master or Pir (Sufism) or fakeer or Wali in the language of the people by dancing and singing and incorporating various philosophies, theologies, ideologies and religions together (e.g., Christianity, Judaism, Paganism, Platonism, Zoroastrianism, Buddhism, Hinduism, Sikhism and so forth with time). Famous Sufi masters are Rumi, Shadhili, Sheikh Farid, Bulleh Shah, Shah Hussain, Shams Tabrizi, Waris Shah, Ghazali, Mian Mir, Attar of Nishapur, Amir Khusrow, Salim Chishti. See many more famous Sufis at the List of Sufis. The Sufis were considered by many to have divine revelations with messages of peace, tolerance, equality, pluralism, love for all and hate for no one, humanitarians, philosophers, psychologists and much more. Many had the teaching if you want to change the world, change yourself and you will change the whole world. The views of the Sufi poets, philosophers and theologians have inspired multiple forms of modern-day academia as well as philosophers of other religions. See also Blind men and an elephant. But undoubtedly, the most influential Sufi scholar to have embraced the world is Jalaluddin Muhammad Rumi. He was born in 1207 AD in a northern province of Afghanistan, however, he later had to seek refuge in Turkey following the invasion of Afghanistan by Mongols. Rumi, through his poetry and teachings, propagated inter-faith harmony like none other. He served as a uniting figure for people of different faiths and his followers included Muslims, Christians and Jews. Even today, Rumiâs popularity does not cease to exist within the Sufi Muslim community and his message of peace and harmony transcends religious and geographical boundaries.

Rumi says:

I looked for God. I went to a temple, and I didn't find him there. Then I went to a church, and I didn't find him there. And then I went to a mosque, and I didn't find him there. And then finally I looked in my heart, and there he was. 
Rumi also says:

How many paths are there to God? There are as many paths to God as there are souls on the Earth.
Rumi also says:
A true Lover doesn't follow any one religion,
be sure of that.
Since in the religion of Love,
there is no irreverence or faith.
When in Love,
body, mind, heart and soul don't even exist.
Become this,
fall in Love,
and you will not be separated again.

Ahmadis recognize many founders of world religions to be from God, who all brought teaching and guidance from God to all peoples. According to the Ahmadiyya understanding of the Quran, every nation in the history of mankind has been sent a prophet, as the Quran states: "And there is a guide for every people". Though the Quran mentions only 24 prophets, the founder of Islam, Muhammad states that the world has seen 124,000 prophets. Thus other than the prophets mentioned in the Quran, Ahmadis, with support from theological study also recognize Buddha, Krishna, founders of Chinese religions to be divinely appointed individuals.

The Second Khalifatul Maish of the Ahmadiyya Muslim Community writes:
"According to this teaching there has not been a single people at any time in history or anywhere in the world who have not had a warner from God, a teacher, a prophet. According to the Quran there have been prophets at all times and in all countries. India, China, Russia, Afghanistan, parts of Africa, Europe, Americaâall had prophets according to the theory of divine guidance taught by the Quran. When, therefore, Muslims hear about prophets of other peoples or other countries, they do not deny them. They do not brand them as liars. Muslims believe that other peoples have had their teachers. If other peoples have had prophets, books, and laws, these constitute no difficulty for Islam."

Mirza Ghulam Ahmad, founder of the Ahmadiyya Muslim Community wrote in his book "A Message of Peace":
"Our God has never discriminated between one people and another. This is illustrated by the fact that all the potentials and capabilities (Prophets) which have been granted to the Aryans (Hindus) have also been granted to the races inhabiting Arabia, Persia, Syria, China, Japan, Europe and America."

Religious pluralism is a contested issue in modern Islamic countries. Twenty three (23) Islamic countries have laws, as of 2014, which make it a crime, punishable with death penalty or prison, for a Muslim, by birth or conversion, to leave Islam or convert to another religion. In Muslim countries such as Algeria, it is illegal to preach, persuade or attempt to convert a Muslim to another religion. Saudi Arabia and several Islamic nations have strict laws against the construction of Christian churches, Jewish synagogues, Hindu temples and Buddhist stupas anywhere inside the country, by anyone including minorities working there. Brunei in southeast Asia adopted Sharia law in 2013 that prescribes a death penalty for any Muslim who converts from Islam to another religion. Other Islamic scholars state Sharia does not allow non-Muslim minorities to enjoy religious freedoms in a Muslim-majority nation, but other scholars disagree.

"AnekÄntavÄda", the principle of relative pluralism, is one of the basic principles of Jainism. In this view, the truth or the reality is perceived differently from different points of view, and no single point of view is the complete truth. Jain doctrine states that an object has infinite modes of existence and qualities and they cannot be completely perceived in all its aspects and manifestations, due to inherent limitations of the humans. Only the Kevalinsâthe omniscient beingsâcan comprehend the object in all its aspects and manifestations, and all others are capable of knowing only a part of it. Consequently, no one view can claim to represent the absolute truthâonly relative truths. Jains compare all attempts to proclaim absolute truth with " andhgajnyaya " or the "maxim of the blind men and elephant", wherein all the blind men claimed to explain the true appearance of the elephant, but could only partly succeed due to their narrow perspective. For Jains, the problem with the blind men is not that they claim to explain the true appearance of the elephant; the problem is doing so to the exclusion of all other claims. Since absolute truth is many-sided, embracing any truth to the exclusion of others is to commit the error of "ekÄnta" (one-sidedness). Openness to the truths of others is one way in which Jainism embodies religious pluralism.

The Mosaic law categorically warns the Jews to refrain from polytheism. First and the second commandment, you shall not have another God except me, worship your God with all your heart and with all your soul. Throughout the Hebrew Bible the sovereignty of Yahweh as the only God is the key pillar of a chosen community of Israel.

The Sikh gurus have propagated the message of "many paths" leading to the one God and ultimate salvation for all souls who treading on the path of righteousness. They have supported the view that proponents of all faiths, by doing good and virtuous deeds and by remembering the Lord, can certainly achieve salvation. Sikhs are told to accept all leading faiths as possible vehicles for attaining spiritual enlightenment, provided the faithful study, ponder and practice the teachings of their prophets and leaders. Sikhism had many interactions with Sufism as well as Hinduism, influenced them and was influenced by them.

The Sri Guru Granth Sahib, the holy book of the Sikhs, says:
As well as:

Some call the Lord "Ram, Ram", and some "Khuda". Some serve Him as "Gusain", others as "Allah". He is the Cause of causes, and Generous. He showers His Grace and Mercy upon us. Some pilgrims bathe at sacred shrines, others go on Hajj to Mecca. Some do devotional worship, whilst others bow their heads in prayer. Some read the Vedas, and some the Koran. Some wear blue robes, and some wear white. Some call themselves Muslim, and some call themselves Hindu. Some yearn for paradise, and others long for heaven. Says Nanak, one who realizes the Hukam of God's Will, knows the secrets of his Lord Master. (Sri Guru Granth Sahib Page:885) 

One who recognizes that all spiritual paths lead to the One shall be emancipated. One who speaks lies shall fall into hell and burn. In all the world, the most blessed and sanctified are those who remain absorbed in Truth. (SGGS Ang 142) 

The seconds, minutes, and hours, days, weeks and months and various seasons originate from One Sun; O nanak, in just the same way, the many forms originate from the Creator. (Guru Granth Sahib page 12,13)
The Guru Granth Sahib also says that Bhagat Namdev and Bhagat Kabir, who were both believed to be Hindus, both attained salvation though they were born before Sikhism took root and were clearly not Sikhs. This highlights and reinforces the Guru's saying that "peoples of other faiths" can join with God as true and also at the same time signify that Sikhism is not the exclusive path for liberation.

Additionally the Guru Granth Sahib says:

First, Allah (God) created the Light; then, by His Creative Power, He made all mortal beings. From the One Light, the entire universe welled up. So who is good, and who is bad? ||1|| 

Again, the Guru Granth Sahib Ji provides this verse:

Naam Dayv the printer, and Kabeer the weaver, obtained salvation through the Perfect Guru. Those who know God and recognize His Shabad ("word") lose their ego and class consciousness. (Guru Granth Sahib page 67) 
Most of the 15 Sikh Bhagats who are mentioned in their holy book were non-Sikhs and belonged to Hindu and Muslim faiths, which were the most prevalent religions of this region.

The pluralistic dialogue of Sikhism began with the founder of Sikhism Guru Nanak after becoming enlightened saying the words "Na koi hindu na koi musalman" - "There is no Hindu, there is no Muslim". He recognised that religious labels held no value and it is the deeds of human that will be judged in the hereafter what we call ourselves religiously holds no value.

Sikhs have been considered eager exponents of interfaith dialogue and not only accept the right of others to practice their faith but have in the past fought and laid down their lives to protect this right for others; the Martyrdom of Guru Tegh Bahadar, who on the pleas of a pandit of the Kashmiris, agreed to fight against a tyrannic Moghul Empire (that was forcing them to convert to Islam) in order that they might gain the freedom to practice their religion, which differed from his own.

The concept of religious pluralism is also relevant to human service professions, such as psychology and social work, as well as medicine and nursing, in which trained professionals may interact with clients from diverse faith traditions. For example, psychologist Kenneth Pargament has described four possible stances toward client religious and spiritual beliefs, which he called "rejectionist", "exclusivist", "constructivist", and "pluralist". Unlike the constructivist stance, the pluralist stance:
...recognizes the existence of a religious or spiritual absolute reality but allows for multiple interpretations and paths toward it. In contrast to the exclusivist who maintains that there is a single path "up the mountain of God," the pluralist recognizes many paths as valid. Although both the exclusivist and the pluralist may agree on the existence of religious or spiritual reality, the pluralist recognizes that this reality is expressed in different cultures and by different people in different ways. Because humans are mortal and limited, a single human religious system cannot encompass all of the religious or spiritual absolute reality... (p. 167)
Importantly, "the pluralistic therapist can hold personal religious beliefs while appreciating those of a client with different religious beliefs. The pluralist recognizes that religious value differences can and will exist between counselors and clients without adversely affecting therapy" (p.Â 168). The stances implied by these four helping orientations on several key issues, such as "should religious issues be discussed in counseling?", have also been presented in tabular form (p.Â 362, Table 12.1).

The profession of chaplaincy, a religious profession, must also deal with issues of pluralism and the relevance of a pluralistic stance. For example, Friberg (2001) argues: "With growing populations of immigrants and adherents of religions not previously seen in significant numbers in North America, spiritual care must take religion and diversity seriously. Utmost respect for the residents' spiritual and religious histories and orientations is imperative" (p.Â 182).










</doc>
<doc id="26485" url="https://en.wikipedia.org/wiki?curid=26485" title="Calendar-based contraceptive methods">
Calendar-based contraceptive methods

Calendar-based methods are various methods of estimating a woman's likelihood of fertility, based on a record of the length of previous menstrual cycles. Various methods are known as the KnausâOgino method and the rhythm method. The standard days method is also considered a calendar-based method, because when using it, a woman tracks the days of her menstrual cycle without observing her physical fertility signs. The standard days method is based on a fixed formula taking into consideration the timing of ovulation, the functional life of the sperm and the ovum, and the resulting likelihood of pregnancy on particular days of the menstrual cycle. These methods may be used to achieve pregnancy by timing unprotected intercourse for days identified as fertile, or to avoid pregnancy by avoiding unprotected intercourse during fertile days.

The first formalized calendar-based method was developed in 1930 by John Smulders, a Roman Catholic physician from the Netherlands. It was based on knowledge of the menstrual cycle. This method was independently discovered by Hermann Knaus (Austria), and Kyusaku Ogino (Japan). This system was a main form of birth control available to Catholic couples for several decades, until the popularization of symptoms-based fertility awareness methods. A new development in calendar-based methods occurred in 2002, when Georgetown University introduced the Standard Days Method. The Standard Days Method is promoted in conjunction with a product called CycleBeads, a ring of colored beads which are meant to help the user keep track of her fertile and non-fertile days.

Some sources may treat the terms "rhythm method" and "fertility awareness" as synonymous. However, fertility awareness is usually used as a broad term that includes tracking basal body temperature and cervical mucus as well as cycle length. The World Health Organization considers the rhythm method to be a specific type of calendar-based method, and calendar-based methods to be only one form of fertility awareness.

More effective than calendar-based methods, systems of fertility awareness that track basal body temperature, cervical mucus, or both, are known as symptoms-based methods. Teachers of symptoms-based methods take care to distance their systems from the poor reputation of the rhythm method. Many consider the rhythm method to have been obsolete for at least 20 years, and some even exclude calendar-based methods from their definition of fertility awareness.

Some sources may treat the terms "rhythm method" and "natural family planning" as synonymous. In the early 20th century, the calendar-based method known as the "rhythm method" was promoted by members of the Roman Catholic Church as the only morally acceptable form of family planning. Methods accepted by this church are referred to as natural family planning (NFP): so at one time, the term "the rhythm method" was synonymous with NFP. Today, NFP is an umbrella term that includes symptoms-based fertility awareness methods and the lactational amenorrhea method as well as calendar-based methods such as rhythm. This overlap between uses of the terms "the rhythm method" and "natural family planning" may contribute to confusion.

The term "the rhythm method" is sometimes used, in error, to describe the behavior of any people who have unprotected vaginal intercourse, yet wish to avoid pregnancy.

The first day of bleeding is considered day one of the menstrual cycle.

It is not known if historical cultures were aware of what part of the menstrual cycle is most fertile. In the year 388, Augustine of Hippo wrote of periodic abstinence. Addressing followers of Manichaeism, his former religion, he said, "Is it not you who used to counsel us to observe as much as possible the time when a woman, after her purification, is most likely to conceive, and to abstain from cohabitation at that time...?" If the Manichaieans practiced something like the Jewish observances of menstruation, then the "time... after her purification" would have indeed been when "a woman... is most likely to conceive." Over a century previously, however, the influential Greek physician Soranus had written that "the time directly before and after menstruation" was the most fertile part of a woman's cycle; this inaccuracy was repeated in the 6th century by the Byzantine physician AÃ«tius. Similarly, a Chinese sex manual written close to the year 600 stated that only the first five days following menstruation were fertile. Some historians believe that Augustine, too, incorrectly identified the days immediately after menstruation as the time of highest fertility.

Written references to a "safe period" do not appear again for over a thousand years. Scientific advances prompted a number of secular thinkers to advocate periodic abstinence to avoid pregnancy: in the 1840s it was discovered that many animals ovulate during estrus. Because some animals (such as dogs) have a bloody discharge during estrus, it was assumed that menstruation was the corresponding most fertile time for women. This inaccurate theory was popularized by physicians Bischoff, FÃ©lix ArchimÃ¨de Pouchet, and Adam Raciborski. In 1854, an English physician named George Drysdale correctly taught his patients that the days near menstruation are the "least" fertile, but this remained the minority view for the remainder of the 19th century.

In 1905 Theodoor Hendrik van de Velde, a Dutch gynecologist, showed that women only ovulate once per menstrual cycle. In the 1920s, Kyusaku Ogino, a Japanese gynecologist, and Hermann Knaus, from Austria, working independently, each made the discovery that ovulation occurs about fourteen days before the next menstrual period. Ogino used his discovery to develop a formula for use in aiding infertile women to time intercourse to achieve pregnancy.

In 1930, Johannes Smulders, a Roman Catholic physician from the Netherlands, used Knaus and Ogino's discoveries to create a method for "avoiding" pregnancy. Smulders published his work with the Dutch Roman Catholic medical association, and this was the official rhythm method promoted over the next several decades. In 1932 a Catholic physician, Dr. Leo J Latz, published a book titled "The Rhythm of Sterility and Fertility in Women" describing the method, and the 1930s also saw the first U.S. Rhythm Clinic (founded by John Rock) to teach the method to Catholic couples.

In the first half of the 20th century, most users of the rhythm method were Catholic; they were following their church's teaching that all other methods of birth control were sinful. In 1968 the encyclical "Humanae vitae" included the statement, "It is supremely desirable... that medical science should by the study of natural rhythms succeed in determining a sufficiently secure basis for the chaste limitation of offspring." This is interpreted as favoring the then-new, more reliable symptoms-based fertility awareness methods over the rhythm method. Currently, many fertility awareness teachers consider the rhythm method to have been obsolete for at least 20 years.

New attention was drawn to calendar-based methods in 2002, when the Institute for Reproductive Health at Georgetown University introduced the Standard Days Method. Designed to be simpler to teach and use than the older rhythm method, the Standard Days Method is being successfully integrated into family planning programs worldwide.

Most menstrual cycles have several days at the beginning that are infertile (pre-ovulatory infertility), a period of fertility, and then several days just before the next menstruation that are infertile (post-ovulatory infertility). The first day of red bleeding is considered day one of the menstrual cycle. To use these methods, a woman is required to know the length of her menstrual cycles.

Imperfect use of calendar-based methods would consist of not correctly tracking the length of the woman's cycles, thus using the wrong numbers in the formula, or of having unprotected intercourse on an identified fertile day. The discipline required to keep accurate records of menstrual cycles, and to abstain from unprotected intercourse, makes imperfect use fairly common. The typical-use failure rate of calendar-based methods is 25% per year.

To find the estimated length of the pre-ovulatory infertile phase, nineteen (19) is subtracted from the length of the woman's shortest cycle. To find the estimated start of the post-ovulatory infertile phase, ten (10) is subtracted from the length of the woman's longest cycle. A woman whose menstrual cycles ranged in length from 30 to 36 days would be estimated to be infertile for the first 11 days of her cycle (30-19=11), to be fertile on days 12-25, and to resume infertility on day 26 (36-10=26). When used to avoid pregnancy, the rhythm method has a perfect-use failure rate of up to 9% per year.

Developed by Georgetown University's Institute for Reproductive Health, the Standard Days Method has a simpler rule set and is more effective than the rhythm method. A product called CycleBeads was developed alongside the method to help the user keep track of estimated high and low fertility points during her menstrual cycle. The Standard Days Method may only be used by women whose cycles are usually between 26 and 32 days in length. In this system:

When used to avoid pregnancy, the Standard Days Method has been claimed to have perfect-use efficacy of 95+% and typical-use efficacy of 88%. However, independent researchers have shown that these figures are probably too optimistic and its efficacy is likely to be much lower.

Several web-based implementations of the cycle method exist, as well as mobile apps such as Natural Cycles.

The Standard Days method (SDM) is increasingly being introduced as part of family planning programs in developing countries. The method is satisfactory for many women and men. The low cost of the method may also enable it to play a useful role in countries that lack funding to provide other methods of birth control.

One concern related to the use of calendar-based methods is their relatively high failure rate, compared to other methods of birth control. Even when used perfectly, calendar-based methods, especially the rhythm method, result in a high pregnancy rate among couples intending to avoid pregnancy. Of commonly known methods of birth control, only the cervical cap and contraceptive sponge have comparably high failure rates. This lower level of reliability of calendar-based methods is because their formulas make several assumptions that are not always true.

The postovulatory (luteal) phase has a normal length of 12 to 16 days, and the rhythm method formula assumes all women have luteal phase lengths within this range. However, many women have shorter luteal phases, and a few have longer luteal phases. For these women, the rhythm method formula incorrectly identifies a few fertile days as being in the infertile period.

Calendar-based methods use records of past menstrual cycles to predict the length of future cycles. However, the length of the pre-ovulatory phase can vary significantly, depending on the woman's typical cycle length, stress factors, medication, illness, menopause, breastfeeding, and whether she is just coming off hormonal contraception. If a woman with previously regular cycles has a delayed ovulation due to one of these factors, she will still be fertile when the method tells her she is in the post-ovulatory infertile phase. If she has an unusually early ovulation, calendar-based methods will indicate she is still in the pre-ovulatory infertile phase when she has actually become fertile.

Finally, calendar-based methods assume that all bleeding is true menstruation. However, mid-cycle or anovulatory bleeding can be caused by a number of factors. Incorrectly identifying bleeding as menstruation will cause the method's calculations to be incorrect.

It has been suggested that pregnancies resulting from method failures of periodic abstinence methods are at increased risk of miscarriage and birth defects due to aged gametes at the time of conception. Other research suggests that timing of conception has no effect on miscarriage rates, low birth weight, or preterm delivery.

Luc Bovens has suggested that unprotected intercourse in the infertile periods of the menstrual cycle may still result in conceptions, but create zygotes incapable of implanting. Bovens maintains that, if one defines abortion to include any destruction of fertilized eggs, then the use of the rhythm method probably results in a large number of abortions.


</doc>
<doc id="26487" url="https://en.wikipedia.org/wiki?curid=26487" title="Roulette">
Roulette

Roulette is a casino game named after the French word meaning "little wheel". In the game, players may choose to place bets on either a single number, various groupings of numbers, the colors red or black, whether the number is odd or even, or if the numbers are high (19â36) or low (1â18).

To determine the winning number and color, a croupier spins a wheel in one direction, then spins a ball in the opposite direction around a tilted circular track running around the outer edge of the wheel. The ball eventually loses momentum, passes through an area of deflectors, and falls onto the wheel and into one of 37 (single zero French/European style roulette) or 38 (double zero American style roulette) colored and numbered pockets on the wheel. The winnings are then paid to anyone who has placed a successful bet.

The first form of roulette was devised in 18th century France. Many historians believe Blaise Pascal introduced a primitive form of roulette in the 17th century in his search for a perpetual motion machine. The roulette mechanism is a hybrid of a gaming wheel invented in 1720 and the Italian game Biribi.

The game has been played in its present form since as early as 1796 in Paris. An early description of the roulette game in its current form is found in a French novel "La Roulette, ou le Jour" by Jaques Lablee, which describes a roulette wheel in the Palais Royal in Paris in 1796. The description included the house pockets, "There are exactly two slots reserved for the bank, whence it derives its sole mathematical advantage." It then goes on to describe the layout with, "...two betting spaces containing the bank's two numbers, zero and double zero". The book was published in 1801. An even earlier reference to a game of this name was published in regulations for New France (QuÃ©bec) in 1758, which banned the games of "dice, hoca, faro, and roulette".

The roulette wheels used in the casinos of Paris in the late 1790s had red for the single zero and black for the double zero. To avoid confusion, the color green was selected for the zeros in roulette wheels starting in the 1800s.

In 1843, in the German spa casino town of Bad Homburg, fellow Frenchmen FranÃ§ois and Louis Blanc introduced the single "0" style roulette wheel in order to compete against other casinos offering the traditional wheel with single and double zero house pockets.

In some forms of early American roulette wheels, there were numbers 1 through 28, plus a single zero, a double zero, and an American Eagle. The Eagle slot, which was a symbol of American liberty, was a house slot that brought the casino extra edge. Soon, the tradition vanished and since then the wheel features only numbered slots. According to Hoyle "the single 0, the double 0, and eagle are never bars; but when the ball falls into either of them, the banker sweeps every thing upon the table, except what may happen to be bet on either one of them, when he pays twenty-seven for one, which is the amount paid for all sums bet upon any single figure".
In the 19th century, roulette spread all over Europe and the US, becoming one of the most famous and most popular casino games. When the German government abolished gambling in the 1860s, the Blanc family moved to the last legal remaining casino operation in Europe at Monte Carlo, where they established a gambling mecca for the elite of Europe. It was here that the single zero roulette wheel became the premier game, and over the years was exported around the world, except in the United States where the double zero wheel had remained dominant.
In the United States, the French double zero wheel made its way up the Mississippi from New Orleans, and then westward. It was here, because of rampant cheating by both operators and gamblers, that the wheel was eventually placed on top of the table to prevent devices being hidden in the table or wheel, and the betting layout was simplified. This eventually evolved into the American-style roulette game. The American game was developed in the gambling dens across the new territories where makeshift games had been set up, whereas the French game evolved with style and leisure in Monte Carlo.

During the first part of the 20th century, the only casino towns of note were Monte Carlo with the traditional single zero French wheel, and Las Vegas with the American double zero wheel. In the 1970s, casinos began to flourish around the world. By 2008, there were several hundred casinos worldwide offering roulette games. The double zero wheel is found in the U.S., Canada, South America, and the Caribbean, while the single zero wheel is predominant elsewhere.

In 2016, The Venetian Las Vegas introduced the first triple-zero wheel, which has since spread to a few additional casinos.

The sum of all the numbers on the roulette wheel (from 0 to 36) is 666, which is the "Number of the Beast".

Roulette players have a variety of betting options. Placing inside bets is either selecting the exact number of the pocket the ball will land in, or a small range of pockets based on their proximity on the layout. Players wishing to bet on the 'outside' will select bets on larger positional groupings of pockets, the pocket color, or whether the winning number is odd or even. The payout odds for each type of bet are based on its probability.

The roulette table usually imposes minimum and maximum bets, and these rules usually apply separately for all of a player's inside and outside bets for each spin. For inside bets at roulette tables, some casinos may use separate roulette table chips of various colors to distinguish players at the table. Players can continue to place bets as the ball spins around the wheel until the dealer announces "no more bets" or "rien ne va plus".
When a winning number and color is determined by the roulette wheel, the dealer will place a marker, also known as a dolly, on that winning number on the roulette table layout. When the dolly is on the table, no players may place bets, collect bets, or remove any bets from the table. The dealer will then sweep away all other losing bets either by hand or rake, and determine all of the payouts to the remaining inside and outside winning bets. When the dealer is finished making payouts, the marker is removed from the board where players collect their winnings and make new bets. The winning chips remain on the board.

In 2004, California legalized a form of roulette known as California Roulette. By law, the game must use cards and not slots on the roulette wheel to pick the winning number.

The pockets of the roulette wheel are numbered from 0 to 36.

In number ranges from 1 to 10 and 19 to 28, odd numbers are red and even are black. In ranges from 11 to 18 and 29 to 36, odd numbers are black and even are red.

There is a green pocket numbered 0 (zero). In American roulette, there is a second green pocket marked 00. Pocket number order on the roulette wheel adheres to the following clockwise sequence in most casinos:


The cloth-covered betting area on a roulette table is known as the "layout". The layout is either single-zero or double-zero. The European-style layout has a single zero, and the American style layout is usually a double-zero. The American-style roulette table with a wheel at one end is now used in most casinos. The French style table with a wheel in the centre and a layout on either side is rarely found outside of Monte Carlo.

In roulette, bets can either be inside or outside bets.

Outside bets typically have smaller payouts with better odds at winning. Except as noted, all of these bets lose if a zero comes up.

In the United Kingdom, the farthest outside bets (low/high, red/black, even/odd) result in the player losing only half of his/her bet if a zero comes up.

The expected value of a $1 bet (except for the special case of Top line bets), for American and European roulette, can be calculated as

where "n" is the number of pockets in the wheel. The initial bet is returned in addition to the mentioned payout. It can be easily demonstrated that this payout formula would lead to a zero expected value of profit if there were only 36 numbers. Having 37 or more numbers gives the casino its edge.

Note that Top line (0, 00, 1, 2, 3) has a different expected value because of approximation of the correct -to-1 payout obtained by the formula to 6-to-1. Note also that 0 and 00 are not odd or even, or high or low.

En prison rules, when used, reduce the house advantage.

The "house average" or "house edge or house advantage" (also called the expected value) is the amount the player loses relative for any bet made, on average. If a player bets on a single number in the American game there is a probability of that the player wins 35 times the bet, and a chance that the player loses his bet. The expected value is:

For European roulette, a single number wins and loses :

For triple-zero wheels, a single number wins and loses :

The presence of the green squares on the roulette wheel and on the table is technically the only house edge. Outside bets will always lose when a single or double zero comes up. However, the house also has an edge on inside bets because the pay outs (including the original player's bet) are always set at 36 to 1 when you mathematically have a 1 out of 38 (1 out of 37 for French/European roulette) chance at winning a straight bet on a single number. To demonstrate the house edge on inside bets, imagine placing straight $1 wagers on all inside numbers (including 0 and 00) to assure a win: you would only get back $36, having spent $38. The only exceptions are the five numbers bet where the house edge is considerably higher (7.89% on an American wheel), and the "even money" bets in some European games (French Roulette) where the house edge is halved because only half the stake is lost when a zero comes up. This is commonly called the "la partage" rule, and it is considered the main difference between European and French roulette. There is also a modification of this rule, which is called the "en prison" rule. These rules cut the house edge into half (1.35%) in French roulette, when playing even-money bets, as half of the even-money bets are given back to the player if the zero is drawn in the wheel.

The house edge should not be confused with the "hold". The hold is the average percentage of the money originally brought to the table that the player loses before he leavesâthe actual "win" amount for the casino. The Casino Control Commission in Atlantic City releases a monthly report showing the win/hold amounts for each casino. The average win/hold for double zero wheels is between 21% to 30%, significantly more than the 5.26% house edge. This reflects the fact that the player is churning the same money over and over again. A 23.6% hold, for example, would imply that, on average, the player bets the total he brought to the table five times, as 23.6% is approximately equal to . For example, a player with $100 making $10 bets on red (which has a near 50/50 chance of winning) is highly unlikely to lose all his money after only 10 bets, and will most likely continue to bet until he has lost all of his money or decides to leave. A player making $10 bets on a single number (with only 1/38 chance of success) with a $100 bankroll is far more likely to lose all of his money after only 10 bets.

In the early frontier gambling saloons, the house would set the odds on roulette tables at 27 for 1. This meant that on a $1 bet you would get $27 and the house would keep your initial dollar. Today most casino odds are set by law, and they have to be either 34 to 1 or 35 to 1. This means that the house pays you $34 or $35 and you get to keep your original $1 bet.

As an example, we can examine the European roulette model, that is, roulette with only one zero. Since this roulette has 37 cells with equal odds of hitting, this is a final model of field probability formula_2, where formula_3, formula_4 for all formula_5.

Call the bet formula_6 a triple formula_7, where formula_8 is the set of chosen numbers, formula_9 is the size of the bet, and formula_10 determines the return of the bet.

The rules of European roulette have 10 types of bets. First we can examine the 'Straight Up' bet. In this case, formula_11, for some formula_12, and formula_13 is determined by
The bet's expected net return, or profitability, is equal to
Without details, for a bet, black (or red), the rule is determined as
and the profitability
For similar reasons it is simple to see that the profitability is also equal for all remaining types of bets. formula_18.

In reality this means that, the more bets a player makes, the more he is going to lose independent of the strategies (combinations of bet types or size of bets) that he employs:
Here, the profit margin for the roulette owner is equal to approximately 2.7%. Nevertheless, several roulette strategy systems have been developed despite the losing odds. These systems can not change the odds of the game in favor of the player.

It is worth noting that the odds for the player in American roulette are even worse, as the bet profitability is at worst formula_20, and never better than formula_21.

For a roulette wheel with formula_22 green numbers and 36 other unique numbers the chance of the ball landing on a given number is formula_23. For a betting option with formula_24 numbers that define a win, the chance of winning a bet is formula_25

For example, betting on "red", there are 18 red numbers, formula_26, the chance of winning is formula_27.

The payout given by the casino for a win is based on the roulette wheel having 36 outcomes and the payout for a bet is given by formula_28.

For example, betting on 1-12 there are 12 numbers that define a win, formula_29, the payout is formula_30, so the better wins 3 times their bet.

The average return on a player's bet is given by formula_31

For formula_32 the average return is always lower than 1 so on average a player will lose money.
With 1 green number formula_33 the average return is formula_34, that is, after a bet the player will on average have formula_34 of their original bet returned to them.
With 2 green numbers formula_36 the average return is formula_37.

This shows that the expected return is independent of the choice of bet.

Although most often named "call bets" technically these bets are more accurately referred to as "announced bets". The legal distinction between a "call bet" and an "announced bet" is that a "call bet" is a bet called by the player without him placing any money on the table to cover the cost of the bet. In many jurisdictions (most notably the United Kingdom) this is considered gambling on credit and is illegal in some jurisdictions around the world. An "announced bet" is a bet called by the player for which he immediately places enough money to cover the amount of the bet on the table, prior to the outcome of the spin or hand in progress being known.

There are different number series in roulette that have special names attached to them. Most commonly these bets are known as "the French bets" and each covers a section of the wheel. For the sake of accuracy, zero spiel, although explained below, is not a French bet, it is more accurately "the German bet". Players at a table may bet a set amount per series (or multiples of that amount). The series are based on the way certain numbers lie next to each other on the roulette wheel. Not all casinos offer these bets, and some may offer additional bets or variations on these.

This is a name, more accurately "grands voisins du zÃ©ro", for the 17 numbers that lie between 22 and 25 on the wheel, including 22 and 25 themselves. The series is 22-18-29-7-28-12-35-3-26-0-32-15-19-4-21-2-25 (on a single-zero wheel).

Nine chips or multiples thereof are bet. Two chips are placed on the 0-2-3 trio; one on the 4-7 split; one on 12-15; one on 18-21; one on 19-22; two on the 25-26-28-29 corner; and one on 32-35.

Zero game, also known as zero spiel ("Spiel" is German for game or play), is the name for the numbers closest to zero. All numbers in the zero game are included in the voisins, but are placed differently. The numbers bet on are 12-35-3-26-0-32-15.

The bet consists of four chips or multiples thereof. Three chips are bet on splits and one chip straight-up: one chip on 0-3 split, one on 12-15 split, one on 32-35 split and one straight-up on number 26.

This type of bet is popular in Germany and many European casinos. It is also offered as a 5-chip bet in many Eastern European casinos. As a 5-chip bet, it is known as "zero spiel naca" and includes, in addition to the chips placed as noted above, a straight-up on number 19.

This is the name for the 12 numbers that lie on the opposite side of the wheel between 27 and 33, including 27 and 33 themselves. On a single-zero wheel, the series is 27-13-36-11-30-8-23-10-5-24-16-33. The full name (although very rarely used, most players refer to it as "tiers") for this bet is "le tiers du cylindre" (translated from French into English meaning one third of the wheel) because it covers 12 numbers (placed as 6 splits), which is as close to of the wheel as one can get.

Very popular in British casinos, tiers bets outnumber voisins and orphelins bets by a massive margin.

Six chips or multiples thereof are bet. One chip is placed on each of the following splits: 5-8, 10-11, 13-16, 23-24, 27-30, and 33-36.

The tiers bet is also called the "small series" and in some casinos (most notably in South Africa) "series 5-8".

A variant known as "tiers 5-8-10-11" has an additional chip placed straight up on 5, 8, 10, and 11m and so is a 10-piece bet. In some places the variant is called "gioco Ferrari" with a straight up on 8, 11, 23 and 30, the bet is marked with a red G on the racetrack.

These numbers make up the two slices of the wheel outside the tiers and voisins. They contain a total of 8 numbers, comprising 17-34-6 and 1-20-14-31-9.

Five chips or multiples thereof are bet on four splits and a straight-up: one chip is placed straight-up on 1 and one chip on each of the splits: 6-9, 14-17, 17-20, and 31-34.

A number may be backed along with the two numbers on the either side of it in a 5-chip bet. For example, "0 and the neighbors" is a 5-chip bet with one piece straight-up on 3, 26, 0, 32, and 15. Neighbors bets are often put on in combinations, for example "1, 9, 14, and the neighbors" is a 15-chip bet covering 18, 22, 33, 16 with one chip, 9, 31, 20, 1 with two chips and 14 with three chips.

Any of the above bets may be combined, e.g. "orphelins by 1 and zero and the neighbors by 1". The "...and the neighbors" is often assumed by the croupier.

Another bet offered on the single-zero game is "final", "finale" or "finals".

Final 4, for example, is a 4-chip bet and consists of one chip placed on each of the numbers ending in 4, that is 4, 14, 24, and 34. Final 7 is a 3-chip bet, one chip each on 7, 17, and 27. Final bets from final 0 (zero) to final 6 cost four chips. Final bets 7, 8 and 9 cost three chips.

Some casinos also offer split-final bets, for example final 5-8 would be a 4-chip bet, one chip each on the splits 5-8, 15-18, 25-28, and one on 35.

A complete bet places all of the inside bets on a certain number. Full complete bets are most often bet by high rollers as "maximum bets".

The maximum amount allowed to be wagered on a single bet in European roulette is based on a progressive betting model. If the casino allows a maximum bet of $1,000 on a 35-to-1 straight-up, then on each 17-to-1 split connected to that straight-up, $2,000 may be wagered. Each 8-to-1 corner that covers four numbers) may have $4,000 wagered on it. Each 11-to-1 street that covers three numbers may have $3,000 wagered on it. Each 5-to-1 six-line may have $6,000 wagered on it. Each $1,000 incremental bet would be represented by a marker that is used to specifically identify the player and the amount bet.

For instance, if a patron wished to place a full complete bet on 17, the player would call "17 to the maximum". This bet would require a total of 40 chips, or $40,000. To manually place the same wager, the player would need to bet:

The player calls his bet to the croupier (most often after the ball has been spun) and places enough chips to cover the bet on the table within reach of the croupier. The croupier will immediately announce the bet (repeat what the player has just said), ensure that the correct monetary amount has been given while simultaneously placing a matching marker on the number on the table and the amount wagered.

The payout for this bet if the chosen number wins is 392 chips, in the case of a $1000 straight-up maximum, $40,000 bet, a payout of $392,000. The player's wagered 40 chips, as with all winning bets in roulette, are still his property and in the absence of a request to the contrary are left up to possibly win again on the next spin.

Based on the location of the numbers on the layout, the number of chips required to "complete" a number can be determined.

Most typically (Mayfair casinos in London and other top-class European casinos) with these "maximum" or "full complete" bets, nothing (except the aforementioned maximum button) is ever placed on the layout even in the case of a win. Experienced gaming staff, and the type of customers playing such bets, are fully aware of the payouts and so the croupier simply makes up the correct payout, announces its value to the table inspector (floor person in the U.S.) and the customer, and then passes it to the customer, but only after a verbal authorization from the inspector has been received.

Also typically at this level of play (house rules allowing) the experienced croupier caters to the needs of the customer and will most often add the customer's winning bet to the payout, as the type of player playing these bets very rarely bets the same number two spins in succession. For example, the winning 40-chip / $40,000 bet on "17 to the maximum" pays 392 chips / $392,000. The experienced croupier would pay the player 432 chips / $432,000, that is 392 + 40, with the announcement that the payout "is with your bet down".

There are also several methods to determine the payout when a number adjacent to a chosen number is the winner, for example, player bets 40 chips on "23 to the maximum" and number 26 is the winning number. The most notable method is known as the "station" system or method. When paying in stations, the dealer counts the number of ways or stations that the winning number hits the complete bet. In the example above, 26 hits 4 stations - 2 different corners, 1 split and 1 six-line. The dealer takes the number 4, multiplies it by 30 and adds the remaining 8 to the payout: 4 Ã 30 = 120, 120 + 8 = 128. If calculated as stations, they would just multiply 4 by 36, making 144 with the players bet down.

In some casinos, a player may bet full complete for less than the table straight-up maximum, for example, "number 17 full complete by $25" would cost $1000, that is 40 chips each at $25 value.

Over the years, many people have tried to beat the casino, and turn rouletteâa game designed to turn a profit for the houseâinto one on which the player expects to win. Most of the time this comes down to the use of betting systems, strategies which say that the house edge can be beaten by simply employing a special pattern of bets, often relying on the "Gambler's fallacy", the idea that past results are any guide to the future (for example, if a roulette wheel has come up 10 times in a row on red, that red on the next spin is any more or less likely than if the last spin was black).

All betting systems that rely on patterns, when employed on casino edge games will result, on average, in the player losing money. In practice, players employing betting systems may win, and may indeed win very large sums of money, but the losses (which, depending on the design of the betting system, may occur quite rarely) will outweigh the wins. Certain systems, such as the Martingale, described below, are extremely risky, because the worst-case scenario (which is mathematically certain to happen, at some point) may see the player chasing losses with ever-bigger bets until he runs out of money.

The American mathematician Patrick Billingsley said that no betting system can convert a subfair game into a profitable enterprise.
At least in the 1930s, some professional gamblers were able to consistently gain an edge in roulette by seeking out rigged wheels (not difficult to find at that time) and betting opposite the largest bets.

Whereas betting systems are essentially an attempt to beat the fact that a geometric series with initial value of 0.95 (American roulette) or 0.97 (European roulette) will inevitably over time tend to zero, engineers instead attempt to overcome the house edge through predicting the mechanical performance of the wheel, most notably by Joseph Jagger at Monte Carlo in 1873. These schemes work by determining that the ball is more likely to fall at certain numbers. If effective, they raise the return of the game above 100%, defeating the betting system problem.

Edward O. Thorp (the developer of card counting and an early hedge-fund pioneer) and Claude Shannon (a mathematician and electronic engineer best known for his contributions to information theory) built the first wearable computer to predict the landing of the ball in 1961. This system worked by timing the ball and wheel, and using the information obtained to calculate the most likely octant where the ball would fall. Ironically, this technique works best with an unbiased wheel though it could still be countered quite easily by simply closing the table for betting before beginning the spin.

In 1982, several casinos in Britain began to lose large sums of money at their roulette tables to teams of gamblers from the USA. Upon investigation by the police, it was discovered they were using a legal system of biased wheel-section betting. As a result of this, the British roulette wheel manufacturer John Huxley manufactured a roulette wheel to counteract the problem.

The new wheel, designed by George Melas, was called "low profile" because the pockets had been drastically reduced in depth, and various other design modifications caused the ball to descend in a gradual approach to the pocket area. In 1986, when a professional gambling team headed by Billy Walters won $3.8 million using the system on an old wheel at the Golden Nugget in Atlantic City, every casino in the world took notice, and within one year had switched to the new low-profile wheel.

Thomas Bass, in his book "The Eudaemonic Pie" (1985) (published as "The Newtonian Casino" in Britain), has claimed to be able to predict wheel performance in real time. The book describes the exploits of a group of University of California Santa Cruz students, who called themselves "the Eudaemons", who in the late 1970s used computers in their shoes to win at roulette. This is an updated and improved version of Edward O. Thorp's approach, where Newtonian
Laws of Motion are applied to track the roulette ball's deceleration; hence the British title.

In the early 1990s, Gonzalo Garcia-Pelayo believed that casino roulette wheels were not perfectly random, and that by recording the results and analysing them with a computer, he could gain an edge on the house by predicting that certain numbers were more likely to occur next than the 1-in-36 odds offered by the house suggested. This he did at the Casino de Madrid in Madrid, Spain, winning 600,000 euros in a single day, and one million euros in total. Legal action against him by the casino was unsuccessful, it being ruled that the casino should fix its wheel.

To defend against exploits like these, many casinos use tracking software, use wheels with new designs, rotate wheel heads, and randomly rotate pocket rings.

At the Ritz London casino in March 2004, two Serbs and a Hungarian used a laser scanner hidden inside a mobile phone linked to a computer to predict the sector of the wheel where the ball was most likely to drop. They netted Â£1.3m in two nights. They were arrested and kept on police bail for nine months, but eventually released and allowed to keep their winnings as they had not interfered with the casino equipment.

The numerous even-money bets in roulette have inspired many players over the years to attempt to beat the game by using one or more variations of a martingale betting strategy, wherein the gambler doubles the bet after every loss, so that the first win would recover all previous losses, plus win a profit equal to the original bet. The problem with this strategy is that, remembering that past results do not affect the future, it is possible for the player to lose so many times in a row, that the player, doubling and redoubling his bets, either runs out of money or hits the table limit. A large financial loss is certain in the long term if the player continued to employ this strategy. Another strategy is the Fibonacci system, where bets are calculated according to the Fibonacci sequence. Regardless of the specific progression, no such strategy can statistically overcome the casino's advantage, since the expected value of each allowed bet is negative.

The LabouchÃ¨re System is a progression betting strategy like the martingale but does not require the gambler to risk his stake as quickly with dramatic double-ups. The Labouchere System involves using a series of numbers in a line to determine the bet amount, following a win or a loss. Typically, the player adds the numbers at the front and end of the line to determine the size of the next bet. When he wins, he crosses out numbers and continues working on the smaller line. If he loses, then he adds his previous bet to the end of the line and continues to work on the longer line. This is a much more flexible progression betting system and there is much room for the player to design his initial line to his own playing preference.

This system is one that is designed so that when the player has won over a third of his bets (less than the expected 18/38), he will win. Whereas the martingale will cause ruin in the event of a long sequence of successive losses, the LabouchÃ¨re system will cause bet size to grow quickly even where a losing sequence is broken by wins. This occurs because as the player loses, the average bet size in the line increases.

As with all other betting systems, the average value of this system is negative.

The system, also called "montant et demontant" (from French, meaning upwards and downwards), is often called a pyramid system. It is based on a mathematical equilibrium theory devised by a French mathematician of the same name. Like the martingale, this system is mainly applied to the even-money outside bets, and is favored by players who want to keep the amount of their bets and losses to a minimum. The betting progression is very simple: After each loss, you add one unit to the next bet, and after each win, one unit is deducted from the next bet. Starting with an initial bet of, say, 1 unit, a loss would raise the next bet to 2 units. If this is followed by a win, the next bet would be 1 units.

This betting system relies on the gambler's fallacyâthat the player is more likely to lose following a win, and more likely to win following a loss.

There are numerous other betting systems that rely on this fallacy, or that attempt to follow 'streaks' (looking for patterns in randomness), varying bet size accordingly.

Many betting systems are sold online and purport to enable the player to 'beat' the odds. One such system was advertised by Jason Gillon of Rotherham, UK, who claimed you could 'earn Â£200 daily' by following his betting system, described as a 'loophole'. As the system was advertised in the UK press, it was subject to Advertising Standards Authority regulation, and following a complaint, it was ruled by the ASA that Mr. Gillon had failed to support his claims you could earn Â£200 daily, and that he had failed to show that there was any loophole.








</doc>
<doc id="26488" url="https://en.wikipedia.org/wiki?curid=26488" title="Reformation (disambiguation)">
Reformation (disambiguation)

The Reformation, also known as the Protestant Reformation, was the 16th century schism within Western Christianity initiated by Martin Luther, John Calvin, and others

Reformation may also refer to:







</doc>
<doc id="26490" url="https://en.wikipedia.org/wiki?curid=26490" title="Reference counting">
Reference counting

In computer science, reference counting is a programming technique of storing the number of references, pointers, or handles to a resource, such as an object, a block of memory, disk space, and others.

In garbage collection algorithms, reference counts may be used to deallocate objects which are no longer needed.

The main advantage of the reference counting over tracing garbage collection is that objects are reclaimed "as soon as" they can no longer be referenced, and in an incremental fashion, without long pauses for collection cycles and with clearly defined lifetime of every object. In real-time applications or systems with limited memory, this is important to maintain responsiveness. Reference counting is also among the simplest forms of memory management to implement. It also allows for effective management of non-memory resources such as operating system objects, which are often much scarcer than memory (tracing garbage collection systems use finalizers for this, but the delayed reclamation may cause problems). Weighted reference counts are a good solution for garbage collecting a distributed system.

Tracing garbage collection cycles are triggered too often if the set of live objects fills most of the available memory; it requires extra space to be efficient. Reference counting performance does not deteriorate as the total amount of free space decreases.

Reference counts are also useful information to use as input to other runtime optimizations. For example, systems that depend heavily on immutable objects such as many functional programming languages can suffer an efficiency penalty due to frequent copies. However, if the compiler (or runtime system) knows that a particular object has only one reference (as most do in many systems), and that the reference is lost at the same time that a similar new object is created (as in the string append statement codice_1), it can replace the operation with a mutation on the original object.

Reference counting in naive form has two main disadvantages over the tracing garbage collection, both of which require additional mechanisms to ameliorate:

In addition to these, if the memory is allocated from a free list, reference counting suffers from poor locality. Reference counting alone cannot move objects to improve cache performance, so high performance collectors implement a tracing garbage collector as well. Most implementations (such as the ones in PHP and Objective-C) suffer from poor cache performance since they do not implement copying objects.

When dealing with garbage collection schemes, it is often helpful to think of the reference graph, which is a directed graph where the vertices are objects and there is an edge from an objectÂ A to an objectÂ B if A holds a reference toÂ B. We also have a special vertex or vertices representing the local variables and references held by the runtime system, and no edges ever go to these nodes, although edges can go from them to other nodes.

In this context, the simple reference count of an object is the in-degree of its vertex. Deleting a vertex is like collecting an object. It can only be done when the vertex has no incoming edges, so it does not affect the out-degree of any other vertices, but it can affect the in-degree of other vertices, causing their corresponding objects to be collected as well if their in-degree also becomes 0 as a result.

The connected component containing the special vertex contains the objects that can't be collected, while other connected components of the graph only contain garbage. If a reference-counting garbage collection algorithm is implemented, then each of these garbage components must contain at least one cycle; otherwise, they would have been collected as soon as their reference count (i.e., the number of incoming edges) dropped to zero.

Incrementing and decrementing reference counts every time a reference is created or destroyed can significantly impede performance. Not only do the operations take time, but they damage cache performance and can lead to pipeline bubbles. Even read-only operations like calculating the length of a list require a large number of reads and writes for reference updates with naive reference counting.

One simple technique is for the compiler to combine a number of nearby reference updates into one. This is especially effective for references which are created and quickly destroyed. Care must be taken, however, to put the combined update at the right position so that a premature free can be avoided.

The Deutsch-Bobrow method of reference counting capitalizes on the fact that most reference count updates are in fact generated by references stored in local variables. It ignores these references, only counting references in data structures, but before an object with reference count zero can be deleted, the system must verify with a scan of the stack and registers that no other reference to it still exists.

Another technique devised by Henry Baker involves deferred increments, in which references which are stored in local variables do not immediately increment the corresponding reference count, but instead defer this until it is necessary. If such a reference is destroyed quickly, then there is no need to update the counter. This eliminates a large number of updates associated with short-lived references (such as the above list-length-counting example). However, if such a reference is copied into a data structure, then the deferred increment must be performed at that time. It is also critical to perform the deferred increment before the object's count drops to zero, resulting in a premature free.

A dramatic decrease in the overhead on counter updates was obtained by Levanoni and Petrank. They introduce the update coalescing method which coalesces many of the redundant reference count updates. Consider a pointer that in a given interval of the execution is updated several times. It first points to an object codice_2, then to an object codice_3, and so forth until at the end of the interval it points to some object codice_4. A reference counting algorithm would typically execute codice_5, codice_6, codice_7, codice_8, codice_9, ..., codice_10. But most of these updates are redundant. In order to have the reference count properly evaluated at the end of the interval it is enough to perform codice_5 and codice_10. The rest of the updates are redundant.

Levanoni and Petrank showed in 2001 how to use such update coalescing in a reference counting collector. When using update coalescing with an appropriate treatment of new objects, more than 99% of the counter updates are eliminated for typical Java benchmarks. In addition, the need for atomic operations during pointer updates on parallel processors is eliminated. Finally, they presented an enhanced algorithm that may run concurrently with multithreaded applications employing only fine synchronization.

Blackburn and McKinley's "ulterior reference counting" method in 2003 combines deferred reference counting with a copying nursery, observing that the majority of pointer mutations occur in young objects. This algorithm achieves throughput comparable with the fastest generational copying collectors with the low bounded pause times of reference counting.

Perhaps the most obvious way to handle reference cycles is to design the system to avoid creating them. A system may explicitly forbid reference cycles; file systems with hard links often do this. Judicious use of "weak" (non-counted) references may also help avoid retain cycles; the Cocoa framework, for instance, recommends using "strong" references for parent-to-child relationships and "weak" references for child-to-parent relationships.

Systems may also be designed to tolerate or correct the cycles they create in some way. Developers may design code to explicitly "tear down" the references in a data structure when it is no longer needed, though this has the cost of requiring them to manually track that data structure's lifetime. This technique can be automated by creating an "owner" object that does the tearing-down when it is destroyed; for instance, a Graph object's destructor could delete the edges of its GraphNodes, breaking the reference cycles in the graph. Cycles may even be ignored in systems with short lives and a small amount of cyclic garbage, particularly when the system was developed using a methodology of avoiding cyclic data structures wherever possible, typically at the expense of efficiency.

Computer scientists have also discovered ways to detect and collect reference cycles automatically, without requiring changes in the data structure design. One simple solution is to periodically use a tracing garbage collector to reclaim cycles; since cycles typically constitute a relatively small amount of reclaimed space, the collector can be run much less often than with an ordinary tracing garbage collector.

Bacon describes a cycle-collection algorithm for reference counting with similarities to tracing collectors, including the same theoretical time bounds. It is based on the observation that a cycle can only be isolated when a reference count is decremented to a nonzero value. All objects which this occurs on are put on a "roots" list, and then periodically the program searches through the objects reachable from the roots for cycles. It knows it has found a cycle that can be collected when decrementing all the reference counts on a cycle of references brings them all down to zero. An enhanced version of this algorithm by Paz et al.
is able to run concurrently with other operations and improve its efficiency by using the update coalescing method of Levanoni and Petrank.

Although it is possible to augment simple reference counts in a variety of ways, often a better solution can be found by performing reference counting in a fundamentally different way. Here we describe some of the variants on reference counting and their benefits and drawbacks.

In weighted reference counting, each reference is assigned a "weight", and each object tracks not the number of references referring to it, but the total weight of the references referring to it. The initial reference to a newly created object has a large weight, such as 2. Whenever this reference is copied, half of the weight goes to the new reference, and half of the weight stays with the old reference. Since the total weight does not change, the object's reference count does not need to be updated.

Destroying a reference decrements the total weight by the weight of that reference. When the total weight becomes zero, all references have been destroyed. If an attempt is made to copy a reference with a weight of 1, the reference has to "get more weight" by adding to the total weight and then adding this new weight to the reference, and then splitting it. An alternative in this situation is to create an "indirection" reference object, the initial reference to which is created with a large weight which can then be split.

The property of not needing to access a reference count when a reference is copied is particularly helpful when the object's reference count is expensive to access, for example because it is in another process, on disk, or even across a network. It can also help increase concurrency by avoiding many threads locking a reference count to increase it. Thus, weighted reference counting is most useful in parallel, multiprocess, database, or distributed applications.

The primary problem with simple weighted reference counting is that destroying a reference still requires accessing the reference count, and if many references are destroyed this can cause the same bottlenecks we seek to avoid. Some adaptations of weighted reference counting seek to avoid this by attempting to give weight back from a dying reference to one which is still active.

Weighted reference counting was independently devised by Bevan and Watson & Watson in 1987.

In indirect reference counting, it is necessary to keep track of from whom the reference was obtained. This means that two references are kept to the object: a direct one which is used for invocations; and an indirect one which forms part of a diffusion tree, such as in the Dijkstra-Scholten algorithm, which allows a garbage collector to identify dead objects. This approach prevents an object from being discarded prematurely.

As a collection algorithm, reference counting tracks, for each object, a count of the number of references to it held by other objects. If an object's reference count reaches zero, the object has become inaccessible, and can be destroyed.

When an object is destroyed, any objects referenced by that object also have their reference counts decreased. Because of this, removing a single reference can potentially lead to a large number of objects being freed. A common modification allows reference counting to be made incremental: instead of destroying an object as soon as its reference count becomes zero, it is added to a list of unreferenced objects, and periodically (or as needed) one or more items from this list are destroyed.

Simple reference counts require frequent updates. Whenever a reference is destroyed or overwritten, the reference count of the object it references is decremented, and whenever one is created or copied, the reference count of the object it references is incremented.

Reference counting is also used in file systems and distributed systems, where full non-incremental tracing garbage collection is too time consuming because of the size of the object graph and slow access speed. 

Microsoft's Component Object Model (COM) and WinRT makes pervasive use of reference counting. In fact, two of the three methods that all COM objects must provide (in the IUnknown interface) increment or decrement the reference count. Much of the Windows Shell and many Windows applications (including MS Internet Explorer, MS Office, and countless third-party products) are built on COM, demonstrating the viability of reference counting in large-scale systems.

One primary motivation for reference counting in COM is to enable interoperability across different programming languages and runtime systems. A client need only know how to invoke object methods in order to manage object life cycle; thus, the client is completely abstracted from whatever memory allocator the implementation of the COM object uses. As a typical example, a Visual Basic program using a COM object is agnostic towards whether that object was allocated (and must later be deallocated) by a C++ allocator or another Visual Basic component.

C++ does not perform reference-counting by default, fulfilling its philosophy of not adding functionality that might incur overheads where the user has not explicitly requested it. Objects that are shared but not owned can be accessed via a reference, raw pointer, or iterator (a conceptual generalisation of pointers).

However, by the same token, C++ provides native ways for users to opt-into such functionality: C++11 provides reference counted smart pointers, via the class, enabling automatic shared memory-management of dynamically allocated objects. Programmers can use this in conjunction with weak pointers (via ) to break cyclic dependencies. Objects that are dynamically allocated but not intended to be shared can have their lifetime automatically managed using a .

In addition, C++11's move semantics further reduce the extent to which reference counts need to be modified by removing the deep copy normally used when a function returns an object, as it allows for a simple copy of the pointer of said object.

Apple's Cocoa and Cocoa Touch frameworks (and related frameworks, such as Core Foundation) use manual reference counting, much like COM. Traditionally this was accomplished by the programmer manually sending codice_13 and codice_14 messages to objects, but Automatic Reference Counting, a Clang compiler feature that automatically inserts these messages as needed, was added in iOS 5 and Mac OS X 10.7. Mac OS X 10.5 introduced a tracing garbage collector as an alternative to reference counting, but it was deprecated in OS X 10.8 and removed from the Objective-C runtime library in macOS Sierra. iOS has never supported a tracing garbage collector.

One language that uses reference counting for garbage collection is Delphi. Delphi is mostly not a garbage collected language, in that user-defined types must still be manually allocated and deallocated. It does provide automatic collection, however, for a few built-in types, such as strings, dynamic arrays, and interfaces, for ease of use and to simplify the generic database functionality. It is up to the programmer to decide whether to use the built-in types or not; Delphi programmers have complete access to low-level memory management like in C/C++. So all potential cost of Delphi's reference counting can, if desired, be easily circumvented.

Some of the reasons reference counting may have been preferred to other forms of garbage collection in Delphi include:


The GObject object-oriented programming framework implements reference counting on its base types, including weak references. Reference incrementing and decrementing uses atomic operations for thread safety. A significant amount of the work in writing bindings to GObject from high-level languages lies in adapting GObject reference counting to work with the language's own memory management system.

The Vala programming language uses GObject reference counting as its primary garbage collection system, along with copy-heavy string handling.

Perl also uses reference counting, without any special handling of circular references, although (as in Cocoa and C++ above), Perl does support weak references, which allows programmers to avoid creating a cycle.

PHP uses a reference counting mechanism for its internal variable management. Since PHP 5.3, it implements the algorithm from Bacon's above mentioned paper. PHP allows you to turn on and off the cycle collection with user-level functions. It also allows you to manually force the purging mechanism to be run.

Python also uses reference counting and offers cycle detection as well (and can reclaim them).

Squirrel also uses reference counting and offers cycle detection as well.
This tiny language is relatively unknown outside the video game industry; however, it is a concrete example of how reference counting can be practical and efficient (especially in realtime environments).

Tcl 8 uses reference counting for memory management of values (Tcl Obj structs). Since Tcl's values are immutable, reference cycles are impossible to form and no cycle detection scheme is needed. Operations that would replace a value with a modified copy are generally optimized to instead modify the original when its reference count indicates it to be unshared. The references are counted at a data structure level, so the problems with very frequent updates discussed above do not arise.

Xojo also uses reference counting, without any special handling of circular references, although (as in Cocoa and C++ above), Xojo does support weak references, which allows programmers to avoid creating a cycle.

Many file systems maintain a count of the number of references to any particular block or file, for example the inode "link count" on Unix-style file systems. These references are usually called as hard links. When the count falls to zero, the file can be safely deallocated. In addition, while references can still be made from directories, some Unixes allow that the referencing can be solely made by live processes, and there can be files that do not exist in the file system hierarchy.



</doc>
<doc id="26491" url="https://en.wikipedia.org/wiki?curid=26491" title="Red-eye effect">
Red-eye effect

The red-eye effect in photography is the common appearance of red pupils in color photographs of the eyes of humans and several other animals. It occurs when using a photographic flash is very close to the camera lens (as with most compact cameras) in ambient low light.

In flash photography the light of the flash occurs too fast for the pupil to close, so much of the very bright light from the flash passes into the eye through the pupil, reflects off the fundus at the back of the eyeball and out through the pupil. The camera records this reflected light. The main cause of the red color is the ample amount of blood in the choroid which nourishes the back of the eye and is located behind the retina. The blood in the retinal circulation is far less than in the choroid, and plays virtually no role. The eye contains several photostable pigments that all absorb in the short wavelength region, and hence contribute somewhat to the red eye effect. The lens cuts off deep blue and violet light, below 430Â nm (depending on age), and macular pigment absorbs between 400 and 500Â nm, but this pigment is located exclusively in the tiny fovea. Melanin, located in the retinal pigment epithelium (RPE) and the choroid, shows a gradually increasing absorption towards the short wavelengths. But blood is the main determinant of the red color, because it is completely transparent at long wavelengths and abruptly starts absorbing at 600Â nm. The amount of red light emerging from the pupil depends on the amount of melanin in the layers behind the retina. This amount varies strongly between individuals. Light-skinned people with blue eyes have relatively low melanin in the fundus and thus show a much stronger red-eye effect than dark-skinned people with brown eyes. The same holds for animals. The color of the iris itself is of virtually no importance for the red-eye effect. This is obvious because the red-eye effect is most apparent when photographing dark-adapted subjects, hence with fully dilated pupils. Photographs taken with infrared light through night vision devices always show very bright pupils because, in the dark, the pupils are fully dilated and the infrared light is not absorbed by any ocular pigment.

The role of melanin in red-eye effect is demonstrated in animals with heterochromia: only the blue eye displays the effect. The effect is still more pronounced in humans and animals with albinism. All forms of albinism involve abnormal production and/or deposition of melanin.

Red-eye effect is seen in photographs of children also because children's eyes have more rapid dark adaption: in low light a child's pupils enlarge sooner, and an enlarged pupil accentuates the red-eye effect.

Theatrical followspot operators, positioned nearly coincidentally with a very bright light and somewhat distant from the actors, occasionally witness red-eye in actors on stage. The effect is not visible to the rest of the audience because it is reliant on the very small angle between the followspot operator and the light.

Similar effects, some related to red-eye effect, are of several kinds:


The red-eye effect can be prevented in a number of ways.

If direct flash must be used, a good rule of thumb is to separate the flash from the lens by 1/20 of the distance of the camera to the subject. For example, if the subject is 2 meters (6Â feet) away, the flash head should be at least 10Â cm (4Â inches) away from the lens.

Professional photographers prefer to use ambient light or indirect flash, as the red-eye reduction system does not always prevent red eyesÂ â for example, if people look away during the pre-flash. In addition, people do not look natural with small pupils, and direct lighting from close to the camera lens is considered to produce unflattering photographs.

Red-eye removal is built into many popular consumer graphics editing software packages, or is supported through red-eye reduction plug-ins; examples include Adobe Photoshop, Apple iPhoto, Corel Photo-Paint, GIMP, Google Picasa, Paint.NET and Microsoft Windows Photo Gallery. Some can automatically find eyes in the image and perform color correction, and can apply it to many photos at once. Others may require the operator to manually select the regions of the pupils to which correction is to be applied. When performed manually, correction may consist of simply converting the red area of pupils to grayscale (desaturating), leaving surface reflections and highlights intact.
In a photograph of a child's face, if there is red-eye in one eye but not the other, it may be leukocoria, which may be caused by the cancer retinoblastoma. The child's eyes should be examined by a general physician.


</doc>
<doc id="26492" url="https://en.wikipedia.org/wiki?curid=26492" title="Ramsay Hunt syndrome type 2">
Ramsay Hunt syndrome type 2

Ramsay Hunt syndrome type 2, also known as herpes zoster oticus, is a disorder that is caused by the reactivation of varicella zoster virus in the geniculate ganglion, a nerve cell bundle of the facial nerve.

Ramsay Hunt syndrome type 2 typically presents with inability to move many facial muscles, pain in the ear, taste loss on the front of the tongue, dry eyes and mouth, and a vesicular rash.

Symptoms include acute facial nerve paralysis, pain in the ear, taste loss in the front two-thirds of the tongue, dry mouth and eyes, and an erythematous vesicular rash in the ear canal, the tongue, and/or hard palate.

Since the vestibulocochlear nerve is in proximity to the geniculate ganglion, it may also be affected, and patients may also suffer from tinnitus, hearing loss, and vertigo. Involvement of the trigeminal nerve can cause numbness of the face.

Ramsay Hunt syndrome type 2 refers to shingles of the geniculate ganglion. After initial infection, varicella zoster virus lies dormant in nerve cells in the body, where it is kept in check by the immune system. Given the opportunity, for example during an illness that suppresses the immune system, the virus travels to the end of the nerve cell, where it causes the symptoms described above.

The affected ganglion is responsible for the movements of facial muscles, the touch sensation of a part of ear and ear canal, the taste function of the frontal two-thirds of the tongue, and the moisturization of the eyes and the mouth. The syndrome specifically refers to the combination of this entity with weakness of the muscles activated by the facial nerve. In isolation, the latter is called Bell's palsy.

However, as with shingles, the lack of lesions does not definitely exclude the existence of a herpes infection. Even before the eruption of vesicles, varicella zoster virus can be detected from the skin of the ear.

Ramsay Hunt Syndrome Type 2 can be diagnosed based on clinical features, however, in ambiguous cases PCR or direct immunofluorescent assay of vesicular fluid can help with the diagnosis. Laboratory studies such as WBC count, ERS, and electrolytes should be obtained to distinguish infectious versus inflammatory etiologies. 

On physical exam look for vesicular exanthema on the external auditory canal, concha and or pinna. Dry eyes with possible lower cornea epithelium damage due to incomplete closure of eye lids.

Ramsay Hunt Syndrome type 2 can usually be diagnosed based on clinical features. However, for suspected cases with unclear presentation, varicella zoster virus can be isolated from vesicle fluid. Tear culture PCR can have positive varicella zoster virus. However 25-35% of patients with Bell palsy can have false positive varicellar zoster virus detected in tears. If central nervous system complications such as meningitis, ventriculitis or meningoencephalitis are suspected, prompt lumbar puncture with spinal fluid analysis and imaging (CT head) are recommended.

Shingles is prevented by immunizing against the causal virus, varicella zoster, for example through Zostavax, a stronger version of chickenpox vaccine.

Treatment with prednisone and the antiviral drug acyclovir 800mg 5 times a day is controversial, with some studies showing to achieve complete recovery in patients if started within the first three days of facial paralysis, with chances of recovery decreasing as treatment was delayed. Delay of treatment may result in permanent facial nerve paralysis. However, some studies demonstrate that even when steroids are started promptly, only 22% of all patient achieve full recovery of facial paralysis.

Treatment apparently has no effect on the recovery of hearing loss. Diazepam is sometimes used to treat the vertigo.

The syndrome is named for James Ramsay Hunt, the eminent neurologist who first described it.



</doc>
<doc id="26494" url="https://en.wikipedia.org/wiki?curid=26494" title="Race and intelligence">
Race and intelligence

The connection between race and intelligence has been a subject of debate in both popular science and academic research since the inception of IQ testing in the early 20th century. There remains some debate as to whether and to what extent differences in intelligence test scores reflect environmental factors as opposed to genetic ones, as well as to the definitions of what "race" and "intelligence" are, and whether they can be objectively defined. Currently, there is no non-circumstantial evidence that these differences in test scores have a genetic component, although some researchers believe that the existing circumstantial evidence makes it at least plausible that hard evidence for a genetic component will eventually be found.

The first test showing differences in IQ test results between different population groups in the US was the tests of United States Army recruits in World War I. In the 1920s, groups of eugenics lobbyists argued that this demonstrated that African-Americans and certain immigrant groups were of inferior intellect to Anglo-Saxon whites due to innate biological differences, using this as an argument for policies of racial segregation. Soon, other studies appeared, contesting these conclusions and arguing instead that the Army tests had not adequately controlled for environmental factors such as socio-economic and educational inequality between blacks and whites.

The debate reemerged again in 1969, when Arthur Jensen championed the view that for genetic reasons, Africans were less intelligent than whites and that compensatory education for African-American children was therefore doomed to be ineffective. In 1994, the book "The Bell Curve" argued that social inequality in the United States could largely be explained as a result of IQ differences between races and individuals, and rekindled the public and scholarly debate with renewed force. During the debates following the book's publication, the American Anthropological Association and the American Psychological Association (APA) published official statements regarding the issue, both highly skeptical of many of the book's claims, and the APA report called for more empirical research on the issue.

Claims of races having different intelligence were used to justify colonialism, slavery, racism, social Darwinism, and racial eugenics. Racial thinkers such as Arthur de Gobineau relied crucially on the assumption that black people were innately inferior to whites in developing their ideologies of white supremacy. Even enlightenment thinkers such as Thomas Jefferson, a slave owner, believed blacks to be innately inferior to whites in physique and intellect.

The first practical intelligence test was developed between 1905 and 1908 by Alfred Binet in France for school placement of children. Binet warned that results from his test should not be assumed to measure innate intelligence or used to label individuals permanently. Binet's test was translated into English and revised in 1916 by Lewis Terman (who introduced IQ scoring for the test results) and published under the name the StanfordâBinet Intelligence Scales. As Terman's test was published, there was great concern in the United States about the abilities and skills of recent immigrants. Different immigrant nationalities were sometimes thought to belong to different races, such as Slavs. A different set of tests developed by Robert Yerkes were used to evaluate draftees for World War I, and researchers found that people from southern and eastern Europe scored lower than native-born Americans, that Americans from northern states had higher scores than Americans from southern states, and that black Americans scored lower than white Americans. The results were widely publicized by a lobby of anti-immigration activists, including the New York patrician and conservationist Madison Grant, who considered the Nordic race to be superior, but under threat of immigration by inferior breeds. In his influential work, "A Study of American Intelligence," psychologist Carl Brigham used the results of the Army tests to argue for a stricter immigration policy, limiting immigration to countries considered to belong to the "Nordic race".

In the 1920s, states like Virginia enacted eugenic laws, such as its 1924 Racial Integrity Act, which established the one-drop rule as law. Many scientists reacted negatively to eugenicist claims linking abilities and moral character to racial or genetic ancestry. They pointed to the contribution of environment to test results (such as speaking English as a second language). By the mid-1930s, many United States psychologists adopted the view that environmental and cultural factors played a dominant role in IQ test results, among them Carl Brigham, who repudiated his own previous arguments on the grounds that he realized that the tests were not a measure of innate intelligence. Discussion of the issue in the United States also influenced German Nazi claims of the "Nordics" being a "master race", influenced by Grant's writings. As the American public sentiment shifted against the Germans, claims of racial differences in intelligence increasingly came to be regarded as problematic. Anthropologists such as Franz Boas, and Ruth Benedict and Gene Weltfish, did much to demonstrate the unscientific status of many of the claims about racial hierarchies of intelligence. Nonetheless, a powerful eugenics and segregation lobby funded largely by textile-magnate Wickliffe Draper continued to publicize studies using intelligence studies as an argument for eugenics, segregation, and anti-immigration legislation.

As the de-segregation of the American South gained traction in the 1950s, the debate about black intelligence resurfaced. Audrey Shuey, funded by Draper's Pioneer Fund, published a new analysis of Yerkes' tests, concluding that blacks really were of inferior intellect to whites. This study was used by segregationists as an argument that it was to the advantage of black children to be educated separately from the superior white children. In the 1960s, the debate was further revived when William Shockley publicly defended the argument that black children were innately unable to learn as well as white children. Arthur Jensen stimulated scholarly discussion of the issue with his "Harvard Educational Review" article, "How Much Can We Boost IQ and Scholastic Achievement?" Jensen's article questioned remedial education for African-American children; he suggested their poor educational performance reflected an underlying genetic cause rather than lack of stimulation at home. Jensen continued to publish on the issue until his death in 2012.

Another revival of public debate followed the appearance of "The Bell Curve" (1994), a book by Richard Herrnstein and Charles Murray, who strongly emphasized the societal effects of low IQ (focusing in most chapters strictly on the non-Hispanic white population of the United States). In 1994, a group of 52 researchers (mostly psychologists) signed an editorial statement "Mainstream Science on Intelligence" in response to the book. "The Bell Curve" also led to a 1995 report from the American Psychological Association, "", acknowledging a difference between mean IQ scores of whites and blacks as well as the absence of any adequate explanation of it, either environmental or genetic. "The Bell Curve" prompted the publication of several multiple-author books responding from a variety of points of view. They include "The Bell Curve Debate" (1995), "" (1996) and a second edition of "The Mismeasure of Man" (1996) by Stephen Jay Gould. Jensen's last book-length publication, "", was published a few years later in 1998.

The review article "Thirty Years of Research on Race Differences in Cognitive Ability" by Rushton and Jensen was published in 2005. The article was followed by a series of responses, some in support, some critical. Richard Nisbett, another psychologist who had also commented at the time, later included an amplified version of his critique as part of the book "Intelligence and How to Get It: Why Schools and Cultures Count" (2009). Rushton and Jensen in 2010 made a point-by-point reply to this thereafter. A comprehensive review article on the issue was published in the journal "American Psychologist" in 2012.

Some of the authors proposing genetic explanations for group differences have received funding from the Pioneer Fund, which was headed by Rushton until his death in 2012. The Southern Poverty Law Center lists the Pioneer Fund as a hate group, citing the fund's history, its funding of race and intelligence research, and its connections with racist individuals. Other researchers have criticized the Pioneer Fund for promoting scientific racism, eugenics and white supremacy.

The concept of intelligence and the degree to which intelligence is measurable is a matter of debate. While there is some consensus about how to define intelligence, it is not universally accepted that it is something that can be unequivocally measured by a single figure. A recurring criticism is that different societies value and promote different kinds of skills and that the concept of intelligence is therefore culturally variable and cannot be measured by the same criteria in different societies. Consequently, some critics argue that proposed relationships to other variables are necessarily tentative.

In relation to the study of racial differences in IQ test scores, it becomes a crucial question of what exactly it is that IQ tests measure. Arthur Jensen was a proponent of the view that there is a correlation between scores on all the known types of IQ tests, and that this correlation points to an underlying factor of general intelligence, or "g". In most conceptions of "g," it is considered to be fairly fixed in a given individual and unresponsive to training or other environmental influences. In this view, test score differences, especially in those tasks considered to be particularly "g-loaded", reflect the test taker's innate capability.

Other psychometricians argue that, while there may or may not be a general intelligence factor, performance on tests relies crucially on knowledge acquired through prior exposure to the types of tasks that such tests contain. This view would mean that tests cannot be expected to reflect only the innate abilities of a given individual, because the expression of potential will always be mediated by experience and cognitive habits. It also means that comparison of test scores from persons with widely different life experiences and cognitive habits is not an expression of their relative innate potentials.

The majority of anthropologists today consider race to be a sociopolitical phenomenon rather than a biological one, a view supported by considerable genetics research. The current mainstream view in the social sciences and biology is that race is a social construction based on folk ideologies that construct groups based on social disparities and superficial physical characteristics. state, "Race is a socially constructed concept, not a biological one. It derives from people's desire to classify." The concept of human "races" as natural and separate divisions within the human species has also been rejected by the American Anthropological Association. The official position of the AAA, adopted in 1998, is that advances in scientific knowledge have made it "clear that human populations are not unambiguous, clearly demarcated, biologically distinct groups" and that "any attempt to establish lines of division among biological populations [is] both arbitrary and subjective."

Race in studies of human intelligence is almost always determined using self-reports, rather than based on analyses of the genetic characteristics of the tested individuals. According to psychologist David Rowe, self-report is the preferred method for racial classification in studies of racial differences because classification based on genetic markers alone ignore the "cultural, behavioral, sociological, psychological, and epidemiological variables" that distinguish racial groups. Hunt and Carlson write that "Nevertheless, self-identification is a surprisingly reliable guide to genetic composition. applied mathematical clustering techniques to sort genomic markers for over 3,600 people in the United States and Taiwan into four groups. There was almost perfect agreement between cluster assignment and individuals' self-reports of racial/ethnic identification as white, black, East Asian, or Latino." Sternberg and Grigorenko disagree with Hunt and Carlson's interpretation of Tang, "Tang et al.'s point was that ancient geographic ancestry rather than current residence is associated with self-identification and not that such self-identification provides evidence for the existence of biological race."

Anthropologist C. Loring Brace and geneticist Joseph Graves disagree with the idea that cluster analysis and the correlation between self-reported race and genetic ancestry support biological race. They argue that while it is possible to find biological and genetic variation corresponding roughly to the groupings normally defined as races, this is true for almost all geographically distinct populations. The cluster structure of the genetic data is dependent on the initial hypotheses of the researcher and the populations sampled. When one samples continental groups, the clusters become continental; if one had chosen other sampling patterns, the clusters would be different. therefore concludes that, while differences in particular allele frequencies can be used to identify populations that loosely correspond to the racial categories common in Western social discourse, the differences are of no more biological significance than the differences found between any human populations (e.g., the Spanish and Portuguese).

Earl B. Hunt agrees that racial categories are defined by social conventions, though he points out that they also correlate with clusters of both genetic traits and cultural traits. Hunt explains that, due to this, racial IQ differences are caused by these variables that correlate with race, and race itself is rarely a causal variable. Researchers who study racial disparities in test scores are studying the relationship between the scores and the many race-related factors which could potentially affect performance. These factors include health, wealth, biological differences, and education.

The study of human intelligence is one of the most controversial topics in psychology. It remains unclear whether group differences in intelligence test scores are caused by heritable factors or by "other correlated demographic variables such as socioeconomic status, education level, and motivation."
Hunt and Carlson outlined four contemporary positions on differences in IQ based on race or ethnicity. The first is that these reflect real differences in average group intelligence, which is caused by a combination of environmental factors and heritable differences in brain function. A second position is that differences in average cognitive ability between races are caused entirely by social and/or environmental factors. A third position holds that differences in average cognitive ability between races do not exist, and that the differences in average test scores are the result of inappropriate use of the tests themselves. Finally, a fourth position is that either or both of the concepts of race and general intelligence are poorly constructed and therefore any comparisons between races are meaningless.

In the United States, individuals identifying themselves as East Asian tend to have higher average IQ scores than do Caucasians, who, in turn, have higher average IQs than African Americans. Nevertheless, greater variation in IQ scores exists within each ethnic group than between them.

In response to the controversial 1994 book "The Bell Curve", the American Psychological Association (APA) formed a task-force of eleven experts, which issued a report, "" in 1995. Regarding group differences, the report reaffirmed the consensus that differences within groups are much wider than difference between groups, and that that claims of ethnic difference in intelligence should be scrutinized carefully, as this had been used to justify racial discrimination. It also acknowledged limitations in the racial categories used, as these categories are neither consistently applied, nor homogeneous (see also race and ethnicity in the United States).

, in a review of the results of a total of 6,246,729 participants on other tests of cognitive ability or aptitude, found a difference in mean IQ scores between blacks and whites of 1.1 SD. Consistent results were found for college and university application tests such as the Scholastic Aptitude Test (N = 2.4 million) and Graduate Record Examination (N = 2.3 million), as well as for tests of job applicants in corporate sections (N = 0.5 million) and in the military (N = 0.4 million). According to the same study, East Asians have tended to score relatively higher on visuospatial subtests with lower scores in verbal subtests while Ashkenazi Jews score higher in verbal subtests with lower scores in visuospatial subtests. The few Amerindian populations who have been systematically tested, including Arctic Natives, tend to score worse on average than white populations but better on average than black populations.

The racial groups studied in the United States and Europe are not necessarily representative samples for populations in other parts of the world. Cultural differences may also factor in IQ test performance and outcomes. Therefore, results in the United States and Europe do not necessarily correlate to results in other populations.

A number of studies have compared average IQ scores between the world's nations, finding patterns of difference between continental populations similar to those associated with race. Richard Lynn and Tatu Vanhanen have argued that populations in the third world, particularly populations in Africa, tend to have limited intelligence because of their genetic composition and that, consequently, education cannot be effective in creating social and economic development in third world countries. Lynn and Vanhanen's studies have been severely criticized for relying on low quality data and for choosing sources in ways that seem to be biased severely towards underestimating the average IQ potential of developing nations, particularly in Africa. Nonetheless there is a general consensus that the average IQ in developing countries is lower than in developed countries, but subsequent research has favored environmental explanations for this fact, such as lack of basic infrastructure related to health and education.

In the 2002 book "IQ and the Wealth of Nations", and "IQ and Global Inequality" in 2006, Richard Lynn and Tatu Vanhanen created estimates of average IQs for 113 nations. They estimated IQs of 79 other nations based on neighboring nations or via other means. They saw a consistent correlation between national development and national IQ averages. They found the highest national IQs among Western and East Asian developed nations and the lowest national IQs in the world's least developed nations among the indigenous peoples in the regions of Sub-Saharan Africa, Southeast Asia and Latin America. In a meta-analysis of studies of IQ estimates in Sub-Saharan Africa, concluded that Lynn and Vanhanen had relied on unsystematic methodology by failing to publish their criteria for including or excluding studies. They found that Lynn and Vanhanen's exclusion of studies had depressed their IQ estimate for sub-Saharan Africa, and that including studies excluded in "IQ and Global Inequality" resulted in average IQ of 82 for sub-Saharan Africa, lower than the average in Western countries, but higher than Lynn and Vanhanen's estimate of 67. Wicherts at al. conclude that this difference is likely due to sub-Saharan Africa having limited access to modern advances in education, nutrition and health care.

A 2010 systematic review by the same research team, along with Jerry S. Carlson, found that compared to American norms, the average IQ of sub-Saharan Africans was about 80. The same review concluded that the Flynn effect had not yet taken hold in sub-Saharan Africa.

A 2007 meta-analysis by Rindermann found many of the same groupings and correlations found by Lynn and Vanhanen, with the lowest scores in sub-Saharan Africa, and a correlation of .60 between cognitive skill and GDP per capita. considers Rindermann's analysis to be much more reliable than Lynn and Vanhanen's. By measuring the relationship between educational data and social wellbeing over time, this study also performed a causal analysis, finding that when nations invest in education this leads to increased well-being later on. Kamin (2006) has also criticized Lynn and Vanhanen's work on the IQs of sub-Saharan Africans.

Wicherts, Borsboom & Dolan (2010) argue that studies reporting support for evolutionary theories of intelligence based on national IQ data suffer from multiple fatal methodological flaws. For example, they state that such studies "...assume that the Flynn Effect is either nonexistent or invariant with respect to different regions of the world, that there have been no migrations and climatic changes over the course of evolution, and that there have been no trends over the last century in indicators of reproductive strategies (e.g., declines in fertility and infant mortality)." They also showed that a strong degree of confounding exists between national IQs and current national development status.

Similarly, Pesta & Poznanski (2014) showed that the average temperature of a given U.S. state is strongly associated with that state's average IQ and other well-being variables, despite the fact that evolution has not had enough time to operate on non-Native American residents of the United States. They also noted that this association persisted even after controlling for race, and concluded that "Evolution is therefore not necessary for temperature and IQ/well-being to co-vary meaningfully across geographic space."

For the past century raw scores on IQ tests have been rising; this score increase is known as the "Flynn effect", named after James R. Flynn. In the United States, the increase was continuous and approximately linear from the earliest years of testing to about 1998 when the gains stopped and some tests even showed decreasing test scores. For example, in the United States the average scores of blacks on some IQ tests in 1995 were the same as the scores of whites in 1945. As one pair of academics phrased it, "the typical African American today probably has a slightly higher IQ than the grandparents of today's average white American."

Flynn has argued that given that these changes take place between one generation and the next it is highly unlikely that genetic factors could account for the increasing scores, which must then be caused by environmental factors. The Flynn Effect has often been used as an argument that the racial gap in IQ test scores must be environmental too, but this is not generally agreed â others have asserted that the two may have entirely different causes. A meta-analysis by Te Nijenhuis and van der Flier (2013) concluded that the Flynn effect and group differences in intelligence were likely to have different causes. They stated that the Flynn effect is caused primarily by environmental factors and that it's unlikely these same environmental factors play an important role in explaining group differences in IQ. The importance of the Flynn effect in the debate over the causes for the IQ gap lies in demonstrating that environmental factors may cause changes in test scores on the scale of 1 SD. This had previously been doubted.

A separate phenomenon from the Flynn effect has been the discovery that the IQ gap has been gradually closing over the last decades of the 20th century, as black test-takers increased their average scores relative to white test-takers. For instance, Vincent reported in 1991 that the black-white IQ gap was decreasing among children, but that it was remaining constant among adults. Similarly, a 2006 study by Dickens and Flynn estimated that the difference between mean scores of blacks and whites closed by about 5 or 6 IQ points between 1972 and 2002, a reduction of about one-third. In the same period, the educational achievement disparity also diminished. However, this was challenged by Rushton & Jensen who claim the difference remains stable. In a 2006 study, Murray agreed with Dickens and Flynn that there has been a narrowing of the difference; "Dickens' and Flynn's estimate of 3â6 IQ points from a base of about 16â18 points is a useful, though provisional, starting point". But he argued that this has stalled and that there has been no further narrowing for people born after the late 1970s. A subsequent study by Murray, based on the WoodcockâJohnson Tests of Cognitive Abilities, estimated that the black-white IQ difference decreased by about one-half of one standard deviation from those born in the 1920s to those born in the second half of the 1960s and early 1970s. Recent reviews by Flynn and Dickens (2006), Mackintosh (2011), and Nisbett "et al." 2012 accept the gradual closing of the gap as a fact. In his review of the historical trends, states: "There is some variety in the results, but not a great deal. The African American means are about 1 standard deviation unit (15 points on the IQ scale) below the white means, and the Hispanic means fall in between."

Some studies reviewed by found that rise in the average achievement of African Americans was caused by a reduction in the number of African American students in the lowest range of scores without a corresponding increase in the number of students in the highest ranges. A 2012 review of the literature found that the IQ gap had diminished by 0.33 standard deviations since first reported.

A 2013 analysis of the National Assessment of Educational Progress found that from 1971 to 2008, the size of the blackâwhite IQ gap in the United States decreased from 16.33 to 9.94 IQ points. It has also concluded however that, while IQ means are continuing to rise in all ethnic groups, this growth is occurring more slowly among 17-year-old students than among younger students and the black-white IQ gap is no longer narrowing. As of 2008, a study published in 2013 by Heiner Rindermann, Stefan Pinchelmann, and James Thompson have estimated the IQ means of 17-year-old black, white, and Hispanic students to range respectively from 90.45â94.15, 102.29â104.57 and 92.30â95.90 points. They explain that the gap may persist due to the crack epidemic, the degradation of African-American family structure, the rise of fraud in the educational system (especially with respect to "No Child Left Behind"), the decrease in unskilled real wages and employment among African-Americans due to globalization and minimum wage increases, differences in parental practices (such as breastfeeding or reading to children), and "environmental conditions shaped by [African-Americans] themselves." To resolve this, they ultimately recommend the reestablishment of "meritoric principles" and "blindly graded objective central exams," as opposed to "ethnically based policies," in education.

The following environmental factors are some of those suggested as explaining a portion of the differences in average IQ between races. These factors are not mutually exclusive with one another, and some may, in fact, contribute directly to others. Furthermore, the relationship between genetics and environmental factors may be complicated. For example, the differences in socioeconomic environment for a child may be due to differences in genetic IQ for the parents, and the differences in average brain size between races could be the result of nutritional factors. All recent reviews agree that some environmental factors that are unequally distributed between racial groups have been shown to affect intelligence in ways that could contribute to the test score gap. However, currently, the question is whether these factors can account for the entire gap between white and black test scores, or only part of it. One group of scholars, including Richard E. Nisbett, James R. Flynn, Joshua Aronson, Diane Halpern, William Dickens, Eric Turkheimer (2012) have argued that the environmental factors so far demonstrated are sufficient to account for the entire gap. Nicholas Mackintosh (2011) considers this a reasonable argument, but argues that probably it is impossible to ever know for sure; another group including Earl B. Hunt (2010), Arthur Jensen, J. Philippe Rushton and Richard Lynn have argued that this is impossible. Jensen and Rushton consider that it may account for as little as 20% of the gap. Meanwhile, while Hunt considers this a vast overstatement, he nonetheless considers it likely that some portion of the gap will eventually be shown to be caused by genetic factors.

Environmental factors including childhood lead exposure, low rates of breast feeding, and poor nutrition can significantly affect cognitive development and functioning. For example, childhood exposure to lead, associated with homes in poorer areas causes an average IQ drop of 7 points, and iodine deficiency causes a fall, on average, of 12 IQ points. Such impairments may sometimes be permanent, sometimes be partially or wholly compensated for by later growth. The first two years of life is the critical time for malnutrition, the consequences of which are often irreversible and include poor cognitive development, educability, and future economic productivity. The African American population of the United States is statistically more likely to be exposed to many detrimental environmental factors such as poorer neighborhoods, schools, nutrition, and prenatal and postnatal health care. Mackintosh points out that for American blacks infant mortality is about twice as high as for whites, and low birthweight is twice as prevalent. At the same time white mothers are twice as likely to breastfeed their infants, and breastfeeding is highly correlated with IQ for low birthweight infants. In this way a wide number of health related factors that influence IQ are unequally distributed between the two groups.

The Copenhagen consensus in 2004 stated that lack of both iodine and iron has been implicated in impaired brain development, and this can affect enormous numbers of people: it is estimated that one-third of the total global population are affected by iodine deficiency. In developing countries, it is estimated that 40% of children aged four and under suffer from anaemia because of insufficient iron in their diets.

Other scholars have found that simply the standard of nutrition has a significant effect on population intelligence, and that the Flynn effect may be caused by increasing nutrition standards across the world. James Flynn has himself argued against this view.

Some recent research has argued that the retardation caused in brain development by infectious diseases, many of which are more prevalent in non-white populations, may be an important factor in explaining the differences in IQ between different regions of the world. The findings of this research, showing the correlation between IQ, race and infectious diseases was also shown to apply to the IQ gap in the US, suggesting that this may be an important environmental factor.

A 2013 meta-analysis by the World Health Organization found that, after controlling for maternal IQ, breastfeeding was associated with IQ gains of 2.19 points. The authors suggest that this relationship is causal but state that the practical significance of this gain is debatable; however, they highlight one study suggesting an association between breastfeeding and academic performance in Brazil, where "breastfeeding duration does not present marked variability by socioeconomic position." Colen and Ramey (2014) similarly find that controlling for sibling comparisons within families, rather than between families, reduces the correlation between breastfeeding status and WISC IQ scores by nearly a third, but further find the relationship between breastfeeding duration and WISC IQ scores to be insignificant. They suggest that "much of the beneficial long-term effects typically attributed to breastfeeding, per se, may primarily be due to selection pressures into infant feeding practices along key demographic characteristics such as race and socioeconomic status." Reichman estimates that no more than 3 to 4% of the black-white IQ gap can be explained by black-white disparities in low birth weight.

Several studies have proposed that a large part of the gap can be attributed to differences in quality of education. Racial discrimination in education has been proposed as one possible cause of differences in educational quality between races. According to a paper by Hala Elhoweris, Kagendo Mutua, Negmeldin Alsheikh and Pauline Holloway, teachers' referral decisions for students to participate in gifted and talented educational programs were influenced in part by the students' ethnicity.

The Abecedarian Early Intervention Project, an intensive early childhood education project, was also able to bring about an average IQ gain of 4.4 points at age 21 in the black children who participated in it compared to controls. Arthur Jensen agreed that the Abecedarian project demonstrates that education can have a significant effect on IQ, but also said that no educational program thus far has been able to reduce the black-white IQ gap by more than a third, and that differences in education are thus unlikely to be its only cause.

Rushton and Jensen argue that long-term follow-up of the Head Start Program found large immediate gains for blacks and whites but that these were quickly lost for the blacks although some remained for whites. They argue that also other more intensive and prolonged educational interventions have not produced lasting effects on IQ or scholastic performance. Nisbett argues that they ignore studies such as which found that at the age 12, 87% of black infants exposed to an intervention had IQs in the normal range (above 85) compared to 56% of controls, and none of the intervention-exposed children were mildly retarded compared to 7% of controls. Other early intervention programs have shown IQ effects in the range of 4â5 points, which are sustained until at least age 8â15. Effects on academic achievement can also be substantial. Nisbett also argues that not only early age intervention can be effective, citing other successful intervention studies from infancy to college.

A series of studies by Joseph Fagan and Cynthia Holland measured the effect of prior exposure to the kind of cognitive tasks posed in IQ tests on test performance. Assuming that the IQ gap was the result of lower exposure to tasks using the cognitive functions usually found in IQ tests among African American test takes, they prepared a group of African Americans in this type of tasks before taking an IQ test. The researchers found that there was no subsequent difference in performance between the African-Americans and white test takers. Daley and Onwuegbuzie conclude that Fagan and Holland demonstrate that "differences in knowledge between blacks and whites for intelligence test items can be erased when equal opportunity is provided for exposure to the information to be tested". A similar argument is made by David Marks who argues that IQ differences correlate well with differences in literacy suggesting that developing literacy skills through education causes an increase in IQ test performance.

A 2003 study found that two variables â stereotype threat and the degree of educational attainment of children's fathers â partially explained the black-white gap in cognitive ability test scores, undermining the hereditarian view that they stemmed from immutable genetic factors.

Different aspects of the socioeconomic environment in which children are raised have been shown to correlate with part of the IQ gap, but they do not account for the entire gap. According to a 2006 review, these factors account for slightly less than half of one standard deviation of the gap. Generally the difference between mean test scores of blacks and whites is not eliminated when individuals and groups are matched on socioeconomic status (SES), suggesting that the relationship between IQ and SES is not simply one in which SES determines IQ. Rather it may be the case that differences in intelligence, particularly parental intelligence, may also cause differences in SES, making separating the two factors difficult. summarises data showing that, jointly, SES and parental IQ account for the full gap (in populations of young children, after controlling parental IQ and parental SES, the gap is not statistically different from zero). He argues the SES-linked components reflect parental occupation status, mother's verbal comprehension score and parent-child interaction quality. Hunt also reviews data showing that the correlation between home environment and IQ becomes weaker with age. Hart and Risley argue that in welfare, working-class, and professional families, children hear a large disparity in the amount of language (between 13 million and 45 million words) in the age range of 0â3, and that by age 9 these differences lead to large differences in child outcomes.

Other research has focussed on different causes of variation within low SES and high SES groups.
In the US, among low-SES groups, genetic differences account for a smaller proportion variance in IQ than among higher SES populations. Such effects are predicted by the "bioecological" hypothesis â that genotypes are transformed into phenotypes through nonadditive synergistic effects of the environment. suggest that high SES individuals are more likely to be able to develop their full biological potential, whereas low SES individuals are likely to be hindered in their development by adverse environmental conditions. The same review also points out that adoption studies generally are biased towards including only high and high middle SES adoptive families, meaning that they will tend to overestimate average genetic effects. They also note that studies of adoption from lower-class homes to middle-class homes have shown that such children experience a 12â18 pt gain in IQ relative to children who remain in low SES homes. A 2015 study found that environmental factors (namely, family income, maternal education, maternal verbal ability/knowledge, learning materials in the home, parenting factors (maternal sensitivity, maternal warmth and acceptance, and safe physical environment), child birth order, and child birth weight) accounted for the black-white gap in cognitive ability test scores.

A number of studies have reached the conclusion that IQ tests may be biased against certain groups. The validity and reliability of IQ scores obtained from outside the United States and Europe have been questioned, in part because of the inherent difficulty of comparing IQ scores between cultures. Several researchers have argued that cultural differences limit the appropriateness of standard IQ tests in non-industrialized communities.

A 1996 report by the American Psychological Association states that intelligence can be difficult to compare across cultures, and notes that differing familiarity with test materials can produce substantial differences in test results; it also says that tests are accurate predictors of future achievement for black and white Americans, and are in that sense unbiased. The view that tests accurately predict future educational attainment is reinforced by Nicholas Mackintosh in his 1998 book "IQ and Human Intelligence", and by a 1999 literature review by .

James R. Flynn, surveying studies on the topic, notes that the weight and presence of many test questions depends on what sorts of information and modes of thinking are culturally valued.

Stereotype threat is the fear that one's behavior will confirm an existing stereotype of a group with which one identifies or by which one is defined; this fear may in turn lead to an impairment of performance. Testing situations that highlight the fact that intelligence is being measured tend to lower the scores of individuals from racial-ethnic groups who already score lower on average or are expected to score lower. Stereotype threat conditions cause larger than expected IQ differences among groups. Psychometrician Nicholas Mackintosh considers that there is little doubt that the effects of stereotype threat contribute to the IQ gap between blacks and whites.

A large number of studies have shown that systemically disadvantaged minorities, such as the African American minority of the United States, generally perform worse in the educational system and in intelligence tests than the majority groups or less disadvantaged minorities such as immigrant or "voluntary" minorities. The explanation of these findings may be that children of caste-like minorities, due to the systemic limitations of their prospects of social advancement, do not have "effort optimism", i.e. they do not have the confidence that acquiring the skills valued by majority society, such as those skills measured by IQ tests, is worthwhile. They may even deliberately reject certain behaviors that are seen as "acting white."

Research published in 1997 indicates that part of the black-white gap in cognitive ability test scores is due to racial differences in test motivation.

However, attempts to replicate studies evincing significant effects of stereotype threat have not yielded the same results. In 2004 Sackett et al. found that eliminating stereotype threat does not eliminate the racial test performance gap, and in 2005 Tyson et al. found that African Americans have motivation similar to or even better than that of white Americans. Self-affirmation exercises promoted by research scientists such as Geoffrey L. Cohen have not been shown to be effective by attempts to replicate his studies. A 2015 meta-analysis conducted by Flore & Wicherts of studies on the relationship between gender and stereotype threat found the observed estimates to be inflated by publication bias, arguing the true effect to be most likely near zero.

Ongoing research aims to understand the contribution of genes to differences in intelligence. Currently there is no non-circumstantial evidence that the test score gap has a genetic component, although some researchers believe that the existing circumstantial evidence makes it plausible to believe that hard evidence for a genetic component will eventually appear. Growing evidence indicates that environmental factors, not genetic ones, are more important in explaining the racial IQ gap. Several lines of investigation have been followed in the attempt to ascertain whether there is a genetic component to the test score gap as well as its relative contribution to the magnitude of the gap.

Geneticist Alan R. Templeton argues that the question about the possible genetic effects on the test score gap is muddled by the general focus on "race" rather than on populations defined by gene frequency or by geographical proximity, and by the general insistence on phrasing the question in terms of heritability. Templeton points out that racial groups neither represent sub-species nor distinct evolutionary lineages, and that therefore there is no basis for making claims about the general intelligence of races. From this point of view the search for possible genetic influences on the black-white test score gap is a priori flawed, because there is no genetic material shared by all Africans or by all Europeans. , however, argues that by using genetic cluster analysis to correlate gene frequencies with continental populations it might be possible to show that African populations have a higher frequency of certain genetic variants that contribute to an average lower intelligence. Such a hypothetical situation could hold without all Africans carrying the same genes or belonging to a single evolutionary lineage. According to Mackintosh, a biological basis for the gap thus cannot be ruled out on a priori grounds.

Intelligence is a polygenic trait. This means that intelligence is under the influence of several genes, possibly several thousand. The effect of most individual genetic variants on intelligence is thought to be very small, well below 1% of the variance in "g". Current studies using quantitative trait loci have yielded little success in the search for genes influencing intelligence. Robert Plomin is confident that QTLs responsible for the variation in IQ scores exist, but due to their small effect sizes, more powerful tools of analysis will be required to detect them. Others assert that no useful answers can be reasonably expected from such research before an understanding of the relation between DNA and human phenotypes emerges. Several candidate genes have been proposed to have a relationship with intelligence. However, a review of candidate genes for intelligence published in failed to find evidence of an association between these genes and general intelligence, stating "there is still almost no replicated evidence concerning the individual genes, which have variants that contribute to intelligence differences". In 2001, a review in the "Journal of Black Psychology" refuted eight major premises on which the hereditarian view regarding race and intelligence is based.

A 2005 literature review article by Sternberg, Grigorenko and Kidd stated that no gene has been shown to be linked to intelligence, "so attempts to provide a compelling genetic link of race to intelligence are not feasible at this time". and concurred, both scholars noting that while several environmental factors have been shown to influence the IQ gap, the evidence for a genetic influence has been circumstantial, and according to Mackintosh negligible. Mackintosh however suggests that it may never become possible to account satisfyingly for the relative contributions of genetic and environmental factors. The 2012 review by concluded that "Almost no genetic polymorphisms have been discovered that are consistently associated with variation in IQ in the normal range". Hunt and several other researchers however maintain that genetic causes cannot be ruled out, and that new evidence may yet show a genetic contribution to the gap. Hunt concurs with Rushton and Jensen who considered the 100% environmental hypothesis to be impossible. Nonetheless, Nisbett and colleagues (2012) consider the entire IQ gap to be explained by the environmental factors that have thus far been demonstrated to influence it, and Mackintosh does not find this view to be unreasonable.

Twin studies of intelligence have reported high heritability values. However, these studies are based on questionable assumptions. When used in the context of human behavior genetics, the term "heritability" is highly misleading, as it does not convey any information about the relative importance of genetic or environmental factors on the development of a given trait, nor does it convey the extent to which that trait is genetically determined. Arguments in support of a genetic explanation of racial differences in IQ are sometimes fallacious. For instance, hereditarians have sometimes cited the failure of known environmental factors to account for such differences, or the high heritability of intelligence within races, as evidence that racial differences in IQ are genetic.

Psychometricians have found that intelligence is substantially heritable within populations, with 30â50% of variance in IQ scores in early childhood being attributable to genetic factors in analyzed US populations, increasing to 75â80% by late adolescence. In biology heritability is defined as the ratio of variation attributable to genetic differences in an observable trait to the trait's total observable variation. The heritability of a trait describes the proportion of variation in the trait that is attributable to genetic factors within a particular population. A heritability of 1 indicates that variation correlates fully with genetic variation and a heritability of 0 indicates that there is no correlation between the trait and genes at all. In psychological testing, heritability tends to be understood as the degree of correlation between the results of a test taker and those of their biological parents. However, since high heritability is simply a correlation between traits and genes, it does not describe the causes of heritability which in humans can be either genetic or environmental.

Therefore, a high heritability measure does not imply that a trait is genetic or unchangeable, however, as environmental factors that affect all group members equally will not be measured by heritability and the heritability of a trait may also change over time in response to changes in the distribution of genes and environmental factors. High heritability also doesn't imply that all of the heritability is genetically determined, but can also be due to environmental differences that affect only a certain genetically defined group (indirect heritability). The figure to the left demonstrates how heritability works. In both gardens the difference between tall and short cornstalks is 100% heritable as cornstalks that are genetically disposed for growing tall will become taller than those without this disposition, but the difference in height between the cornstalks to the left and those on the right is 100% environmental as it is due to different nutrients being supplied to the two gardens. Hence the causes of differences within a group and between groups may not be the same, even when looking at traits that are highly heritable. In his criticism of "the Bell Curve", Noam Chomsky further illustrated this with the example of women wearing earrings:

In regards to the IQ gap the question becomes whether racial groups can be shown to be influenced by different environmental factors that may account for the observed differences between them. Jensen originally argued that given the high heritability of IQ the only way that the IQ gap could be explained as caused by the environment would be if it could be shown that all blacks were subject to a single "x-factor" which affected no white populations while affecting all black populations equally. Jensen considered the existence of such an x-factor to be extremely improbable, but Flynn's discovery of the Flynn effect showed that in spite of high heritability environmental factors could cause considerable disparities in IQ between generations of the same population, showing that the existence of such an x-factor was not only possible but real.

Jensen has also argued that heritability of traits rises with age as the genetic potential of individuals becomes expressed. He sees this as related to the fact that the IQ gap between white and black test takers has been shown to appear gradually, with the gap widening as cohorts reach adulthood. This he sees as a further argument in favor of Spearman's hypothesis (see section below).

In contrast, Dickens and Flynn argued that the conventional interpretation ignores the role of feedback between factors, such as those with a small initial IQ advantage, genetic or environmental, seeking out more stimulating environments which will gradually greatly increase their advantage, which, as one consequence in their alternative model, would mean that the "heritability" figure is only in part due to direct effects of genotype on IQ.

Today researchers such as , and consider that rather than a single factor accounting for the entire gap, probably many different environmental factors differ systematically between the environments of white and black people converge to create part of the gap and perhaps all of it. They argue that it does not make sense to talk about a single universal heritability figure for IQ, rather, they state, heritability of IQ varies between and within groups. They point specifically to studies showing a higher heritability of test scores in white and medium-high SES families, but considerably lower heritability for black and low-SES families. This they interpret to mean that children who grow up with limited resources do not get to develop their full genetic potential.

Multiple studies have been conducted over the past several decades to survey scientific estimates on the heritability of the IQ gap. A review by Snydermann and Rothman in 1988 found that 45% of the scientists they questioned believed the gap to be "a product of genetic and environmental variation," 15% and 1% respectively "entirely to environmental" and "genetic variation," while the remaining 38% either declined to answer or stated that the evidence was inconclusive. The heritability of intelligence was estimated on average to be 59.6% for white Americans and 57.0% for black Americans among those who answered that the evidence was sufficiently conclusive. The "Wall Street Journal" published an editorial by Linda Gottfredson in 1994, signed by 52 professors specializing in intelligence and allied fields, that estimated the heritability of individual variation to range between 40â80%, but also stating that "there is no definitive answer" to explain the racial gap. Social psychologist Donald T. Campbell criticized the report, arguing that it overstated the plausibility of genetic explanations and underestimated the extent of environmental differences between races. A stated that there is more plausible evidence for an environmental than for a genetic explanation, but that there was "no adequate explanation" for the black-white IQ gap. In a 2013 followup on Snyderman & Rothman, Rindermann et al. found the average and median estimates of the black-white IQ gap to be heritable by 47% and 50% respectively among surveyed scientists who believed that the available evidence allowed for a reasonable estimate. This survey however yielded a response rate of 18% (228 participants) compared to Snyderman & Rothman's 65% (661 participants).

Spearman's hypothesis states that the magnitude of the black-white difference in tests of cognitive ability is entirely or mainly a function of the extent to which a test measures general mental ability, or "g". The hypothesis was first formalized by Arthur Jensen who devised the statistical Method of Correlated Vectors to test it. Jensen holds that if Spearman's hypothesis holds true then some cognitive tasks have a higher g-load than others, and that these tasks are exactly the tasks in which the gap between black and white test takers are greatest. Jensen, and other psychometricians such as Rushton and Lynn, take this to show that the cause of "g" and the cause of the gap are the sameâin their view genetic differences.

James argues that there is an inherent flaw in Jensen's argument that the correlation between g-loadings, test scores and heritability support a genetic cause of the gap. He points out that as the difficulty of a task increases a low performing group will naturally fall further behind, and heritability will therefore also naturally increase. The same holds for increases in performance which will first affect the least difficult tasks, but only gradually affect the most difficult ones. Flynn thus sees the correlation between in g-loading and the test score gap to offer no clue to the cause of the gap.

A number of studies have been done on the effect of similar rearing conditions on children from different races. The hypothesis is that by investigating whether black children adopted into white families demonstrated gains in IQ test scores relative to black children reared in black families. Depending on whether their test scores are more similar to their biological or adoptive families, that could be interpreted as either supporting a genetic or an environmental hypothesis. The main point of critique in studies like these however is whether the environment of black childrenâeven when raised in white familiesâis truly comparable to the environment of white children. Several reviews of the adoption study literature has pointed out that it is perhaps impossible to avoid confounding of biological and environmental factors in this type of studies. Given the differing heritability estimates in medium-high SES and low-SES families, argue that adoption studies on the whole tend to overstate the role of genetics because they represent a restricted set of environments, mostly in the medium-high SES range.

The Minnesota Transracial Adoption Study (1976) examined the IQ test scores of 122 adopted children and 143 nonadopted children reared by advantaged white families. The children were restudied ten years later. The study found higher IQ for whites compared to blacks, both at age 7 and age 17. cite the Minnesota study as providing support to a genetic explanation. Nonetheless, acknowledging the existence of confounding factors, Scarr and Weinberg the authors of the original study, did not themselves consider that it provided support for either the hereditarian or environmentalist view.

Three other adoption studies found contrary evidence to the Minnesota study, lending support to a mostly environmental hypothesis: 

Rushton and Jensen have argued that unlike the Minnesota Transracial Adoption Study, these studies did not retest the children post-adolescence when heritability of IQ would presumably be higher. however point out that the difference in heritability between ages 7 and 17 are quite small, and that consequently this is no reason to disregard Moore's findings.

Frydman and Lynn (1989) showed a mean IQ of 119 for Korean infants adopted by Belgian families. After correcting for the Flynn effect, the IQ of the adopted Korean children was still 10 points higher than the indigenous Belgian children.

Reviewing the evidence from adoption studies Mackintosh considers the studies by Tizard and Eyferth to be inconclusive, and the Minnesota study to be consistent only with a partial genetic hypothesis. On the whole he finds that environmental and genetic variables remain confounded and considers evidence from adoption studies inconclusive on the whole, and fully compatible with a 100% environmental explanation.

Most people have an ancestry from different geographic regions, particularly African Americans typically have ancestors from both Africa and Europe, with, on average, 20% of their genome inherited from European ancestors. If racial IQ gaps have a partially genetic basis, one might expect blacks with a higher degree of European ancestry to score higher on IQ tests than blacks with less European ancestry, because the genes inherited from European ancestors would likely include some genes with a positive effect on IQ. Geneticist Alan Templeton has argued that an experiment based on the Mendelian "common garden" design where specimens with different hybrid compositions are subjected to the same environmental influences, would be the only way to definitively show a causal relation between genes and IQ. Summarizing the findings of admixture studies, he concludes that it has shown no significant correlation between any cognitive and the degree of African or European ancestry.

Studies have employed different ways of measuring or approximating relative degrees of ancestry from Africa and Europe. One set of studies have used skin color as a measure, and other studies have used blood groups. surveys the literature and argues that the blood groups studies may be seen as providing some support to the genetic hypothesis, even though the correlation between ancestry and IQ was quite low. He finds that studies by , Willerman, Naylor & Myrianthopoulos (1970) did not find a correlation between degree of African/European ancestry and IQ. The latter study did find a difference based on the race of the mother, with children of white mothers with black fathers scoring higher than children of black mothers and white fathers. Loehlin considers that such a finding is compatible with either a genetic or an environmental cause. All in all Loehlin finds admixture studies inconclusive and recommends more research.

Another study cited by , and by , was study which found that adopted mixed-race children's has test scores identical to children with two black parentsâreceiving no apparent "benefit" from their white ancestry. Rushton and Jensen consider admixture studies to have provided overall support for a genetic explanation though this view is not shared by , , , , nor by .

Reviewing the evidence from admixture studies considers it to be inconclusive because of too many uncontrolled variables. quotes a statement by to the effect that admixture studies have not provided a shred of evidence in favor of a genetic basis for the gap.

Mental chronometry measures the elapsed time between the presentation of a sensory stimulus and the subsequent behavioral response by the participant. This reaction time (RT) is considered a measure of the speed and efficiency with which the brain processes information. Scores on most types of RT tasks tend to correlate with scores on standard IQ tests as well as with "g", and no relationship has been found between RT and any other psychometric factors independent of "g". The strength of the correlation with IQ varies from one RT test to another, but Hans Eysenck gives 0.40 as a typical correlation under favorable conditions. According to Jensen individual differences in RT have a substantial genetic component, and heritability is higher for performance on tests that correlate more strongly with IQ. Nisbett argues that some studies have found correlations closer to 0.2, and that the correlation is not always found.

Several studies have found differences between races in average reaction times. These studies have generally found that reaction times among black, Asian and white children follow the same pattern as IQ scores. Black-white differences in reaction time, however, tend to be small (average effect size .18). have argued that reaction time is independent of culture and that the existence of race differences in average reaction time is evidence that the cause of racial IQ gaps is partially genetic instead of entirely cultural. Responding to this argument in "Intelligence and How to Get It", Nisbett has pointed to the study in which a group of Chinese Americans had longer reaction times than a group of European Americans, despite having higher IQs. Nisbett also mentions findings in and suggesting that movement time (the measure of how long it takes a person to move a finger after making the decision to do so) correlates with IQ just as strongly as reaction time, and that average movement time is faster for blacks than for whites. considers reaction time evidence unconvincing and points out that other cognitive tests that also correlate well with IQ show no disparity at all, for example the habituation/dishabituation test. And he points out that studies show that rhesus monkeys have shorter reaction times than American college students, suggesting that different reaction times may not tell us anything useful about intelligence.

A number of studies have reported a moderate statistical correlation between differences in IQ and brain size between individuals in the same group. And some scholars have reported differences in average brain sizes between Africans, Europeans, and Asians. J. P. Rushton has argued that Africans on average have smaller brain cases and brains than Europeans, that Europeans have smaller brains than East Asians, and that this is evidence that the gap is biological in nature. Critics of Rushton have argued that Rushton's arguments rest on outdated data collected by unsound methods and should be considered invalid. Recent reviews by and consider that current data does show an average difference in brain size and head-circumference between American blacks and whites, but question whether this has any relevance for the IQ gap. Nisbett et al. argue that crude brain size is unlikely to be a good measure of IQ; for example, brain size also differs between men and women, but without well-documented differences in IQ. At the same time newborn black children have the same average brain size as whites, suggesting that the difference in average size could be accounted for by differences in postnatal environment. Several factors that reduce brain size have been demonstrated to disproportionately affect black children.

Earl Hunt states that brain size is found to have a correlation of about .35 with intelligence among whites and cites studies showing that genes may account for as much as 90% of individual variation in brain size. According to Hunt, race differences in average brain size could potentially be an important argument for a possible genetic contribution to racial IQ gaps. Nonetheless, Hunt notes that Rushton's head size data would account for a difference of .09 standard deviations between black and white average test scores, less than a tenth of the 1.0 standard deviation gap in average scores that is observed. Wicherts, Borsboom, & Dolan (2010) argue that black-white differences in brain size are insufficient to explain 91% to 95% of the black-white IQ gap.

Archaeological evidence does not support claims by Rushton and others that blacks' cognitive ability was inferior to whites' during prehistoric times as a result of evolution.

The 1996 report of the APA commented on the ethics of research on race and intelligence. as well as have also discussed different possible ethical guidelines. "Nature" in 2009 featured two editorials on the ethics of research in race and intelligence by Steven Rose (against) and Stephen J. Ceci and Wendy M. Williams (for).

According to critics, research on group differences in IQ will reproduce the negative effects of social ideologies (such as Nazism or social Darwinism) that were justified in part on claimed hereditary racial differences. Steven Rose maintains that the history of eugenics makes this field of research difficult to reconcile with current ethical standards for science.

Linda Gottfredson argues that suggestion of higher ethical standards for research into group differences in intelligence is a double standard applied in order to undermine disliked results. James R. Flynn has argued that had there been a ban on research on possibly poorly conceived ideas, much valuable research on intelligence testing (including his own discovery of the Flynn effect) would not have occurred.

Jensen and Rushton argued that the existence of biological group differences does not rule out, but raises questions about the worthiness of policies such as affirmative action or placing a premium on diversity. They also argued for the importance of teaching people not to overgeneralize or stereotype individuals based on average group differences, because of the significant overlap of people with varying intelligence between different races.

Some who hold the environmentalist viewpoint argue for increased interventions in order to close the gaps. Nisbett argues that schools can be greatly improved and that many interventions at every age level are possible. Flynn, arguing for the importance of the black subculture, writes that "America will have to address all the aspects of black experience that are disadvantageous, beginning with the regeneration of inner city neighbourhoods and their schools. A resident police office and teacher in every apartment block would be a good start." Researchers from both sides agree that interventions should be better researched.

Especially in developing nations, society has been urged to take on the prevention of cognitive impairment in children as of the highest priority. Possible preventable causes include malnutrition, infectious diseases such as meningitis, parasites, cerebral malaria, in utero drug and alcohol exposure, newborn asphyxia, low birth weight, head injuries, lead poisoning and endocrine disorders.






</doc>
<doc id="26495" url="https://en.wikipedia.org/wiki?curid=26495" title="Retirement">
Retirement

Retirement is the withdrawal from one's position or occupation or from one's active working life. A person may also semi-retire by reducing work hours.

An increasing number of individuals are choosing to put off this point of total retirement, by selecting to exist in the emerging state of pre-tirement. Some people who have retired from a position with a pre-nominal title, particularly military officers, are often listed with a post-nominal indicating retirement, e.g., "Admiral John Smith (ret)" (or rtd or retd).

Many people choose to retire when they are eligible for private or public pension benefits, although some are forced to retire when bodily conditions no longer allow the person to work any longer (by illness or accident) or as a result of legislation concerning their position. In most countries, the idea of retirement is of recent origin, being introduced during the late 19th and early 20th centuries. Previously, low life expectancy and the absence of pension arrangements meant that most workers continued to work until death. Germany was the first country to introduce retirement benefits in 1889.

Nowadays, most developed countries have systems to provide pensions on retirement in old age, funded by employers or the state. In many poorer countries there is no support for the old beyond that provided through the family. Today, retirement with a pension is considered a right of the worker in many societies; hard ideological, social, cultural and political battles have been fought over whether this is a right. In many western countries, this is a right embodied in national constitutions.

Retirement, or the practice of leaving one's job or ceasing to work after reaching a certain age, has been around since around the 18th century. Prior to the 18th century, humans had an average life expectancy between 26 and 40 years.
In consequence, only a small percentage of the population reached an age where physical impairments began to be obstacles to working. Countries began to adopt government policies on retirement during the late 19th century and the 20th century, beginning in Germany under Otto von Bismarck.

A person may retire at whatever age they please. However, a country's tax laws or state old-age pension rules usually mean that in a given country a certain age is thought of as the "standard" retirement age. As life expectancy increases and more and more people live to an advanced age, in many countries the age at which a pension is awarded has been increased in the 21st century, often progressively.

The "standard" retirement age varies from country to country but it is generally between 50 and 70 (according to latest statistics, 2011). In some countries this age is different for men and women, although this has recently been challenged in some countries (e.g., Austria), and in some countries the ages are being brought into line. The table below shows the variation in eligibility ages for public old-age benefits in the United States and many European countries, according to the OECD.

"The retirement age in many countries is increasing, often starting in the 2010s and continuing until the late 2020s."

Notes: Parentheses indicate eligibility age for women when different. Sources: Cols. 1â2: OECD Pensions at a Glance (2005), Cols. 3â6: Tabulations from HRS, ELSA and SHARE. Square brackets indicate early retirement for some public employees.

In the United States, while the normal retirement age for Social Security, or Old Age Survivors Insurance (OASI) was age 65 to receive unreduced benefits, it is gradually increasing to age 67 by 2027. Public servants are often not covered by Social Security but have their own pension programs. Police officers in the United States may typically retire at half pay after 20 years of service, or three-quarter pay after 30 years, allowing retirement from the early forties. Military members of the US Armed Forces may elect to retire after 20 years of active duty. Their retirement pay (not a pension since they can be recalled to active duty at any time) is calculated on number of years on active duty, final pay grade and the retirement system in place when they entered service. Members awarded the Medal of Honor qualify for a separate stipend. Retirement pay for military members in the reserve and US National Guard is based on a point system. 

Recent advances in data collection have vastly improved our ability to understand important relationships between retirement and factors such as health, wealth, employment characteristics and family dynamics, among others. The most prominent study for examining retirement behavior in the United States is the ongoing Health and Retirement Study (HRS), first fielded in 1992. The HRS is a nationally representative longitudinal survey of adults in the U.S. ages 51+, conducted every two years, and contains a wealth of information on such topics as labor force participation (e.g., current employment, job history, retirement plans, industry/occupation, pensions, disability), health (e.g., health status and history, health and life insurance, cognition), financial variables (e.g., assets and income, housing, net worth, wills, consumption and savings), family characteristics (e.g., family structure, transfers, parent/child/grandchild/sibling information) and a host of other topics (e.g., expectations, expenses, internet use, risk taking, psychosocial, time use).

2002 and 2004 saw the introductions of the English Longitudinal Study of Ageing (ELSA) and the Survey of Health, Ageing and Retirement in Europe (SHARE), which includes respondents from 14 continental European countries plus Israel. These surveys were closely modeled after the HRS in sample frame, design and content. A number of other countries (e.g., Japan, South Korea) also now field HRS-like surveys, and others (e.g., China, India) are currently fielding pilot studies. These data sets have expanded the ability of researchers to examine questions about retirement behavior by adding a cross-national perspective.
Notes: MHAS discontinued in 2003; ELSA numbers exclude institutionalized (nursing homes). Source: Borsch-Supan et al., eds. (November 2008). Health, Ageing and Retirement in Europe (2004â2007): Starting the Longitudinal Dimension.

Many factors affect people's retirement decisions. Retirement funding education is a big factor that affects the success of an individualâs retirement experience. Social Security clearly plays an important role because most individuals solely rely on Social Security as their only retirement option, when Social Securityâs both trust funds are expected to be depleted by 2034. Knowledge affects an individualâs retirement decisions by simply finding more reliable retirement options such as, Individual Retirement Accounts or Employer-Sponsored Plans. In countries around the world, people are much more likely to retire at the early and normal retirement ages of the public pension system (e.g., ages 62 and 65 in the U.S.). This pattern cannot be explained by different financial incentives to retire at these ages since typically retirement benefits at these ages are approximately actuarially fair; that is, the present value of lifetime pension benefits (pension wealth) conditional on retiring at age "a" is approximately the same as pension wealth conditional on retiring one year later at age "a"+1. Nevertheless, a large literature has found that individuals respond significantly to financial incentives relating to retirement (e.g., to discontinuities stemming from the Social Security earnings test or the tax system).

Greater wealth tends to lead to earlier retirement, since wealthier individuals can essentially "purchase" additional leisure. Generally the effect of wealth on retirement is difficult to estimate empirically since observing greater wealth at older ages may be the result of increased saving over the working life in anticipation of earlier retirement. However, a number of economists have found creative ways to estimate wealth effects on retirement and typically find that they are small. For example, one paper exploits the receipt of an inheritance to measure the effect of wealth shocks on retirement using data from the HRS. The authors find that receiving an inheritance increases the probability of retiring earlier than expected by 4.4 percentage points, or 12 percent relative to the baseline retirement rate, over an eight-year period.

A great deal of attention has surrounded how the Financial crisis of 2007â2008 and subsequent Great Recession are affecting retirement decisions, with the conventional wisdom saying that fewer people will retire since their savings have been depleted; however recent research suggests that the opposite may happen. Using data from the HRS, researchers examined trends in defined benefit (DB) vs. defined contribution (DC) pension plans and found that those nearing retirement had only limited exposure to the recent stock market decline and thus are not likely to substantially delay their retirement. At the same time, using data from the Current Population Survey (CPS), another study estimates that mass layoffs are likely to lead to an increase in retirement almost 50% larger than the decrease brought about by the stock market crash, so that on net retirements are likely to increase in response to the crisis.

More information tells of how many who retire will continue to work, but not in the career they have had for the majority of their life. Job openings will increase in the next 5 years due to retirements of the baby boomer generation. The Over 50 population is actually the fastest growing labor groups in the US.

A great deal of research has examined the effects of health status and health shocks on retirement. It is widely found that individuals in poor health generally retire earlier than those in better health. This does not necessarily imply that poor health status leads people to retire earlier, since in surveys retirees may be more likely to exaggerate their poor health status to justify their earlier decision to retire. This justification bias, however, is likely to be small. In general, declining health over time, as well as the onset of new health conditions, have been found to be positively related to earlier retirement. Health conditions that can cause someone to retire include hypertension, diabetes mellitus, sleep apnea, joint diseases, and hyperlipidemia.

Most people are married when they reach retirement age; thus, spouse's employment status may affect one's decision to retire. On average, husbands are three years older than their wives in the U.S., and spouses often coordinate their retirement decisions. Thus, men are more likely to retire if their wives are also retired than if they are still in the labor force, and vice versa.

Researchers analyzed factors affecting retirement decisions in EU Member States:



Overall, income after retirement can come from state pensions, occupational pensions, private savings and investments (private pension funds, owned housing), donations (e.g., by children), and social benefits. In some countries an additional lump sum is granted, according to the years of work and the average pay; this is usually provided by the employer. On a personal level, the rising cost of living during retirement is a serious concern to many older adults. Health care costs play an important role.

Provision of state pensions is a significant drain on a government's budget. As life expectancy increases and the health of older people improves with medical advances, the age of entitlement to a pension has been increasing progressively since about 2010.

Older people have poorer health, and the cost of health care in retirement is large. Most countries provide universal health insurance coverage for seniors, although in the United States many people retire before they become eligible for Medicare health cover at age 65.

A useful and straightforward calculation can be done if we assume that interest, after expenses, taxes, and inflation is zero. Assume that in real (after-inflation) terms, your salary never changes during your "w" years of working life. During your "p" years of pension, you have a living standard that costs a replacement ratio "R" times as much as your living standard in your working life. Your working life living standard is your salary less the proportion of salary Z that you need to save. Calculations are per unit salary (e.g., assume salary = 1).

Then after "w" years work, retirement age accumulated savings = "wZ". To pay for pension for "p" years, necessary savings at retirement = "Rp(1-Z)"

Equate these: "wZ" = "Rp"("1-Z") and solve to give "Z" = "Rp" / ("w + Rp"). For example, if "w" = 35, "p" = 30 and "R" = 0.65 we find that we need to save a proportion "Z" = 35.78% of our salary.

Retirement calculators generally accumulate a proportion of salary up to retirement age. This shows a straightforward case, which nonetheless could be practically useful for optimistic people hoping to work for only as long as they are likely to be retired. References relevant to the zero real interest assumption are listed here

For more complicated situations, there are several online retirement calculators on the Internet. Many retirement calculators project how much an investor needs to save, and for how long, to provide a certain level of retirement expenditures. Some retirement calculators, appropriate for safe investments, assume a constant, unvarying rate of return. Monte Carlo retirement calculators take volatility into account and project the probability that a particular plan of retirement savings, investments, and expenditures will outlast the retiree. Retirement calculators vary in the extent to which they take taxes, social security, pensions, and other sources of retirement income and expenditures into account.

The assumptions keyed into a retirement calculator are critical. One of the most important assumptions is the assumed rate of real (after inflation) investment return. A conservative return estimate could be based on the real yield of Inflation-indexed bonds offered by some governments, including the United States, Canada, and the United Kingdom. The TIP$TER retirement calculator projects the retirement expenditures that a portfolio of inflation-linked bonds, coupled with other income sources like Social Security, would be able to sustain. Current real yields on United States Treasury Inflation Protected Securities (TIPS) are available at the US Treasury site. Current real yields on Canadian 'Real Return Bonds' are available at the Bank of Canada's site. As of December 2011, US Treasury inflation-linked bonds (TIPS) were yielding about 0.8% real per annum for the 30-year maturity and a noteworthy slightly negative real return for the 7-year maturity.
Many individuals use "retirement calculators" on the Internet to determine the proportion of their pay they should be saving in a tax advantaged-plan (e.g., IRA or 401-K in the US, RRSP in Canada, personal pension in the UK, superannuation in Australia).
After expenses and any taxes, a reasonable (though arguably pessimistic) long-term assumption for a safe real rate of return is zero. So in real terms, interest does not help the savings grow. Each year of work must pay its share of a year of retirement. For someone planning to work for 40 years and be retired for 20 years, each year of work pays for itself and for half a year of retirement. Hence, 33.33% of pay must be saved, and 66.67% can be spent when earned. After 40 years of saving 33.33% of pay, we have accumulated assets of 13.33 years of pay, as in the graph. In the graph to the right, the lines are straight, which is appropriate given the assumption of a zero real investment return.

The graph above can be compared with those generated by many retirement calculators. However, most retirement calculators use nominal (not "real" dollars) and therefore require a projection of both the expected inflation rate and the expected nominal rate of return. One way to work around this limitation is to, for example, enter "0% return, 0% inflation" inputs into the calculator. The Bloomberg retirement calculator gives the flexibility to specify, for example, zero inflation and zero investment return and to reproduce the graph above. The MSN retirement calculator in 2011 has as the defaults a realistic 3% per annum inflation rate and optimistic 8% return assumptions; consistency with the December 2011 US nominal bond and inflation-protected bond market rates requires a change to about 3% inflation and 4% investment return before and after retirement.

Ignoring tax, someone wishing to work for a year and then relax for a year on the same living standard needs to save 50% of pay. Similarly, someone wishing to work from age 25 to 55 and be retired for 30 years till 85 needs to save 50% of pay if government and employment pensions are not a factor and if it is considered appropriate to assume a zero real investment return. The problem that the lifespan is not known in advance can be reduced in some countries by the purchase at retirement of an inflation-indexed life annuity.

To pay for your pension, assumed for simplicity to be received at the end of each year, and taking discounted values in the manner of a net present value calculation, you need a lump sum available at retirement of:

Above we have used the standard mathematical formula for the sum of a geometric series. (Or if i =0 then the series in braces sums to p since it then has p equal terms). As an example, assume that S=60,000 per year and that it is desired to replace R=0.80, or 80%, of pre-retirement living standard for p=30 years. Assume for current purposes that a proportion z =0.25 (25%) of pay was being saved. Using i=0.02, or 2% per year real return on investments, the necessary lump sum is given by the formula as (1-0.25)*0.80*60,000*annuity-series-sum(30)=36,000*22.396=806,272 in the nation's currency in 2008â2010 terms. To allow for inflation in a straightforward way, it is best to talk of the 806,272 as being '13.43 years of retirement age salary'. It may be appropriate to regard this as being the necessary lump sum to fund 36,000 of annual supplements to any employer or government pensions that are available. It is common to not include any house value in the calculation of this necessary lump sum, so for a homeowner the lump sum pays primarily for non-housing living costs.

Will you have saved enough at retirement? Use our necessary but unrealistic assumption of a constant after-pay-rises rate of interest. At retirement you have accumulated

Â = z S ((1+i )- 1)/i 

To make the accumulation match with the lump sum needed to pay your pension:

z S (((1+i )) - 1)/i = (1-z ) R S (1 â ((1+i )) )/i

Bring z to the left hand side to give our answer, under this rough and unguaranteed method, for the proportion of pay that we should be saving:

z = R (1 â ((1+i )) )/i / [(((1+i )) - 1)/i + R (1 â ((1+i )) )/i ] (Ret-03)

Note that the special case i =0 = i means that we instead sum the geometric series by noting that we have p or w identical terms and hence z = p/(w+p). This corresponds to our graph above with the straight line real-terms accumulation.

The result for the necessary z given by (Ret-03) depends critically on the assumptions that you make. As an example, you might assume that price inflation will be 3.5% per year forever and that your pay will increase only at that same rate of 3.5%. If you assume a 4.5% per year nominal rate of interest, then (using 1.045/1.035 in real terms ) your pre-retirement and post-retirement net interest rates will remain the same, i = 0.966 percent per year and i = 0.966 percent per year. These assumptions may be reasonable in view of the market returns available on inflation-indexed bonds, after expenses and any tax. Equation (Ret-03) is readily coded in Excel and with these assumptions gives the required savings rates in the accompanying picture.

Finally, a newer method for determining the adequacy of a retirement plan is Monte Carlo simulation. This method has been gaining popularity and is now employed by many financial planners. Monte Carlo retirement calculators allow users to enter savings, income and expense information and run simulations of retirement scenarios. The simulation results show the probability that the retirement plan will be successful.

Retirement is generally considered to be "early" if it occurs before the age (or tenure) needed for eligibility for support and funds from government or employer-provided sources. Early retirees typically rely on their own savings and investments to be self-supporting, either indefinitely or until they begin receiving external support. Early retirement can also be used as a euphemistic term for being terminated from employment before typical retirement age.

While conventional wisdom has it that one can retire and take 7% or more out of a portfolio year after year, this strategy would not have worked very often in the past.

The chart at the right shows the year-to-year portfolio balances after taking $35,000 (and adjusting for inflation) from a $750,000 portfolio every year for 30 years, starting in 1973 (red line), 1974 (blue line), or 1975 (green line). While the overall market conditions and inflation affected all three about the same (since all three experienced exactly the same conditions between 1975 and 2003), the chance of making the funds last for 30 years depended heavily on what happened to the stock market in the first few years.

Those contemplating early retirement will want to know if they have enough to survive possible bear markets such as the one that would cause the hypothetical 1973 retiree's fund to be exhausted after only 20 years.

The history of the US stock market shows that one would need to live on about 4% of the initial portfolio per year to ensure that the portfolio is not depleted before the end of the retirement; this rule of thumb is a summary of one conclusion of the Trinity study, though the report is more nuanced and the conclusions and very approach have been heavily criticized (see Trinity study for details). This allows for increasing the withdrawals with inflation to maintain a consistent spending ability throughout the retirement, and to continue making withdrawals even in dramatic and prolonged bear markets. (The 4% figure does not assume any pension or change in spending levels throughout the retirement.)

When retiring prior to age , there is a 10% IRS penalty on withdrawals from a retirement plan such as a 401(k) plan or a Traditional IRA. Exceptions apply under certain circumstances. At age 59 and six months, the penalty-free status is achieved and the 10% IRS penalty no longer applies.

To avoid the 10% penalty prior to age , a person should consult a lawyer about the use of IRS rule 72 T. This rule must be applied for with the IRS. It allows the distribution of an IRA account prior to age in equal amounts of a period of either 5 years or until the age of , whichever is the longest time period, without a 10% penalty. Taxes still must be paid on the distributions.

Although the 4% initial portfolio withdrawal rate described above can be used as a rough gauge, it is often desirable to use a retirement planning tool that accepts detailed input and can render a result that has more precision. Some of these tools model only the retirement phase of the plan while others can model both the savings or accumulation phase as well as the retirement phase of the plan. For example, an analysis by "Forbes" reckoned that in 90% of historical markets, a 4% rate would have lasted for at least 30 years, while in 50% of the historical markets, a 4% rate would have been sustained for more than 40 years.

The effects of making inflation-adjusted withdrawals from a given starting portfolio can be modeled with a downloadable spreadsheet that uses historical stock market data to estimate likely portfolio returns. Another approach is to employ a retirement calculator that also uses historical stock market modeling, but adds provisions for incorporating pensions, other retirement income, and changes in spending that may occur during the course of the retirement.

Retirement might coincide with important life changes; a retired worker might move to a new location, for example a retirement community, thereby having less frequent contact with their previous social context and adopting a new lifestyle. Often retirees volunteer for charities and other community organizations. Tourism is a common marker of retirement and for some becomes a way of life, such as for so-called grey nomads. Some retired people even choose to go and live in warmer climates in what is known as retirement migration.

It has been found that Americans have six lifestyle choices as they age: continuing to work full-time, continuing to work part-time, retiring from work and becoming engaged in a variety of leisure activities, retiring from work and becoming involved in a variety of recreational and leisure activities, retiring from work and later returning to work part-time, and retiring from work and later returning to work full-time. An important note to make from these lifestyle definitions are that four of the six involve working. America is facing an important demographic change in that the Baby Boomer generation is now reaching retirement age. This poses two challenges: whether there will be a sufficient number of skilled workers in the work force, and whether the current pension programs will be sufficient to support the growing number of retired people. The reasons that some people choose to never retire, or to return to work after retiring include not only the difficulty of planning for retirement but also wages and fringe benefits, expenditure of physical and mental energy, production of goods and services, social interaction, and social status may interact to influence an individualâs work force participation decision.

Often retirees are called upon to care for grandchildren and occasionally aged parents. For many it gives them more time to devote to a hobby or sport such as golf or sailing. On the other hand, many retirees feel restless and suffer from depression as a result of their new situation. Although it is not scientifically possible to directly show that retirement either causes or contributes to depression, the newly retired are one of the most vulnerable societal groups when it comes to depression most likely due to confluence of increasing age and deteriorating health status. Retirement coincides with deterioration of one's health that correlates with increasing age and this likely plays a major role in increased rates of depression in retirees. Longitudinal and cross-sectional studies have shown that healthy elderly and retired people are as happy or happier and have an equal quality of life as they age as compared to younger employed adults, therefore retirement in and of itself is not likely to contribute to development of depression.

Many people in the later years of their lives, due to failing health, require assistance, sometimes in extremely expensive treatments â in some countries â being provided in a nursing home. Those who need care, but are not in need of constant assistance, may choose to live in a retirement home.





</doc>
<doc id="26501" url="https://en.wikipedia.org/wiki?curid=26501" title="Boeing RC-135">
Boeing RC-135

The Boeing RC-135 is a family of large reconnaissance aircraft built by Boeing and modified by a number of companies, including General Dynamics, Lockheed, LTV, E-Systems, and L3 Technologies, and used by the United States Air Force and Royal Air Force to support theater and national level intelligence consumers with near real-time on-scene collection, analysis and dissemination capabilities. Based on the C-135 Stratolifter airframe, various types of RC-135s have been in service since 1961. Unlike the C-135 and KC-135 which are recognized by Boeing as the Model 717, most of the current RC-135 fleet (with the exception of the RAF's RC-135Ws) is internally designated as the Model 739 by the company. Many variants have been modified numerous times, resulting in a large variety of designations, configurations, and program names.

The first RC-135 variant, the RC-135A, was ordered in 1962 by the United States Air Force to replace the Boeing RB-50 Superfortress. Originally nine were ordered but this was later reduced to four. Boeing allocated the variant the designation "Boeing 739-700" but they were a modified variant of the KC-135A then in production. They used the same Pratt & Whitney J57 turbojet engines as the tanker, but carried cameras in a bay just aft of the nose wheel well where the forward fuel tank was normally located. They had no in-flight refueling system and they were to be used for photographic and surveying tasks. Although the RC-135A was the first designation in the RC-135 family, it was not the first RC-135 in service. That distinction belongs to the RC-135S, which began operational reconnaissance missions in 1961, followed by the RC-135D in 1962.

The next variant ordered was the RC-135B, to be used as an electronic intelligence aircraft to replace the Boeing RB-47H Stratojet, a SIGINT platform. Unlike the earlier variants, the RC-135Bs had Pratt & Whitney TF33 turbofans rather than the older J57s. These ten aircraft were delivered directly to Martin Aircraft beginning in 1965 for installation of their operational electronics suite. By 1967, they emerged as RC-135Cs and all entered service that year. The refueling boom was not fitted and the boom operator station was used as a camera bay for a KA-59 camera. Externally, the aircraft were distinguished by the large âcheekâ antenna fairings on the forward fuselage.

The RC-135Bs were the last of the new aircraft built. All further reconnaissance variants that followed were modified aircraft, either from earlier RC-135 variants or from tankers and transports.

In 2005, the RC-135 fleet completed a series of significant airframe, navigation and powerplant upgrades which include re-engining from the TF33 to the CFM International CFM-56 (F108) engines used on the KC-135R and T Stratotanker and upgrade of the flight deck instrumentation and navigation systems to the AMP standard. The AMP standard includes conversion from analog readouts to a digital "glass cockpit" configuration.

The current RC-135 fleet is the latest iteration of modifications to this pool of aircraft dating back to the early 1960s. Initially employed by Strategic Air Command for reconnaissance, the RC-135 fleet has participated in every armed conflict involving U.S. forces during its tenure. RC-135s supported operations in Vietnam War, the Mediterranean for Operation El Dorado Canyon, Grenada for Operation Urgent Fury, Panama for Operation Just Cause, the Balkans for Operations Deliberate Force and Allied Force, and Southwest Asia for Operations Desert Shield, Desert Storm, Enduring Freedom and Iraqi Freedom. RC-135s have maintained a constant presence in Southwest Asia since the early 1990s. They were stalwarts of Cold War operations, with missions flown around the periphery of the USSR and its client states in Europe and around the world.

Originally, all RC-135s were operated by Strategic Air Command. Since 1992 they have been assigned to Air Combat Command. The RC-135 fleet is permanently based at Offutt Air Force Base, Nebraska and operated by the 55th Wing, using forward operating locations worldwide. The 55th Wing operates 22 platforms in three variants: three RC-135S Cobra Ball, two RC-135U Combat Sent, and 17 RC-135V/W Rivet Joint.

On August 9, 2010, the Rivet Joint program recognized its 20th anniversary of continuous service in Central Command, dating back to the beginning of Desert Shield. This represents the longest unbroken presence of any aircraft in the Air Force inventory. During this time it has flown over 8,000 combat missions supporting air and ground forces of Operations Desert Storm, Desert Shield, Northern Watch, Southern Watch, Iraqi Freedom and Enduring Freedom, which continues to this day.

On 22 March 2010 the British Ministry of Defence announced that it had reached agreement with the US Government to purchase three RC-135W Rivet Joint aircraft to replace the Nimrod R1, which was subsequently retired in June 2011. The aircraft, to be styled as 'Airseeker', are scheduled to be delivered by 2017 at a total cost of around Â£650 million, including provision of ground infrastructure, training of personnel and ground supporting systems. In 2013, the UK government confirmed that crews from the RAF's 51 Squadron had been training and operating alongside their USAF colleagues since 2011, having achieved in excess of 32,000 flying hours and 1,800 sorties as part of the US 55th Strategic Reconnaissance Wing at Offutt AFB. The RAF received the first RC-135W in September 2013, which was deployed from July 2014 to support coalition action against combat Islamic State of Iraq and the Levant militants in Iraq. The second aircraft was delivered seven months ahead of schedule in September 2015, with over sixty improvements incorporated ranging from upgrades to the aircraft's mission systems to engine improvements providing increased fuel efficiency and durability. In due course, the first Airseeker will receive the same upgrades. The aircraft will be air-to-air refuelled in service by USAF tankers based in Europe, as the UK does not operate boom-equipped refueling aircraft and has no plans to adapt drogue-equipped aircraft.

At least four KC-135A tankers were converted into makeshift reconnaissance platforms with no change of Mission Design Series (MDS) designation. KC-135As 55-3121, 55-3127, 59-1465, and 59-1514 were modified beginning in 1961. That year the Soviet Union announced its intention to detonate a 100 megaton thermonuclear device on Novaya Zemlya, the so-called Tsar Bomba. A testbed KC-135A (55-3127) was modified under the Big Safari program to the SPEED LIGHT BRAVO configuration in order to obtain intelligence information on the test. The success of the mission prompted conversion of additional aircraft for intelligence gathering duties.

Not to be confused with the CFM F108-powered KC-135R tanker, the KC-135R MDS was applied in 1963 to the three KC-135A reconnaissance aircraft under the Rivet Stand program. The three aircraft were 55-3121, 59-1465, and 59-1514; a fourth, serial no. 58-0126, was converted in 1969 to replace 1465 which had crashed in 1967. Externally the aircraft had varied configurations throughout their careers, but generally they were distinguished by five "towel bar" antennas along the spine of the upper fuselage and a radome below the forward fuselage. The first three aircraft retained the standard tanker nose radome, while 58-0126 was fitted with the 'hog nose' radome commonly associated with an RC-135. A trapeze-like structure in place of the refueling boom which was used to trail an aerodynamic shape housing a specialized receiver array (colloquially known as a "blivet") on a wire was installed. This was reported to be used for "Briar Patch" and "Combat Lion" missions. There were four small optically flat windows on each side of the forward fuselage. On some missions a small wing-like structure housing sensors was fitted to each side of the forward fuselage, with a diagonal brace below it. With the loss of 59-1465, KC-135A 58-0126 was modified to this standard under the Rivet Quick operational name. All four aircraft have now been lost or converted to KC-135R tanker configuration. They are among the few KC-135 tankers equipped with an aerial refueling receptacle above the cockpit, a remnant of their service as intelligence gathering platforms.

KC-135R 55-3121 was modified in 1969 by Lockheed Air Services to the unique KC-135T configuration under the Cobra Jaw program name. Externally distinguished by the 'hog nose' radome, the aircraft also featured spinning "fang" receiver antennas below the nose radome, a large blade antenna above the forward fuselage, a single 'towel bar' antenna on the spine, teardrop antennas forward of the horizontal stabilizers on each side, and the trapeze-like structure in place of the refueling boom. The aircraft briefly carried nose art consisting of the Ford Cobra Jet cartoon cobra. It was later modified into an RC-135T Rivet Dandy.

Four RC-135As (63-8058 through 8061) were photo mapping platforms utilized briefly by the Air Photographic & Charting Service, based at Turner Air Force Base, Georgia and later at Forbes Air Force Base, Kansas as part of the 1370th Photographic Mapping Wing. The mission was soon assumed by satellites, and the RC-135As were de-modified and used as staff transports. In the early 1980s they were further converted to tankers with the designation KC-135D (of the same basic configuration as the KC-135E, plus some remaining special mission equipment). Due to delays in reinstalling their original equipment, the RC-135As were the last of the entire C-135 series delivered to the USAF. The Boeing model number for the RC-135A is 739-700.

The as-delivered version of the RC-135. The RC-135B was never used operationally, as it had no mission equipment installed by Boeing. The entire RC-135B production run of ten aircraft was delivered directly to Martin Aircraft in Baltimore, Maryland for modification and installation of mission equipment under the Big Safari program. Upon completion, the RC-135Bs were re-designated RC-135C. The Boeing model number for the RC-135B is 739-445B.

Modified and re-designated RC-135B aircraft used for strategic reconnaissance duties, equipped with the AN/ASD-1 electronic intelligence (ELINT) system. This system was characterized by the large 'cheek' pods on the forward fuselage containing the Automated ELINT Emitter Locating System (AEELSÂ â not Side Looking Airborne RadarÂ â SLAR, as often quoted), as well as numerous other antennae and a camera position in the refuelling pod area of the aft fuselage. The aircraft was crewed by two pilots, two navigators, numerous intelligence gathering specialists, inflight maintenance technicians and airborne linguists. When the RC-135C was fully deployed, SAC was able to retire its fleet of RB-47H Stratojets from active reconnaissance duties. All ten continue in active service as either RC-135V Rivet Joint or RC-135U Combat Sent platforms.

The RC-135Ds, originally designated KC-135A-II, were the first reconnaissance configured C-135s given the "R" MDS designation, although they were not the first reconnaissance-tasked members of the C-135 family. They were delivered to Eielson Air Force Base, Alaska in 1962 as part of the Office Boy Project. Serial numbers were 60-0356, 60-0357, and 60-0362. The aircraft began operational missions in 1963. These three aircraft were ordered as KC-135A tankers, but delivered without refueling booms, and known as "falsie C-135As" pending the delivery of the first actual C-135A cargo aircraft in 1961. The primary Rivet Brass mission flew along the northern border of the Soviet Union, often as a shuttle mission between Eielson and RAF Upper Heyford, Oxfordshire, and later RAF Mildenhall, Suffolk, UK. The RC-135D was also used in Southeast Asia during periods when the RC-135M (see below) was unavailable. In the late 1970s, with the expansion of the RC-135 fleet powered by TF33 turbofan engines, the RC-135Ds were converted into tankers, and remain in service as receiver-capable KC-135Rs.

Originally designated C-135B-II, project name Lisa Ann, the RC-135E Rivet Amber was a one-of-a-kind aircraft equipped with a large 7 MW Hughes Aircraft phased-array radar system. Originally delivered as a C-135B, 62-4137 operated from Shemya Air Force Station, Alaska from 1966 to 1969. Its operations were performed in concert with the RC-135S Rivet Ball aircraft (see below). The radar system alone weighed over 35,000 pounds and cost over US$35 million (1960 dollars), making Rivet Amber both the heaviest C-135-derivative aircraft flying and the most expensive Air Force aircraft for its time. This prevented the forward and aft crew areas from having direct contact after boarding the aircraft. The system could track an object the size of a soccer ball from a distance of , and its mission was to monitor Soviet ballistic missile testing in the reentry phase. The power requirement for the phased array radar was enormous, necessitating an additional power supply. This took the form of a podded Lycoming T55-L5 turboshaft engine in a pod under the left inboard wing section, driving a 350kVA generator dedicated to powering mission equipment. On the opposite wing in the same location was a podded heat exchanger to permit cooling of the massive electronic components on board the aircraft. This configuration has led to the mistaken impression that the aircraft had six engines. On June 5, 1969, Rivet Amber was lost at sea on a ferry flight from Shemya to Eielson AFB for maintenance, and no trace of the aircraft or its crew was ever found.

The RC-135M was an interim type with more limited ELINT capability than the RC-135C but with extensive additional COMINT capability. They were converted from Military Airlift Command C-135B transports, and operated by the 82d Reconnaissance Squadron during the Vietnam War from Kadena AB, gathering signals intelligence over the Gulf of Tonkin and Laos with the program name Combat Apple (originally Burning Candy). There were six RC-135M aircraft, 62-4131, 62-4132, 62-4134, 62-4135, 62-4138 and 62-4139, all of which were later modified to and continue in active service as RC-135W Rivet Joints by the early 1980s.

Rivet Ball was the predecessor program to Cobra Ball and was initiated with a single RC-135S (serial 59-1491, formerly a JKC-135A) on December 31, 1961. The aircraft first operated under the Nancy Rae project as an asset of Air Force Systems Command and later as an RC-135S reconnaissance platform with Strategic Air Command under project Wanda Belle. The name Rivet Ball was assigned in January 1967. The aircraft operated from Shemya AFB, Alaska. Along with most other RC-135 variants, the RC-135S had an elongated nose radome housing an S band receiving antenna. The aircraft was characterized by ten large optically flat quartz windows on the right side of the fuselage used for tracking cameras. Unlike any other RC-135S, Rivet Ball also had a plexiglass dome mounted top center on its fuselage for the Manual Tracker position. It holds the distinction of obtaining the very first photographic documentation of Soviet Multiple Reentry vehicle (MRV) testing on October 4, 1968. On January 13, 1969 Rivet Ball was destroyed in a landing accident at Shemya when it overran the runway with no fatalities.

The RC-135S Cobra Ball is a measurement and signature intelligence MASINT collector equipped with special electro-optical instruments designed to observe ballistic missile flights at long range. The Cobra Ball monitors missile-associated signals and tracks missiles during boost and re-entry phases to provide reconnaissance for treaty verification and theater ballistic missile proliferation. The aircraft are extensively modified C-135Bs. The right wing and engines are traditionally painted black to reduce sun glare for tracking cameras.

There are three aircraft in service and they are part of the 55th Wing, 45th Reconnaissance Squadron based at Offutt Air Force Base, Nebraska. Cobra Ball aircraft were originally assigned to Shemya and used to observe ballistic missile tests on the Kamchatka peninsula in conjunction with Cobra Dane and Cobra Judy. Two aircraft were converted for Cobra Ball in 1969 and following the loss of an aircraft in 1981 another aircraft was converted in 1983. The sole RC-135X was also converted into an RC-135S in 1995 to supplement the other aircraft.

KC-135T 55-3121 was modified to RC-135T Rivet Dandy configuration in 1971. It was used to supplement the RC-135C/D/M fleet, then in short supply due to ongoing upgrades requiring airframes to be out of service. It operated under the Burning Candy operational order. In 1973 the aircraft's SIGINT gear was removed and transferred to KC-135R 58-0126, resulting in 55-3121 assuming the role of trainer, a role which it fulfilled for the remainder of its operational existence. Externally the aircraft retained the 'hog nose' radome and some other external modifications, but the aerial refueling boom and trapeze below the tail were removed, and it had no operational reconnaissance role. In this configuration it operated variously with the 376th Strategic Wing at Kadena AB, Okinawa, the 305th AREFW at Grissom AFB, Indiana, and the 6th Strategic Wing at Eielson AFB, Alaska. In 1982 the aircraft was modified with Pratt & Whitney TF33-PW102 engines and other modifications common to the KC-135E tanker program, and returned to Eielson AFB. It crashed while on approach to Valdez Airport, Alaska on 25 February 1985 with the loss of three crew members. The wreckage was not found until August 1985, six months after the accident.

The RC-135U Combat Sent is designed to collect technical intelligence on adversary radar emitter systems. Combat Sent data is collected to develop new or upgraded radar warning receivers, radar jammers, decoys, anti-radiation missiles, and training simulators.

Distinctly identified by the antenna arrays on the fuselage chin, tailcone, and wing tips, three RC-135C aircraft were converted to RC-135U (63-9792, 64-14847, & 64-14849) in the early 1970s. 63-9792 was later converted into a Rivet Joint in 1978, and all aircraft remain in service based at Offutt Air Force Base, Nebraska. Minimum crew requirements are 2 pilots, 2 navigators, 3 systems engineers, 10 electronic warfare officers, and 6 area specialists.

The RC-135V/W is the USAF's standard airborne SIGINT platform. Missions flown by the RC-135s are designated either Burning Wind or Misty Wind. Its sensor suite allows the mission crew to detect, identify and geolocate signals throughout the electromagnetic spectrum. The mission crew can then forward gathered information in a variety of formats to a wide range of consumers via Rivet Joint's extensive communications suite. The crew consists of the cockpit crew, electronic warfare officers, intelligence operators, and airborne systems maintenance personnel. All Rivet Joint airframe and mission systems modifications are performed by L-3 Communications in Greenville, Texas, under the oversight of the Air Force Materiel Command.

All RC-135s are assigned to Air Combat Command. The RC-135 is permanently based at Offutt Air Force Base, Nebraska, and operated by the 55th Wing, using various forward deployment locations worldwide.

Under the "BIG SAFARI" program name, RC-135Vs were upgraded from the RC-135C "Big Team" configuration. RC-135Ws were originally delivered as C-135B transports, and most were modified from RC-135Ms. This is the only difference between the V and W variants; both carry the same mission equipment. For many years, the RC-135V/W could be identified by the four large disc-capped MUCELS antennas forward, four somewhat smaller blade antennae aft and myriad of smaller underside antennas. Baseline 8 Rivet Joints (in the 2000s) introduced the first major change to the external RC-135V/W configuration replacing the MUCELS antennas with plain blade antennas. The configuration of smaller underside antennas was also changed significantly.

The sole RC-135X Cobra Eye was converted during the mid-to-late-1980s from a C-135B Telemetry/Range Instrumented Aircraft, serial number 62-4128, with the mission of tracking ICBM reentry vehicles. In 1993, it was converted into an additional RC-135S Cobra Ball.

The United Kingdom bought three KC-135R aircraft for conversion to RC-135W Rivet Joint standard under the Airseeker project. Acquisition of the three aircraft was budgeted at Â£634m, with entry into service in October 2014. The aircraft formed No. 51 Squadron RAF, based at RAF Waddington along with the RAF's other ISTAR assets. They are expected to remain in service until 2045.

Previously, the Royal Air Force had gathered signals intelligence with three Nimrod R1 aircraft. When the time came to upgrade the maritime Nimrods to MRA4 standard, Project Helix was launched in August 2003 to study options for extending the life of the R1 out to 2025. The option of switching to Rivet Joint was added to Helix in 2008, and the retirement of the R1 became inevitable when the MRA4 was cancelled under the UK's 2010 budget cuts. The R1's involvement over Libya in Operation Ellamy delayed its retirement until June 2011.

Helix became Project Airseeker, under which three KC-135R airframes were converted to RC-135W standard by L-3 Communications. L-3 also provides ongoing maintenance and upgrades under a long-term agreement. The three airframes are former United States Air Force KC-135Rs, all of which first flew in 1964 but will be modified to the latest RC-135W standard before delivery. The three airframes on offer to the UK are the youngest KC-135s in the USAF fleet. As of September 2010 the aircraft had approximately 23,200 flying hours, 22,200 hours and 23,200 hours.

51 Sqn personnel began training at Offutt in January 2011 for conversion to the RC-135. The first RC-135W (ZZ664) was delivered ahead of schedule to the Royal Air Force on 12 November 2013, for final approval and testing by the Defence Support and Equipment team prior to its release to service from the UK MAA. The second one was once again delivered ahead of schedule on 4 September 2015 at RAF Mildenhall in Suffolk. The third was delivered in June 2017, and entered operational service in December 2017.

Three aircraft are in service for crew training, and lack fully functional mission equipment. One TC-135S (62-4133) provides training capability for the Cobra Ball mission, and is distinguishable from combat-ready aircraft by the lack of cheeks on the forward fuselage. It was converted from an EC-135B in 1985 following the crash of the former RC-135T 55-3121, which had been used as a trainer up to that point. In addition, two TC-135Ws (62-4127 and 4129) serve as training aircraft primarily for the Rivet Joint mission, but can also provide some training capability for RC-135U Combat Sent crews. They carry considerably fewer antennas than the fully equipped aircraft, but are otherwise similar in appearance to other Rivet Joint aircraft.

United States Air Force â Air Combat Command
Royal Air Force





</doc>
<doc id="26502" url="https://en.wikipedia.org/wiki?curid=26502" title="Rumiko Takahashi">
Rumiko Takahashi

Rumiko Takahashi was born in Niigata, Japan. Although she showed little interest in manga during her childhood, she was said to occasionally doodle in the margins of her papers while attending Niigata ChÅ«Å High School. Takahashi's interest in manga did not start until later. In an interview in 2000, Takahashi said that she had always wanted to become a professional comic author since she was a child. During her university years, she enrolled in Gekiga Sonjuku, a manga school founded by Kazuo Koike, author of "Crying Freeman" and "Lone Wolf and Cub". Under his guidance Takahashi began to publish her first "dÅjinshi" creations in 1975, such as "Bye-Bye Road" and "Star of Futile Dust". Koike often urged his students to create well-thought out, interesting characters, and this influence would greatly impact Rumiko Takahashi's works throughout her career.

Takahashi's professional career began in 1978. Her first published work was the one-shot "Katte na Yatsura" ("Those Selfish Aliens"), which garnered her an honorable mention at that year's Shogakukan New Comics Contest. Later that same year, she began her first serialized story in "Weekly ShÅnen Sunday"; "Urusei Yatsura", a comedic science fiction story. She had difficulty meeting deadlines to begin with, so chapters were published sporadically until 1980. During the run of the series, she shared a small apartment with two assistants, and often slept in a closet due to a lack of space. During the same year, she published "Time Warp Trouble", "Shake Your Buddha", and the "Golden Gods of Poverty" in "Weekly ShÅnen Sunday" magazine, which would remain the home to most of her major works for the next twenty years.

During 1980, Takahashi started her second major series, "Maison Ikkoku", in "Big Comic Spirits" magazine. Written for an older audience, "Maison Ikkoku" is a romantic comedy, and Takahashi used her own experience living in an apartment complex to create the series. Takahashi managed to work on the series on and off simultaneously with "Urusei Yatsura". She concluded both series in 1987, with "Urusei Yatsura" ending at 34 volumes, and "Maison Ikkoku" at 15.

During the 1980s, Takahashi became a prolific writer of short story manga. Her stories "Laughing Target", "Maris the Chojo", and "Fire Tripper" all were adapted into original video animations (OVAs). In 1984, during the writing of "Urusei Yatsura" and "Maison Ikkoku", Takahashi took a different approach to storytelling and began the dark, macabre "Mermaid Saga". This series of short segments was published sporadically until 1994.

Another short work of Takahashi's to be published sporadically was "One-Pound Gospel". Takahashi concluded the series in 2007 after publishing chapters in 1998, 2001 and 2006. One-Pound Gospel was adapted into a live-action TV drama.

Later, in 1987, Takahashi began her third major series, "Ranma Â½". Following the late 1980s and early 1990s trend of "shÅnen" martial arts manga, "Ranma Â½" features a gender-bending twist. The series continued for nearly a decade until 1996, when it ended at 38 volumes. "Ranma Â½" and its anime adaption are cited as some of the first of their mediums to have become popular in the United States.

During the latter half of the 1990s, Rumiko Takahashi continued with short stories and her installments of "Mermaid Saga" and "One-Pound Gospel" until beginning her fourth major work, "Inuyasha". Unlike the majority of her works, "Inuyasha" has a darker tone more akin to "Mermaid Saga" and, having been serialized in "Weekly ShÅnen Sunday" from 1996 to 2008, is her longest to date. On March 5, 2009, Rumiko Takahashi released her one-shot "Unmei No Tori". On March 16, 2009, she collaborated with Mitsuru Adachi, creator of "Touch" and "Cross Game", to release a one-shot called "My Sweet Sunday". Her latest manga series, "KyÅkai no Rinne" started on April 22, 2009. This is Rumiko Takahashi's first new manga series since her previous manga series "Inuyasha" ended in June 2008. Takahashi will start her new series "MAO" in the issue #23 of "Weekly ShÅnen Sunday" on May 8, 2019.

"Urusei Yatsura", "Maison Ikkoku", "Ranma Â½", and "Inuyasha" manga were all published in English in the United States by Viz Comics. Their 1989 release of "Urusei Yatsura" halted after only a few volumes were translated, but will see a re-print in 2019.

Rumiko Takahashi started a new manga series entitled "MAO" in "Weekly ShÅnen Sunday" issue #23 released on May 8, 2019.

In 1981, "Urusei Yatsura" became the first of Takahashi's works to be animated. This series first aired on Japanese television on October 14, and went through multiple director changes during its run. Though the 195-episode TV series ended in March 1986, "Urusei Yatsura" was kept alive in anime form through OVA and movie releases through 1991. Most notable of the series directors was Mamoru Oshii, who made "Beautiful Dreamer", the second "Urusei Yatsura" movie. AnimEigo has released the entire TV series and all of the OVAs and movies except for "Beautiful Dreamer" (which was released by Central Park Media in the U.S.) in the United States in English-subtitled format, with English dubs also made for the first two TV episodes (as "Those Obnoxious Aliens") and for all of the movies.

Kitty Films, the studio that produced "Urusei Yatsura" with animation assistance from Studio Pierrot and then Studio Deen, continued their cooperation and adapted Rumiko Takahashi's second work, "Maison Ikkoku" in 1986; it debuted the week after the final TV episode of "UY". The TV series ran for 96 episodes, 3 OVAs, a movie and also a live-action movie. Studio Deen also provided animation duties on "Maison Ikkoku" and "Ranma".

"Maris the Chojo", "Fire Tripper", and "Laughing Target" were all made into OVAs during the mid-80s. Her stories "Mermaid's Forest" and "Mermaid's Scar" were also made as OVAs in Japan on 1991. They were all released, subtitled in English, in the U.S.

In 1989, Kitty Animation produced its last major series, "Ranma Â½". The series went through ups and downs in ratings until Kitty Animation finally went out of business. "Ranma Â½" was never concluded in animated form despite being 161 episodes and two movies in length. The TV series ended in 1992 amid internal turmoil within Kitty; Kitty and Studio Deen continued to produce "Ranma" OVAs until 1996.

Sunrise was the first studio after Kitty Animation to adapt a major Rumiko Takahashi series. "Inuyasha" debuted in 2000 and ended in 2004. The TV series went on for 167 episodes and spawned four major films. The first anime ended before the manga did, thus wrapping up inconclusively. However, a second Inuyasha anime series called "Inuyasha the Final Act" debuted in Japan in the fall of 2009 and ended in March 2010, finishing the series.

Viz Communications has released the anime of "Maison Ikkoku", "Ranma" and "Inuyasha" in English, in both subtitled and dubbed formats.

The year 2008 marked the 50th anniversary of "Weekly ShÅnen Sunday" and the 30th anniversary of the first publication of "Urusei Yatsura", and Rumiko Takahashi's manga work was honoured in "It's a Rumic World", a special exhibition held from July 30 to August 11 at the Matsuya Ginza department store in Tokyo. Several new pieces of animation accompanied the exhibit, including new half-hour "Ranma Â½" and "Inuyasha" ("Black Tetsusaiga") OVAs and an introductory sequence featuring characters from "Urusei Yatsura", "Ranma" and "Inuyasha" (starring the characters' original anime voice talents), which has become a popular video on YouTube. The "It's a Rumic World" exhibit was scheduled to re-open in Sendai in December 2008, at which time a new half-hour "Urusei Yatsura" OVA was scheduled to premiere. A special DVD release containing all three new OVAs was announced as coming out on January 29, 2010, with a trailer posted in September 2009. However, it is not known whether any of the new episodes will ever be released outside Japan.

"Rumiko Takahashi Anthology", animated by TMS Entertainment adapts many of her short stories from the 80s. It features her stories "The Tragedy of P", "The Merchant of Romance", "Middle-Aged Teen", "Hidden in the Pottery", "Aberrant Family F", "As Long As You Are Here", "One Hundred Years of Love", "In Lieu of Thanks", "Living Room Lovesong", "House of Garbage", "One Day Dream", "Extra-Large Size Happiness", and "The Executive's Dog". Also, a TV series of "Mermaid Saga" was produced in 2003, animating 8 of her stories.

Many of Takahashi's works have been translated into English, as well as other European languages. Takahashi said that she did not know why her works are relatively popular with English speakers. Takahashi said "Sure, there are cultural differences in my work. When I see an American comedy, even though the jokes are translated, there's always a moment when I feel puzzled and think, 'Ah, Americans would probably laugh at this more'. I suppose the same thing must happen with my books. It's inevitable. And yet, that doesn't mean my books can't be enjoyed by English-speaking readers. I feel confident that there's enough substance to them that people from a variety of cultural backgrounds can have a lot of fun reading them."

Artists that have cited Takahashi and her work as an influence include Canadian Bryan Lee O'Malley on his series "Scott Pilgrim", American Colleen Coover on her erotic series "Small Favors", Japanese Chihiro Tamaki on her manga "Walkin' Butterfly", Chinese-Australian Queenie Chan, and Thai Wisut Ponnimit. Scottish rock band Urusei Yatsura named themselves after her first work. Matt Bozon, creator of the "Shantae" video game series, cited "Ranma Â½" as a big influence on his work.

Takahashi was one of the recipients of the Inkpot Award at the 1994 San Diego Comic-Con.

In 2016, ComicsAlliance listed Takahashi as one of twelve women cartoonists deserving of lifetime achievement recognition, stating that "Any one of her projects would be the career highlight of another talent." In 2017, Takahashi was inducted into the Science Fiction and Fantasy Hall of Fame as part of the 2016 class.

In July 2018, Takahashi was inducted into the Eisner Hall of Fame. She was previously nominated for entry in 2014, 2016 and 2017.

In January 2019, Takahashi won the Grand Prix de la ville d'AngoulÃªme, becoming the second woman and second manga artist to win the award at the AngoulÃªme International Comics Festival.



</doc>
<doc id="26509" url="https://en.wikipedia.org/wiki?curid=26509" title="Riverside">
Riverside

Riverside describes anything on the bank of or alongside a river. It may refer to:










</doc>
<doc id="26511" url="https://en.wikipedia.org/wiki?curid=26511" title="RPN">
RPN

RPN may refer to:





</doc>
<doc id="26513" url="https://en.wikipedia.org/wiki?curid=26513" title="Reverse Polish notation">
Reverse Polish notation

Reverse Polish notation (RPN), also known as Polish postfix notation or simply postfix notation, is a mathematical notation in which operators follow their operands, in contrast to Polish notation (PN), in which operators precede their operands. It does not need any parentheses as long as each operator has a fixed number of operands. The description "Polish" refers to the nationality of logician Jan Åukasiewicz, who invented Polish notation in 1924.

The reverse Polish scheme was proposed in 1954 by Arthur Burks, Don Warren, and Jesse Wright and was independently reinvented by Friedrich L. Bauer and Edsger W. Dijkstra in the early 1960s to reduce computer memory access and utilize the stack to evaluate expressions. The algorithms and notation for this scheme were extended by Australian philosopher and computer scientist Charles L. Hamblin in the mid-1950s.

During the 1970s and 1980s, Hewlett-Packard used RPN in all of their desktop and hand-held calculators, and continued to use it in some models into the 2010s. In computer science, reverse Polish notation is used in stack-oriented programming languages such as Forth and PostScript.

Note: Most of what follows is about binary operators. An example of a unary operator whose standard notation may be interpreted as reverse Polish notation is the factorial, ""n"!".

In reverse Polish notation, the operators follow their operands; for instance, to add 3 and 4, one would write rather than . If there are multiple operations, operators are given immediately after their second operands; so the expression written in conventional notation would be written in reverse Polish notation: 4 is first subtracted from 3, then 5 is added to it. An advantage of reverse Polish notation is that it removes the need for parentheses that are required by infix notation. While can also be written , that means something quite different from . In reverse Polish notation, the former could be written , which unambiguously means which reduces to (which can further be reduced to -17); the latter could be written (or , if keeping similar formatting), which unambiguously means .

In comparison testing of reverse Polish notation with algebraic notation, reverse Polish has been found to lead to faster calculations, for two reasons. The first reason is that reverse Polish calculators do not need expressions to be parenthesized, so fewer operations need to be entered to perform typical calculations. Additionally, users of reverse Polish calculators made fewer mistakes than for other types of calculators. Later research clarified that the increased speed from reverse Polish notation may be attributed to the smaller number of keystrokes needed to enter this notation, rather than to a smaller cognitive load on its users. However, anecdotal evidence suggests that reverse Polish notation is more difficult for users to learn than algebraic notation.

The following algorithm evaluates postfix expressions using a stack, with the expression processed from left to right:

The following algorithm produces the same results of the previous one, but the expression is processed from right to left:

A significant difference between the two algorithms is that the first one holds only numbers on a stack, and so it is possible to write a long calculation that uses minimal stack space by putting operators as early as possible in the expression. For example, will accumulate the subtotal on the stack and the stack never has more than two items.

The infix expression can be written like this in reverse Polish notation:



The following table shows the state of the operand stack at each stage of the above left-to-right algorithm:

The above example could be rewritten by following the "chain calculation" method described by HP for their series of reverse Polish notation calculators:
As was demonstrated in the Algebraic mode, it is usually easier (fewer keystrokes) in working a problem like this to begin with the arithmetic operations inside the parentheses first.

Edsger Dijkstra invented the shunting-yard algorithm to convert infix expressions to postfix expressions (reverse Polish notation), so named because its operation resembles that of a railroad shunting yard.

There are other ways of producing postfix expressions from infix expressions. Most operator-precedence parsers can be modified to produce postfix expressions; in particular, once an abstract syntax tree has been constructed, the corresponding postfix expression is given by a simple post-order traversal of that tree.

The first computers to implement architectures enabling reverse Polish notation were the English Electric Company's KDF9 machine, which was announced in 1960 and delivered (i.e. made available commercially) in 1963, and the American Burroughs B5000, announced in 1961 and also delivered in 1963. One of the designers of the B5000, Robert S. Barton, later wrote that he developed reverse Polish notation independently of Hamblin sometime in 1958 after reading a 1954 textbook on symbolic logic by Irving Copi, where he found a reference to Polish notation, which made him read the works of Jan Åukasiewicz as well, and before he was aware of Hamblin's work. Designed by Robert "Bob" Appleby Ragen, Friden introduced reverse Polish notation to the desktop calculator market with the EC-130 supporting a four-level stack in June 1963. The successor EC-132 added a square root function in April 1965. Around 1966, the Monroe Epic calculator supported an unnamed input scheme resembling RPN as well.

Hewlett-Packard engineers designed the 9100A Desktop Calculator in 1968 with reverse Polish notation with only three stack levels, a reverse Polish notation variant later referred to as "three-level RPN". This calculator popularized reverse Polish notation among the scientific and engineering communities. The HP-35, the world's first handheld scientific calculator, introduced the classical "four-level RPN" in 1972. HP used reverse Polish notation on every handheld calculator it sold, whether scientific, financial, or programmable, until it introduced the HP-10 adding machine calculator in 1977. By this time, HP was the leading manufacturer of calculators for professionals, including engineers and accountants.

Later calculators with LCD displays in the early 1980s, such as the HP-10C, HP-11C, HP-15C, HP-16C, and the financial HP-12C calculator also used reverse Polish notation. In 1988, Hewlett-Packard introduced a business calculator, the HP-19B, without reverse Polish notation, but its 1990 successor, the HP-19BII, gave users the option of using algebraic or reverse Polish notation.

Around 1987, HP introduced RPL, an object-oriented successor to reverse Polish notation. It deviates from classical reverse Polish notation by utilizing a stack only limited by the amount of available memory (instead of three or four fixed levels) and which can hold all kinds of data objects (including symbols, strings, lists, matrices, graphics, programs, etc.) instead of just numbers. It also changed the behaviour of the stack to no longer duplicate the top register on drops (since in an unlimited stack there is no longer a top register) and the behaviour of the key so that it no longer duplicates values into Y under certain conditions, both part of the specific ruleset of the so-called "automatic memory stack" or "operational (memory) stack" in classical reverse Polish notation in order to ease some calculations and to save keystrokes, but which had shown to also sometimes cause confusion among users not familiar with these properties. From 1990 to 2003 HP manufactured the HP-48 series of graphing RPL calculators, and in 2006 introduced the HP 50g.

As of 2011, Hewlett-Packard was offering the calculator models 12C, 12C Platinum, 17bII+, 20b, 30b, 33s, 35s, 48gII (RPL) and 50g (RPL) which support reverse Polish notation. While calculators emulating classical models continue to support classical reverse Polish notation, new reverse Polish notation models feature a variant of reverse Polish notation, where the key behaves as in RPL. This latter variant is sometimes known as "entry RPN". In 2013, the HP Prime introduced a "128-level" form of entry RPN called "advanced RPN". By late 2017, only the 12C, 12C Platinum, 17bii+, 35s and Prime remain active HP models supporting reverse Polish notation.

The community-developed calculators WP 31S and WP 34S, which are based on the HP 20b/HP 30b hardware platform, support Hewlett-Packard-style classical reverse Polish notation with either a four- or an eight-level stack. A seven-level stack had been implemented in the MITS 7400C scientific desktop calculator in 1972 and an eight-level stack was already suggested by John A. Ball in 1978.

In Britain, Clive Sinclair's Sinclair Scientific and Scientific Programmable models used reverse Polish notation.

In 1974 Commodore produced the Minuteman *6 (MM6) without key and the Minuteman *6X (MM6X) with key, both implementing a form of "two-level RPN". The SR4921 RPN came with a variant of "four-level RPN" with stack levels named X, Y, Z, and W (rather than T). In contrast to Hewlett-Packard's reverse Polish notation implementation, W filled with 0 instead of its contents being duplicated on stack drops.

Prinz and Prinztronic were own-brand trade names of the British Dixons photographic and electronic goods stores retail chain, later rebranded as Currys Digital stores, and became part of DSG International. A variety of calculator models was sold in the 1970s under the Prinztronic brand, all made for them by other companies.

Among these was the PROGRAM Programmable Scientific Calculator which featured reverse Polish notation.

The Aircraft Navigation Computer Heathkit OC-1401/OCW-1401 used "five-level RPN" in 1978.

Soviet programmable calculators (MK-52, MK-61, B3-34 and earlier B3-21 models) used reverse Polish notation for both automatic mode and programming. Modern Russian calculators MK-161 and MK-152, designed and manufactured in Novosibirsk since 2007 and offered by Semico, are backwards compatible with them. Their extended architecture is also based on reverse Polish notation.

Existing implementations using reverse Polish notation include:





</doc>
<doc id="26514" url="https://en.wikipedia.org/wiki?curid=26514" title="Roald Hoffmann">
Roald Hoffmann

Roald Hoffmann (born Roald Safran; July 18, 1937) is a Polish-American theoretical chemist who won the 1981 Nobel Prize in Chemistry. He has also published plays and poetry. He is the Frank H. T. Rhodes Professor of Humane Letters, Emeritus, at Cornell University, in Ithaca, New York.

Hoffmann was born in ZÅoczÃ³w, Second Polish Republic (now Zolochiv, Ukraine), to a Polish-Jewish family, and was named in honor of the Norwegian explorer Roald Amundsen. His parents were Clara (Rosen), a teacher, and Hillel Safran, a civil engineer. After Germany invaded Poland and occupied the town, his family was placed in a labor camp where his father, who was familiar with much of the local infrastructure, was a valued prisoner. As the situation grew more dangerous, with prisoners being transferred to extermination camps, the family bribed guards to allow an escape and arranged with a Ukrainian neighbor named Mykola Dyuk for Hoffmann, his mother, two uncles and an aunt to hide in the attic and a storeroom of the local schoolhouse, where they remained for eighteen months, from January 1943 to June 1944, while Hoffmann was aged 5 to 7.

His father remained at the labor camp, but was able to occasionally visit, until he was tortured and killed by the Germans for his involvement in a plot to arm the camp prisoners. When she received the news, his mother attempted to contain her sorrow by writing down her feelings in a notebook her husband had been using to take notes on a relativity textbook he had been reading. While in hiding his mother kept Hoffmann entertained by teaching him to read and having him memorize geography from textbooks stored in the attic, then quizzing him on it. He referred to the experience as having been enveloped in a cocoon of love.

Most of the rest of the family perished in the Holocaust, though one grandmother and a few others survived. They migrated to the United States on the troop carrier "Ernie Pyle" in 1949.

Hoffmann married Eva BÃ¶rjesson in 1960. They have two children, Hillel Jan and Ingrid Helena. Hoffmann visited Zolochiv with his adult son (by then a parent of a five-year-old) in 2006 and found that the attic where he had hidden was still intact, but the storeroom had been incorporated, ironically enough, into a chemistry classroom. In 2009, a monument to Holocaust victims was built in Zolochiv on Hoffmann's initiative. He is an atheist.

Hoffmann graduated in 1955 from New York City's Stuyvesant High School, where he won a Westinghouse science scholarship. He received his bachelor of arts degree at Columbia University (Columbia College) in 1958. He earned his master of arts degree in 1960 from Harvard University. He earned his doctor of philosophy degree from Harvard University while working under joint supervision of Martin Gouterman and subsequent 1976 Nobel Prize in Chemistry winner William N. Lipscomb, Jr. Hoffman worked on the molecular orbital theory of polyhedral molecules. Under Lipscomb's direction the Extended HÃ¼ckel method was developed by Lawrence Lohr and by Roald Hoffmann. This method was later extended by Hoffmann. He went to Cornell in 1965 and has remained there, becoming professor emeritus.

Hoffmann's research and interests have been in the electronic structure of stable and unstable molecules, and in the study of transition states in reactions. He has investigated the structure and reactivity of both organic and inorganic molecules, and examined problems in organo-metallic and solid-state chemistry. Hoffman has developed semiempirical and nonempirical computational tools and methods such as the extended HÃ¼ckel method for determining molecular orbitals, which he proposed in 1963.

With Robert Burns Woodward he developed the WoodwardâHoffmann rules for elucidating reaction mechanisms and their stereochemistry. They realized that chemical transformations could be approximately predicted from subtle symmetries and asymmetries in the electron orbitals of complex molecules. Their rules predict differing outcomes, such as the types of products that will be formed when two compounds are activated by heat compared with those produced under activation by light. For this work Hoffmann received the 1981 Nobel Prize in chemistry, sharing it with Japanese chemist Kenichi Fukui, who had independently resolved similar issues. (Woodward was not included in the prize, which is given only to living persons, although he had won the 1965 prize for other work.) In his Nobel Lecture, Hoffmann introduced the isolobal analogy for predicting the bonding properties of organometallic compounds.

Some of Hoffman's most recent work, with Neil Ashcroft and Vanessa Labet, examines bonding in matter under extreme high pressure.

In 1988 Hoffmann became the series host in a 26-program PBS education series by Annenberg/CPB, "The World of Chemistry", opposite with series demonstrator Don Showalter. While Hoffmann introduced a series of concepts and ideas, Showalter provided a series of demonstrations and other visual representations to help students and viewers to better understand the information.

Since the spring of 2001, Hoffmann has been the host of the monthly series "Entertaining Science" at New York City's Cornelia Street Cafe, which explores the juncture between the arts and science.

He has published books on the connections between art and science: "Roald Hoffmann on the Philosophy, Art, and Science of Chemistry" and "Beyond the Finite: The Sublime in Art and Science".

Hoffmann is also a writer of poetry. His collections include "The Metamict State" (1987, ), "Gaps and Verges" (1990, ), and "Chemistry Imagined", co-produced with artist Vivian Torrence.

He co-authored with Carl Djerassi the play "Oxygen", about the discovery of oxygen and the experience of being a scientist. Hoffman's play, "Should've" (2006) about ethics in science in art, has been produced in workshops; as has a play based on his experiences in the holocaust, "We Have Something That Belongs to You" (2009), later retitled "Something That Belongs to You.

In 1981, Hoffmann received the Nobel Prize in Chemistry, which he shared with Kenichi Fukui "for their theories, developed independently, concerning the course of chemical reactions".

Hoffmann has won many other awards, and is the recipient of more than 25 honorary degrees.


Hoffmann is a member of the International Academy of Quantum Molecular Science and the Board of Sponsors of The Bulletin of the Atomic Scientists.

In August 2007, the American Chemical Society held a symposium at its biannual national meeting to honor Hoffmann's 70th birthday.

In 2008, the GÃ¶ttingen Academy of Sciences and Humanities awarded him its Lichtenberg Medal.

In August 2017, another symposium was held at the 254th American Chemical Society National Meeting in Washington DC, to honor Hoffmann's 80th birthday.


</doc>
<doc id="26515" url="https://en.wikipedia.org/wiki?curid=26515" title="Rhotic consonant">
Rhotic consonant

In phonetics, rhotic consonants, or "R-like" sounds, are liquid consonants that are traditionally represented orthographically by symbols derived from the Greek letter rho, including , in the Latin script and , in the Cyrillic script. They are transcribed in the International Phonetic Alphabet by upper- or lower-case variants of Roman , : , , , , , , , and .

This class of sounds is difficult to characterise phonetically; from a phonetic standpoint, there is no single articulatory correlate (manner or place) common to rhotic consonants. Rhotics have instead been found to carry out similar phonological functions or to have certain similar phonological features across different languages. Although some have been found to share certain acoustic peculiarities, such as a lowered third formant, further study has revealed that this does not hold true across different languages. For example, the acoustic quality of lowered third formants pertains almost exclusively to American varieties of English. Being "R-like" is an elusive and ambiguous concept phonetically and the same sounds that function as rhotics in some systems may pattern with fricatives, semivowels or even stops in othersâfor example, the alveolar tap is a rhotic consonant in many languages; but in American English it is an allophone of the stop phoneme /t/, as in "water". It is likely that rhotics, then, are not a phonetically natural class, but a phonological one instead.

Some languages have rhotic and non-rhotic varieties, which differ in the incidence of rhotic consonants. In non-rhotic accents of English, /r/ is not pronounced unless it is followed directly by a vowel.

The most typical rhotic sounds found in the world's languages are the following:



In broad transcription rhotics are usually symbolised as unless there are two or more types of rhotic in the same language; for example, most Australian Aboriginal languages, which contrast approximant and trill , use the symbols "r" and "rr" respectively. The IPA has a full set of different symbols which can be used whenever more phonetic precision is required: an "r" rotated 180Â° for the alveolar approximant, a small capital "R" for the uvular trill, and a flipped small capital "R" for the voiced uvular fricative or approximant.

The fact that the sounds conventionally classified as "rhotics" vary greatly in both place and manner in terms of articulation, and also in their acoustic characteristics, has led several linguists to investigate what, if anything, they have in common that justifies grouping them together. One suggestion that has been made is that each member of the class of rhotics shares certain properties with other members of the class, but not necessarily the same properties with all; in this case, rhotics have a "family resemblance" with each other rather than a strict set of shared properties. Another suggestion is that rhotics are defined by their behaviour on the sonority hierarchy, namely, that a rhotic is any sound that patterns as being more sonorous than a lateral consonant but less sonorous than a vowel. The potential for variation within the class of rhotics makes them a popular area for research in sociolinguistics.

English has rhotic and non-rhotic accents. Rhotic speakers pronounce a historical in all instances, while non-rhotic speakers only pronounce before or between vowels.

The rhotic consonant is dropped or vocalized under similar conditions in other Germanic languages, notably German, Danish and Dutch from the eastern Netherlands (because of Low German influence) and southern Sweden (possibly because of its Danish history). In most varieties of German (with the notable exception of Swiss Standard German), in the syllable coda is frequently realized as a vowel or a semivowel, or . In the traditional standard pronunciation, this happens only in the unstressed ending "-er" and after long vowels: for example "besser" , "sehr" . In common speech, the vocalization is usual after short vowels as well, and additional contractions may occur: for example "Dorn" ~ , "hart" ~ . Similarly, Danish after a vowel is, unless followed by a stressed vowel, either pronounced ("mor" "mother" , "nÃ¦ring" "nourishment" ) or merged with the preceding vowel while usually influencing its vowel quality ( and or are realised as long vowels and , and , and are all pronounced ) ("lÃ¸ber" "runner" , "SÃ¸ren Kierkegaard" (personal name) ).

In Asturian, word final is always lost in infinitives if they are followed by an enclitic pronoun, and this is reflected in the writing; e.g. The infinitive form "dar" plus the 3rd plural dative pronoun "-yos" "da-yos" (give to them) or the accusative form "los" "dalos" (give them). This will happen even in southern dialects where the infinitive form will be "dare" , and both the and the vowel will drop (da-yos, not *dÃ¡re-yos). However, most of the speakers also drop the rhotics in the infinitive before a lateral consonant of a different word, and this doesn't show in the writing. e.g. "dar los dos" (give the two [things]). This doesn't occur in the middle of words. e.g. the name "Carlos" .

In some Catalan dialects, word final is lost in coda position not only in suffixes on nouns and adjectives denoting the masculine singular and plural (written as "-r", "-rs") but also in the "-"ar", -"er", -"ir"" suffixes of infinitives; e.g. "forner" "(male) baker", "forners" , "fer" "to do", "lluir" "to shine, to look good". However, rhotics are "recovered" when followed by the feminine suffix "-a" , and when infinitives have single or multiple enclitic pronouns (notice the two rhotics are neutralized in the coda, with a tap occurring between vowels, and a trill elsewhere); e.g. "fornera" "(female) baker", "fer-lo" "to do it (masc.)", "fer-ho" "to do it/that/so", "lluir-se" "to excel, to show off".

Final R is generally not pronounced in words ending in -er. The R in "parce que" (because) is not pronounced in informal speech in French.

In Indonesian, which is a form of Malay, the final is pronounced, it has varying forms of Malay spoken on the Malay Peninsula. In Indonesia, it is usually a tap version, but for some Malaysian, it is a retroflex r.

Historical final has been lost from all Khmer dialects but Northern.

In some dialects of Brazilian Portuguese, is unpronounced or aspirated. This occurs most frequently with verbs in the infinitive, which is always indicated by a word-final . In some states, however, it happens mostly with any when preceding a consonant. The "Carioca" accent (from the city of Rio de Janeiro) is notable for this.

Among the Spanish dialects, Andalusian Spanish, Caribbean Spanish (descended from and still very similar to Andalusian and Canarian Spanish), CastÃºo (the Spanish dialect of Extremadura), Northern Colombian Spanish (in cities like Cartagena, MonterÃ­a, San AndrÃ©s and Santa Marta, but not Barranquilla, which is mostly rhotic) and the Argentine dialect spoken in the TucumÃ¡n province may have an unpronounced word-final , especially in infinitives, which mirrors the situation in some dialects of Brazilian Portuguese. However, in Antillean Caribbean forms, word-final in infinitives and non-infinitives is often in free variation with word-final and may relax to the point of being articulated as .

L and R are used interchangeably in Thai. That is, Thai speakers generally replace an R(à¸£) with an L(à¸¥) and when they hear L(à¸¥) they may write R (à¸£). 

Among the Turkic languages, Turkish displays more or less the same feature, as syllable-final is dropped. For example, it is very common to hear phrases like "gidiyo" instead of "gidiyor", in spoken Turkish. In some parts of Turkey, e.g. Kastamonu, the syllable-final is almost never pronounced, e.g. "gidiya" instead of "gidiyor" (meaning "she/he is going"), "gide" instead of "gider" (meaning "she/he goes"). In "gide", the preceding vowel e is lengthened and pronounced somewhat between an e and a.

Among the Turkic languages, Uyghur displays more or less the same feature, as syllable-final is dropped, while the preceding vowel is lengthened: for example "Uyghurlar" âUyghursâ. The may, however, sometimes be pronounced in unusually "careful" or "pedantic" speech; in such cases, it is often mistakenly inserted after long vowels even when there is no phonemic there.

Similarly in Yaqui, an indigenous language of northern Mexico, intervocalic or syllable-final is often dropped with lengthening of the previous vowel: "pariseo" becomes , "sewaro" becomes .

Lacid, whose exonyms in various literature include Lashi, Lachik, Lechi, and Leqi, is a Tibeto-Burman language spoken by the Lacid people. There are various reports of their population size ranging from 30,000 to 60,000 people. The majority are in Myanmar but there are also small groups located in China and Thailand. Noftz (2017) reports finding an example of a rhotic alveolar fricative in Lacid while doing phonological research at Payap University in Thailand in 2015. He was not able to continue his research and expressed the need for further examination of the segment to verify his results. It is postulated that the segment is a remnant of the rhotic fricative in Proto-Tibeto-Burman.

The Shekaki accent of the Kurmanji dialect of Kurdish is non-rhotic, that is the postvocalic flap "r" is not pronounced but the trill "R" is. When r is omitted, a "compensatory lengthening" of the preceding vowel takes place. For example:
Shekaki retains morphological syllables instead of phonological syllables in non-rhotic pronunciation. 



</doc>
<doc id="26517" url="https://en.wikipedia.org/wiki?curid=26517" title="Richard Hell">
Richard Hell

Richard Lester Meyers (born October 2, 1949), better known by his stage name Richard Hell, is an American singer, songwriter, bass guitarist and writer.

Richard Hell was an innovator of punk music and fashion. He was one of the first to spike his hair and wear torn, cut and drawn-on shirts, often held together with safety pins.

Malcolm McLaren, manager of the Sex Pistols, credited Hell as a source of inspiration for the Sex Pistols' look and attitude, as well as the safety-pin and graphics accessorized clothing that McLaren sold in his London shop, Sex.
Hell was in several important, early punk bands, including Neon Boys, Television and The Heartbreakers, after which he formed Richard Hell & the Voidoids. Their 1977 album "Blank Generation" influenced many other punk bands. Its title track was named "One of the 500 Songs That Shaped Rock" by music writers in the Rock and Roll Hall of Fame listing and is ranked as one of the all-time Top 10 punk songs by a 2006 poll of original British punk figures, as reported in the "Rough Guide to Punk".

Since the late 1980s, Hell has devoted himself primarily to writing, publishing two novels and several other books. He was the film critic for "BlackBook" magazine from 2004 to 2006.

Richard Lester Meyers was born in Lexington, Kentucky in 1949. His father, a secular Jew, was an experimental psychologist, researching animal behavior. He died when Hell was 7 years old. Hell was then raised by his mother, who came from Methodists of Welsh and English ancestry. After her husband's death, she returned to school and became a professor.

Hell attended the Sanford School in Delaware for one year, where he became friends with Tom Miller, who later changed his name to Tom Verlaine. They ran away from school together and a short time later were arrested in Alabama for arson and vandalism.

Hell never finished high school, instead moving to New York City to make his way as a poet. In New York he met fellow young poet David Giannini, and moved to Santa Fe, New Mexico for several months, where Giannini and Meyers co-founded "Genesis:Grasp". They used an AM VariTyper with changeable fonts to publish the magazine. They began publishing books and magazines, but decided to go their separate ways in 1971, after which Hell created and published Dot Books.

Before he was 21, his own poems were published in numerous periodicals, ranging from "Rolling Stone" to the New Directions "Annual"s. In 1971, along with Verlaine, Hell also published under the pseudonym Theresa Stern, a fictional poet whose photo was actually a combination of both his and Verlaine's faces in drag, superimposed over one another to create a new identity. A book of poems credited to "Stern", "Wanna Go Out?", was released by Dot in 1973.

In 1972, Verlaine joined Hell in New York and formed the Neon Boys. In 1974, the band added a second guitarist, Richard Lloyd, and changed their name to Television.

Television's performances at CBGB helped kick-start the first wave of punk bands, inspiring a number of different artists including Patti Smith, who wrote the first press review of Television for the "SoHo Weekly News" in June 1974. She formed a highly successful band of her own, the Patti Smith Group. Television was one of the early bands to play at CBGB because their manager, Terry Ork, persuaded owner Hilly Kristal to book them alongside the Ramones. They also built the club's first stage. Hell started playing his punk rock anthem "Blank Generation" during his time in Television. In early 1975, Hell parted ways with Television after a dispute over creative control. Hell claimed that he and Verlaine had originally divided the songwriting evenly but that later Verlaine sometimes refused to play Hell's songs. Verlaine remained silent on the subject.

Hell left Television the same week that Jerry Nolan and Johnny Thunders quit the New York Dolls. In May 1975, the three of them formed the Heartbreakers (not to be confused with Tom Petty's band, which adopted the same name the following year). After one show, Walter Lure joined the Heartbreakers as a second guitarist. Four Heartbreakers demo tracks, recorded while Hell was still in the band, were later released on that band's "L.A.M.F. Definitive Edition" reissue. A live album recorded with Hell in 1975 was released as "What Goes Around..." in 1991.

In early 1976, Hell quit the Heartbreakers and started Richard Hell and the Voidoids with Robert Quine, Ivan Julian and Marc Bell. The band released two albums, though the second, "Destiny Street", retained only Quine from the original group, with Naux (Juan Maciel) on guitar and Fred Maher on drums. Hell's best known songs with the Voidoids included "Blank Generation", "Love Comes in Spurts", "The Kid With the Replaceable Head" and "Time". In 2009, the guitar tracks on "Destiny Street" were re-recorded and released as "Destiny Street Repaired", with guitarists Julian, Marc Ribot and Bill Frisell playing to the original rhythm tracks.
Also in 2009, Hell gave his blessing to the public access program Pancake Mountain to create an animated music video for "The Kid with the Replaceable Head". It was the Voidoids' first and only official music video. The cut used for the animation appears on Hell's 2005 retrospective album, "Spurts, The Richard Hell Story".

Hell's only other album release was as part of the band Dim Stars, for which he came out of retirement for a month in the early 1990s. Dim Stars featured guitarist Thurston Moore and drummer Steve Shelley from Sonic Youth, Gumball's guitarist Don Fleming, and Quine. They formed only to record a 1991 EP and a 1992 album, both titled "Dim Stars", and played one show in public, a WFMU benefit at The Ritz in Manhattan. Hell played bass, sang lead vocals and wrote the lyrics for the album.

Hell also guested on the 1993 "Roller Coaster" album by Shotgun Rationale, and co-wrote and sang lead vocals on the song "Never Mind" by the Heads, a 1996 collaborative effort between three former members of Talking Heads.

"The Voidoid", a novella written in 1973, was finally published by CodeX in 1993. It was reissued in 2009 by 38th Street Publishers with illustrations by Kier Cooke Sandvik.

Early poetry collections by Hell include "I Was a Spiral on the Floor" (1988) and "Across the Years" (1992), both published by Soyo Publications.

"Artifact: Notebooks from Hell 1974â1980", a collection of Hell's punk-era journals, was released in 1990 by Hanuman Books.

In 1996, Scribner published Hell's first full-length novel, "Go Now", set in 1980 and drawn largely from his own experiences.

Hell released a collection of short pieces (poems, essays and drawings) called "Hot and Cold" in 2001. His second novel, "Godlike", was published in 2005 by Akashic Books as part of Dennis Cooper's Little House on the Bowery Series. Also published in 2005 was "Rabbit Duck", a book of 13 poems written in collaboration with David Shapiro. More recent works include "Psychopts" (2008), a collaboration with artist Christopher Wool, as well as "Disgusting" (2010) and "I Dreamed I Was a Very Clean Tramp" (2013).

Hell's nonfiction has been widely anthologized, including a number of appearances in "best music writing" collections. "The Toilet Paper Columns" (2007) compiled his columns for the Colorado alternative magazine "Toilet Paper", while "Massive Pissed Love: Nonfiction 2001-2014" was issued by Soft Skull Press in 2015.

Hell's archive of his manuscripts, tapes, correspondence (written and email), journals and other documents of his life was purchased for $50,000 by New York University's Fales Library in 2003.

A mural in Hell's hometown of Lexington, Kentucky, created by students from Lexington Montessori High School, was completed in June 2019. The mural, located in the city's North Limestone neighborhood, has three parts: two profiles of Hell, and a quote from his autobiography, "I Dreamed I Was a Very Clean Tramp". "This was in Lexington, Ky. when everybody was a kid. I looked for caves and birds and ran away from home. My favorite thing to do was to run away. The words âletâs run awayâ still sounds magical to me."

Hell has appeared in several low-budget films, most notably Susan Seidelman's "Smithereens". Other acting appearances include Ulli Lommel's "Blank Generation", Nick Zedd's "Geek Maggot Bingo", Rachel Amadeo's "What About Me?" and Rachid Kerdouche's "Final Reward". Hell had a non-speaking cameo role as Madonna's murdered boyfriend in Seidelman's 1985 "Desperately Seeking Susan".

Hell was married to Scandal's Patty Smyth for two years during 1985â86, and they had a daughter, Ruby. Hell married Sheelagh Bevan in 2002.











</doc>
<doc id="26520" url="https://en.wikipedia.org/wiki?curid=26520" title="Rob Roy (cocktail)">
Rob Roy (cocktail)

The Rob Roy is a cocktail consisting primarily of whisky and vermouth, created in 1894 by a bartender at the Waldorf Astoria in Manhattan, New York City. The drink was named in honor of the premiere of "Rob Roy", an operetta by composer Reginald De Koven and lyricist Harry B. Smith loosely based upon Scottish folk hero Rob Roy MacGregor.

A Rob Roy is similar to a Manhattan, but is made exclusively with Scotch whisky, while the Manhattan is traditionally made with rye and today commonly made with bourbon or Canadian whisky.

Like the Manhattan, the Rob Roy can be made "sweet", "dry", or "perfect". The standard Rob Roy is the sweet version, made with sweet vermouth, so there is no need to specify a "sweet" Rob Roy when ordering. A "dry" Rob Roy is made by replacing the sweet vermouth with dry vermouth. A "perfect" Rob Roy is made with equal parts sweet and dry vermouth.

The Rob Roy includes a dash of Angostura bitters (mostly for color) and is usually served in a cocktail glass and garnished with two maraschino cherries on a skewer (for the standard version) or a lemon twist (for the perfect and dry versions).



</doc>
<doc id="26521" url="https://en.wikipedia.org/wiki?curid=26521" title="Rob Roy">
Rob Roy

Rob Roy most often refers to the Scottish hero Rob Roy MacGregor (, 1671â1734).

Rob Roy may also refer to:







</doc>
<doc id="26524" url="https://en.wikipedia.org/wiki?curid=26524" title="Rogue">
Rogue

A rogue is a person or entity that flouts accepted norms of behavior. 

Rogue or rogues may also refer to:













</doc>
<doc id="26526" url="https://en.wikipedia.org/wiki?curid=26526" title="Referential transparency">
Referential transparency

Referential transparency and referential opacity are properties of parts of computer programs. An expression is called referentially transparent if it can be replaced with its corresponding value without changing the program's behavior. This requires that the expression be pure, that is to say the expression value must be the same for the same inputs and its evaluation must have no side effects. An expression that is not referentially transparent is called referentially opaque.

In mathematics all function applications are referentially transparent, by the definition of what constitutes a mathematical function. However, this is not always the case in programming, where the terms procedure and method are used to avoid misleading connotations. In functional programming only referentially transparent functions are considered. Some programming languages provide means to guarantee referential transparency. Some functional programming languages enforce referential transparency for all functions.

The importance of referential transparency is that it allows the programmer and the compiler to reason about program behavior as a rewrite system. This can help in proving correctness, simplifying an algorithm, assisting in modifying code without breaking it, or optimizing code by means of memoization, common subexpression elimination, lazy evaluation, or parallelization.

The concept seems to have originated in Alfred North Whitehead and Bertrand Russell's "Principia Mathematica" (1910â13). It was adopted in analytical philosophy by Willard Van Orman Quine. In Â§30 of "Word and Object" (1960) Quine gives this definition:

A mode of containment Ï is referentially transparent if, whenever an occurrence of a singular term t is purely referential in a term or sentence Ï(t), it is purely referential also in the containing term or sentence Ï(Ï(t)).

The term appeared in its contemporary computer science usage, in the discussion of variables in programming languages, in Christopher Strachey's seminal set of lecture notes "Fundamental Concepts in Programming Languages" (1967). The lecture notes referenced Quine's "Word and Object" in the bibliography.

If all functions involved in the expression are pure functions, then the expression is referentially transparent.

Consider a function that returns the input from some source. In pseudocode, a call to this function might be codice_1 where codice_2 might identify a particular disk file, the keyboard, etc. Even with identical values of codice_2, the successive return values will be different. Therefore, function codice_4 is neither deterministic nor referentially transparent.

A more subtle example is that of a function that has a free variable, i.e., depends on some input that is not explicitly passed as a parameter. This is then resolved according to name binding rules to a non-local variable, such as a global variable, a variable in the current execution environment (for dynamic binding), or a variable in a closure (for static binding). Since this variable can be altered without changing the values passed as parameter, the results of subsequent calls to the function may differ even if the parameters are identical. However, in pure functional programming, destructive assignment is not allowed, and thus if the free variable is statically bound to a value, the function is still referentially transparent, as neither the non-local variable nor its value can change, due to static binding and immutability, respectively.

Arithmetic operations are referentially transparent: codice_5 can be replaced by codice_6, for instance. In fact, all functions in the mathematical sense are referentially transparent: codice_7 is transparent, since it will always give the same result for each particular codice_8.

Assignments are not transparent. For instance, the C expression codice_9 changes the value assigned to the variable codice_8. Assuming codice_8 initially has value codice_12, two consecutive evaluations of the expression yield, respectively, codice_13 and codice_14. Clearly, replacing codice_9 with either codice_13 or codice_14 gives a program with different meaning, and so the expression is not referentially transparent. However, calling a function such as codice_18 "is" transparent, as it will not implicitly change the input x and thus has no such side effects.

codice_19 is not transparent, as if you evaluate it and replace it by its value (say, "Jan 1, 2001"), you don't get the same result as you will if you run it tomorrow. This is because it depends on a state (the date).

In languages with no side-effects, like Haskell, we can substitute equals for equals: i.e. if codice_20 then codice_21. This is a property also known as indistinguishable identicals, see Identity of indiscernibles. Such properties need not hold in general for languages with side-effects. Even so it is important to limit such assertions to so-called judgmental equality, that is the equality of the terms as tested by the system, not including user defined equivalence for types. For instance, if codice_22 and the type codice_23 has overridden the notion of equality, e.g. making all terms equal, then it is possible to test codice_24 and yet find codice_25. This is because systems like Haskell do not verify that functions defined on types with user defined equivalence relations be well-defined with respect to that equivalence. Thus the referential transparency is limited to types without equivalence relations. To extend referential transparency to user-defined equivalence relations can be done for example with a Martin-Lof Identity Type, but requires a dependently typed system such as in Agda, Coq or Idris.

If the substitution of an expression with its value is valid only at a certain point in the execution of the program, then the expression is not referentially transparent. The definition and ordering of these sequence points are the theoretical foundation of imperative programming, and part of the semantics of an imperative programming language.

However, because a referentially transparent expression can be evaluated at any time, it is not necessary to define sequence points nor any guarantee of the order of evaluation at all. Programming done without these considerations is called purely functional programming.

One advantage of writing code in a referentially transparent style is that given an intelligent compiler, static code analysis is easier and better code-improving transformations are possible automatically. For example, when programming in C, there will be a performance penalty for including a call to an expensive function inside a loop, even if the function call could be moved outside of the loop without changing the results of the program. The programmer would be forced to perform manual code motion of the call, possibly at the expense of source code readability. However, if the compiler is able to determine that the function call is referentially transparent, it can perform this transformation automatically.

The primary disadvantage of languages that enforce referential transparency is that they make the expression of operations that naturally fit a sequence-of-steps imperative programming style more awkward and less concise. Such languages often incorporate mechanisms to make these tasks easier while retaining the purely functional quality of the language, such as definite clause grammars and monads.

As an example, let's use two functions, one which is referentially opaque, and the other which is referentially transparent:

The function codice_26 is referentially transparent, which means that codice_27 if codice_28. For instance, codice_29, and so on. However, we can't say any such thing for codice_30 because it uses a global variable that it modifies.

The referential opacity of codice_30 makes reasoning about programs more difficult. For example, say we wish to reason about the following statement:

One may be tempted to simplify this statement to:

However, this will not work for codice_32 because each occurrence of codice_33 evaluates to a different value. Remember that the return value of codice_30 is based on a global value that isn't passed in and which gets modified on each call to codice_30. This means that mathematical identities such as formula_1 no longer hold.

Such mathematical identities "will" hold for referentially transparent functions such as codice_26.

However, a more sophisticated analysis can be used to simplify the statement to:

This takes more steps and requires a degree of insight into the code infeasible for compiler optimization.

Therefore, referential transparency allows us to reason about our code which will lead to more robust programs, the possibility of finding bugs that we couldn't hope to find by testing, and the possibility of seeing opportunities for optimization.





</doc>
<doc id="26529" url="https://en.wikipedia.org/wiki?curid=26529" title="Reaganomics">
Reaganomics

Reaganomics (; a portmanteau of "[Ronald] Reagan" and "economics" attributed to Paul Harvey), or Reaganism, refers to the economic policies promoted by U.S. President Ronald Reagan during the 1980s. These policies are commonly associated with and characterized as supply-side economics or trickle-down economics or voodoo economics by political opponents, while Reagan and his political advocates preferred to call it free-market economics.

The four pillars of Reagan's economic policy were to reduce the growth of government spending, reduce the federal income tax and capital gains tax, reduce government regulation, and tighten the money supply in order to reduce inflation.

The results of Reaganomics are still debated. Supporters point to the end of stagflation, stronger GDP growth, and an entrepreneur revolution in the decades that followed. Critics point to the widening income gap, what they described as an atmosphere of greed, and the national debt tripling in eight years which ultimately reversed the post-World War II trend of a shrinking national debt as percentage of GDP.

Prior to the Reagan administration, the United States economy experienced a decade of high unemployment and persistently high inflation (known as stagflation). Attacks on Keynesian economic orthodoxy as well as empirical economic models such as the Phillips Curve grew. Political pressure favored stimulus resulting in an expansion of the money supply. President Richard Nixon's wage and price controls were phased out. The federal oil reserves were created to ease any future short term shocks. President Jimmy Carter had begun phasing out price controls on petroleum while he created the Department of Energy. Much of the credit for the resolution of the stagflation is given to two causes: a three-year contraction of the money supply by the Federal Reserve Board under Paul Volcker, initiated in the last year of Carter's presidency, and long-term easing of supply and pricing in oil during the 1980s oil glut.

In stating that his intention was to lower taxes, Reagan's approach was a departure from his immediate predecessors. Reagan enacted lower marginal tax rates as well as simplified income tax codes and continued deregulation. During Reagan's eight year presidency, the annual deficits averaged 4.0% of GDP, compared to a 2.2% average during the preceding eight years. The real (inflation adjusted) average rate of growth in federal spending fell from 4% under Jimmy Carter to 2.5% under Ronald Reagan. GDP per employed person increased at an average 1.5% rate during the Reagan administration, compared to an average 0.6% during the preceding eight years. Private sector productivity growth, measured as real output per hour of all persons, increased at an average rate of 1.9% during Reagan's eight years, compared to an average 1.3% during the preceding eight years. Federal net outlays as a percent of GDP averaged 21.4% under Reagan, compared to 19.1% during the preceding eight years.

During the Nixon and Ford Administrations, before Reagan's election, a combined supply and demand side policy was considered unconventional by the moderate wing of the Republican Party. While running against Reagan for the Presidential nomination in 1980, George H. W. Bush had derided Reaganomics as "voodoo economics". Similarly, in 1976, Gerald Ford had severely criticized Reagan's proposal to turn back a large part of the Federal budget to the states.

In his 1980 campaign speeches, Reagan presented his economic proposals as a return to the free enterprise principles, free market economy that had been in favor before the Great Depression and FDR's New Deal policies. At the same time he attracted a following from the supply-side economics movement, which formed in opposition to Keynesian demand-stimulus economics. This movement produced some of the strongest supporters for Reagan's policies during his term in office.

The contention of the proponents, that the tax rate cuts would more than cover any increases in federal debt, was influenced by a theoretical taxation model based on the elasticity of tax rates, known as the Laffer curve. Arthur Laffer's model predicts that excessive tax rates actually reduce potential tax revenues, by lowering the incentive to produce; the model also predicts that insufficient tax rates (rates below the optimum level for a given economy) lead directly to a reduction in tax revenues.

Ronald Reagan also cited the 14th-century Arab scholar Ibn Khaldun as an influence on his supply-side economic policies, in 1981. Reagan paraphrased Ibn Khaldun, who said that "in the beginning of the dynasty, great tax revenues were gained from small assessments," and that "at the end of the dynasty, small tax revenues were gained from large assessments." Reagan said his goal is "trying to get down to the small assessments and the great revenues."

Reagan lifted remaining domestic petroleum price and allocation controls on January 28, 1981, and lowered the oil windfall profits tax in August 1981. He ended the oil windfall profits tax in 1988. During the first year of Reagan's presidency, federal income tax rates were lowered significantly with the signing of the Economic Recovery Tax Act of 1981, which lowered the top marginal tax bracket from 70% to 50% and the lowest bracket from 14% to 11%. This act slashed estate taxes and trimmed taxes paid by business corporations by $150 billion over a five-year period. In 1982 Reagan agreed to a rollback of corporate tax cuts and a smaller rollback of individual income tax cuts. The 1982 tax increase undid a third of the initial tax cut. In 1983 Reagan instituted a payroll tax increase on Social Security and Medicare hospital insurance. In 1984 another bill was introduced that closed tax loopholes. According to tax historian Joseph Thorndike, the bills of 1982 and 1984 "constituted the biggest tax increase ever enacted during peacetime".

With the Tax Reform Act of 1986, Reagan and Congress sought to simplify the tax system by eliminating many deductions, reducing the highest marginal rates, and reducing the number of tax brackets. In 1983, Democrats Bill Bradley and Dick Gephardt had offered a proposal; in 1984 Reagan had the Treasury Department produce its own plan. The 1986 act aimed to be revenue-neutral: while it reduced the top marginal rate, it also cleaned up the tax base by removing certain tax write-offs, preferences, and exceptions, thus raising the effective tax on activities previously specially favored by the code. Ultimately, the combination of the decrease in deductions and decrease in rates raised revenue equal to about 4% of existing tax revenue.
Federal revenue share of GDP fell from 19.6% in fiscal 1981 to 17.3% in 1984, before rising back to 18.4% by fiscal year 1989. Personal income tax revenues fell during this period relative to GDP, while payroll tax revenues rose relative to GDP. Reagan's 1981 cut in the top regular tax rate on unearned income reduced the maximum capital gains rate to only 20% â its lowest level since the Hoover administration. The 1986 act set tax rates on capital gains at the same level as the rates on ordinary income like salaries and wages, with both topping out at 28%.

Reagan significantly increased public expenditures, primarily the Department of Defense, which rose (in constant 2000 dollars) from $267.1 billion in 1980 (4.9% of GDP and 22.7% of public expenditure) to $393.1 billion in 1988 (5.8% of GDP and 27.3% of public expenditure); most of those years military spending was about 6% of GDP, exceeding this number in 4 different years. All these numbers had not been seen since the end of U.S. involvement in the Vietnam War in 1973. In 1981, Reagan significantly reduced the maximum tax rate, which affected the highest income earners, and lowered the top marginal tax rate from 70% to 50%; in 1986 he further reduced the rate to 28%. The federal deficit under Reagan peaked at 6% of GDP in 1983, falling to 3.2% of GDP in 1987 and to 3.1% of GDP in his final budget. The inflation-adjusted rate of growth in federal spending fell from 4% under Jimmy Carter to 2.5% under Ronald Reagan. This was the slowest rate of growth in inflation adjusted spending since Eisenhower. However, federal deficit as percent of GDP was up throughout the Reagan presidency from 2.7% at the end of (and throughout) the Carter administration. As a short-run strategy to reduce inflation and lower nominal interest rates, the U.S. borrowed both domestically and abroad to cover the Federal budget deficits, raising the national debt from $997 billion to $2.85 trillion. This led to the U.S. moving from the world's largest international creditor to the world's largest debtor nation. Reagan described the new debt as the "greatest disappointment" of his presidency.

According to William A. Niskanen, one of the architects of Reaganomics, "Reagan delivered on each of his four major policy objectives, although not to the extent that he and his supporters had hoped", and notes that the most substantial change was in the tax code, where the top marginal individual income tax rate fell from 70.1% to 28.4%, and there was a "major reversal in the tax treatment of business income", with effect of "reducing the tax bias among types of investment but increasing the average effective tax rate on new investment". Roger Porter, another architect of the program, acknowledges that the program was weakened by the many hands that changed the President's calculus, such as Congress. President Reagan raised taxes eleven times over the course of his presidency, but the overall tax burden went down during his presidency. According to Paul Krugman, "Over all, the 1982 tax increase undid about a third of the 1981 cut; as a share of GDP, the increase was substantially larger than Mr. Clinton's 1993 tax increase." According to historian and domestic policy adviser Bruce Bartlett, Reagan's tax increases over the course of his presidency took back half of the 1981 tax cut. Though since the Reagan tax reductions, top marginal tax rates have remained lower than at any point in US history since 1931, when the top marginal rate was raised from 25% to 63%.

Spending during the years Reagan budgeted (FY 1982â89) averaged 21.6% GDP, roughly tied with President Obama for the highest among any recent President. Each faced a severe recession early in their administration. In addition, the public debt rose from 26% GDP in 1980 to 41% GDP by 1988. In dollar terms, the public debt rose from $712 billion in 1980 to $2.052 trillion in 1988, a roughly three-fold increase. The unemployment rate rose from 7% in 1980 to 11% in 1982, then declined to 5% in 1988. The inflation rate declined from 10% in 1980 to 4% in 1988.

Some economists have stated that Reagan's policies were an important part of bringing about the third longest peacetime economic expansion in U.S. history. During the Reagan administration, real GDP growth averaged 3.5%, compared to 2.9% during the preceding eight years. The annual average unemployment rate declined by 1.7 percentage points, from 7.2% in 1980 to 5.5% in 1988, after it had increased by 1.6 percentage points over the preceding eight years. Nonfarm employment increased by 16.1 million during Reagan's presidency, compared to 15.4 million during the preceding eight years, while manufacturing employment declined by 582,000 after rising 363,000 during the preceding eight years. Reagan's administration is the only one not to have raised the minimum wage. The inflation rate, 13.5% in 1980, fell to 4.1% in 1988, due to the Federal Reserve increasing interest rates (prime rate peaking at 20.5% in August 1981). The latter contributed to a recession from July 1981 to November 1982 during which unemployment rose to 9.7% and GDP fell by 1.9%. Additionally, income growth slowed for middle- and lower-class (2.4% to 1.8%) and rose for the upper-class (2.2% to 4.83%).

The misery index, defined as the inflation rate added to the unemployment rate, shrank from 19.33 when he began his administration to 9.72 when he left, the greatest improvement record for a President since Harry S. Truman left office. In terms of American households, the percentage of total households making less than $10,000 a year (in real 2007 dollars) shrank from 8.8% in 1980 to 8.3% in 1988 while the percentage of households making over $75,000 went from 20.2% to 25.7% during that period, both signs of progress.

The job growth (measured for non-farm payrolls) under the Reagan administration averaged 168,000 per month, versus 216,000 for Carter, 55,000 for H.W. Bush, and 239,000 for Clinton. Measuring the number of jobs created per month is limited for longer time periods as the population grows. To address this, we can measure annual job growth percentages, comparing the beginning and ending number of jobs during their time in office to determine an annual growth rate. Jobs grew by 2.0% annually under Reagan, versus 3.1% under Carter, 0.6% under H.W. Bush, and 2.4% under Clinton.

The unemployment rate averaged 7.5% under Reagan, compared to an average 6.6% during the preceding eight years. Declining steadily after December 1982, the rate was 5.4% the month Reagan left office.

The average real hourly wage for production and nonsupervisory workers continued the decline that had begun in 1973, albeit at a slower rate, and remained below the pre-Reagan level in every Reagan year.

The labor force participation rate increased by 2.6 percentage points during Reagan's eight years, compared to 3.9 percentage points during the preceding eight years.

Some commentators have asserted that over one million jobs were created in a single month â September 1983. Although official data support that figure, it was caused by nearly 700,000 AT&T workers going on strike and being counted as job losses in August 1983, with a quick resolution of the strike leading workers to return in September, then being counted as job gains.

Following the 1981 recession, the unemployment rate had averaged slightly higher (6.75% vs. 6.35%), productivity growth lower (1.38% vs. 1.92%), and private investment as a percentage of GDP slightly less (16.08% vs. 16.86%). In the 1980s, industrial productivity growth in the United States matched that of its trading partners after trailing them in the 1970s. By 1990, manufacturing's share of GNP exceeded the post-World War II low hit in 1982 and matched "the level of output achieved in the 1960s when American factories hummed at a feverish clip".

Real GDP grew over one-third during Reagan's presidency, an over $2 trillion increase. The compound annual growth rate of GDP was 3.6% during Reagan's eight years, compared to 2.7% during the preceding eight years. Real GDP per capita grew 2.6% under Reagan, compared to 1.9% average growth during the preceding eight years.

In nominal terms, median household income grew at a compound annual growth rate (CAGR) of 5.5% during the Reagan presidency, compared to 8.5% during the preceding five years (pre-1975 data are unavailable). Real median family income grew by $4,492 during the Reagan period, compared to a $1,270 increase during the preceding eight years. After declining from 1974 through 1980, real mean personal income rose $4,708 by 1988. Nominal household net worth increased by a CAGR of 8.4%, compared to 9.3% during the preceding eight years.

The percentage of the total population below the poverty level increased from 13.0% in 1980 to 15.2% in 1983, then declined back to 13.0% in 1988. During Reagan's first term, critics noted homelessness as a visible problem in U.S. urban centers. In the closing weeks of his presidency, Reagan told David Brinkley that the homeless "make it their own choice for staying out there," noting his belief that there "are shelters in virtually every city, and shelters here, and those people still prefer out there on the grates or the lawn to going into one of those shelters". He also stated that "a large proportion" of them are "mentally impaired." A result (he believed) of ACLU (and similar organizations) lawsuits against institutions. His policies became widely known as "trickle-down economics", due to the significant cuts in the upper tax brackets, as that extra money for the wealthy could trickle along to low-income groups.

During the Reagan administration, fiscal year federal receipts grew from $599 billion to $991 billion (an increase of 65%) while fiscal year federal outlays grew from $678 billion to $1144 billion (an increase of 69%). According to a 1996 report of the Joint Economic Committee of the United States Congress, during Reagan's two terms, and through 1993, the top 10% of taxpayers paid an increased share of income taxes (not including payroll taxes) to the Federal government, while the lowest 50% of taxpayers paid a reduced share of income tax revenue. Personal income tax revenues declined from 9.4% GDP in 1981 to 8.3% GDP in 1989, while payroll tax revenues increased from 6.0% GDP to 6.7% GDP during the same period.

According to a 2003 Treasury study, the tax cuts in the Economic Recovery Tax Act of 1981 resulted in a significant decline in revenue relative to a baseline without the cuts, approximately $111 billion (in 1992 dollars) on average during the first four years after implementation or nearly 3% GDP annually. Other tax bills had neutral or, in the case of the Tax Equity and Fiscal Responsibility Act of 1982, a (~+1% of GDP) increase in revenue as a share of GDP. The study did not examine the longer-term impact of Reagan tax policy, including sunset clauses and "the long-run, phased-in effect of the tax bills". The fact that tax receipts "as a percentage of GDP" fell following the Economic Recovery Tax Act of 1981 shows a decrease in tax burden as share of GDP and a commensurate increase in the deficit, as spending did not fall relative to GDP. Total federal tax receipts increased in every Reagan year except 1982, at an annual average rate of 6.2% compared to 10.8% during the preceding eight years.

The effect of Reagan's 1981 tax cuts (reduced revenue relative to a baseline without the cuts) were at least partially offset by phased in Social Security payroll tax increases that had been enacted by President Jimmy Carter and the 95th Congress in 1977, and further increases by Reagan in 1983 and following years, also to counter the uses of tax shelters. An accounting indicated nominal tax receipts increased from $599 billion in 1981 to $1.032 trillion in 1990, an increase of 72% in current dollars. In 2005 dollars, the tax receipts in 1990 were $1.5 trillion, an increase of 20% above inflation.

Reagan was inaugurated in January 1981, so the first fiscal year (FY) he budgeted was 1982 and the final year was 1989. 

Nominal after-tax corporate profits grew at a compound annual growth rate of 3.0% during Reagan's eight years, compared to 13.0% during the preceding eight years. The S&P 500 Index increased 113.3% during the 2024 trading days under Reagan, compared to 10.4% during the preceding 2024 trading days. The business sector share of GDP, measured as gross private domestic investment, declined by 0.7 percentage points under Reagan, after increasing 0.7 percentage points during the preceding eight years.

The federal government's share of GDP increased 0.2 percentage points under Reagan, while it decreased 1.5 percentage points during the preceding eight years. The number of federal civilian employees increased 4.2% during Reagan's eight years, compared to 6.5% during the preceding eight years.

As a candidate, Reagan asserted he would shrink government by abolishing the Cabinet-level departments of energy and education. He abolished neither, but elevated veterans affairs from independent agency status to Cabinet-level department status.

Continuing a trend that began in the 1970s, income inequality grew and accelerated in the 1980s. "The Economist" wrote in 2006: "After the 1973 oil shocks, productivity growth suddenly slowed. A few years later, at the start of the 1980s, the gap between rich and poor began to widen." According to the CBO: 

According to a 1996 study by the Cato Institute, a libertarian think tank, on 8 of the 10 key economic variables examined, the American economy performed better during the Reagan years than during the pre- and post-Reagan years. The study asserted that real median family income grew by $4,000 and during the eight Reagan years and experienced a loss of almost $1,500 in the post-Reagan years. Interest rates, inflation, and unemployment fell faster under Reagan than they did immediately before or after his presidency. The only economic variable that was lower during period than in both the pre- and post-Reagan years was the savings rate, which fell rapidly in the 1980s. The productivity rate was higher in the pre-Reagan years but lower in the post-Reagan years. The Cato study was dismissive of any positive effects of tightening, and subsequent loosening, of Federal Reserve monetary policy under "inflation hawk" Paul Volcker, whom President Carter had appointed in 1979 to halt the persistent inflation of the 1970s.

Economic analyst Stephen Moore stated in the Cato analysis, "No act in the last quarter century had a more profound impact on the U.S. economy of the eighties and nineties than the Reagan tax cut of 1981." He argued that Reagan's tax cuts, combined with an emphasis on federal monetary policy, deregulation, and expansion of free trade created a sustained economic expansion, the greatest American sustained wave of prosperity ever. He also claims that the American economy grew by more than a third in size, producing a $15 trillion increase in American wealth. Consumer and investor confidence soared. Cutting federal income taxes, cutting the U.S. government spending budget, cutting useless programs, scaling down the government work force, maintaining low interest rates, and keeping a watchful inflation hedge on the monetary supply was Ronald Reagan's formula for a successful economic turnaround.

Milton Friedman stated, "Reaganomics had four simple principles: Lower marginal tax rates, less regulation, restrained government spending, noninflationary monetary policy. Though Reagan did not achieve all of his goals, he made good progress."

The Tax Reform Act of 1986 and its impact on the alternative minimum tax (AMT) reduced nominal rates on the wealthy and eliminated tax deductions, while raising tax rates on lower-income individuals. The across the board tax system reduced marginal rates and further reduced bracket creep from inflation. The highest income earners (with incomes exceeding $1,000,000) received a tax break, restoring a flatter tax system. In 2006, the IRS's National Taxpayer Advocate's report characterized the effective rise in the AMT for individuals as a problem with the tax code. Through 2007, the revised AMT had brought in more tax revenue than the former tax code, which has made it difficult for Congress to reform.

Economist Paul Krugman argued the economic expansion during the Reagan administration was primarily the result of the business cycle and the monetary policy by Paul Volcker. Krugman argues that there was nothing unusual about the economy under Reagan because unemployment was reducing from a high peak and that it is consistent with Keynesian economics for the economy to grow as employment increases if inflation remains low.

The CBO Historical Tables indicate that federal spending during Reagan's two terms (FY 1981â88) averaged 22.4% GDP, well above the 20.6% GDP average from 1971 to 2009. In addition, the public debt rose from 26.1% GDP in 1980 to 41.0% GDP by 1988. In dollar terms, the public debt rose from $712 billion in 1980 to $2,052 billion in 1988, a three-fold increase. Krugman argued in June 2012 that Reagan's policies were consistent with Keynesian stimulus theories, pointing to the significant increase in per-capita spending under Reagan.

William Niskanen noted that during the Reagan years, privately held federal debt increased from 22% to 38% of GDP, despite a long peacetime expansion. Second, the savings and loan problem led to an additional debt of about $125 billion. Third, greater enforcement of U.S. trade laws increased the share of U.S. imports subjected to trade restrictions from 12% in 1980 to 23% in 1988.

Economists Raghuram Rajan and Luigi Zingales pointed out that many deregulation efforts had either taken place or had begun before Reagan (note the deregulation of airlines and trucking under Carter, and the beginning of deregulatory reform in railroads, telephones, natural gas, and banking). They stated, "The move toward markets preceded the leader [Reagan] who is seen as one of their saviors." Economists Paul Joskow and Roger Noll made a similar contention.

Economist William A. Niskanen, a member of Reagan's Council of Economic Advisers wrote that deregulation had the "lowest priority" of the items on the Reagan agenda given that Reagan "failed to sustain the momentum for deregulation initiated in the 1970s" and that he "added more trade barriers than any administration since Hoover." By contrast, economist Milton Friedman has pointed to the number of pages added to the Federal Register each year as evidence of Reagan's anti-regulation presidency (the Register records the rules and regulations that federal agencies issue per year). The number of pages added to the Register each year declined sharply at the start of the Ronald Reagan presidency breaking a steady and sharp increase since 1960. The increase in the number of pages added per year resumed an upward, though less steep, trend after Reagan left office. In contrast, the number of pages being added each year increased under Ford, Carter, George H. W. Bush, Clinton, George W. Bush, and Obama. The number of pages in Federal Register is however criticized as an extremely crude measure of regulatory activity, because it can be easily manipulated (e.g. font sizes have been changed to keep page count low). The apparent contradiction between Niskanen's statements and Friedman's data may be resolved by seeing Niskanen as referring to "statutory" deregulation (laws passed by Congress) and Friedman to "administrative" deregulation (rules and regulations implemented by federal agencies). A 2016 study by the Congressional Research Service found that Reagan's average annual number of final federal regulatory rules published in the Federal Register was higher than during the Clinton, George W. Bush or Obama's administrations, even though the Reagan economy was considerably smaller than during those later presidents. Another study by the QuantGov project of the libertarian Mercatus Center found that the Reagan administration added restrictive regulations â containing such terms as "shall," "prohibited" or "may not" â at a faster average annual rate than did Clinton, Bush or Obama.

Greg Mankiw, a conservative Republican economist who served as chairman of the Council of Economic Advisors under President George W. Bush, wrote in 2007:
I used the phrase "charlatans and cranks" in the first edition of my principles textbook to describe some of the economic advisers to Ronald Reagan, who told him that broad-based income tax cuts would have such large supply-side effects that the tax cuts would raise tax revenue. I did not find such a claim credible, based on the available evidence. I never have, and I still don't ... My other work has remained consistent with this view. In a paper on dynamic scoring, written while I was working at the White House, Matthew Weinzierl and I estimated that a broad-based income tax cut (applying to both capital and labor income) would recoup only about a quarter of the lost revenue through supply-side growth effects. For a cut in capital income taxes, the feedback is larger â about 50 percent â but still well under 100 percent. A chapter on dynamic scoring in the 2004 Economic Report of the President says about the same thing.

Glenn Hubbard, who preceded Mankiw as Bush's CEA chair, also disputed the assertion that tax cuts increase tax revenues, writing in his 2003 Economic Report of the President: "Although the economy grows in response to tax reductions (because of higher consumption in the short run and improved incentives in the long run), it is unlikely to grow so much that lost tax revenue is completely recovered by the higher level of economic activity."

In 1986, Martin Feldstein â a self-described "traditional supply sider" who served as Reagan's chairman of the Council of Economic Advisors from 1982 to 1984 â characterized the "new supply siders" who emerged circa 1980:

What distinguished the new supply siders from the traditional supply siders as the 1980s began was not the policies they advocated but the claims that they made for those policies ... The "new" supply siders were much more extravagant in their claims. They projected rapid growth, dramatic increases in tax revenue, a sharp rise in saving, and a relatively painless reduction in inflation. The height of supply side hyperbole was the "Laffer curve" proposition that the tax cut would actually increase tax revenue because it would unleash an enormously depressed supply of effort. Another remarkable proposition was the claim that even if the tax cuts did lead to an increased budget deficit, that would not reduce the funds available for investment in plant and equipment because tax changes would raise the saving rate by enough to finance the increased deficit ... Nevertheless, I have no doubt that the loose talk of the supply side extremists gave fundamentally good policies a bad name and led to quantitative mistakes that not only contributed to subsequent budget deficits but that also made it more difficult to modify policy when those deficits became apparent.







</doc>
<doc id="26531" url="https://en.wikipedia.org/wiki?curid=26531" title="Roland Corporation">
Roland Corporation

Roland has manufactured numerous instruments that have had lasting impacts on popular music, such as the Juno-106 synthesizer, TB-303 bass synthesizer, and TR-808 and TR-909 drum machines. Roland was also instrumental in the development of MIDI, a standardized means of synchronizing electronic instruments manufactured by different companies. In 2016, "Fact" wrote that Roland "arguably did more to shape electronic music than any other [company] in history".

Having created Ace Electronic Industries Inc in 1960, Ikutaro Kakehashi founded Roland in Osaka on April 18, 1972. While rival companies Moog and ARP targeted professional musicians and academics, Kakehashi, who had no musical training, wanted to appeal to amateurs and hobbyists, and focused on miniaturization, affordability, and simplicity.

The "Roland" name was selected for export purposes, as Kakehashi was interested in a name that was easy to pronounce for his worldwide target markets. The name was found in a telephone directory, and Kakehashi was satisfied with the simple two-syllable word and its soft consonants. The letter "R" was chosen because it was not used by many other music equipment companies, and would therefore stand out in trade show directories and industry listings. Kakehashi did not learn of the French epic poem "The Song of Roland" until later.

With seven employees from his former company, a rented shed, and $100,000, Kakehashi built on his experience at Ace, introducing a drum machine, the TR-77 or Rhythm 77, as Roland's first product, followed by the TR-33 and TR-55 released that same year. In 1973, Roland introduced the first compact synthesizer produced in Japan and the first synthesizer produced by Roland, the SH-1000, as well as their first non-preset synthesizer, the SH-3.

The company was also manufacturing effects pedals, introducing the RE-201 Space Echo in 1974, and expanding into guitar amplifiers the following year with the JC-60 and JC-120 Jazz Chorus, whose chorus circuit would become the first Boss Corporation product, the CE-1 Chorus Ensemble, the following year. In 1976, Roland introduced the semi-modular System 100 and the modular System 700 synthesizers.

In 1977, the company introduced one of the earliest microprocessor-driven music sequencers, the MC-8 MicroComposer, and the first guitar synthesizer, the GR-500. Just one year later, they introduced the CompuRhythm CR-78, the first drum machine that enabled users to program and store their own drum patterns.

During the 1980s and 1990s, Roland released several instruments that have had a lasting influence on popular music. After Kakehashi realized microprocessors could be used to program drum machines, Roland launched the TR-808 drum machine, its first programmable drum machine, in 1980. Although it was not an immediate commercial success, the 808 was eventually used on more hit records than any other drum machine and became a cornerstone of the emerging electronic and hip hop genres. It has been described as hip hop's equivalent to the Fender Stratocaster guitar, which dramatically influenced the development of rock music. The 808 was followed in 1983 by the TR-909, which, alongside the TB-303 synthesizer, influenced the development of dance music such as techno, house and acid.

Roland played a key role in the development of MIDI, a standardized means of synchronizing electronic musical instruments manufactured by different companies. Kakehashi proposed developing a standard with representatives from Oberheim Electronics, Sequential Circuits, Yamaha, Korg and Kawai. He and Dave Smith of Sequential Circuits unveiled MIDI in 1983. It remains the industry standard.

In 1994, Kakehashi founded the Roland Foundation and became Chairman. In 1995 he was appointed the chairman of Roland Corporation. In 2001 he resigned from the position and was appointed as Special Executive Adviser of Roland Corporation. In 2002, Kakehashi published an autobiography, "I Believe in Music." His second book, "An Age Without Samples: Originality and Creativity in the Digital World", was published in 2017.

Roland markets products under a number of brand names, each of which are used on products geared toward a different niche.




</doc>
<doc id="26532" url="https://en.wikipedia.org/wiki?curid=26532" title="Rhys ap Gruffydd">
Rhys ap Gruffydd

Rhys ap Gruffydd or ap Gruffudd (often anglicised to "Griffith"; 1132 â 28 April 1197) was the ruler of the kingdom of Deheubarth in south Wales from 1155 to 1197. Today, he is commonly known as The Lord Rhys, in Welsh "Yr Arglwydd Rhys", although this title may have not been used in his lifetime. He usually used the title "Proprietary Prince of Deheubarth" or "Prince of South Wales", but two documents have been discovered in which he uses the title "Prince of Wales" or "Prince of the Welsh". Rhys was one of the most successful and powerful Welsh princes, and, after the death of Owain Gwynedd of Gwynedd in 1170, the dominant power in Wales.

Rhys's grandfather, Rhys ap Tewdwr, was king of Deheubarth, and was killed at Brecon in 1093 by Bernard de NeufmarchÃ©. Following his death, most of Deheubarth was taken over by the Normans. Rhys's father, Gruffydd ap Rhys, eventually was able to become ruler of a small portion, and more territory was won back by Rhys's older brothers after Gruffydd's death. Rhys became ruler of Deheubarth in 1155. He was forced to submit to King Henry II of England in 1158. Henry invaded Deheubarth in 1163, stripped Rhys of all his lands and took him prisoner. A few weeks later he was released and given back a small part of his holdings. Rhys made an alliance with Owain Gwynedd and, after the failure of another invasion of Wales by Henry in 1165, was able to win back most of his lands.

In 1171 Rhys made peace with King Henry and was confirmed in possession of his recent conquests as well as being named Justiciar of South Wales. He maintained good relations with King Henry until the latter's death in 1189. Following Henry's death Rhys revolted against Richard I and attacked the Norman lordships surrounding his territory, capturing a number of castles. In his later years Rhys had trouble keeping control of his sons, particularly Maelgwn and Gruffydd, who maintained a feud with each other. Rhys launched his last campaign against the Normans in 1196 and captured a number of castles. The following year he died unexpectedly and was buried in St David's Cathedral.

Rhys was the fourth son of Gruffydd ap Rhys, ruler of part of Deheubarth, by his wife Gwenllian ferch Gruffydd, daughter of Gruffudd ap Cynan, king of Gwynedd. His next older brother was Maredudd ap Gruffydd, and there were older brothers, Morgan and Maelgwn, who were killed in battle with their mother in 1136. He also had two older half-brothers, Anarawd and Cadell, from his father's first marriage. Rhys married Gwenllian ferch Madog, daughter of Madog ap Maredudd, the last Prince of all Powys.
His grandfather, Rhys ap Tewdwr, had been king of all Deheubarth until his death in 1093. Rhys ap Tewdwr was killed in Brycheiniog, and most of his kingdom was taken over by Norman lords. Gruffydd ap Rhys was forced to flee to Ireland. He later returned to Deheubarth and ruled a portion of the kingdom, but was forced to flee to Ireland again in 1127. When Rhys was born in 1132, his father held only the commote of Caeo in Cantref Mawr.

The death of King Henry I of England, and the ensuing Anarchy arising from the rival claims of Stephen and Matilda to the English throne, gave the Welsh the opportunity to rise against the Normans. A revolt spread through south Wales in 1136, and Gruffydd ap Rhys, aided by his two eldest sons, Anarawd and Cadell, defeated the Normans in a battle near Loughor, killing over five hundred. After driving Walter de Clifford out of Cantref Bychan, Gruffydd set off to Gwynedd to enlist the help of his father-in-law, Gruffudd ap Cynan. In the absence of her husband, Gwenllian led an army against the Norman lordship of Cydweli (Kidwelly), taking along her two oldest sons, Morgan and Maelgwn. She was defeated and killed by an army commanded by Maurice de Londres of Oystermouth Castle. Morgan was also killed and Maelgwn captured.

Gruffydd formed an alliance with Gwynedd, and later in 1136 the sons of Gruffudd ap Cynan, Owain Gwynedd and Cadwaladr ap Gruffydd, led an army to Ceredigion. Their combined forces won a decisive victory over the Normans at the Battle of Crug Mawr. Ceredigion was reclaimed from the Normans, but was annexed by Gwynedd as the senior partner in the alliance. Gruffydd ap Rhys continued his campaign against the Normans in 1137, but died later that year. The leadership of the family now passed to Rhys's half-brother Anarawd ap Gruffydd. In 1143, when Rhys was eleven, Anarawd was murdered, a death arranged for by Cadwaladr ap Gruffydd, brother of Owain Gwynedd, king of Gwynedd. Owain punished Cadwaladr by depriving him of his lands in Ceredigion.

Anarawd's brother, Cadell ap Gruffydd, took over as head of the family. Gilbert de Clare, Earl of Pembroke, rebuilt Carmarthen castle in 1145 then began a campaign to reclaim Ceredigion. He built a castle in the commote of Mabudryd, but Cadell, aided by Hywel ab Owain Gwynedd who held Ceredigion for Gwynedd, destroyed it in 1146. Rhys appears in the annals for the first time in 1146, fighting alongside his brothers Cadell and Maredudd in the capture by assault of Llansteffan Castle. This was followed by the capture of Wiston in 1147, Carmarthen in 1150 and Loughor in 1151. In 1151 Cadell was attacked while out hunting by a group of Norman and Flemish knights from Tenby, and left for dead. He survived, but suffered injuries which left him unable to play an active role, and in 1153 he left on a pilgrimage to Rome.

Maredudd became ruler of Deheubarth and continued a campaign, begun in 1150, aimed at recovering Ceredigion, which had been held by Gwynedd since 1136. Maredudd and Rhys were able to drive Hywel ab Owain Gwynedd from Ceredigion by 1153. The same year Rhys is recorded as an independent commander for the first time, leading an army to capture the Norman castle of St Clears. Maredudd and Rhys also destroyed the castles at Tenby and Aberafan that year. Maredudd died in 1155 at the age of twenty-five and left Rhys as ruler of Deheubarth. Around this time he married Gwenllian ferch Madog, daughter of Madog ap Maredudd, prince of Powys.

Shortly after becoming ruler of Deheubarth, Rhys heard rumours that Owain Gwynedd was planning to invade Ceredigion in order to reclaim it for Gwynedd. Rhys responded by building a castle at Aberdyfi in 1156. The threatened invasion did not take place, and Turvey claims that Owain's intention may have been to test the resolve of the new ruler.

King Stephen had died in October 1154, bringing to an end the long dispute with the Empress Matilda which had helped Anarawd, Cadell and Maredudd to extend their rule in Deheubarth. With disunity within the realm no longer a problem, the new king of England, Henry II, soon turned his attention to Wales. He began with an invasion of Gwynedd in 1157. This invasion was not entirely successful, but Owain Gwynedd was induced to seek terms and to give up some territory in the north-east of Wales.

The following year, Henry prepared an invasion of Deheubarth. Rhys made plans to resist, but was persuaded by his council to meet the king to discuss peace terms. The terms were much harsher than those offered to Owain: Rhys was stripped of all his possessions apart from Cantref Mawr, though he was promised one other cantref. The other territories were returned to their Norman lords.

Among the Normans who returned to their holdings was Walter de Clifford, who reclaimed Cantref Bychan, then invaded Rhys's lands in Cantref Mawr. An appeal to the king produced no response, and Rhys resorted to arms, first capturing Clifford's castle at Llandovery then seizing Ceredigion. King Henry responded by preparing another invasion, and Rhys submitted without resistance. He was obliged to give hostages, probably including his son Hywel.

The king was absent in France in 1159, and Rhys took the opportunity to attack Dyfed and then to lay siege to Carmarthen, which was saved by a relief force led by Earl Reginald of Cornwall. Rhys retreated to Cantref Mawr, where an army led by five earls, the Earls of Cornwall, Gloucester, Hertford, Pembroke and Salisbury, marched against him. The earls were assisted by Cadwaladr, brother of Owain Gwynedd, and Owain's sons, Hywel and Cynan. However they were forced to withdraw and a truce was arranged. In 1162, Rhys again attempted to recover some of his lost lands, and captured Llandovery castle. The following year Henry II returned to England after an absence of four years and prepared for another invasion of Deheubarth. Rhys met the king to discuss terms and was obliged to give more hostages, including another son, Maredudd. He was then seized and taken to England as a prisoner. Henry appears to have been uncertain what to do with Rhys, but after a few weeks decided to free him and allow him to rule Cantref Mawr. Rhys was summoned to appear before Henry at Woodstock to do homage together with Owain Gwynedd and Malcolm IV of Scotland.

In 1164 all the Welsh princes united in an uprising. Warren suggests that when Rhys and Owain were obliged to do homage to Henry in 1163 they were forced to accept a status of dependent vassalage instead of their previous client status, and that this led to the revolt. Rhys had other reasons for rebellion, for he had returned to Deheubarth from England to find that the neighbouring Norman lords were threatening Cantref Mawr. His nephew, Einion ab Anarawd, who was the captain of his bodyguard, had been murdered at the instigation of Roger de Clare, Earl of Hertford. The murderer had been given the protection of the Clares in Ceredigion. Rhys first appealed to the king to intercede; when this failed, he invaded Ceredigion and recaptured all of it apart from the town and castle of Cardigan. The Welsh revolt led to another invasion of Wales by King Henry in 1165. Henry attacked Gwynedd first, but instead of following the usual invasion route along the north coast he attacked from the south, following a route over the Berwyn hills. He was met by the united forces of the Welsh princes, led by Owain Gwynedd and including Rhys. According to "Brut y Tywysogion":

Torrential rain forced Henry's army to retreat in disorder without fighting a major battle, and Henry vented his spleen on the hostages, having Rhys's son Maredudd blinded. Rhys's other son, Hywel, was not among the victims. Rhys returned to Deheubarth where he captured and burned Cardigan Castle. He allowed the garrison to depart, but held the castellan, Robert Fitz-Stephen, as a prisoner. Shortly afterwards Rhys captured Cilgerran castle.

In 1167 he joined Owain Gwynedd in an attack on Owain Cyfeiliog of southern Powys, and spent three weeks helping Owain besiege the Norman castle of Rhuddlan. In 1168 he attacked the Normans at Builth, destroying its castle. Rhys benefited from the Norman invasion of Ireland in 1169 and 1170, which was largely led by the Cambro-Norman lords of south Wales. In 1167 the King of Leinster, Diarmait Mac Murchada, who had been driven out of his kingdom, had asked Rhys to release Robert Fitz-Stephen from captivity to take part in an expedition to Ireland. Rhys did not oblige at the time, but released him the following year and in 1169 Fitz-Stephen led the vanguard of a Norman army which landed in Wexford. The leader of the Norman forces, Richard de Clare, 2nd Earl of Pembroke, known as "Strongbow", followed in 1170. According to Warren:

The departure of the Norman lords enabled Rhys to strengthen his position, and the death of Owain Gwynedd in late 1170 left him as the acknowledged leader of the Welsh princes.

In 1171 King Henry II arrived in England from France, on his way to Ireland. Henry wished to ensure that Richard de Clare, who had married Diarmait's daughter and become heir to Leinster, did not establish an independent Norman kingdom in Ireland. His decision to try a different approach in his dealings with the Welsh was influenced by the events in Ireland, although Warren suggests that "it seems likely that Henry began rethinking his attitude to the Welsh soon after the dÃ©bÃ¢cle of 1165". Henry now wished to make peace with Rhys, who came to Newnham to meet him. Rhys was to pay a tribute of 300 horses and 4,000 head of cattle, but was confirmed in possession of all the lands he had taken from Norman lords, including the Clares. They met again in October that year at Pembroke as Henry waited to cross to Ireland. Rhys had collected 86 of the 300 horses, but Henry agreed to take only 36 of them and remitted the remainder of the tribute until after his return from Ireland. Rhys's son, Hywel, who had been held as a hostage for many years, was returned to him. Henry and Rhys met once more at Laugharne as Henry returned from Ireland in 1172, and shortly afterwards Henry appointed Rhys "justice on his behalf in all Deheubarth". According to A. D. Carr:

The agreement between Henry and Rhys was to last until Henry's death in 1189. When Henry's sons rebelled against him in 1173 Rhys sent his son Hywel Sais to Normandy to aid the king, then in 1174 personally led an army to Tutbury in Staffordshire to assist at the siege of the stronghold of the rebel Earl William de Ferrers. When Rhys returned to Wales after the fall of Tutbury, he left a thousand men with the king for service in Normandy. King Henry held a council at Gloucester in 1175 which was attended by a large gathering of Welsh princes, led by Rhys. It appears to have concluded with the swearing of a mutual assistance pact for the preservation of peace and order in Wales. In 1177 Rhys, Dafydd ab Owain, who had emerged as the main power in Gwynedd, and Cadwallon ap Madog from Rhwng Gwy a Hafren swore fealty and liege homage to Henry at a council held at Oxford. At this council the king gave Meirionnydd, part of the kingdom of Gwynedd, to Rhys. There was some fighting in Meirionnydd the following year, but Rhys apparently made no serious attempt to annex it.

Rhys built a number of stone castles, starting with Cardigan castle, which was the earliest recorded native-built stone castle in Wales. He also built Carreg Cennen castle near Llandeilo, a castle set in a spectacular position on a mountain top. He held a festival of poetry and song at his court at Cardigan over Christmas 1176. This is generally regarded as the first recorded Eisteddfod. The festival was announced a year in advance throughout Wales and in England, Scotland, Ireland and possibly France. Two chairs were awarded as prizes, one for the best poem and the other for the best musical performance. J. E. Caerwyn Williams suggests that this event may be an adaptation of the similar French "puys". R.R. Davies suggests that the texts of Welsh law, traditionally codified by Hywel Dda at Whitland, were first assembled in book form under the aegis of Rhys.
Rhys founded two religious houses during this period. Talley Abbey was the first Premonstratensian abbey in Wales, while Llanllyr was a Cistercian nunnery, only the second nunnery to be founded in Wales and the first to prosper. He became the patron of the abbeys of Whitland and Strata Florida and made large grants to both houses. Giraldus Cambrensis, who was related to Rhys, gives an account of his meetings with Rhys in 1188 when Giraldus accompanied Archbishop Baldwin around Wales to raise men for the Third Crusade. Some Welsh clerics were not happy about this visit, but Rhys was enthusiastic and gave the Archbishop a great deal of assistance. Giraldus says that Rhys decided to go on crusade himself and spent several weeks making preparations, but was eventually persuaded to change his mind by his wife Gwenllian, "by female artifices".

Henry II died in 1189 and was succeeded by Richard I. Rhys considered that he was no longer bound by the agreement with King Henry and attacked the Norman lordships surrounding his territory. He ravaged Pembroke, Haverfordwest, and Gower and captured the castles of St. Clear's, Laugharne, and Llansteffan. Richard's brother, Prince John (later King John), came to Wales in September and tried to make peace. He persuaded Rhys to raise the siege of Carmarthen and accompany him to Oxford to meet Richard. Rhys arrived at Oxford to discover that Richard was not prepared to travel there to meet him, and hostilities continued.

In his later years Rhys had trouble keeping control of his sons, particularly Maelgwn and Gruffydd. In 1189 Gruffydd persuaded Rhys to imprison Maelgwn, and he was given into Gruffydd's keeping at Dinefwr. Gruffydd handed him over to his father-in-law, William de Braose. Gruffydd is also said to have persuaded his father to annex the lordship of Cemais and its chief castle of Nevern, held by William FitzMartin, in 1191. This action was criticized by Giraldus Cambrensis, who describes Gruffydd as "a cunning and artful man". William FitzMartin was married to Rhys's daughter Angharad, and, according to Giraldus, Rhys "had solemnly sworn, by the most precious relics, that his indemnity and security should be faithfully maintained". Rhys had also annexed the Norman lordships of Cydweli and Carnwyllion in 1190. In 1192 Rhys secured Maelgwn's release, but by now Maelgwn and Gruffydd were bitter enemies. In 1194 Rhys was defeated in battle by Maelgwn and Hywel, who imprisoned him in Nevern castle, though Hywel later released his father without Maelgwn's consent. Giraldus suggests that Rhys's incarceration in Nevern castle was divine vengeance for the dispossession of William FitzMartin. In 1195 two other sons, Rhys Gryg and Maredudd, seized Llanymddyfri and Dinefwr, and Rhys responded by imprisoning them. Rhys launched his last campaign against the Normans in 1196. He captured a number of castles, including Carmarthen, Colwyn, Radnor and Painscastle, and defeated an army led by Roger de Mortimer and Hugh de Say near Radnor, with forty knights among the dead. This was Rhys' last battle. William de Braose offered terms, and Painscastle was returned to him.

In April 1197 Rhys died unexpectedly and was buried in St David's Cathedral. The chronicler of "Brut y Tywysogion" records for 1197:

Rhys died excommunicate, having quarreled with the Bishop of St. David's, Peter de Leia, over the theft of some of the bishop's horses some years previously. Before he could be buried in the cathedral, the bishop had his corpse scourged in posthumous penance.

Rhys had nominated his eldest legitimate son, Gruffydd ap Rhys, as his successor, and soon after his father's death Gruffydd met the Justiciar, Archbishop Hubert Walter, on the border and was confirmed as heir. Maelgwn, the eldest son but illegitimate, refused to accept this and was given military assistance by Gwenwynwyn ab Owain of Powys. Maelgwn took the town and castle of Aberystwyth and captured Gruffydd, whom he handed over to the custody of Gwenwynwyn. Gwenwynwyn later handed him over to the king, who imprisoned him at Corfe Castle. Gruffydd was set free the following year and regained most of Ceredigion. In 1201 Gruffydd died, but this did not end the fighting between rival claimants. In 1216 Llywelyn the Great of Gwynedd held a council at Aberdyfi where he allocated parts of Deheubarth to several sons and grandsons of Rhys.

Giraldus Cambrensis frequently mentions Rhys in his writings and describes him as "a man of excellent wit and quick in repartee". Gerald tells the story of a banquet at Hereford in 1186 where Rhys sat between two members of the Clare family. What could have been a tense affair, since Rhys had seized lands in Ceredigion previously held by the Clare family, passed off with an exchange of courteous compliments, followed by some good-natured banter between Rhys and Gerald about their family connections. Rhys gave Gerald and Archbishop Baldwin a great deal of assistance when they visited Wales to raise troops for the crusade in 1188, and Gerald several times refers to his "kindness" and says that Rhys accompanied them all the way from Cardigan to the northern border of Ceredigion "with a liberality peculiarly praiseworthy in so illustrious a prince".

Another contemporary writer also wrote of Rhys if Roger Turvey is correct in stating that Walter Map's piece "Of the King Appollonides" deals with Rhys under a pseudonym. Map was less favourably disposed toward Rhys, describing him as "This king I have seen and know, and hate", but goes on to say "I would not have my hatred blacken his worth; it is not my wish ever to suppress any man's excellence through envy". He tells the following story about Apollonides/Rhys:

Davies provides the following assessment of Rhys:

Davies also notes two flaws in Rhys's achievement. One was the personal nature of his accord with Henry II, which meant that it did not survive Henry's death. The other was his inability to control his sons and to force the other sons to accept Gruffydd as his successor.

Rhys had at least nine sons and eight daughters. Confusingly, three of the sons were named Maredudd and two of the daughters were named Gwenllian.






</doc>
